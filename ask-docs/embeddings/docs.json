[
  {
    "title": "Get Started",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Start here.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/get-started/",
    "relURI": "/latest/get-started/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "2e5f30201d9cce32b16beaba71bfe5a6"
  },
  {
    "title": "Beginner Tutorial",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Get Started",
    "description": "Learn how to quickly ingest photos, trace their outlines, and output a collage using the transformed data.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/get-started/beginner-tutorial/",
    "relURI": "/latest/get-started/beginner-tutorial/",
    "body": " Before You Start # Install Pachyderm either locally our within the cloud. Install Pachyderm Shell. Join our Slack Community so you can ask any questions you may have! Context # Pachyderm creates a Kubernetes cluster that you interact with using either the pachctl CLI or through Console, a GUI.\npachctl is great for users already experienced with using a CLI. Console is great for beginners and helps with visualizing relationships between projects, repos, and pipelines. Within the cluster, you can create projects that contain repos and pipelines. Pipelines can be single-stage or multi-stage; multi-stage pipelines are commonly referred to as DAGs.\nTutorial: Image processing with OpenCV # In this tutorial you\u0026rsquo;ll create an image edge detection pipeline that processes new data as it is added and outputs the results.\n1. Create a Project # Create Project: CLI Console To keep our work organized, we\u0026rsquo;re going to create a project named openCV and set it to our currently active context.\npachctl create project openCV pachctl config update context --project openCV You can always check to confirm which project has been set to your context by running the following commands:\n# prints current context name (local) pachctl config get active-context # prints local\u0026#39;s context details pachctl config get context local # { # \u0026#34;source\u0026#34;: \u0026#34;IMPORTED\u0026#34;, # \u0026#34;cluster_name\u0026#34;: \u0026#34;docker-desktop\u0026#34;, # \u0026#34;auth_info\u0026#34;: \u0026#34;docker-desktop\u0026#34;, # \u0026#34;cluster_deployment_id\u0026#34;: \u0026#34;dev\u0026#34;, # \u0026#34;project\u0026#34;: \u0026#34;openCV\u0026#34; # } Open a tab in your browser and go to localhost. Select Create Project. Provide details for the following values: Name Description Select Create. 2. Create a Repo # Repos should be dedicated to a single source of data such as log messages from a particular service, a users table, or training data.\nCreate Repo: CLI Console pachctl create repo images You can verify that the repository was created by running the following command:\npachctl list repo # NAME CREATED SIZE (MASTER) ACCESS LEVEL # images 4 seconds ago ‚â§ 0B [repoOwner] In Console, select View Project. Select Create Your First Repo. Provide details for the following values: Name Description Select Create. 3. Add Data # In Pachyderm, you write data to an explicit commit. Commits are immutable snapshots of your data which give Pachyderm its version control properties. You can add, remove, or update files in a given commit.\nAdd Data: CLI Console We\u0026rsquo;re going to use the pachctl put file command, along with the -f flag, to upload an image.\npachctl put file images@master:liberty.png -f http://imgur.com/46Q8nDz.png pachctl put file automatically starts and finishes a commit for you so you can add files more easily.\nüí° If you want to add many files over a period of time, you can do pachctl start commit and pachctl finish commit yourself.\nYou can confirm the commit using the following command:\npachctl list commit images # REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION # openCV/images master 37559e89ed0c4a0cb354649524050851 10 seconds ago 57.27KiB USER You can also view the filename in the commit using the following command:\npachctl list file images@master # NAME TYPE SIZE # /liberty.png file 57.27KiB From your Project\u0026rsquo;s DAG view in console, select your repo. Select the upload icon from the slideout menu. Complete one of the following: Provide the path of the file on your local machine. Select Browse Files and choose the file from your local machine. Drag-and-drop the file. Select Done. Bonus: View Image # You can view the files you\u0026rsquo;ve uploaded in the Console or in your Terminal.\nIn Terminal # Operating System: MacOS Linux pachctl get file images@master:liberty.png | open -f -a Preview.app pachctl get file images@master:liberty.png | display In Console # In your Console, click on the images repo to visualize its commit and inspect its file:\n4. Create a Pipeline # Now that you have some data in your repo, it is time to do something with it using a pipeline.\nPipelines process data and are defined using a JSON pipeline specification. For this tutorial, we\u0026rsquo;ve already created the spec for you.\nReview Pipeline Spec # Take a moment to review the details of the provided pipeline spec so that you\u0026rsquo;ll know how to create one on your own in the future.\n{ // The `pipeline` section contains a `name` for identification; this name is also used to create a corresponding output repo. \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;edges\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;A pipeline that performs image edge detection by using the OpenCV library.\u0026#34;, // The `transform` section allows you to specify the docker `image` you want to use (`pachyderm/opencv:1.0`)and the `cmd` that defines the entry point (`edges.py`). \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python3\u0026#34;, \u0026#34;/edges.py\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/opencv:1.0\u0026#34; }, // The input section specifies repos visible to the running pipeline, and how to process the data from the repos. // Commits to these repos trigger the pipeline to create new processing jobs. In this case, `images` is the repo, and `/*` is the glob pattern. \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;images\u0026#34;, // The glob pattern defines how the input data will be transformed into datum if you want to distribute computation. `/*` means that each file can be processed individually. \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } } } The following extract is the Python User Code run in this pipeline:\nimport cv2 import numpy as np from matplotlib import pyplot as plt import os # make_edges reads an image from /pfs/images and outputs the result of running # edge detection on that image to /pfs/out. Note that /pfs/images and # /pfs/out are special directories that Pachyderm injects into the container. def make_edges(image): img = cv2.imread(image) tail = os.path.split(image)[1] edges = cv2.Canny(img,100,200) plt.imsave(os.path.join(\u0026#34;/pfs/out\u0026#34;, os.path.splitext(tail)[0]+\u0026#39;.png\u0026#39;), edges, cmap = \u0026#39;gray\u0026#39;) # walk /pfs/images and call make_edges on every file found for dirpath, dirs, files in os.walk(\u0026#34;/pfs/images\u0026#34;): for file in files: make_edges(os.path.join(dirpath, file)) The code simply walks over all the images in /pfs/images, performs edge detection, and writes the result to /pfs/out.\n/pfs/images and /pfs/out are special local directories that Pachyderm creates within the container automatically. Input data is stored in /pfs/\u0026lt;input_repo_name\u0026gt;. ‚ÑπÔ∏è Your code must write out to /pfs/out (see the function make_edges(image) above). Pachyderm gathers data written to /pfs/out, versions it, and maps it to the pipeline\u0026rsquo;s output repo of the same name.\nNow, let\u0026rsquo;s create the pipeline in Pachyderm:\npachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/2.6.x/examples/opencv/edges.json Again, check the end result in your Console: What Happens When You Create a Pipeline # When you create a pipeline, Pachyderm transforms all current and future data added to your input repo using your user code. This process is known as a job. The initial job downloads the specified Docker image that is used for all future jobs.\nView the job: pachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # 23378d899d3d45738f55df3809841145 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 5 seconds ago 5 seconds ago Check the state of your pipeline: pachctl list pipeline # NAME VERSION INPUT CREATED STATE / LAST JOB DESCRIPTION # edges 1 images:/* 2 minutes ago running / success A pipeline that performs image edge detection by using the OpenCV library. List your repositories: pachctl list repo # NAME CREATED SIZE (MASTER) ACCESS LEVEL # edges 10 minutes ago ‚â§ 22.22KiB [repoOwner] Output repo for pipeline edges. # images 3 hours ago ‚â§ 57.27KiB [repoOwner] Reading the Output # We can view the output data from the edges repo in the same fashion that we viewed the input data.\nOperating System: MacOS Linux pachctl get file edges@master:liberty.png | open -f -a Preview.app pachctl get file edges@master:liberty.png | display Processing More Data # Create two new commits: pachctl put file images@master:AT-AT.png -f http://imgur.com/8MN9Kg0.png pachctl put file images@master:kitten.png -f http://imgur.com/g2QnNqa.png View the list of jobs that have started: pachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # 1c1a9d7d36944eabb4f6f14ebca25bf1 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 31 seconds ago 31 seconds ago # fe5c4f70ac4347fd9c5934f0a9c44651 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 47 seconds ago 47 seconds ago # 23378d899d3d45738f55df3809841145 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 12 minutes ago 12 minutes ago View the output data: Operating System: MacOS Linux pachctl get file edges@master:AT-AT.png | open -f -a Preview.app pachctl get file edges@master:kitten.png | open -f -a Preview.app pachctl get file edges@master:AT-AT.png | display pachctl get file edges@master:kitten.png | display 5. Create a DAG # Currently, we\u0026rsquo;ve only set up a single-stage pipeline. Let\u0026rsquo;s create a multi-stage pipeline (also known as a DAG) by adding a montage pipeline that takes our both original and edge-detected images and arranges them into a single montage of images:\nBelow is the pipeline spec for this new pipeline:\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;montage\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;A pipeline that combines images from the `images` and `edges` repositories into a montage.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;cross\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;images\u0026#34; } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;edges\u0026#34; } } ] }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;sh\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;v4tech/imagemagick\u0026#34;, \u0026#34;stdin\u0026#34;: [ \u0026#34;montage -shadow -background SkyBlue -geometry 300x300+2+2 $(find /pfs -type f | sort) /pfs/out/montage.png\u0026#34; ] } } This montage pipeline spec is similar to our edges pipeline except for the following differences:\nWe are using a different Docker image that has imagemagick installed. We are executing a sh command with stdin instead of a python script in the pipeline\u0026rsquo;s transform section. We have multiple input data repositories (images and edges). In the montage pipeline we are combining our multiple input data repositories using a cross pattern. This cross pattern creates a single pairing of our input images with our edge detected images.\nCreate the montage pipeline: pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/2.6.x/examples/opencv/montage.json View the triggered jobs: pachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # 01e0c8040e18429daf7f67ce34c3a5d7 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 11 seconds ago 11 seconds ago # 1c1a9d7d36944eabb4f6f14ebca25bf1 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 12 minutes ago 12 minutes ago # fe5c4f70ac4347fd9c5934f0a9c44651 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 12 minutes ago 12 minutes ago # 23378d899d3d45738f55df3809841145 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 24 minutes ago 24 minutes ago View the generated montage image: Operating System: MacOS Linux pachctl get file montage@master:montage.png | open -f -a Preview.app pachctl get file montage@master:montage.png | display ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["tutorials"],
    "id": "a219978dbbcf8f025593aff1ebc84a77"
  },
  {
    "title": "First-Time Setup",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Get Started",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/get-started/first-time-setup/",
    "relURI": "/latest/get-started/first-time-setup/",
    "body": "Pachyderm can be deployed in Kubernetes using a wide variety of container orchestrators, but to get you set up for the very first time, we recommend using Docker Desktop. This installation method is very fast and will provide you with everything you need to start the Beginner Tutorial.\nBefore You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:\nOpen a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each of the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; All installation steps after 1. Install Docker Desktop must be run through the WSL terminal (Ubuntu) and not in Powershell.\nYou are now ready to continue to Step 1.\nYou must have Homebrew installed. /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; 1. Install Docker Desktop # Install Docker Desktop for your machine. Navigate to Settings for Mac, Windows, or Linux. Adjust your resources (~4 CPUs and ~12GB Memory) Enable Kubernetes Select Apply \u0026amp; Restart. 2. Install Pachctl CLI # Operating System: MacOs, Windows, \u0026amp; Darwin Debian brew tap pachyderm/tap \u0026amp;\u0026amp; brew install pachyderm/tap/pachctl@2.6 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.6.1/pachctl_2.6.1_amd64.deb \u0026amp;\u0026amp; sudo dpkg -i /tmp/pachctl.deb 3. Install \u0026amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install PachD: Version: Community Edition Enterprise helm install pachyderm pachyderm/pachyderm \\ --set deployTarget=LOCAL \\ --set proxy.enabled=true \\ --set proxy.service.type=LoadBalancer \\ --set proxy.host=localhost Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:\nhelm install pachyderm pachyderm/pachyderm \\ --set deployTarget=LOCAL \\ --set pachd.enterpriseLicenseKey=\u0026#34;$(cat license.txt)\u0026#34; \\ --set proxy.enabled=true \\ --set proxy.service.type=LoadBalancer \\ --set proxy.host=localhost This unlocks Enterprise features but also requires user authentication to access Console. A mock user is created by default to get you started, with the username: admin and password: password.\nThis may take several minutes to complete.\n4. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 5. Connect to Cluster # pachctl connect http://localhost:80 ‚ÑπÔ∏è If the connection commands did not work together, run each separately.\nOptionally open your browser and navigate to the Console UI.\nüí° You can check your Pachyderm version and connection to pachd at any time with the following command:\npachctl version COMPONENT VERSION pachctl 2.6.1 pachd 2.6.1 ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "820f8709482fa5cee161363bde0c6d11"
  },
  {
    "title": "Connect to Existing Instance",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Get Started",
    "description": "Learn how to connect to your organization's existing instance.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/get-started/connect-to-existing/",
    "relURI": "/latest/get-started/connect-to-existing/",
    "body": " Before You Start # This guide assumes you have already installed Pachyderm. You should know the URL of your organization\u0026rsquo;s Pachyderm instance, located in your Helm Chart at proxy.host. How to Log in to a Cluster via IdP # Open a terminal. Connect to your organization\u0026rsquo;s instance. Method: HTTP HTTPS (TLS) pachctl connect http://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; pachctl connect https://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; Input the following command: pachctl auth login Select the connector you wish to use. Provide your credentials. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9e689eb2c259978dbd89dcfc1055bdb1"
  },
  {
    "title": "Language Clients",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Get Started",
    "description": "Learn about our available language clients.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/get-started/clients/",
    "relURI": "/latest/get-started/clients/",
    "body": "pachctl is the command-line tool you use to interact with a Pachyderm cluster in your terminal. However, external applications might need to interact with Pachyderm directly through our APIs.\nIn this case, Pachyderm offers language specific SDKs in Go, Python, and JS.\nGo Client # The Pachyderm team officially supports the Go client. It implements most of the functionalities provided with the pachctl CLI tool.\nGenerate And Serve The godocs Locally # Golang\u0026rsquo;s package (godoc), installed by default by the Go installer, can generate the Go client\u0026rsquo;s documentation from the go code.\nTo generate the docs:\nSet your GOPATH:\nexport PATH=$(go env GOPATH)/bin:$PATH In Pachyderm\u0026rsquo;s root directory, start the godocs server:\ngo run golang.org/x/tools/cmd/godoc -http=:6060 -goroot=\u0026#34;\u0026lt;your go root directory - for example: /Users/yourusername/pachyderm\u0026gt;\u0026#34; See https://pkg.go.dev/golang.org/x/tools/cmd/godoc for the complete list of flags available.\nIn your favorite browser, run localhost:6060/pkg/\n‚ö†Ô∏è A compatible version of gRPC is needed when using the Go client. You can identify the compatible version by searching for the version number next to replace google.golang.org/grpc =\u0026gt; google.golang.org/grpc in https://github.com/pachyderm/pachyderm/blob/master/go.mod then:\ngo get google.golang.org/grpc cd $GOPATH/src/google.golang.org/grpc git checkout v1.29.1 Running Go Examples # The Pachyderm godocs reference (see generation instructions above) provides examples of how you can use the Go client API. You need to have a running Pachyderm cluster to run these examples.\nMake sure that you use your pachd_address in client.NewFromAddress(\u0026quot;\u0026lt;your-pachd-address\u0026gt;:30650\u0026quot;). For example, if you are testing on minikube, run minikube ip to get this information.\nSee the OpenCV Example in Go for more information.\nPython Client # The Python client python-pachyderm is officially supported by the Pachyderm team. It implements most of the functionalities provided with the pachctl CLI tool allowing you to easily integrate operations like create repo, put a file, or create pipeline into your python applications.\n‚ÑπÔ∏è Use python-pachyderm v7.3 with Pachyderm 2.6.x.\nYou will find all you need to get you started or dive into the details of the available modules and functions in the API documentation, namely:\nThe installation instructions and links to PyPI. A quick \u0026ldquo;Hello World\u0026rdquo; example to jumpstart your understanding of the API. Links to python-pachyderm main Github repository with a list of useful examples. As well as the entire reference API. Node Client # Our Javascript client node-pachyderm is a library officially supported by Pachyderm and used in production by Pachyderm Console.\nToday, we provide only read operations as shown in Console. Over time, we will add additional functionality to the SDK. However, there are no near-term plans to reach parity with python-pachyderm yet.\nPlease get in touch with us if you are interested in contributing or ask your questions on our dedicated slack channel.\nYou will find installations instructions and a first quick overview of how to use the library in our public repository. Check also our opencv example.\nOther languages # Pachyderm uses a simple protocol buffer API. Protobufs support other languages, any of which can be used to programmatically use Pachyderm. We have not built clients for them yet. It is an easy way to contribute to Pachyderm if you are looking to get involved.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["sdks", "golang", "python", "javascript", "developers"],
    "id": "6ae5b5f0cd79fdc7db272499c2269f23"
  },
  {
    "title": "Learn",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Learn about how our platform works.",
    "date": "January 30, 2023",
    "uri": "https://docs.pachyderm.com/latest/learn/",
    "relURI": "/latest/learn/",
    "body": "Pachyderm is a data science platform that provides data-driven pipelines with version control and autoscaling. It is container-native, allowing developers to use the languages and libraries that are best suited to their needs, and runs across all major cloud providers and on-premises installations.\nThe platform is built on Kubernetes and integrates with standard tools for CI/CD, logging, authentication, and data APIs, making it scalable and incredibly flexible. Pachyderm‚Äôs data-driven pipelines allow you to automatically trigger data processing based on changes in your data, and the platform‚Äôs autoscaling capabilities ensure that resource utilization is optimized, maximizing developer efficiency.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "35086b30a659a4efdfd10f56bdc9440f"
  },
  {
    "title": "Key Features",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Learn",
    "description": "Learn about the key features and benefits of our powerful data processing platform.",
    "date": "January 30, 2023",
    "uri": "https://docs.pachyderm.com/latest/learn/key-features/",
    "relURI": "/latest/learn/key-features/",
    "body": " Key Features and Benefits # The following are the key features of Pachyderm that make it a powerful data processing platform.\nData-driven Pipelines # Automatically trigger pipelines based on changes in the data. Orchestrate batch or real-time data pipelines. Only process dependent changes in the data. Reproducibility and data lineage across all pipelines. Version Control # Track every change to your data automatically. Works with any file type. Supports collaboration through a git-like structure of commits. Autoscaling and Deduplication # Autoscale jobs based on resource demand. Automatically parallelize large data sets. Automatically deduplicate data across repositories. Flexibility and Infrastructure Agnosticism # Use existing cloud or on-premises infrastructure. Process any data type, size, or scale in batch or real-time pipelines. Container-native architecture allows for developer autonomy. Integrates with existing tools and services, including CI/CD, logging, authentication, and data APIs. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "79bf63b0b065c446cad46c138b62fd3c"
  },
  {
    "title": "Target Audience",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Learn",
    "description": "Discover if our platform is the right solution for your large-scale data processing and analysis needs.",
    "date": "January 30, 2023",
    "uri": "https://docs.pachyderm.com/latest/learn/target-audience/",
    "relURI": "/latest/learn/target-audience/",
    "body": " Target Audience # Pachyderm is designed for data engineers and data scientists who are managing and processing large amounts of data in a scalable and efficient manner. Pachyderm is ideal for organizations working with big data and require robust, version-controlled, reproducible, and distributed data pipelines.\nIt is particularly useful for large unstructured data processing jobs, such as dataset curation for computer vision, speech recognition, video analytics, NLP, and many others.\nNon-Target Audience # Pachyderm is not intended for users who do not require large-scale data processing and analysis. For instance, data scientists who are just starting with a small project may not need Pachyderm\u0026rsquo;s distributed system. Additionally, users with limited experience with containerization, cloud computing, and distributed systems may find it challenging to use Pachyderm effectively.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8aed82b2a12515cc7188bcf7b6ecae04"
  },
  {
    "title": "Basic Concepts",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Learn",
    "description": "Discover how our platform provides a secure, scalable, and version-controlled solution for storing and processing large amounts of data through its most basic concepts.",
    "date": "January 30, 2023",
    "uri": "https://docs.pachyderm.com/latest/learn/basic-concepts/",
    "relURI": "/latest/learn/basic-concepts/",
    "body": " Pachyderm File System # The Pachyderm File System (PFS) is the backbone of the Pachyderm data platform, providing a secure, scalable, and efficient way to store and manage large amounts of data. It is a version-controlled data management system that enables users to store any type of data in any format and scale, from a single file to a directory of files. The PFS is built on top of Postgres and S3, ensuring that your data is secure, consistent, and easily accessible. With PFS, users can version their data and work collaboratively with their teams, using branches and commits to manage and track changes over time.\nRepositories (Repo) # Pachyderm repositories are version controlled, meaning that they keep track of changes to the data stored within them. Each repository can contain any type of data, including individual files or directories of files, and can handle data of any scale.\nLearn more about Input Repositories and Output Repositories.\nBranches # Branches in Pachyderm are similar to branches in Git. They are pointers to commits that move along a growing chain of commits. This allows you to work with different versions of your data within the same repository.\nLearn more about Branches\nCommits # A commit in Pachyderm is created automatically whenever data is added to or deleted from a repository. Each commit preserves the state of all files in the repository at the time of the commit, similar to a snapshot. Each commit is uniquely identifiable by a UUID and is immutable, meaning that the source data can never change.\nLearn more about Commits\nPachyderm Pipeline System # The Pachyderm Pipeline System (PPS) is a core component of the Pachyderm platform, designed to run robust data pipelines in a scalable and reproducible manner. With PPS, you can define, execute, and monitor complex data transformations using code that is run in Docker containers. The output of each pipeline is version-controlled in a Pachyderm data repository, providing a complete, auditable history of all processing steps. In this way, PPS provides a flexible, data-driven solution for managing your data processing needs, while keeping data and processing results secure, reproducible, and scalable.\nLearn more about the PPS\nPipelines # Pachyderm pipelines are used to transform data from Pachyderm repositories. The output data is versioned in a Pachyderm data repository, and the code for the transformation is run in Docker containers. Pipelines are triggered by new commits to a branch, making them data-driven.\nLearn more about Pipelines\nJobs # A job in Pachyderm is the execution of a pipeline with a new commit. The data is distributed and parallelized computation is performed across a cluster. Each job is uniquely identified, making it possible to reproduce the results of a specific job.\nLearn more about Jobs\nDatum # A datum in Pachyderm is a unit of computation for a job. It is used to distribute the processing workloads and to define how data can be split for parallel processing.\nLearn more about Datums\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "eea3afced4fcb5aee77f4e3edeaf00c1"
  },
  {
    "title": "Intro to Data Versioning",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Learn",
    "description": "Learn how to interact with versioned data, including creating and managing data repositories, creating and navigating commits, and branching to manage the evolution of data.",
    "date": "January 30, 2023",
    "uri": "https://docs.pachyderm.com/latest/learn/intro-data-versioning/",
    "relURI": "/latest/learn/intro-data-versioning/",
    "body": " Introduction to Data Versioning # On this page we want to give a brief overview of how to use and interact with versioned data inside Pachyderm. Collectively, this is often referred to as the Pachyderm File System (PFS).\nRepositories # Data versioning in Pachyderm starts with creating a data repository. Pachyderm data repos are similar to Git repositories in that they provide a place to track changes made to a set of files.\nUsing the Pachyderm CLI (pachctl) we would create a repository called data with the create repo command.\npachctl create repo data Once a repo is created, data can be added, deleted, or updated to a branch and all changes are versioned with commits.\nCommits # In Pachyderm, commits are made to branches of a repo. For example, in the following session if we add a file to our data repository, that file will be captured in a commit.\n$ pachctl put file data@master -f my_file.bin $ pachctl list commit images@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master 6806cce 4 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE /my_file.bin file 57.27KiB If we then delete that file, it is removed from the active state of the branch, but the commit still exists.\n$ pachctl delete file data@master:/my_file.bin $ pachctl list commit data@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master ff1867a 3 seconds ago 0B USER data master 6806cce 20 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE Then if we add the file back, we\u0026rsquo;ll see a third commit.\n$ pachctl create file data@master:/my_file.bin $ pachctl list commit data@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master 0ec029b 20 seconds ago 57.27KiB USER data master ff1867a 3 seconds ago 0B USER data master 6806cce 20 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE /my_file.bin file 57.27KiB Visualizing the commit history for the master branch looks like the following.\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;data\u0026#39;}} }%% gitGraph commit id:\u0026#34;6806cce\u0026#34; commit id:\u0026#34;ff1867a\u0026#34; commit id:\u0026#34;0ec029b\u0026#34; tag: \u0026#34;master\u0026#34; Branches are a critical for tracking commits. The branch functions as a pointer to the most recent commit to the branch. For instance, when we create a new commit on the master branch (pachctl put file data@master -f my_new_file), we would create a new commit and our master branch would point at it.\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;data\u0026#39;}} }%% gitGraph commit id:\u0026#34;6806cce\u0026#34; commit id:\u0026#34;ff1867a\u0026#34; commit id:\u0026#34;0ec029b\u0026#34; commit id:\u0026#34;b69b3e3\u0026#34; tag: \u0026#34;master\u0026#34; As we\u0026rsquo;ve already seen, we can reference the HEAD of the branch, with the syntax, data@master.\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;data@master\u0026#39;}} }%% gitGraph commit id:\u0026#34;6806cce\u0026#34; commit id:\u0026#34;ff1867a\u0026#34; commit id:\u0026#34;0ec029b\u0026#34; commit id:\u0026#34;b69b3e3\u0026#34; type:HIGHLIGHT tag: \u0026#34;HEAD\u0026#34; Navigating Commits # Here we\u0026rsquo;ll introduce the basics of how to navigate commits. Navigating these commits is an important aspect of working with PFS, and allows you to easily manage the history and evolution of your data.\nOne useful feature for navigating commits in PFS is the ability to refer to a previous commit using ancestry syntax. This syntax allows you to specify a commit relative to the current one, making it easy to compare and manipulate different versions of your data.\nThis makes it simple to switch between different versions of your data, and to perform operations like diffing, branching, and merging.\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;data@master^\u0026#39;}} }%% gitGraph commit id:\u0026#34;6806cce\u0026#34; commit id:\u0026#34;ff1867a\u0026#34; commit id:\u0026#34;0ec029b\u0026#34; type:HIGHLIGHT commit id:\u0026#34;b69b3e3\u0026#34; tag: \u0026#34;HEAD\u0026#34; To refer to the commit 2 before the HEAD:\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;data@master^^\u0026#39;}} }%% gitGraph commit id:\u0026#34;6806cce\u0026#34; commit id:\u0026#34;ff1867a\u0026#34; type:HIGHLIGHT commit id:\u0026#34;0ec029b\u0026#34; commit id:\u0026#34;b69b3e3\u0026#34; tag: \u0026#34;HEAD\u0026#34; Similarly, we can abbreviate this with the following syntax:\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;data@master^2\u0026#39;}} }%% gitGraph commit id:\u0026#34;6806cce\u0026#34; commit id:\u0026#34;ff1867a\u0026#34; type:HIGHLIGHT commit id:\u0026#34;0ec029b\u0026#34; commit id:\u0026#34;b69b3e3\u0026#34; tag: \u0026#34;HEAD\u0026#34; We can reference the commits in numerical order using .n, where n is the commit number.\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;data@master.1\u0026#39;}} }%% gitGraph commit id:\u0026#34;6806cce\u0026#34; type:HIGHLIGHT commit id:\u0026#34;ff1867a\u0026#34; commit id:\u0026#34;0ec029b\u0026#34; commit id:\u0026#34;b69b3e3\u0026#34; tag: \u0026#34;HEAD\u0026#34; %%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;data@master.-1\u0026#39;}} }%% gitGraph commit id:\u0026#34;6806cce\u0026#34; commit id:\u0026#34;ff1867a\u0026#34; commit id:\u0026#34;0ec029b\u0026#34; type:HIGHLIGHT commit id:\u0026#34;b69b3e3\u0026#34; tag: \u0026#34;HEAD\u0026#34; Branches # In Pachyderm, branches are used to track changes in a repository. You can think of a branch as a tag on a specific commit. Branches are associated with a particular commit and are updated as new commits are made (moving the HEAD of that branch to its most recent commit). This also means that at any time, you can change the commit that a branch is associated with, affecting branch history.\nHere\u0026rsquo;s an example of a repo with three branches, each with its own history of commits:\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;master\u0026#39;}} }%% gitGraph commit commit branch v1.0 commit commit commit branch v1.1 commit commit commit tag:\u0026#34;v1.1:HEAD\u0026#34; checkout v1.0 commit tag:\u0026#34;v1.0:HEAD\u0026#34; checkout master commit tag:\u0026#34;master:HEAD\u0026#34; \u0026ldquo;Merging\u0026rdquo; Branches # The concept of merging binary data from different commits is complex. Ultimately, there are too many edge cases to do it reliably for every type of binary data, because computing a diff between two commits is ultimately meaningless unless you know how to compare the data. For example, we know that text files can be compared line-by-line or a bitmap image pixel by pixel, but how would we compute a diff for, say, binary model files?\nAdditionally, the output of a merge is usually a master copy, the official set of files desired. We rarely combine multiple pieces of image data to make one image, and if we are, we have usually created a technique for doing so. In the end, some files will be deleted, some updated, and some added.\nInstead, merging data, means creating a new commit with the desired combination of files and pointing our branch at that commit. In order to maintain a proper history, we would also want to make sure that the parent of that commit is relevant to what we want as well.\nFor example, in this situation, we have created a branch, dev, based on the 1-2833cd3 commit. We have committed multiple times to the dev branch, but nothing to master.\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;master\u0026#39;}} }%% gitGraph commit id:\u0026#34;0-96e9b89\u0026#34; commit id:\u0026#34;1-2833cd3\u0026#34; tag:\u0026#34;master:HEAD\u0026#34; branch dev commit id:\u0026#34;2-25a8daf\u0026#34; commit id:\u0026#34;3-6413afc\u0026#34; commit id:\u0026#34;4-41a750b\u0026#34; tag:\u0026#34;dev:HEAD\u0026#34; In this case it is simple to simply move the master branch to follow the most recent commit on dev, 4-41a750b.\npachctl create branch data@master --head 41a750b\nWhich would look like this:\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;master\u0026#39;}} }%% gitGraph commit id:\u0026#34;0-96e9b89\u0026#34; commit id:\u0026#34;1-2833cd3\u0026#34; branch dev commit id:\u0026#34;2-25a8daf\u0026#34; commit id:\u0026#34;3-6413afc\u0026#34; commit id:\u0026#34;4-41a750b\u0026#34; tag:\u0026#34;master:HEAD, dev:HEAD\u0026#34; Or from the history perspective of the respective branches:\n%%{init: { \u0026#39;logLevel\u0026#39;: \u0026#39;debug\u0026#39;, \u0026#39;theme\u0026#39;: \u0026#39;base\u0026#39;, \u0026#39;gitGraph\u0026#39;: {\u0026#39;showBranches\u0026#39;: true, \u0026#39;showCommitLabel\u0026#39;:true,\u0026#39;mainBranchName\u0026#39;: \u0026#39;master\u0026#39;}} }%% gitGraph commit id:\u0026#34;0-96e9b89\u0026#34; commit id:\u0026#34;1-2833cd3\u0026#34; branch dev commit id:\u0026#34;2-25a8daf\u0026#34; commit id:\u0026#34;3-6413afc\u0026#34; checkout dev commit tag:\u0026#34;dev:HEAD\u0026#34; id:\u0026#34;4-41a750b\u0026#34; checkout master commit id:\u0026#34;2-25a8daf \u0026#34; commit id:\u0026#34;3-6413afc \u0026#34; commit tag:\u0026#34;master:HEAD\u0026#34; id:\u0026#34;4-41a750b \u0026#34; Branches are useful for many reasons, but in Pachyderm they also form the foundation of the pipeline system. New commits on branches can be used to trigger pipelines to run, resulting in one of the key differentiators, data-driven pipelines.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b8fb7c346bc31748da8be17677c38a15"
  },
  {
    "title": "Intro to Pipelines",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Learn",
    "description": "Learn about the Pipeline System and how to define pipelines in YAML for data transformation and processing, including datums, jobs, and advanced glob patterns.",
    "date": "January 30, 2023",
    "uri": "https://docs.pachyderm.com/latest/learn/intro-pipelines/",
    "relURI": "/latest/learn/intro-pipelines/",
    "body": " Introduction to Pipelines # The Pachyderm Pipeline System (PPS) is a powerful tool for automating data transformations. With PPS, pipelines can be automatically triggered whenever input data changes, meaning that data transformations happen automatically in response to changes in your data, without the need for manual intervention.\nPipelines in Pachyderm are defined by a pipeline specification and run on Kubernetes. The output of a pipeline is stored in a versioned data repository, which allows you to reproduce any transformation that occurs in Pachyderm.\nPipelines can be combined into a computational DAG (directed acyclic graph), with each pipeline being triggered when an upstream commit is finished. This allows you to build complex workflows that can process large amounts of data efficiently and with minimal manual intervention.\nPipeline Specification # This is a Pachyderm pipeline definition in YAML. It describes a pipeline called transform that takes data from the data repository and transforms it using a Python script my_transform_code.py.\npipeline: name: transform input: pfs: repo: data glob: \u0026#34;/*\u0026#34; transform: image: my-transform-image:v1.0 cmd: - python - \u0026#34;/my_transform_code.py\u0026#34; - \u0026#34;--input\u0026#34; - \u0026#34;/pfs/data/\u0026#34; - \u0026#34;--output\u0026#34; - \u0026#34;/pfs/out/\u0026#34; Here\u0026rsquo;s a breakdown of the different sections of the pipeline definition:\npipeline specifies the name of the pipeline (in this case, it\u0026rsquo;s transform). This name will also be used as the name for the output data repository. input specifies the input for the pipeline. In this case, the input is taken from the data repository in Pachyderm. glob is used to specify how the files from the repository map to datums for processing. In this case, /* is used to specify all files in the repository can be processed individually. transform specifies the code and image to use for processing the input data. The image field specifies the Docker image to use for the pipeline. In this example, the image is named my-transform-image with a tag of v1.0. The cmd field specifies the command to run inside the container. In this example, the command is python /my_transform_code.py, which runs a Python script named my_transform_code.py. The script is passed the --input flag pointing to the input data directory, and the --output flag pointing to the output data directory. /pfs/data/ and /pfs/out/ are directories created by Pachyderm. The input directory will contain an individual datum when the job is running, and anything put into the output directory will be committed to the output repositories when the job is complete. So, in summary, this pipeline definition defines a pipeline called transform that takes all files in the data repository, runs a Python script to transform them, and outputs the results to the out repository.\nDatums and Jobs # Pipelines can distribute work across a cluster to parallelize computation. Each time data is committed to a Pachyderm repository, a job is created for each pipeline with that repo as an input to process the data.\nTo determine how to distribute data and computational work, datums are used. A datum is an indivisible unit of data required by the pipeline, defined according to the pipeline spec. The datums will be distributed across the cluster to be processed by workers.\n‚ÑπÔ∏è Only one job per pipeline will be created per commit, but there may be many datums per job.\nFor example, say you have a bunch of images that you want to normalize to a single size. You could iterate through each image and use opencv to change the size of it. No image depends on any other image, so this task can be parallelized by treating each image as an individual unit of work, a datum.\nNext, let‚Äôs say you want to create a collage from those images. Now, we need to consider all of the images together to combine them. In this case, the collection of images would be a single datum, since they are all required for the process.\nPachyderm input specifications can handle both of these situations with the glob section of the Pipeline Specification.\nBasic Glob Patterns # In this section we\u0026rsquo;ll introduce glob patterns and datums in a couple of examples.\nIn the basic glob pattern example below, the input glob pattern is /*. This pattern matches each image at the top level of the images@master branch as an individual unit of work.\npipeline: name: resize description: A pipeline that resizes an image. input: pfs: glob: /* repo: images transform: cmd: - python - resize.py - --input - /pfs/images/* - --output - /pfs/out/ image: pachyderm/opencv When the pipeline is executed, it retrieves the datums defined in the input specification. For each datum, the worker downloads the necessary files into the Docker container at the start of its execution, and then performs the transform. Once the execution is complete, the output for each execution is combined into a commit and written to the output data repository.\nIn this example, the input glob pattern is /. This pattern matches everything at the top level of the images@master branch as an individual unit of work.\npipeline: name: collage description: A pipeline that creates a collage for a collection of images. input: pfs: glob: / repo: images transform: cmd: - python - collage.py - --input - /pfs/images/* - --output - /pfs/out/ image: pachyderm/opencv When this pipeline runs, it retrieves a single datum from the input specification. The job runs the single datum, downloading all the files from the images@master into the Docker container, and performs the transform. The result is then committed to the output data repository.\nAdvanced Glob Patterns # Datums can also be created from advanced operations, such as Join, Cross, Group, Union, and others to combine glob patterns from multiple data repositories. This allows us to create complex datum definitions, enabling sophisticated data processing pipelines.\nPipeline Communication (Advanced) # A much more detailed look at how Pachyderm actually triggers pipelines is shown in the sequence diagram below. This is a much more advanced level of detail, but knowing how the different pieces of the platform interact can be useful.\nBefore we look at the diagram, it may be helpful to provide a brief recap of the main participants involved:\nUser: The user is the person interacting with Pachyderm, typically through the command line interface (CLI) or one of the client libraries. PFS (Pachyderm File System): PFS is the underlying file system that stores all of the data in Pachyderm. It provides version control and lineage tracking for all data inside it. PPS (Pachyderm Pipeline System): PPS is how code gets applied to the data in Pachyderm. It manages the computational graph, which describes the dependencies between different steps of the data processing pipeline. Worker: Workers are Kubernetes pods that executes the jobs defined by PPS. Each worker runs a container image that contains the code for a specific pipeline. The worker will iterate through the datums it is given and apply user code to it. sequenceDiagram participant User participant PPS participant PFS participant Worker User-\u0026gt;\u0026gt;PFS: pachctl create repo foo activate PFS Note over PFS: create branch foo@master deactivate PFS User-\u0026gt;\u0026gt;PPS: pachctl create pipeline bar activate PPS PPS-\u0026gt;\u0026gt;PFS: create branch bar@master \u0026lt;br\u0026gt; (provenant on foo@master) PPS-\u0026gt;\u0026gt;Worker: create pipeline worker master Worker-\u0026gt;\u0026gt;PFS: subscribe to bar@master \u0026lt;br\u0026gt; (because it\u0026#39;s subvenant on foo@master) deactivate PPS User-\u0026gt;\u0026gt;PFS: pachctl put file -f foo@master data.txt activate PFS Note over PFS: start commit PFS-\u0026gt;\u0026gt;PFS: propagate commit \u0026lt;br\u0026gt; (start downstream commits) Note over PFS: copy data.txt to open commit Note over PFS: finish commit PFS--\u0026gt;\u0026gt;Worker: subscribed commit returns deactivate PFS Note over Worker: Pipeline Triggered activate Worker Worker-\u0026gt;\u0026gt;PPS: Create job Worker-\u0026gt;\u0026gt;PFS: request datums for commit PFS--\u0026gt;\u0026gt;Worker: Datum list loop Each datum PFS-\u0026gt;\u0026gt;Worker: download datum Note over Worker: Process datum with user code Worker-\u0026gt;\u0026gt;PFS: copy data to open output commit end Worker-\u0026gt;\u0026gt;PFS: Finish commit Worker-\u0026gt;\u0026gt;PPS: Finish job deactivate Worker This diagram illustrates the data flow and interaction between the user, the Pachyderm Pipeline System (PPS), the Pachyderm File System (PFS), and a worker node when creating and running a Pachyderm pipeline. Note, this is simplified for the single worker case. The multi-worker and autoscaling mechanisms are more complex.\nThe sequence of events begins with the user creating a PFS repo called foo and a PPS pipeline called bar with the foo repo as its input. When the pipeline is created, PPS creates a branch called bar@master, which is provenant on the foo@master branch in PFS. A worker pod is then created in the Kubernetes cluster by PPS, which subscribes to the bar@master branch.\nWhen the user puts a file named data.txt into the foo@master branch, PFS starts a new commit and propagates the commit, opening downstream commits for anything impacted. The worker receives the subscribed commit and when it finishes, triggers the pipeline.\nThe triggered pipeline creates a job for the pipeline, requesting datums for the output commit. For each datum, the worker downloads the data, processes it with the user\u0026rsquo;s code, and writes the output to an open output commit in PFS. Once all datums have been processed, the worker finishes the output commit and the job is marked as complete.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "27cdf186dbecf140c9dcdd80c54ae2b6"
  },
  {
    "title": "Intro to Console",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Learn",
    "description": "Learn how to perform various actions in the Console UI.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/console-guide/",
    "relURI": "/latest/learn/console-guide/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "865523605e5f190c993eab643f945c9c"
  },
  {
    "title": "View Project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Intro to Console",
    "description": "Learn how to view a project in the console UI.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/console-guide/view-project/",
    "relURI": "/latest/learn/console-guide/view-project/",
    "body": " How to View a Project in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Your project is displayed as a DAG (Directed Acyclic Graph) by default.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9a3ccd8dc1e8a2c353f626f0f6b8fe72"
  },
  {
    "title": "View List",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Intro to Console",
    "description": "Learn how to view a list of Repos or Pipelines in the console UI.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/console-guide/view-list/",
    "relURI": "/latest/learn/console-guide/view-list/",
    "body": " How to View a List of Resources in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select View List. The DAG view is converted into a List view, organized by resource types. Select Repositories to view a list of repositories. Select Pipelines to view a list of pipelines. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "5615fd14b0afc2dd920efe604c663e93"
  },
  {
    "title": "View Pipelines",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Intro to Console",
    "description": "Learn how to view pipeline details in the console UI.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/console-guide/view-pipeline/",
    "relURI": "/latest/learn/console-guide/view-pipeline/",
    "body": " How to View Pipeline Details in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select a Pipeline. Scroll and tab through the side panel to review the pipeline\u0026rsquo;s details. Job Overview: Contains details like the number of datums processed, success status, and runtime. Pipeline Info: Contains details like the pipeline\u0026rsquo;s description, number of tries allowed, output branch, and output repos. Pipeline Spec: Contains the pipeline spec which can be copied or downloaded as json/yaml. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e5c2a6b449563f8849124e4da617a28b"
  },
  {
    "title": "View Jobs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Intro to Console",
    "description": "Learn how to view job details in the console UI.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/console-guide/view-jobs/",
    "relURI": "/latest/learn/console-guide/view-jobs/",
    "body": " How to View Jobs From a Pipeline in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select Jobs. Scroll through the list of jobs. Select See Details. Select Read Logs. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "1eaf4f10ef715976d544ad2021459dbd"
  },
  {
    "title": "View Outputs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Intro to Console",
    "description": "Learn how to view output files in the Console UI.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/console-guide/view-outputs/",
    "relURI": "/latest/learn/console-guide/view-outputs/",
    "body": " How to View Output Files in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select an Output. Select View Files. Select See Files. Perform one of the following: Preview Download ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "67435ae0c6a4b4307af2542f56e989a1"
  },
  {
    "title": "View Inputs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Intro to Console",
    "description": "Learn how to view input files in the Console UI.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/console-guide/view-inputs/",
    "relURI": "/latest/learn/console-guide/view-inputs/",
    "body": " How to View Input Files in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select an Input Repo. Select View Files. Perform one of the following: Preview Download ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9a098a8b5fc3df3002159ad52375fe4d"
  },
  {
    "title": "Developer Workflow",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Learn",
    "description": "Learn how to manage and process data in your CI workflow.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/developer-workflow/",
    "relURI": "/latest/learn/developer-workflow/",
    "body": "In general, the developer workflow for Pachyderm involves adding data to versioned data repositories, creating pipelines to read from those repositories, executing the pipeline\u0026rsquo;s code, and writing the pipeline\u0026rsquo;s output to other data repositories. Both the data and pipeline can be iterated on independently with Pachyderm handling the code execution according to the pipeline specfication. The workflow steps are shown below.\nData Workflow # Adding data to Pachyderm is the first step towards building data-driven pipelines. There are multiple ways to add data to a Pachyderm repository:\nBy using the pachctl put file command By using a special type of pipeline, such as a spout or cron By using one of the Pachyderm\u0026rsquo;s language clients By using a compatible S3 client For more information, see Load Your Data Into Pachyderm.\nPipeline Workflow # The fundamental concepts of Pachyderm are very powerful, but the manual build steps mentioned in the pipeline workflow can become cumbersome during rapid-iteration development cycles. We\u0026rsquo;ve created a few helpful developer workflows and tools to automate steps that are error-prone or repetitive:\nThe push images flag or --push-images is a optional flag that can be passed to the create or update pipeline command. This option is most useful when you need to customize your Docker image or are iterating on the Docker image and code together, since it tags and pushes the image before updating the pipeline. CI/CD Integration provides a way to incorporate Pachyderm functions into the CI process. This is most useful when working with a complex project or for code collaboration. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "1286528d0ba6108b5b29bf5aaf9f6b55"
  },
  {
    "title": "CI/CD Integration",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Developer Workflow",
    "description": "Learn how to integrate into your overall CI/CD workflows.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/developer-workflow/ci-cd-integration/",
    "relURI": "/latest/learn/developer-workflow/ci-cd-integration/",
    "body": "Pachyderm is a powerful system for providing data provenance and scalable processing to data scientists and engineers. You can make it even more powerful by integrating it with your existing continuous integration and continuous deployment (CI/CD) workflows and systems. If you are just starting to use Pachyderm and not setting up automation for your Pachyderm build processes, see Working with Pipelines.\nThe following diagram demonstrates automated Pachyderm development workflow with CI:\nAlthough initial CI setup might require extra effort on your side, in the long run, it brings significant benefits to your team, including the following:\nSimplified workflow for data scientists. Data scientists do not need to be aware of the complexity of the underlying containerized infrastructure. They can follow an established Git process, and the CI platform takes care of the Docker build and push process behind the scenes.\nYour CI platform can run additional unit tests against the submitted code before creating the build.\nFlexibility in tagging Docker images, such as specifying a custom name and tag or using the commit SHA for tagging.\nCI Workflow # The CI workflow includes the following steps:\nA new commit triggers a Git hook.\nTypically, Pachyderm users store the following artifacts in a Git repository:\nA Dockerfile that you use to build local images. A pipeline.json specification file that you can use in a Makefile to create local builds, as well as in the CI/CD workflows. The code that performs data transformations. A commit hook in Git for your repository triggers the CI/CD process. It uses the information in your pipeline specification for subsequent steps.\nBuild an image.\nYour CI process automatically starts the build of a Docker container image based on your code and the Dockerfile.\nPush the image tagged with commit ID to an image registry.\nYour CI process pushes a Docker image created in Step 2 to your preferred image registry. When a data scientist submits their code to Git, a CI process uses the Dockerfile in the repository to build, tag with a Git commit SHA, and push the container to your image registry.\nUpdate the pipeline spec with the tagged image.\nIn this step, your CI/CD infrastructure uses your updated pipeline.json specification and fills in the Git commit SHA for the version of the image that must be used in this pipeline. Then, it runs the pachctl update pipeline command to push the updated pipeline specification to Pachyderm. After that, Pachyderm pulls a new image from the registry automatically. When the production pipeline is updated with the pipeline.json file that has the correct image tag in it, Pachyderm restarts all pods for this pipeline with the new image automatically.\nGitHub Actions # GitHub actions are a convenient way to kick off workflows and perform integration. These can be used to:\nManually trigger a pipeline build, or Automatically build a pipeline from a commit or pull request. In our example, we show how to use the Pachyderm GitHub Action to incorporate Pachyderm functions to run on a Pull Request or at other points during development.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["workflows"],
    "id": "d82fdeaabe0648b66353ff5fef0d0f36"
  },
  {
    "title": "Create a Machine Learning Workflow",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Developer Workflow",
    "description": "Learn how to integrate into your Machine Learning workflows.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/developer-workflow/create-ml-workflow/",
    "relURI": "/latest/learn/developer-workflow/create-ml-workflow/",
    "body": "Because Pachyderm is a language and framework agnostic and platform, and because it easily distributes analysis over large data sets, data scientists can use any tooling for creating machine learning workflows. Even if that tooling is not familiar to the rest of an engineering organization, data scientists can autonomously develop and deploy scalable solutions by using containers. Moreover, Pachyderm‚Äôs pipeline logic paired with data versioning make any results reproducible for debugging purposes or during the development of improvements to a model.\nFor maximum leverage of Pachyderm\u0026rsquo;s built functionality, Pachyderm recommends that you combine model training processes, persisted models, and model utilization processes, such as making inferences or generating results, into a single Pachyderm pipeline Directed Acyclic Graph (DAG).\nSuch a pipeline enables you to achieve the following goals:\nKeep a rigorous historical record of which models were used on what data to produce which results. Automatically update online ML models when training data or parameterization changes. Easily revert to other versions of an ML model when a new model does not produce an expected result or when bad data is introduced into a training data set. The following diagram demonstrates an ML pipeline:\nYou can update the training dataset at any time to automatically train a new persisted model. Also, you can use any language or framework, including Apache Spark‚Ñ¢, Tensorflow‚Ñ¢, scikit-learn‚Ñ¢, or other, and output any format of persisted model, such as pickle, XML, POJO, or other. Regardless of the framework, Pachyderm versions the model so that you can track the data that was used to train each model.\nPachyderm processes new data coming into the input repository with the updated model. Also, you can recompute old predictions with the updated model, or test new models on previously input and versioned data. This feature enables you to avoid manual updates to historical results or swapping ML models in production.\nFor examples of ML workflows in Pachyderm see Machine Learning Examples.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["workflows"],
    "id": "c1f92a52541a4583d61d12dc76543389"
  },
  {
    "title": "The Push Images Flag",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Developer Workflow",
    "description": "Learn how to use the --push-images flag to accelerate pipeline development speed.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/developer-workflow/push-images-flag/",
    "relURI": "/latest/learn/developer-workflow/push-images-flag/",
    "body": "The --push-images flag is one way to improve development speed when working with pipelines.\nThe --push-images flag performs the following steps after you have built your image:\nIn your local registry, generates a unique tag for the image named after the transform.image field of your pipeline spec. üí° You must build your image with your username as a prefix (example: pachyderm/example-joins-inner-outer) \u0026ndash; This name must match the one declared in the transform.image field of your pipeline spec.\nPushes the Docker image, with the tag, to your registry Updates the image tag in the pipeline spec json (on the fly) to match the new image Submits the updated pipeline to the Pachyderm cluster The usage of the flag is shown below:\npachctl update pipeline -f \u0026lt;pipeline name\u0026gt; --push-images --registry \u0026lt;registry\u0026gt; --username \u0026lt;registry user\u0026gt; ‚ÑπÔ∏è For more details on the --push-images flag, see Update a Pipeline.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pachctl"],
    "id": "ebbf8b44d3f592b646f88d7619e412dc"
  },
  {
    "title": "Working with Pipelines",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Developer Workflow",
    "description": "Learn about the steps involved in building, testing, and deploying data-transformation pipelines.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/developer-workflow/working-with-pipelines/",
    "relURI": "/latest/learn/developer-workflow/working-with-pipelines/",
    "body": "A typical Pachyderm workflow involves multiple iterations of experimenting with your code and pipeline specs.\nIn general, there are five steps to working with a pipeline. The stages can be summarized in the image below.\nWe will walk through each of the stages in detail.\nStep 1: Write Your Analysis Code # Because Pachyderm is completely language-agnostic, the code that is used to process data in Pachyderm can be written in any language and can use any libraries of choice. Whether your code is as simple as a bash command or as complicated as a TensorFlow neural network, it needs to be built with all the required dependencies into a container that can run anywhere, including inside of Pachyderm. See Examples.\nYour code does not have to import any special Pachyderm functionality or libraries. However, it must meet the following requirements:\nRead files from a local file system. Pachyderm automatically mounts each input data repository as /pfs/\u0026lt;repo_name\u0026gt; in the running containers of your Docker image. Therefore, the code that you write needs to read input data from this directory, similar to any other file system.\nBecause Pachyderm automatically spreads data across parallel containers, your analysis code does not have to deal with data sharding or parallelization. For example, if you have four containers that run your Python code, Pachyderm automatically supplies 1/4 of the input data to /pfs/\u0026lt;repo_name\u0026gt; in each running container. These workload balancing settings can be adjusted as needed through Pachyderm tunable parameters in the pipeline specification.\nWrite files into a local file system, such as saving results. Your code must write to the /pfs/out directory that Pachyderm mounts in all of your running containers. Similar to reading data, your code does not have to manage parallelization or sharding.\nStep 2: Build Your Docker Image # When you create a Pachyderm pipeline, you need to specify a Docker image that includes the code or binary that you want to run. Therefore, every time you modify your code, you need to build a new Docker image, push it to your image registry, and update the image tag in the pipeline spec. This section describes one way of building Docker images, but if you have your own routine, feel free to apply it.\nTo build an image, you need to create a Dockerfile. However, do not use the CMD field in your Dockerfile to specify the commands that you want to run. Instead, you add them in the cmd field in your pipeline specification. Pachyderm runs these commands inside the container during the job execution rather than relying on Docker to run them. The reason is that Pachyderm cannot execute your code immediately when your container starts, so it runs a shim process in your container instead, and then, it calls your pipeline specification\u0026rsquo;s cmd from there.\n‚ÑπÔ∏è The Dockerfile example below is provided for your reference only. Your Dockerfile might look completely different.\nTo build a Docker image, complete the following steps:\nIf you do not have a registry, create one with a preferred provider. If you decide to use DockerHub, follow the Docker Hub Quickstart to create a repository for your project.\nCreate a Dockerfile for your project. See the OpenCV example.\nBuild a new image from the Dockerfile by specifying a tag:\ndocker build -t \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; . For more information about building Docker images, see Docker documentation.\nStep 3: Push Your Docker Image to a Registry # Once your image is built and tagged, you need to upload the image into a public or private image registry, such as DockerHub.\nAlternatively, you can use the Pachyderm\u0026rsquo;s built-in functionality to tag, and push images by running the pachctl update pipeline command with the --push-images flag. For more information, see Update a pipeline.\nLog in to an image registry.\nIf you use DockerHub, run:\ndocker login --username=\u0026lt;dockerhub-username\u0026gt; --password=\u0026lt;dockerhub-password\u0026gt; \u0026lt;dockerhub-fqdn\u0026gt; Push your image to your image registry.\nIf you use DockerHub, run:\ndocker push \u0026lt;image\u0026gt;:tag ‚ÑπÔ∏è Pipelines require a unique tag to ensure the appropriate image is pulled. If a floating tag, such as latest, is used, the Kubernetes cluster may become out of sync with the Docker registry, concluding it already has the latest image.\nStep 4: Create/Edit the Pipeline Config # Pachyderm\u0026rsquo;s pipeline specification files store the configuration information about the Docker image and code that Pachyderm should run, the input repo(s) of the pipeline, parallelism settings, GPU usage etc\u0026hellip; Pipeline specifications are stored in JSON or YAML format.\nA standard pipeline specification must include the following parameters:\nname transform input ‚ÑπÔ∏è Some special types of pipelines, such as a spout pipeline, do not require you to specify all of these parameters. Spout pipelines, for example, do not have input repos.\nCheck our reference pipeline specification page, for a list of all available fields in a pipeline specification file.\nYou can store your pipeline specifications locally or in a remote location, such as a GitHub repository.\nA simple pipeline specification file in JSON would look like the example below. The pipeline takes its data from the input repo data, runs worker containers with the defined image \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; and command, then outputs the resulting processed data in the my-pipeline output repo. During a job execution, each worker sees and reads from the local file system /pfs/data containing only matched data from the glob expression, and writes its output to /pfs/out with standard file system functions; Pachyderm handles the rest.\n# my-pipeline.json { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-pipeline\u0026#34; }, \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;\u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt;\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;command\u0026#34;, \u0026#34;/pfs/data\u0026#34;, \u0026#34;/pfs/out\u0026#34;] }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } } } Step 5: Deploy/Update the Pipeline # As soon as you create a pipeline, Pachyderm spins up one or more Kubernetes pods in which the pipeline code runs. By default, after the pipeline finishes running, the pods continue to run while waiting for the new data to be committed into the Pachyderm input repository. You can configure this parameter, as well as many others, in the pipeline specification.\nCreate a Pachyderm pipeline from the spec:\npachctl create pipeline -f my-pipeline.json You can specify a local file or a file stored in a remote location, such as a GitHub repository. For example, https://raw.githubusercontent.com/pachyderm/pachyderm/2.6.x/examples/opencv/edges.json.\nIf your pipeline specification changes, you can update the pipeline by running\npachctl update pipeline -f my-pipeline.json ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines"],
    "id": "f8ca7229c8c0803757afa3dad597e19a"
  },
  {
    "title": "Diagrams",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Learn",
    "description": "View diagrams to learn about architecture and workflows.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/diagrams/",
    "relURI": "/latest/learn/diagrams/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "aeca865387492859715a7c3594a6152a"
  },
  {
    "title": "High-Level Architecture Diagram",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Diagrams",
    "description": "View a high-level architecture diagram of the product..",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/diagrams/architecture/",
    "relURI": "/latest/learn/diagrams/architecture/",
    "body": " ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f906a14a344d6a12e59ba050f47612ab"
  },
  {
    "title": "Glossary",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Learn",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/",
    "relURI": "/latest/learn/glossary/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9cb763a828820e85df5e862196b3e628"
  },
  {
    "title": "Ancestry Syntax",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of Ancestry Syntax, which is used to reference the history of commits and branches in a repository.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/ancestry-syntax/",
    "relURI": "/latest/learn/glossary/ancestry-syntax/",
    "body": " About # Ancestry syntax in Pachyderm is used to reference the history of commits and branches in a Pachyderm input repository. Ancestry syntax is similar to Git syntax, and it allows users to navigate the history of commits and branches using special characters like ^ and ..\nThe ^ character is used to reference a commit or branch parent, where commit^ refers to the parent of the commit, and branch^\u0026quot; refers to the parent of the branch head. Multiple ^ characters can be used to reference earlier ancestors, for example, commit^^ refers to the grandparent of the commit.\nThe . character is used to reference a specific commit in the history of a branch. For example, branch.1 refers to the first commit on the branch, branch.2 refers to the second commit, and so on.\nAncestry syntax allows users to access historical versions of data stored in Pachyderm, which can be useful for tasks like debugging, testing, and auditing. However, it\u0026rsquo;s important to note that resolving ancestry syntax can be computationally intensive, especially for long chains of commits, so it\u0026rsquo;s best to use this feature judiciously.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b7c8dce2cee0037d63d61f8f7c3dd99a"
  },
  {
    "title": "Branch",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a branch, which is a pointer to a commit that moves along with new commits as they are submitted.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/branch/",
    "relURI": "/latest/learn/glossary/branch/",
    "body": " About # A Pachyderm branch is a pointer to a commit that moves along with new commits. By default, Pachyderm does not create any branches when you create a repository. Most users create a master branch to initiate the first commit.\nBranches allow collaboration between teams of data scientists. However, the master branch is sufficient for most users.\nEach branch stores information about provenance, including input and output branches. Pachyderm pipelines trigger a job when changes are detected in the HEAD of a branch.\nYou can create additional branches with pachctl create branch and view branches with pachctl list branch. Deleting a branch doesn\u0026rsquo;t delete its commits, and all branches require a head commit.\nExample # pachctl list branch images # BRANCH HEAD # master c32879ae0e6f4b629a43429b7ec10ccc ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["concepts", "pachctl", "data-operations"],
    "id": "4ed02e97407f4a5aea6db2c7342977dc"
  },
  {
    "title": "Commit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a commit, which is an atomic operation that snapshots and preserves the state of files/directories within a repository.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/commit/",
    "relURI": "/latest/learn/glossary/commit/",
    "body": " About # In Pachyderm, commits snapshot and preserve the state of files and directories in a repository at a point in time. Unlike Git, Pachyderm commits are centralized and transactional. You can create a commit with pachctl start commit and save it with pachctl finish commit. Once the commit is closed its contents are immutable. Commits may be chained together to represent a sequence of states.\nAll commits have an alphanumeric ID, and you can reference a commit with \u0026lt;repo\u0026gt;@\u0026lt;commitID\u0026gt;. Each commit has an origin that indicates why it was produced (USER or AUTO).\nGlobal Commits # A commit with global scope (global commit) represents the set of all provenance-dependent commits sharing the same ID.\nSub-Commits # A commit with a more focused scope (sub-commit) represents the \u0026ldquo;Git-like\u0026rdquo; record of one commit in a single branch of a repository‚Äôs file system.\nActions # List Commits Squash Commit Delete Commit Track Downstream Provenance ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["concepts", "pachctl", "data-operations"],
    "id": "f756a62f86d8c042a7ad2db5a7ea8536"
  },
  {
    "title": "Commit Set",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a commit set, which is an immutable set of all the commits that resulted from a modification to the system.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/commit-set/",
    "relURI": "/latest/learn/glossary/commit-set/",
    "body": " About # A Commit Set is an immutable set of all the commits that resulted from a modification to the system. The commits within a commit set share a name (i.e. Global ID). This naming scheme enables you to reference data related to a commit anywhere in their provenance graph by simply naming it.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["concepts", "pachctl", "data-operations"],
    "id": "1547a4ac132021229fe7268de174ba81"
  },
  {
    "title": "DAG",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about DAGs, the Directed Acyclic Graphs that define the order in which pipelines are executed and how data flows between them.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/dag/",
    "relURI": "/latest/learn/glossary/dag/",
    "body": " About # In Pachyderm, a Directed Acyclic Graph (DAG) is a collection of pipelines connected by data dependencies. The DAG defines the order in which pipelines are executed and how data flows between them.\nEach pipeline in a DAG processes data from its input repositories and produces output data that can be used as input by downstream pipelines. The input repositories of a pipeline can be the output repositories of other pipelines, allowing data to flow through the DAG.\nTo create a DAG in Pachyderm, you create multiple pipeline specifications and define the dependencies between them. You can define dependencies between pipelines using the input parameter in the pipeline specification. For example, if you have two pipelines named A and B, and B depends on the output of A, you would set the input parameter of B to the name of the output repository of A.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "4334b267b45042fb9a5f6e390f454dd5"
  },
  {
    "title": "Data Parallelism",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of data parallelism.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/data-parallelism/",
    "relURI": "/latest/learn/glossary/data-parallelism/",
    "body": " About # Data parallelism refers to a parallel computing technique where a large dataset is partitioned and processed in parallel across multiple computing resources within a directed acyclic graph (DAG) or pipeline. In data parallelism, each task in the DAG/pipeline operates on a different subset of the dataset in parallel, allowing for efficient processing of large amounts of data. The results of each task are then combined to produce the final output. Data parallelism is often used in machine learning and deep learning pipelines where large datasets need to be processed in parallel using multiple computing resources. By distributing the data across different nodes, data parallelism can help reduce the overall processing time and improve the performance of the pipeline.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["concepts", "parallelism"],
    "id": "35bcaf30aa8f1a56d3ccfab0c0c62379"
  },
  {
    "title": "Datum",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about datums, the smallest indivisible unit of computation within a job.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/datum/",
    "relURI": "/latest/learn/glossary/datum/",
    "body": " About # A datum is the smallest indivisible unit of computation within a job. Datums are used to:\nDivide your input data Distribute processing workloads A datum\u0026rsquo;s scope can be as large as all of your data at once, a directory, a file, or a combination of multiple inputs. The shape and quantity of your datums is determined by a glob pattern defined in your pipeline specification.\nA job can have one, many, or no datums. Each datum is processed independently with a single execution of the user code on one of the pipeline worker pods. The individual output files produced by all of your datums are combined to create the final output commit.\nIf a job is successfully executed but has no matching files to transform, it is considered a zero-datum job.\nDatum Processing States # When a pipeline runs, it processes your datums. Some of them get processed successfully and some might be skipped or even fail. Generally, processed datums fall into either successful or failure state category.\nSuccessful States # State Description Success The datum has been successfully processed in this job. Skipped The datum has been successfully processed in a previous job, has not changed since then, and therefore, it was skipped in the current job. Failure States # State Description Failed The datum failed to be processed. Any failed datum in a job fails the whole job. Recovered The datum failed, but was recovered by the user\u0026rsquo;s error handling code. Although the datum is marked as recovered, Pachyderm does not process it in the downstream pipelines. A recovered datum does not fail the whole job. Just like failed datums, recovered datums are retried on the next run of the pipeline. You can view the information about datum processing states in the output of the pachctl list job \u0026lt;jobID\u0026gt; command:\n‚ÑπÔ∏è Datums that failed are still included in the total, but not shown in the progress indicator.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "bd6fcaef603671919b8d57b6b7c72dc1"
  },
  {
    "title": "Deferred Processing",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of deferred processing, which allows you to commit data more frequently than you process it.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/deferred-processing/",
    "relURI": "/latest/learn/glossary/deferred-processing/",
    "body": " About # Deferred Processing is a technique that allows you to commit data more frequently than you process it. By default, Pachyderm automatically processes any newly committed data added to its input branch. For example, you may want to commit data hourly, but retrain your ML model daily.\nActions: # Defer processing via a staging branch Process specific commits Set branch triggers Set a custom output branch ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["data-operations", "datums", "branch triggers", "trigger"],
    "id": "ad3de9874edff14abb08139c76e80aa5"
  },
  {
    "title": "Distributed Computing",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of distributed computing, which allows you to split your jobs across multiple workers.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/distributed-computing/",
    "relURI": "/latest/learn/glossary/distributed-computing/",
    "body": " About # Distributed computing is a technique that allows you to split your jobs across multiple Pachyderm workers via the Parallelism PPS attribute. Leveraging distributed computing enables you to build production-scale pipelines with adjustable resources to optimize throughput.\nFor each job, all the datums are queued up and then distributed across the available workers. When a worker finishes processing its datum, it grabs a new datum from the queue until all the datums complete processing. If a worker pod crashes, its datums are redistributed to other workers for maximum fault tolerance.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["data-operations", "pipelines"],
    "id": "79b655b3b884c4ede9260365f21cc47f"
  },
  {
    "title": "File",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a file, which is a Unix filesystem object that stores data.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/file/",
    "relURI": "/latest/learn/glossary/file/",
    "body": " About # A file is a Unix filesystem object, which is a directory or file, that stores data. Unlike source code version-control systems that are most suitable for storing plain text files, you can store any type of file in Pachyderm, including binary files.\nOften, data scientists operate with comma-separated values (CSV), JavaScript Object Notation (JSON), images, and other plain text and binary file formats. Pachyderm supports all file sizes and formats and applies storage optimization techniques, such as deduplication, in the background.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["concepts", "pachctl", "data-operations"],
    "id": "4d0aaef70efcce491779da99cf0ddfe9"
  },
  {
    "title": "Glob Pattern",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a glob pattern, which is a string of characters that specifies a set of filenames or paths in a file system.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/glob-pattern/",
    "relURI": "/latest/learn/glossary/glob-pattern/",
    "body": " About # A glob pattern is a string of characters that specifies a set of filenames or paths in a file system. The term \u0026ldquo;glob\u0026rdquo; is short for \u0026ldquo;global,\u0026rdquo; and refers to the fact that a glob pattern can match multiple filenames or paths at once. For Pachyderm, you can use glob patterns to define the shape of your datums against your inputs, which are spread across Pachyderm workers for distributing computing.\nExamples # Glob Pattern Datum created / Pachyderm denotes the whole repository as a single datum and sends all input data to a single worker node to be processed together. /* Pachyderm defines each top-level files / directories in the input repo, as a separate datum. For example, if you have a repository with ten files and no directory structure, Pachyderm identifies each file as a single datum and processes them independently. /*/* Pachyderm processes each file / directory in each subdirectories as a separate datum. /** Pachyderm processes each file in all directories and subdirectories as a separate datum. Glob patterns can also use other special characters, such as the question mark (?) to match a single character, or brackets ([...]) to match a set of characters.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["concepts", "pachctl", "datums", "pipelines", "data-operations"],
    "id": "52e71eea4c735902d712227ab90f4d06"
  },
  {
    "title": "Global Identifier",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a global identifier, which is a unique identifier for a DAG.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/globalid/",
    "relURI": "/latest/learn/glossary/globalid/",
    "body": " About # Global Identifiers provide a simple way to follow the provenance of a DAG. Commits and jobs sharing the same Global ID represent a logically-related set of objects.\nWhen a new commit is made, Pachyderm creates an associated commit ID; all resulting downstream commits and jobs in your DAG will then share that same ID (the Global Identifier).\nThe following diagram illustrates the global commit and its various components: Actions # List all global commits \u0026amp; jobs List all sub-commits associated with a global ID Track provenance downstream, live Delete a Branch Head ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["data-operations", "pipelines"],
    "id": "96d1d7ae145c981764d357e8b6dc061b"
  },
  {
    "title": "History",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of history (version control), which is a record of the changes made to data over time.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/history/",
    "relURI": "/latest/learn/glossary/history/",
    "body": " About # History in Pachyderm is a record of the changes made to data over time, stored as a series of immutable snapshots (commits) that can be accessed using ancestry syntax and branch pointers. Each commit has a parentage structure, where new commits inherit content from their parents, creating a chain of commits that represents the full history of changes to the data.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "6b9fed4e22d89b300c768a1b43dbb257"
  },
  {
    "title": "Input Repository",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of an input repository, which is a location where data resides that is used as input for a pipeline.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/input-repo/",
    "relURI": "/latest/learn/glossary/input-repo/",
    "body": " About # In Pachyderm, an input repository is a location where data resides that is used as input for a Pachyderm pipeline. To define an input repository, you need to fill out the input attribute in pipeline\u0026rsquo;s specification file.\nThere are several ways to structure the content of your input repos, such as:\nCross Group PFS Join Once you have defined an input repository, you can use it as the input source for a Pachyderm pipeline. The pipeline will automatically subscribe to the branch of the input repository and process any new data that is added to the branch according to the pipeline configuration.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "bad5b841213d357f531a38fb0eb29048"
  },
  {
    "title": "Job",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a Job, which is a unit of work that is created by a pipeline.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/job/",
    "relURI": "/latest/learn/glossary/job/",
    "body": " About # A job is an execution of a pipeline triggered by new data detected in an input repository.\nWhen a commit is made to the input repository of a pipeline, jobs are created for all downstream pipelines in a directed acyclic graph (DAG), but they do not run until the prior pipelines they depend on produce their output. Each job runs the user\u0026rsquo;s code against the current commit in a repository at a specified branch and then submits the results to the output repository of the pipeline as a single output commit.\nEach job has a unique alphanumeric identifier (ID) that users can reference in the \u0026lt;pipeline\u0026gt;@\u0026lt;jobID\u0026gt; format. Jobs have the following states:\nSate Description CREATED An input commit exists, but the job has not been started by a worker yet. STARTING The worker has allocated resources for the job (that is, the job counts towards parallelism), but it is still waiting on the inputs to be ready. UNRUNNABLE The job could not be run, because one or more of its inputs is the result of a failed or unrunnable job. As a simple example, say that pipelines Y and Z both depend on the output from pipeline X. If pipeline X fails, both pipeline Y and Z will pass from STARTING to UNRUNNABLE to signify that they had to be cancelled because of upstream failures. RUNNING The worker is processing datums. EGRESS The worker has completed all the datums and is uploading the output to the egress endpoint. FINISHING After all of the datum processing and egress (if any) is done, the job transitions to a finishing state where all of the post-processing tasks such as compaction are performed. FAILURE The worker encountered too many errors when processing a datum. KILLED The job timed out, or a user called StopJob SUCCESS None of the bad stuff happened. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["concepts", "pachctl", "data-operations", "pipelines"],
    "id": "6b09bd4c4235f71e1c021582c19cc072"
  },
  {
    "title": "NLP",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of NLP, which is a subfield of machine learning that focuses on teaching machines to understand and generate human language.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/nlp/",
    "relURI": "/latest/learn/glossary/nlp/",
    "body": "NLP (Natural Language Processing) is a subfield of machine learning that focuses on teaching machines to understand and generate human language. It involves developing algorithms and models that can process, analyze, and generate natural language data, such as text, speech, and other forms of communication.\nNLP has numerous applications in various industries, such as chatbots, voice assistants, machine translation, sentiment analysis, and text classification, among others. Some common techniques used in NLP include text preprocessing, feature extraction, language modeling, sequence-to-sequence models, attention-based models, and transformer-based models like BERT and GPT.\nNLP models and algorithms often require large amounts of labeled data for training, and they can be computationally intensive. However, recent advancements in deep learning have led to significant improvements in NLP models, allowing them to achieve state-of-the-art performance on various natural language tasks.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "53ba5f5ccfa69ab3b5c2af1b5f1e74e8"
  },
  {
    "title": "Output Repository",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of an output repo, which is a repository where the results of a pipeline's processing are stored after being transformed by the provided user code.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/output-repo/",
    "relURI": "/latest/learn/glossary/output-repo/",
    "body": " About # In Pachyderm, an output repo is a repository where the results of a pipeline\u0026rsquo;s processing are stored after being transformed by the provided user code. Every pipeline automatically creates an output repository with the same name as the pipeline.\nWhen a pipeline runs, it creates a new commit in the output repository with the results of the processing. The commit contains a set of files that represent the output of the pipeline. Each commit in the output repository corresponds to a job that was run to generate that output.\nAn output repository can be created or deleted using a pachctl CLI command or the Pachyderm API.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "fb5be24ab78172bedd92706793a70fdb"
  },
  {
    "title": "Pachyderm Worker",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a Pachyderm worker.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/pachyderm-worker/",
    "relURI": "/latest/learn/glossary/pachyderm-worker/",
    "body": " About # Pachyderm workers are kubernetes pods that run the docker image (your user code) specified in the pipeline specification. When you create a pipeline, Pachyderm spins up workers that continuously run in the cluster, waiting for new data to process.\nEach datum goes through the following processing phases inside a Pachyderm worker pod:\nPhase Description Downloading The Pachyderm worker pod downloads the datum contents into Pachyderm. Processing The Pachyderm worker pod runs the contents of the datum against your code. Uploading The Pachyderm worker pod uploads the results of processing into an output repository. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "ea831ec701d968d85a722e0d79cae3f2"
  },
  {
    "title": "Pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a pipeline, which is a primitive responsible for reading data from a specified source, transforming it according to the pipeline specification, and writing the result to an output repo.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/pipeline/",
    "relURI": "/latest/learn/glossary/pipeline/",
    "body": " About # A pipeline is a Pachyderm primitive responsible for reading data from a specified source, such as a Pachyderm repo, transforming it according to the pipeline specification, and writing the result to an output repo.\nPipelines subscribe to a branch in one or more input repositories, and every time the branch has a new commit, the pipeline executes a job that runs user code to completion and writes the results to a commit in the output repository.\nPipelines are defined declaratively using a JSON or YAML file (the pipeline specification), which must include the name, input, and transform parameters at a minimum. Pipelines can be chained together to create a directed acyclic graph (DAG).\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b7b649c7999782fe6d3c572ae334c605"
  },
  {
    "title": "Pipeline Inputs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a pipeline input, which is the source of the data that the pipeline reads and processes.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/pipeline-inputs/",
    "relURI": "/latest/learn/glossary/pipeline-inputs/",
    "body": " About # In Pachyderm, pipeline inputs are defined as the source of the data that the pipeline reads and processes. The input for a pipeline can be a Pachyderm repository (input repo) or an external data source, such as a file in a cloud storage service.\nTo define a pipeline input, you need to specify the source of the data and how the data is organized. This is done in the input section of the pipeline specification file, which is a YAML or JSON file that defines the configuration of the pipeline.\nInput Types # The input section can contain one or more input sources, each specified as a separate block.\nPFS Cron Egress (DB) Egress (Storage) Service Spout S3 ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f9319cd5e230648fe8adca9f5ca56760"
  },
  {
    "title": "Pipeline Specification",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of a pipeline specification, which is a declarative configuration file used to define the behavior of a pipeline.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/pipeline-specification/",
    "relURI": "/latest/learn/glossary/pipeline-specification/",
    "body": " About # A pipeline specification is a declarative configuration file used to define the behavior of a Pachyderm pipeline. It is typically written in YAML or JSON format and contains information about the pipeline\u0026rsquo;s input sources, output destinations, Docker image (user code), command, and other metadata.\nIn addition to simply transforming your data, you can also achieve more advanced techniques though the pipeline specification, such as:\nDeferred professing Distributed computing ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "94c3b7d2c086dad50ba4a1e35b3ae5ec"
  },
  {
    "title": "Provenance",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of provenance, which is the recorded data lineage that tracks the dependencies and relationships between datasets.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/provenance/",
    "relURI": "/latest/learn/glossary/provenance/",
    "body": " About # Provenance in Pachyderm refers to the tracking of the dependencies and relationships between datasets, as well as the ability to go back in time and see the state of a dataset or repository at a particular moment. Pachyderm models both commit provenance and branch provenance to represent the dependencies between data in the pipeline.\nCommit Provenance # Commit provenance refers to the relationship between commits in different repositories. If a commit in a repository is derived from a commit in another repository, the derived commit is provenant on the source commit. Capturing this relationship supports queries regarding how data in a commit was derived.\nBranch Provenance # Branch provenance represents a more general relationship between data. It asserts that future commits in the downstream branch will be derived from the head commit of the upstream branch.\nTraversing Provenance # Pachyderm automatically maintains a complete audit trail, allowing all results to be fully reproducible. To track the direct provenance of commits and learn where the data in the repository originates, you can use the pachctl inspect command to view provenance information, including the origin kind, direct provenance, and size of the data.\nPachyderm\u0026rsquo;s DAG structure makes it easy to traverse the provenance and subvenance in any commit. All related steps in a DAG share the same global identifier, making it possible to run pachctl list commit \u0026lt;commitID\u0026gt; to get the full list of all the branches with commits created due to provenance relationships.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["concepts", "pachctl", "data-operations"],
    "id": "73874675e81c52f17e9f66003f2578c3"
  },
  {
    "title": "Task Parallelism",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of task parallelism.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/task-parallelism/",
    "relURI": "/latest/learn/glossary/task-parallelism/",
    "body": " About # Task parallelism refers to a parallel computing technique where multiple tasks within a directed acyclic graph (DAG) or pipeline are executed simultaneously on different computing resources. In task parallelism, the focus is on executing different tasks in parallel rather than parallelizing a single task. This means that each task in the DAG/pipeline is executed independently of other tasks, allowing for efficient use of resources and faster completion of the overall DAG/pipeline. Task parallelism is often used in data processing pipelines or workflows where tasks can be executed in parallel without any dependency on each other.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["concepts", "parallelism"],
    "id": "c6445c7c4185ec6b2d65e0bde6344873"
  },
  {
    "title": "User Code",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Glossary",
    "description": "Learn about the concept of User Code, which is custom code that users write to process their data in pipelines.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/learn/glossary/user-code/",
    "relURI": "/latest/learn/glossary/user-code/",
    "body": " About # In Pachyderm, user code refers to the custom code that users write to process their data in pipelines. User code can be written in any language and can use any libraries or frameworks.\nPachyderm allows users to define their user code as a Docker image, which can be pushed to a registry and referenced using the Transform attribute of the pipeline\u0026rsquo;s specification. The user code image contains the necessary dependencies and configuration for the code to run in Pachyderm\u0026rsquo;s distributed computing environment.\nUser code can be defined for each pipeline stage in Pachyderm, allowing users to chain together multiple processing steps and build complex data pipelines. Pachyderm also provides a Python library for building pipelines, which simplifies the process of defining user code and specifying pipeline stages.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "71605dff83a19d917130495bdd554348"
  },
  {
    "title": "Set Up",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Set up locally, in the cloud, or on-premises.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/",
    "relURI": "/latest/set-up/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a8e1f5d22c5a60458f74ded48660be1f"
  },
  {
    "title": "Cloud Deploy",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Set Up",
    "description": "Learn how to deploy using your preferred cloud provider.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/cloud-deploy/",
    "relURI": "/latest/set-up/cloud-deploy/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["aws", "azure", "gcp", "cloud-deploy"],
    "id": "b035fa07efcc627cf6181c2b6ce9b9fb"
  },
  {
    "title": "AWS Deployment",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Cloud Deploy",
    "description": "Learn how to deploy to the cloud with AWS.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/cloud-deploy/aws/",
    "relURI": "/latest/set-up/cloud-deploy/aws/",
    "body": " Before You Start # This guide assumes that you have already tried Pachyderm locally and have all of the following installed:\nKubectl Pachctl Helm AWS CLI Eksctl 1. Create an EKS Cluster # Use the eksctl tool to deploy an EKS Cluster: eksctl create cluster --name pachyderm-cluster --region \u0026lt;region\u0026gt; -profile \u0026lt;your named profile\u0026gt; Verify deployment: kubectl get all 2. Create an S3 Bucket # Run the following command: aws s3api create-bucket --bucket ${BUCKET_NAME} --region ${AWS_REGION} Verify. aws s3 ls 3. Enable Persistent Volumes Creation # Create an IAM OIDC provider for your cluster. Install the Amazon EBS Container Storage Interface (CSI) driver on your cluster. Create a gp3 storage class manifest file (e.g., gp3-storageclass.yaml) kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp3 annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; provisioner: kubernetes.io/aws-ebs parameters: type: gp3 fsType: ext4 Set gp3 to your default storage class. kubectl apply -f gp3-storageclass.yaml Verify that it has been set as your default. kubectl get storageclass 4. Set up an RDS PostgreSQL Instance # By default, Pachyderm runs with a bundled version of PostgreSQL. For production environments, it is strongly recommended that you disable the bundled version and use an RDS PostgreSQL instance.\n‚ö†Ô∏è Aurora Serverless PostgreSQL is not supported.\nIn the RDS console, create a database in the region matching your Pachyderm cluster. Choose the PostgreSQL engine. Select a PostgreSQL version \u0026gt;= 13.3. Configure your DB instance as follows: SETTING Recommended value DB instance identifier Fill in with a unique name across all of your DB instances in the current region. Master username Choose your Admin username. Master password Choose your Admin password. DB instance class The standard default should work. You can change the instance type later on to optimize your performances and costs. Storage type and Allocated storage If you select io1, keep the 100 GiB default size. Read more information on Storage for RDS on Amazon\u0026rsquo;s website. Storage autoscaling If your workload is cyclical or unpredictable, enable storage autoscaling to allow RDS to scale up your storage when needed. Standby instance We highly recommend creating a standby instance for production environments. VPC Select the VPC of your Kubernetes cluster. Attention: After a database is created, you can\u0026rsquo;t change its VPC. Read more on VPCs and RDS on Amazon documentation. Subnet group Pick a Subnet group or Create a new one. Read more about DB Subnet Groups on Amazon documentation. Public access Set the Public access to No for production environments. VPC security group Create a new VPC security group and open the postgreSQL port or use an existing one. Password authentication or Password and IAM database authentication Choose one or the other. Database name In the Database options section, enter Pachyderm\u0026rsquo;s Database name (We are using Pachydermin this example.) and click Create database to create your PostgreSQL service. Your instance is running. Warning: If you do not specify a database name, Amazon RDS does not create a database. üìñ Standalone Clusters\nIf you are deploying a standalone cluster, you must create a second database named dex in your RDS instance for Pachyderm\u0026rsquo;s authentication service. Read more about dex on PostgreSQL in Dex\u0026rsquo;s documentation.\nMulti-cluster setups use Enterprise Server to handle authentication, so you do not need to create a dex database.\nCreate a new user account and grant it full CRUD permissions to both Pachydermand (when applicable) dex databases. Read about managing PostgreSQL users and roles in this blog. Pachyderm will use the same username to connect to Pachydermas well as to dex. 5. Create a Values.yaml # Version: Community Edition Enterprise global: postgresql: postgresqlAuthType: \u0026#34;scram-sha-256\u0026#34; # use \u0026#34;md5\u0026#34; if using postgresql \u0026lt; 14 postgresqlUsername: \u0026#34;username\u0026#34; postgresqlPassword: \u0026#34;password\u0026#34; # The name of the database should be Pachyderm\u0026#39;s (\u0026#34;pachyderm\u0026#34; in the example above), not \u0026#34;dex\u0026#34; # See also # postgresqlExistingSecretName: \u0026#34;\u0026lt;yoursecretname\u0026gt;\u0026#34; postgresqlDatabase: \u0026#34;databasename\u0026#34; # The postgresql database host to connect to. Defaults to postgres service in subchart postgresqlHost: \u0026#34;RDS CNAME\u0026#34; # The postgresql database port to connect to. Defaults to postgres server in subchart postgresqlPort: \u0026#34;5432\u0026#34; postgresql: # turns off the install of the bundled postgres. # If not using the built in Postgres, you must specify a Postgresql # database server to connect to in global.postgresql enabled: false deployTarget: \u0026#34;AMAZON\u0026#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: amazon: bucket: \u0026#34;bucket_name\u0026#34; # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) id: \u0026#34;AKIAIOSFODNN7EXAMPLE\u0026#34; # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) secret: \u0026#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\u0026#34; region: \u0026#34;us-east-2\u0026#34; externalService: enabled: true console: enabled: true global: postgresql: postgresqlAuthType: \u0026#34;scram-sha-256\u0026#34; # use \u0026#34;md5\u0026#34; if using postgresql \u0026lt; 14 postgresqlUsername: \u0026#34;username\u0026#34; postgresqlPassword: \u0026#34;password\u0026#34; # The name of the database should be Pachyderm\u0026#39;s (\u0026#34;pachyderm\u0026#34; in the example above), not \u0026#34;dex\u0026#34; # See also # postgresqlExistingSecretName: \u0026#34;\u0026lt;yoursecretname\u0026gt;\u0026#34; postgresqlDatabase: \u0026#34;databasename\u0026#34; # The postgresql database host to connect to. Defaults to postgres service in subchart postgresqlHost: \u0026#34;RDS CNAME\u0026#34; # The postgresql database port to connect to. Defaults to postgres server in subchart postgresqlPort: \u0026#34;5432\u0026#34; postgresql: # turns off the install of the bundled postgres. # If not using the built in Postgres, you must specify a Postgresql # database server to connect to in global.postgresql enabled: false deployTarget: \u0026#34;AMAZON\u0026#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: amazon: bucket: \u0026#34;bucket_name\u0026#34; # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) id: \u0026#34;AKIAIOSFODNN7EXAMPLE\u0026#34; # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) secret: \u0026#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\u0026#34; region: \u0026#34;us-east-2\u0026#34; # Enterprise key enterpriseLicenseKey: \u0026#34;YOUR_ENTERPRISE_TOKEN\u0026#34; console: enabled: true 6. Configure Helm # Run the following to add the Pachyderm repo to Helm:\nhelm repo add pachyderm https://helm.pachyderm.com helm repo update helm install pachyderm pachyderm/pachyderm -f my_pachyderm_values.yaml 7. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 8. Connect to Cluster # You\u0026rsquo;ll need your organization\u0026rsquo;s cluster URL (proxy.host) value to connect.\nRun the following command to get your cluster URL: kubectl get services | grep pachyderm-proxy | awk \u0026#39;{print $4}\u0026#39; Connect to your cluster:\nMethod: HTTP HTTPS (TLS) pachctl connect http://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; pachctl connect https://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; ‚ÑπÔ∏è If the connection commands did not work together, run each separately.\nOptionally open your browser and navigate to the Console UI.\nüí° You can check your Pachyderm version and connection to pachd at any time with the following command:\npachctl version COMPONENT VERSION pachctl 2.6.1 pachd 2.6.1 ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["aws", "cloud-deploy"],
    "id": "3fcb8d7339dafe916b49b116e2814dfd"
  },
  {
    "title": "Azure Deployment",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Cloud Deploy",
    "description": "Learn how to deploy to the cloud with Azure.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/cloud-deploy/azure/",
    "relURI": "/latest/set-up/cloud-deploy/azure/",
    "body": " Before You Start # This guide assumes that you have already tried Pachyderm locally and have all of the following installed:\nKubectl Pachctl Helm Azure CLI. 1. Create an AKS Cluster # You can deploy Kubernetes on Azure by following the official Azure Kubernetes Service documentation, use the quickstart walkthrough, or follow the steps in this section.\nAt a minimum, you will need to specify the parameters below:\nVariable Description RESOURCE_GROUP A unique name for the resource group where Pachyderm is deployed. For example, pach-resource-group. LOCATION An Azure availability zone where AKS is available. For example, centralus. NODE_SIZE The size of the Kubernetes virtual machine (VM) instances. To avoid performance issues, Pachyderm recommends that you set this value to at least Standard_DS4_v2 which gives you 8 CPUs, 28 Gib of Memory, 56 Gib SSD.\nIn any case, use VMs that support premium storage. See Azure VM sizes for details around which sizes support Premium storage. CLUSTER_NAME A unique name for the Pachyderm cluster. For example, pach-aks-cluster. You can choose to follow the guided steps in Azure Service Portal\u0026rsquo;s Kubernetes Services or use Azure CLI.\nLog in to Azure:\naz login This command opens a browser window. Log in with your Azure credentials. Resources can now be provisioned on the Azure subscription linked to your account.\nCreate an Azure resource group or retrieve an existing group.\naz group create --name ${RESOURCE_GROUP} --location ${LOCATION} Example:\naz group create --name test-group --location centralus System Response:\n{ \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/6c9f2e1e-0eba-4421-b4cc-172f959ee110/resourceGroups/pach-resource-group\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;centralus\u0026#34;, \u0026#34;managedBy\u0026#34;: null, \u0026#34;name\u0026#34;: \u0026#34;pach-resource-group\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;provisioningState\u0026#34;: \u0026#34;Succeeded\u0026#34; }, \u0026#34;tags\u0026#34;: null, \u0026#34;type\u0026#34;: null } Create an AKS cluster in the resource group/location:\nFor more configuration options: Find the list of all available flags of the az aks create command.\naz aks create --resource-group ${RESOURCE_GROUP} --name ${CLUSTER_NAME} --node-vm-size ${NODE_SIZE} --node-count \u0026lt;node_pool_count\u0026gt; --location ${LOCATION} Example:\naz aks create --resource-group test-group --name test-cluster --generate-ssh-keys --node-vm-size Standard_DS4_v2 --location centralus Confirm the version of the Kubernetes server by running kubectl version.\n‚ÑπÔ∏è \u0026ldquo;See Also:\u0026rdquo; - Azure Virtual Machine sizes\nOnce your Kubernetes cluster is up, and your infrastructure configured, you are ready to prepare for the installation of Pachyderm. Some of the steps below will require you to keep updating the values.yaml started during the setup of the recommended infrastructure:\n2. Create a Storage Container # Pachyderm needs an Azure Storage Container (Object store) to store your data.\nTo access your data, Pachyderm uses a Storage Account with permissioned access to your desired container. You can either use an existing account or create a new one in your default subscription, then use the JSON key associated with the account and pass it on to Pachyderm.\nSet up the following variables:\nSTORAGE_ACCOUNT: The name of the storage account where you store your data. CONTAINER_NAME: The name of the Azure blob container where you store your data. Create an Azure storage account:\naz storage account create \\ --resource-group=\u0026#34;${RESOURCE_GROUP}\u0026#34; \\ --location=\u0026#34;${LOCATION}\u0026#34; \\ --sku=Premium_LRS \\ --name=\u0026#34;${STORAGE_ACCOUNT}\u0026#34; \\ --kind=BlockBlobStorage System response:\n{ \u0026#34;accessTier\u0026#34;: null, \u0026#34;creationTime\u0026#34;: \u0026#34;2019-06-20T16:05:55.616832+00:00\u0026#34;, \u0026#34;customDomain\u0026#34;: null, \u0026#34;enableAzureFilesAadIntegration\u0026#34;: null, \u0026#34;enableHttpsTrafficOnly\u0026#34;: false, \u0026#34;encryption\u0026#34;: { \u0026#34;keySource\u0026#34;: \u0026#34;Microsoft.Storage\u0026#34;, \u0026#34;keyVaultProperties\u0026#34;: null, \u0026#34;services\u0026#34;: { \u0026#34;blob\u0026#34;: { \u0026#34;enabled\u0026#34;: true, ... Make sure that you set Stock Keeping Unit (SKU) to Premium_LRS and the kind parameter is set to BlockBlobStorage. This configuration results in a storage that uses SSDs rather than standard Hard Disk Drives (HDD). If you set this parameter to an HDD-based storage option, your Pachyderm cluster will be too slow and might malfunction.\nVerify that your storage account has been successfully created:\naz storage account list Obtain the key for the storage account (STORAGE_ACCOUNT) and the resource group to be used to deploy Pachyderm:\nSTORAGE_KEY=\u0026#34;$(az storage account keys list \\ --account-name=\u0026#34;${STORAGE_ACCOUNT}\u0026#34; \\ --resource-group=\u0026#34;${RESOURCE_GROUP}\u0026#34; \\ --output=json \\ | jq \u0026#39;.[0].value\u0026#39; -r )\u0026#34; ‚ÑπÔ∏è Find the generated key in the Storage accounts \u0026gt; Access keys section in the Azure Portal or by running the following command az storage account keys list --account-name=${STORAGE_ACCOUNT}.\nCreate a new storage container within your storage account:\naz storage container create --name ${CONTAINER_NAME} \\ --account-name ${STORAGE_ACCOUNT} \\ --account-key \u0026#34;${STORAGE_KEY}\u0026#34; 3. Create a Values.yaml # Version: Community Edition Enterprise deployTarget: \u0026#34;MICROSOFT\u0026#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: microsoft: # storage container name container: \u0026#34;blah\u0026#34; # storage account name id: \u0026#34;AKIAIOSFODNN7EXAMPLE\u0026#34; # storage account key secret: \u0026#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\u0026#34; externalService: enabled: true console: enabled: true deployTarget: \u0026#34;MICROSOFT\u0026#34; proxy: enabled: true service: type: LoadBalancer host: \u0026lt;insert-external-ip-address-or-dns-name\u0026gt; pachd: storage: microsoft: # storage container name container: \u0026#34;blah\u0026#34; # storage account name id: \u0026#34;AKIAIOSFODNN7EXAMPLE\u0026#34; # storage account key secret: \u0026#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\u0026#34; # Enterprise key enterpriseLicenseKey: \u0026#34;YOUR_ENTERPRISE_TOKEN\u0026#34; console: enabled: true 4. Configure Helm # Run the following to add the Pachyderm repo to Helm:\nhelm repo add pachyderm https://helm.pachyderm.com helm repo update helm install pachyderm pachyderm/pachyderm -f my_pachyderm_values.yaml 5. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 6. Connect to Cluster # You\u0026rsquo;ll need your organization\u0026rsquo;s cluster URL (proxy.host) value to connect.\nRun the following command to get your cluster URL: kubectl get services | grep pachyderm-proxy | awk \u0026#39;{print $4}\u0026#39; Connect to your cluster:\nMethod: HTTP HTTPS (TLS) pachctl connect http://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; pachctl connect https://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["azure", "cloud-deploy"],
    "id": "7d15897c35f27c281e3f4dfef2d93582"
  },
  {
    "title": "GCP Deployment",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Cloud Deploy",
    "description": "Learn how to deploy to the cloud with GCP.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/cloud-deploy/gcp/",
    "relURI": "/latest/set-up/cloud-deploy/gcp/",
    "body": " Before You Start # This guide assumes that:\nYou have already tried Pachyderm locally and have some familiarity with Kubectl, Helm, Google Cloud SDK and jq. You have access to a Google Cloud account linked to an active billing account. 1. Create a New Project # Log in to Google Cloud Console. Create a new project (e.g.,pachyderm-quickstart-project). Enable the Compute Engine API. You are now ready to create a GKE Cluster.\n2. Run Setup Script # You can run this setup script either through the Cloud Shell or in a local terminal via the gcloud cli. Before running the install script change the values PROJECT_ID, to match the project created in step one, and SQL_ADMIN_PASSWORD to a secure value. Running this script creates all of the following:\nOne GKE cluster Workload identity service accounts Permissions A static IP address The cloud SQL instance and databases One cloud storage bucket One file called ${NAME}.values.yaml in the current directory It also installs Pachyderm into the cluster.\n3. Connect to Cluster # You\u0026rsquo;ll need your organization\u0026rsquo;s cluster URL (proxy.host) value to connect.\nRun the following command to get your cluster URL: kubectl get services | grep pachyderm-proxy | awk \u0026#39;{print $4}\u0026#39; Connect to your cluster:\nMethod: HTTP HTTPS (TLS) pachctl connect http://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; pachctl connect https://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; üí° You can also connect to Console via Google\u0026rsquo;s Cloud Shell:\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["gcp", "cloud-deploy"],
    "id": "bded9f53cbb08228d8a1d99143a57fa7"
  },
  {
    "title": "Console Setup",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Cloud Deploy",
    "description": "Learn how to deploy the Console UI from the cloud (AWS, GCP, Azure).",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/cloud-deploy/console/",
    "relURI": "/latest/set-up/cloud-deploy/console/",
    "body": " Before You Start # You must have Pachyderm installed using one of the following guides:\nAWS GCP Azure Deploy # Set up your Proxy and DNS and point your browser to: http://\u0026lt;external-IP-address-or-domain-name\u0026gt;:80 or, https://\u0026lt;external-IP-address-or-domain-name\u0026gt;:443 if TLS is enabled Set up your IDP during deployment. ‚ÑπÔ∏è You can use the mock user (username:admin, password: password) to login to Console when authentication is enabled but no Identity provider was wired (Enterprise).\nConfigure your Identity Provider As Part of Helm: To configure your Identity Provider as a part of helm install, see examples for the oidc.upstreamIDPs value in the helm chart values specification and read our IDP Configuration page for a better understanding of each field. Manually via Values.yaml: You can manually update your values.yaml with oidc.mockIDP = false. Connect. Method: HTTP HTTPS (TLS) pachctl connect http://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; pachctl connect https://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; You are all set! You should land on the Projects page of Console.\nEnterprise + Helm # When Enterprise is enabled through Helm, Auth is automatically activated. This means that you do not need to run pachctl auth activate; a pachyderm-auth Kubernetes secret is created which contains a rootToken key. Use {{\u0026quot;kubectl get secret pachyderm-auth -o go-template='{{.data.rootToken | base64decode }}'\u0026quot;}} to retrieve it and save it where you see fit.\nConsiderations # If you run pachctl auth activate, the secret is not updated. Instead, the rootToken is printed in your STDOUT for you to save; the same behavior applies if you activate enterprise manually (pachctl license activate) and then activate authentication (pachctl auth activate). You can set the helm value pachd.activateAuth to false to prevent the automatic bootstrap of auth on the cluster. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["console", "cloud-deploy"],
    "id": "897a46ff86d8896528daf1e857a8eeff"
  },
  {
    "title": "Set Up AWS Secret Manager",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Cloud Deploy",
    "description": "Learn how to securely manage and centralize your secrets.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/cloud-deploy/aws-secret-manager/",
    "relURI": "/latest/set-up/cloud-deploy/aws-secret-manager/",
    "body": "For production environments, we highly recommend securing and centralizing the storage and management of your secrets (database access, root token, enterprise key, etc\u0026hellip;) in AWS Secrets Manager, then allow your EKS cluster to retrieve those secrets using fine-grained IAM policies.\nThis section will walk you through the steps to enable your EKS cluster to retrieve secrets from AWS Secrets Manager.\nBefore You Start # Make sure you have completed the steps found in the AWS deployment instructions before proceeding.\n1. Install The AWS Secrets and Configuration Provider (ASCP) # To retrieve your secrets through your workloads running on your cluster, you will first need to install:\nA Secrets Store CSI driver AWS Secrets Manager and Config Provider ‚ö†Ô∏è The ASCP works with Amazon Elastic Kubernetes Service (Amazon EKS) 1.17+.\nInstall the Secrets Store CSI Driver # Deploy the Secrets Store CSI driver by following the installation steps.\nüí° Make sure to enable the Sync as Kubernetes Secret feature explicitly by setting the helm parameter syncSecret.enabled to true.\n‚ÑπÔ∏è helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts helm install csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver --namespace kube-system --set syncSecret.enabled=true Install the AWS Provider # AWS provider for the Secrets Store CSI Driver allows you to make secrets stored in Secrets Manager appear as files mounted in Kubernetes pods.\n‚ÑπÔ∏è kubectl apply -f https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml 2. Store Pachyderm\u0026rsquo;s Secrets in Secrets Manager # In your Secret Manager Console, click on Store a new secret, select the Other type of Secret (for generic secrets), provide the following Key/Value pairs, then choose a secret name.\nSecret Key Description Value root_token Root clusterAdmin of your cluster Any postgresql_password Password to your database Any OAUTH_CLIENT_SECRET Oauth client secret for Console Required if you set an Enterprise key Any enterprise_license_key Your enterprise license Your enterprise License key pachd_oauth_client_secret Oauth client secret for pachd Any enterprise_secret Needed if you connect to an enterprise server Any Create your secret, then retrieve its arn. It will be needed in the next phase.\n3. Grant Your EKS Cluster Access To Your Secrets Manager # Your cluster has an OpenID Connect issuer URL associated with it. To use IAM roles for service accounts, an IAM OIDC provider must exist for your cluster.\nCreate an IAM OIDC Provider # Before granting your EKS pods the proper permissions to access your secrets, you need to create an IAM OIDC provider for your cluster or retrieve the arn of your provider if you already have one created.\nFollow the steps in AWS user guide\nExample # eksctl utils associate-iam-oidc-provider --cluster=\u0026#34;\u0026lt;cluster-name\u0026gt;\u0026#34; Create An IAM Policy That Grants Read Access To Your Secret # Create a new Policy from your IAM Console Select the JSON tab. Copy/Paste the following text in the JSON tab { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, ], \u0026#34;Resource\u0026#34;: [ \u0026lt;!-- Copy the arn of your secret HERE - see example below\u0026gt; \u0026#34;arn:aws:secretsmanager:\u0026lt;region\u0026gt;:\u0026lt;account\u0026gt;„äôÔ∏è\u0026lt;your secret name\u0026gt;\u0026#34; ] } ] } This policy limits the access to the secrets that your EKS cluster needs to access.\nAttach Your Policy To An IAM Role and The Role To Your Service Account # Create an IAM role and attach the IAM policy that you specified to it. The role is associated with a Kubernetes service account created in the namespace that you specify (your cluster\u0026rsquo;s) and annotated with eks.amazonaws.com/role-arn:arn:aws:iam::111122223333:role/my-role-name.\nExample # eksctl create iamserviceaccount \\ --name \u0026#34;\u0026lt;my-service-account\u0026gt;\u0026#34; \\ --cluster \u0026#34;\u0026lt;my-cluster\u0026gt;\u0026#34; \\ --attach-policy-arn \\ \u0026#34;\u0026lt;Copy the arn of your policy HERE\u0026gt;\u0026#34; \\ --approve \\ --override-existing-serviceaccounts 4. Mount Your Secrets In Your EKS Cluster # To show secrets in EKS as though they are files on the filesystem, you need to create a SecretProviderClass YAML file that contains information about your secrets as well as information on how to display them in the EKS pod. Use the file provided below and run kubectl apply -f yoursecretclass.yaml.\nThe SecretProviderClass must be in the same namespace as the EKS cluster.\nmetadata: # Insert your secret name name: pach-secrets spec: provider: aws parameters: objects: | - objectName: \u0026#34;pach-secrets\u0026#34; objectType: \u0026#34;secretsmanager\u0026#34; jmesPath: - path: root_token objectAlias: root-token - path: postgresql_password objectAlias: postgresql-password - path: OAUTH_CLIENT_SECRET objectAlias: OAUTH_CLIENT_SECRET - path: enterprise_license_key objectAlias: enterprise-license-key - path: pachd_oauth_client_secret\tobjectAlias: pachd-oauth-client-secret - path: enterprise_secret objectAlias: enterprise-secret secretObjects: - data: - key: root-token objectName: root-token secretName: root-token type: Opaque - data: - key: postgresql-password objectName: postgresql-password secretName: postgresql-password type: Opaque - data: - key: OAUTH_CLIENT_SECRET objectName: OAUTH_CLIENT_SECRET secretName: console-oauth-client-secret type: Opaque - data: - key: enterprise-license-key objectName: enterprise-license-key secretName: enterprise-license-key type: Opaque - data: - key: pachd-oauth-client-secret\tobjectName: pachd-oauth-client-secret\tsecretName: pachd-oauth-client-secret type: Opaque - data: - key: enterprise-secret objectName: enterprise-secret secretName: enterprise-secret type: Opaque 5. Create A Syncer Pod # Once your secret class is configured, a pod needs to request the class to trigger the CSI driver and retrieve the secrets in Kubernetes. Update the file below with your serviceAccountName and secretProviderClass before you run a kubectl apply -f syncerpod.yaml\napiVersion: v1 kind: Pod metadata: name: secret-syncer spec: containers: - name: secret-syncer image: k8s.gcr.io/pause volumeMounts: - name: secrets-store-inline mountPath: \u0026#34;/mnt/secrets-store\u0026#34; readOnly: true terminationGracePeriodSeconds: 3 serviceAccountName: \u0026#34;\u0026lt;Insert your service account name\u0026gt;\u0026#34; volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: # Insert the name of your Secret Provider secretProviderClass: \u0026#34;pach-secrets\u0026#34; Run a quick kubectl get all to check on your new pod.\n6. Update Secrets In Pachyderm Values.YAML # Finally, using the secretName(s) of your SecretProviderClass above, update Pachyderm\u0026rsquo;s values.YAML with the list of secrets you will be needing.\nChoose the ones that apply to your use case.\ndeployTarget: LOCAL global: postgresql: postgresqlExistingSecretName: postgresql-password postgresqlExistingSecretKey: postgresql-password console: enabled: true config: oauthClientSecretSecretName: console-oauth-client-secret pachd: rootTokenSecretName: root-token enterpriseSecretSecretName: enterprise-secret oauthClientSecretSecretName: pachd-oauth-client-secret\tenterpriseLicenseKeySecretName: enterprise-license-key activateEnterprise: true Your Secrets Manager is now configured to provide credential values to your cluster.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["aws", "secrets"],
    "id": "63f7c9a532908c0792559c6fb4395fd3"
  },
  {
    "title": "Pachctl",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn how to install PachCTL.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/pachctl/",
    "relURI": "/latest/set-up/pachctl/",
    "body": " Operating System: MacOs, Windows, \u0026amp; Darwin Debian brew tap pachyderm/tap \u0026amp;\u0026amp; brew install pachyderm/tap/pachctl@2.6 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.6.1/pachctl_2.6.1_amd64.deb \u0026amp;\u0026amp; sudo dpkg -i /tmp/pachctl.deb ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["deployment"],
    "id": "aa262acfc681cc1f1e4437ea727e6b93"
  },
  {
    "title": "Pachctl Auto-completion",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn how to install our auto-completion helper tool (it's great for learning PachCTL commands).",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/pachctl-autocomplete/",
    "relURI": "/latest/set-up/pachctl-autocomplete/",
    "body": "Pachyderm autocompletion allows you to automatically finish partially typed commands by pressing TAB. Autocompletion needs to be installed separately when pachctl is already available on your client machine.\nPachyderm autocompletion is supported for bash and zsh shells. You must have either of them preinstalled before installing Pachyderm autocompletion.\nüí° Type pachctl completion --help to display help information about the command.\nCommand Shell: Zsh Bash Verify that bash-completion is installed on your machine. For example, if you have installed bash completion by using Homebrew, type:\nbrew info bash-completion This command returns information about the directory in which bash-completion and bash completion scripts are installed. For example, /usr/local/etc/bash_completion.d/. You need to specify the path to bash_completion.d as the path to which install pachctl autocompletion. Also, the output of the info command might have a suggestion to include the path to bash-completion into your ~/.bash_profile file.\nInstall pachctl autocompletion:\npachctl completion bash --install --path \u0026lt;path/to/bash-completion\u0026gt; For example, if you specify the path to bash-completion as /usr/local/etc/bash_completion.d/pachctl, your system response looks like this:\nSystem response:\nBash completions installed in /usr/local/etc/bash_completion.d/pachctl, you must restart bash to enable completions. Restart your terminal.\npachctl autocomplete should now be enabled in your system.\nVerify that zsh-completions are installed on your machine. For example, if you have installed zsh completion by using Homebrew, type:\nbrew info zsh-completions You should see the directory in which zsh-completions are installed and instructions to add the correct path in the ~/.zshrc file. Make sure you add the required path. If you do not have the ~/.zshrc file on your computer, create one. For more information about setting up zsh completions, see zsh-completions.\nInstall pachctl autocompletion for zsh:\npachctl completion zsh --install --path \u0026lt;path/to/zsh-completions\u0026gt; Example:\npachctl completion zsh --install --path /usr/local/share/zsh-completions/_pachctl System response:\nCompletions installed in \u0026#34;_pachctl\u0026#34;, you must restart your terminal to enable them. Restart your terminal.\npachctl autocomplete should now be enabled in your system.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["deployment"],
    "id": "8d1212be4f05bf5e44d640aaf9bd7afc"
  },
  {
    "title": "Authentication & IdP Connectors",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Set Up",
    "description": "Learn how to enable users to access a cluster using their preferred identity provider.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/connectors/",
    "relURI": "/latest/set-up/connectors/",
    "body": "MLDM has an embedded Open ID Connect based on Dex, allowing for vendor-neutral authentication using your existing credentials from various back-ends. You can enable users to authenticate to a Pachyderm cluster using their favorite Identity Providers by following the articles in this section.\nBefore You Start # You must be using Enterprise to set up authentication and authorization. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "de5368dbc3d87216edeb8c36527bc41d"
  },
  {
    "title": "Auth0",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Authentication & IdP Connectors",
    "description": "Learn how to authenticate with Auth0.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/connectors/auth0/",
    "relURI": "/latest/set-up/connectors/auth0/",
    "body": " Before You Start # You must have an Enterprise Server set up. This guide assumes you are using the embedded proxy. This guide uses Auth0 as an example; if you do not have an Auth0 account, sign up for one and create your Pool of Users. 1. Register With Your IdP # Log in to your Auth0 account. In Applications, click Create Application. Type the name of your application, such as Pachyderm. In the application type, select Regular Web Application. Click Create. Go to the application settings. Scroll down to Application URIs. In the Allowed Callback URLs, add the Pachyderm callback link in the following format: # Dex\u0026#39;s issuer URL + \u0026#34;/callback\u0026#34; http(s)://\u0026lt;insert-external-ip-or-dns-name\u0026gt;/dex/callback Scroll down to Show Advanced Settings. Select Grant Types. Verify that Authorization Code and Refresh Token are selected. 2. Set Up Connector # Create a JSON or YAML connector config file that matches your IdP. Syntax: json yaml { \u0026#34;type\u0026#34;: \u0026#34;oidc\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;auth0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Auth0\u0026#34;, \u0026#34;version\u0026#34;: 1, \u0026#34;config\u0026#34;:{ \u0026#34;issuer\u0026#34;: \u0026#34;https://dev-k34x5yjn.us.auth0.com/\u0026#34;, \u0026#34;clientID\u0026#34;: \u0026#34;hegmOc5rTotLPu5ByRDXOvBAzgs3wuw5\u0026#34;, \u0026#34;clientSecret\u0026#34;: \u0026#34;7xk8O71Uhp5T-bJp_aP2Squwlh4zZTJs65URPma-2UT7n1iigDaMUD9ArhUR-2aL\u0026#34;, \u0026#34;redirectURI\u0026#34;: \u0026#34;http(s)://\u0026lt;insert-external-ip-or-dns-name\u0026gt;/dex/callback\u0026#34;, \u0026#34;insecureEnableGroups\u0026#34;: true, \u0026#34;insecureSkipEmailVerified\u0026#34;: true, \u0026#34;insecureSkipIssuerCallbackDomainCheck\u0026#34;: false, \u0026#34;forwardedLoginParams\u0026#34;: [\u0026#34;login_hint\u0026#34;] } } type: oidc id: auth0 name: Auth0 version: 1 config: issuer: https://dev-k34x5yjn.us.auth0.com/ clientID: hegmOc5rTotLPu5ByRDXOvBAzgs3wuw5 clientSecret: 7xk8O71Uhp5T-bJp_aP2Squwlh4zZTJs65URPma-2UT7n1iigDaMUD9ArhUR-2aL redirectURI: http(s)://\u0026lt;insert-external-ip-or-dns-name\u0026gt;/dex/callback insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: false, forwardedLoginParams: - login_hint ‚ÑπÔ∏è Note that Pachyderm\u0026rsquo;s YAML format is a simplified version of Dex\u0026rsquo;s sample config.\nUpdate the following attributes: Attribute Description id The unique identifier of your connector (string). name Its full name (string). type The type of connector. (oidc, saml). version The version of your connector (integer - default to 0 when creating a new connector) issuer The domain of your application (here in Auth0). For example, https://dev-k34x5yjn.us.auth0.com/. Note the trailing slash. client_id The Pachyderm Client ID (here in Auth0). The client ID consists of alphanumeric characters and can be found on the application settings page. client_secret The Pachyderm client secret (here in Auth0) located on the application settings page. redirect_uri This parameter should match what you have added to Allowed Callback URLs when registering Pachyderm on your IdP website. Open your Helm values.yml file. Find the oidc.upstreamIDPs section. Input your connector info; Pachyderm stores this value in the platform secret pachyderm-identity in the key upstream-idps. stringData: upstream-idps: | - type: github id: github name: GitHub jsonConfig: \u0026gt;- { \u0026#34;clientID\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;clientSecret\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;redirectURI\u0026#34;: \u0026#34;https://pach.pachdemo.cloud/dex/callback\u0026#34;, \u0026#34;loadAllGroups\u0026#34;: true } Alternatively, you can create a secret containing your dex connectors (Key: upstream-idps) and reference its name in the field oidc.upstreamIDPsSecretName.\n3. Login # The users registered with your IdP are now ready to Log in to Pachyderm\nConsiderations # Ingress # When using an ingress:\nredirect_uri must be changed to point to https://domain-name/dex/callback. (Note the additional /dex/) TLS requires all non-localhost redirectURIs to be HTTPS. AZURE USERS: You must use TLS when deploying on Azure. When using Azure Active Directory, add the following to the oidc config: \u0026#34;config\u0026#34;:{ \u0026#34;claimMapping\u0026#34;: { \u0026#34;email\u0026#34;: \u0026#34;preferred_username\u0026#34; } } ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["identity-providers", "permissions", "management", "integrations"],
    "id": "a6cfda4e2b4f9799c0cc0f1c92ded0e5"
  },
  {
    "title": "Okta",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Authentication & IdP Connectors",
    "description": "Learn how to authenticate with Okta.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/connectors/okta/",
    "relURI": "/latest/set-up/connectors/okta/",
    "body": "If Okta¬Æ access management software is your preferred choice of IdP, you can configure Pachyderm to use Okta as an OpenID Connect (OIDC) identity provider using the following steps.\n‚ÑπÔ∏è Before you can configure Pachyderm to work with Okta, log in or create an account at https://www.okta.com/login/.\nRegister Pachyderm with Okta # For more detailed step by step instructions, follow this documentation.\nSign in to your Okta organization with your administrator account.\nFrom the Admin Console side navigation, click Applications \u0026gt; Applications.\nClick Add Application.\nClick Create New App (or search for your existing app).\nSelect Platform: Web and sign-on method OpenID Connect.\nClick Create.\nType the name of your application, such as Pachyderm.\nAdd the following Login redirect URI.\nhttp://\u0026lt;ip\u0026gt;:30658/callback Note: Your port number should be whatever is routing to the Identity Service:658.\nThe IP address is the address of your Pachyderm host. For example, if you are running Pachyderm in Minikube, you can find the IP address by running minikube ip.\nClick Save\nClick Edit to change the General Settings pane. In the Allowed grant types section, enable Authorization Code and Refresh Token.\nClick Save\nOn the Assignments tab, click Assign to assign the app integration to any user or group in your org. Click Done when the assignments are complete.\nSet up an create an Idp-Pachyderm connector # After you have configured a Pachyderm application in Okta, you need to create an OIDC connector config file with the Okta parameters. All the required parameters, such as client_id, client_secret, and others, are located on the App General tab.\nTo configure Pachyderm Auth, complete the following steps:\nGo to the terminal and forward the pachd pod to the OIDC port:\nGet the pachd pod ID:\nkubectl get pod Example system response:\npachd-79f7f68c65-9qs8g 1/1 Running 0 4h2m ... Forward the pachd pod to the OIDC port:\nExample:\nkubectl port-forward pachd-79f7f68c65-9qs8g 30657 Enable Pachyderm authentication:\npachctl auth activate --initial-admin=robot:admin Pachyderm returns a token.\nWARNING! You must save the token to a secure location to avoid being locked out of your cluster.\nLog in as the admin user with the token you received in the previous step:\npachctl auth use-auth-token Set up the authentication config:\npachctl auth set-config \u0026lt;\u0026lt;EOF { \u0026#34;live_config_version\u0026#34;: 2, \u0026#34;id_providers\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;okta\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;oidc-based authentication with Okta\u0026#34;, \u0026#34;oidc\u0026#34;:{ \u0026#34;issuer\u0026#34;: \u0026#34;https://\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;client_secret\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;redirect_uri\u0026#34;: \u0026#34;your redirect URI\u0026#34;, ignore_email_verified: true } }] } EOF You need to replace the following placeholders with relevant values:\nissuer ‚Äî The domain of your application in Okta. For example, {yourOktaDomain}/. Note the trailing slash.\nclient_id ‚Äî The Pachyderm Client ID in Okta.\nclient_secret - The Pachyderm client secret in Okta.\nredirect_uri - This parameter should match what you have added to redirect URI in the previous step.\nLog in as the user you have created in the Pachyderm application or sign in with Google:\nRun:\npachctl auth login You should be prompted to a web-browser. Log in as the user you have previously created in Okta or sign in with Google.\nYou should see the following message printed out in your browser:\nYou are now logged in. Go back to the terminal to use Pachyderm! In the terminal, check that you are logged in as the Okta user:\npachctl auth whoami Example of System Response:\nYou are \u0026#34;okta:test@pachyderm.com\u0026#34; session expires: 07 Aug 20 14:04 PDT ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["identity-providers", "permissions", "management", "integrations"],
    "id": "1683a7cf49d1bfd290616d206f0a0462"
  },
  {
    "title": "Authorization",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Set Up",
    "description": "Learn how to set up and manage Role-Based Access Control (RBAC).",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/authorization/",
    "relURI": "/latest/set-up/authorization/",
    "body": "You can use Pachyderm\u0026rsquo;s Role-Based Access Control (RBAC) model to configure authorization for your users. Users can be assigned roles that grant certain permissions for interacting with Pachyderm\u0026rsquo;s resources.\nBefore You Start # You must be using Enterprise to set up authentication and authorization. Activate User Access Management # Activate authentication using the following command: pachctl auth activate # Pachyderm root token: # 54778a770c554d0fb84563033c9cb808 Save the root token value in a secure place. You can use this token in the future to log in to the initial root admin user by entering the following comand:\npachctl auth use-auth-token # Please paste your Pachyderm auth token: As a Root User (or initial admin), you can now configure Pachyderm to work with the identity management provider (IdP) of your choice.\nLicense Expiration # When an Enterprise License expires, a Pachyderm cluster with enabled User Access Management goes into an admin-only state. In this state, only ClusterAdmins have access to the data stored in Pachyderm. This safety measure keeps sensitive data protected, even when an enterprise subscription becomes stale. To return the cluster to its previous state, run pachctl license activate and submit your new code.\nUsers Types # Pachyderm has 5 user types:\nUser Type Description clusterAdmin IdP User Any user or group of users authenticated by your Identity Provider to access Pachyderm. Robot User A Service account used for third party applications/systems integrating with Pachyderm APIs/Clients. Pipeline User An internal Service Account used for Pipelines when interacting with Pachyderm resources. All Cluster Users A general subject that represents everyone who has logged in to a cluster. Pachyderm defines 4 prefixes depending on the type of user:\nrobot user group pipeline (as mentioned above, this prefix will not be used in the context of granting privileges to users. However, it does exist. We are listing it here to give an exhauxtive list of all prefixes.) Aditionnally, the \u0026ldquo;everyone\u0026rdquo; user allClusterUsers has no specific prefix. See the example below to learn how to assign repoReader access to allClusterUsers on a repo.\nResource Types # Pachyderm has 3 resource types:\nResource Type Description Cluster A set of nodes for running containerized applications. Containers allow users to run repeatable and standardized code. Project A project is a container of 1 or more DAGs that allows for users to organize their repos. Projects allow multiple teams to work in a cluster. Repo A repository is where data is stored and contains both files and folders. Repos tracks all changes to the data and creates a history of data changes. Role Types # Pachyderm has 3 role types:\nRole Type Description Cluster Roles Granted at the cluster level. Project Roles Granted at the project level. Repo Roles Granted at the repo level or at the cluster level. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "720160f18747e0daa7e5ad86359852d1"
  },
  {
    "title": "Add Roles to User",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Authorization",
    "description": "Learn how to grant and modify permissions on given resources for a user.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/authorization/add-user-roles/",
    "relURI": "/latest/set-up/authorization/add-user-roles/",
    "body": " Before You Start # Review the permissions assigned to each role. Confirm you have the right role(s) to grant a user access to a given resource. This guide assumes resources (projects, repositories) have already been created in your cluster. How to Assign Roles to a User # Open your terminal. Connect as the root user using the following command: pachctl auth use-auth-token Input your root token.\nRun one of the following commands to assign a role:\nResource Type: Project Repo Other All pachctl auth set project \u0026lt;project-name\u0026gt; \u0026lt;role-name\u0026gt; user:\u0026lt;username@email.com\u0026gt; pachctl auth set repo \u0026lt;repo-name\u0026gt; \u0026lt;role-name\u0026gt; user:\u0026lt;username@email.com\u0026gt; pachctl auth set enterprise clusterAdmin user:\u0026lt;email\u0026gt; pachctl auth set \u0026lt;resource\u0026gt; \u0026lt;resource-name\u0026gt; [role1,role2 | none ] \u0026lt;prefix:subject\u0026gt; Confirm access by running the following command: Resource Type: Project Repo pachctl auth get project \u0026lt;project-name\u0026gt; pachctl auth get repo \u0026lt;repo-name\u0026gt; You can also use these steps to update a users permissions.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["permissions", "management"],
    "id": "e720b830c1a8e39a3c5559efc301d534"
  },
  {
    "title": "Add Roles to Group",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Authorization",
    "description": "Learn how to grant and modify permissions on given resources for a group of users.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/authorization/add-group-roles/",
    "relURI": "/latest/set-up/authorization/add-group-roles/",
    "body": " Before You Start # Your IdP must support groups to use these instructions. Review the permissions assigned to each role. This guide assumes resources (projects, repositories) have already been created in your cluster. This guide uses Auth0 as an example IdP. How to Assign Roles to a Group # Enable group management in your IdP of choice . Update your connector config to include the appropriate attributes. Syntax: JSON YAML { \u0026#34;type\u0026#34;: \u0026#34;oidc\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;auth0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Auth0\u0026#34;, \u0026#34;version\u0026#34;: 1, \u0026#34;config\u0026#34;:{ \u0026#34;issuer\u0026#34;: \u0026#34;https://dev-k34x5yjn.us.auth0.com/\u0026#34;, \u0026#34;clientID\u0026#34;: \u0026#34;hegmOc5rTotLPu5ByRDXOvBAzgs3wuw5\u0026#34;, \u0026#34;clientSecret\u0026#34;: \u0026#34;7xk8O71Uhp5T-bJp_aP2Squwlh4zZTJs65URPma-2UT7n1iigDaMUD9ArhUR-2aL\u0026#34;, \u0026#34;redirectURI\u0026#34;: \u0026#34;http(s)://\u0026lt;insert-external-ip-or-dns-name\u0026gt;/dex/callback\u0026#34;, \u0026#34;scopes\u0026#34;: [\u0026#34;groups\u0026#34;, \u0026#34;email\u0026#34;, \u0026#34;profile\u0026#34;], \u0026#34;claimMapping\u0026#34;:{ \u0026#34;groups\u0026#34;: \u0026#34;http://pachyderm.com/groups\u0026#34; }, \u0026#34;insecureEnableGroups\u0026#34;: true } } type: oidc id: auth0 name: Auth0 version: 1 config: issuer: https://dev-k34x5yjn.us.auth0.com/ clientID: hegmOc5rTotLPu5ByRDXOvBAzgs3wuw5 clientSecret: 7xk8O71Uhp5T-bJp_aP2Squwlh4zZTJs65URPma-2UT7n1iigDaMUD9ArhUR-2aL redirectURI: http(s)://\u0026lt;insert-external-ip-or-dns-name\u0026gt;/dex/callback scopes: - groups - email - profile claimMapping: groups: http://pachyderm.com/groups insecureEnableGroups: true Update the config by running the following command: pachctl idp update-connector \u0026lt;connector-id\u0026gt; --version 2 Grant the group roles by running the following command: pachctl auth set \u0026lt;resource-type\u0026gt; \u0026lt;resource-name\u0026gt; \u0026lt;role-name\u0026gt; group:\u0026lt;group-name\u0026gt; Confirm the group\u0026rsquo;s roles were updated for the given resource: Resource Type: Project Repo pachctl auth get project \u0026lt;project-name\u0026gt; pachctl auth get repo \u0026lt;repo-name\u0026gt; üí° The command pachctl auth get-groups lists the groups that have been defined on your cluster.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["permissions", "management"],
    "id": "f3f5e56e64ae22b64d445c191a594a0c"
  },
  {
    "title": "IAM",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Authorization",
    "description": "Learn how to manage access to resources using roles.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/authorization/permissions/",
    "relURI": "/latest/set-up/authorization/permissions/",
    "body": "This page describes how Pachyderm\u0026rsquo;s Identity and Access Management (IAM) system works and how you can use it to manage access in Pachyderm. Use IAM to grant granular access to specific Pachyderm resources.\nHow IAM Works # IAM works by managing access for users (human or robot) through assigned roles. Roles contain a set of granular permissions (create, read, update, delete) for a given resource. In Pachyderm, resources include clusters, projects, and repositories.\nA user can have many roles, and some roles encompass the permissions of other roles. For example, if you have a clusterAdminRole, all other permissions belonging to more restricted roles are included.\nüí° You can use the command pachctl auth roles-for-permission \u0026lt;permission\u0026gt; to look up which roles provide a given permission.\nAdmin Roles # clusterAdminRole # The ClusterAdminRole includes all of the previous permissions, plus the following:\nPermission CLUSTER_MODIFY_BINDINGS CLUSTER_GET_BINDINGS CLUSTER_AUTH_ACTIVATE CLUSTER_AUTH_DEACTIVATE CLUSTER_AUTH_GET_CONFIG CLUSTER_AUTH_SET_CONFIG CLUSTER_AUTH_MODIFY_GROUP_MEMBERS CLUSTER_AUTH_GET_GROUPS CLUSTER_AUTH_GET_GROUP_USERS CLUSTER_AUTH_EXTRACT_TOKENS CLUSTER_AUTH_RESTORE_TOKEN CLUSTER_AUTH_ROTATE_ROOT_TOKEN CLUSTER_AUTH_DELETE_EXPIRED_TOKENS CLUSTER_AUTH_GET_PERMISSIONS_FOR_PRINCIPAL CLUSTER_AUTH_REVOKE_USER_TOKENS CLUSTER_ENTERPRISE_ACTIVATE CLUSTER_ENTERPRISE_HEARTBEAT CLUSTER_ENTERPRISE_GET_CODE CLUSTER_ENTERPRISE_DEACTIVATE CLUSTER_DELETE_ALL CLUSTER_ENTERPRISE_PAUSE oidcAppAdminRole # Permission CLUSTER_IDENTITY_DELETE_OIDC_CLIENT CLUSTER_IDENTITY_CREATE_OIDC_CLIENT CLUSTER_IDENTITY_UPDATE_OIDC_CLIENT CLUSTER_IDENTITY_LIST_OIDC_CLIENTS CLUSTER_IDENTITY_GET_OIDC_CLIENT idpAdminRole # Permission CLUSTER_IDENTITY_CREATE_IDP CLUSTER_IDENTITY_UPDATE_IDP CLUSTER_IDENTITY_LIST_IDPS CLUSTER_IDENTITY_GET_IDP CLUSTER_IDENTITY_DELETE_IDP secretAdminRole # Permission CLUSTER_CREATE_SECRET CLUSTER_LIST_SECRETS SECRET_INSPECT SECRET_DELETE identityAdminRole # Permission CLUSTER_IDENTITY_SET_CONFIG CLUSTER_IDENTITY_GET_CONFIG licenseAdminRole # Permission CLUSTER_LICENSE_ACTIVATE CLUSTER_LICENSE_GET_CODE CLUSTER_LICENSE_ADD_CLUSTER CLUSTER_LICENSE_UPDATE_CLUSTER CLUSTER_LICENSE_DELETE_CLUSTER CLUSTER_LICENSE_LIST_CLUSTERS Project Roles # All users have the PROJECT_LIST_REPO and PROJECT_CREATE_REPO permissions by default.\nüí° You can view your access level by running the command pachctl list project and checking the ACCESS_LEVEL column.\nProjectViewerRole # Permission PROJECT_LIST_REPO ProjectWriterRole # The ProjectWriterRole includes all of the ProjectViewerRole permissions, plus the following:\nPermission PROJECT_CREATE_REPO ProjectOwnerRole # Permission PROJECT_DELETE PROJECT_MODIFY_BINDINGS ProjectCreatorRole # Permission PROJECT_CREATE Repo Roles # RepoReaderRole # Permission REPO_READ REPO_INSPECT_COMMIT REPO_LIST_COMMIT REPO_LIST_BRANCH REPO_LIST_FILE REPO_INSPECT_FILE REPO_ADD_PIPELINE_READER REPO_REMOVE_PIPELINE_READER PIPELINE_LIST_JOB RepoWriterRole # The RepoWriterRole includes all of the RepoReaderRole permissions, plus the following:\nPermission REPO_WRITE REPO_DELETE_COMMIT REPO_CREATE_BRANCH REPO_DELETE_BRANCH REPO_ADD_PIPELINE_WRITER RepoOwnerRole # The RepoOwnerRole includes all of the RepoWriterRole and RepoReaderRole permissions, plus the following:\nPermission REPO_MODIFY_BINDINGS REPO_DELETE Misc Roles # debuggerRole # Permission CLUSTER_DEBUG_DUMP CLUSTER_GET_PACHD_LOGS robotUserRole # Permission CLUSTER_AUTH_GET_ROBOT_TOKEN pachdLogReaderRole # Permission CLUSTER_GET_PACHD_LOGS ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["permissions", "management", "roles"],
    "id": "32aaad4798955907b0d660c5339dd5e1"
  },
  {
    "title": "Connection",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn how to connect to your organization's publicly exposed cluster.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/cluster-pachctl-connect/",
    "relURI": "/latest/set-up/cluster-pachctl-connect/",
    "body": "If you are exposing your cluster publicly (e.g. via a load balancer), you can connect to it using the pachctl connect command. This command will configure your local pachctl to connect to your cluster.\nOpen a terminal. Retrieve the external IP address of your load balancer or your domain name. kubectl get services | grep pachyderm-proxy | awk \u0026#39;{print $4}\u0026#39; Update the context of your cluster using the IP address. pachctl connect https://pachyderm.\u0026lt;your-proxy.host-value\u0026gt; Verify you are using the correct context. pachctl config get active-context Verify you can connect to your cluster by printing the PachD version. pachctl version ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["load balancer", "tcp", "pachctl connect", "ip"],
    "id": "312fc900fdf8b42c0a36b019e856fe5e"
  },
  {
    "title": "Environment Variables",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn how to configure environment variables.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/environment-variables/",
    "relURI": "/latest/set-up/environment-variables/",
    "body": "You can define environment variables that handle required configuration. In Pachyderm, you can define the following types of environment variables:\npachd variables: Used for your Pachyderm daemon container.\nPachyderm worker variables: Used by the Kubernetes pods that run your pipeline code.\nüí° You can reference environment variables in your code. For example, if your code writes data to an external system and you want to know the current job ID, you can use the PACH_JOB_ID environment variable to refer to the current job ID.\npachd Environment Variables # You can find the list of pachd environment variables in the pachd manifest by running the following command:\nkubectl get deploy pachd -o yaml The following tables list all the pachd environment variables.\nGlobal Configuration # Environment Variable Default Value Description ETCD_SERVICE_HOST N/A The host on which the etcd service runs. ETCD_SERVICE_PORT N/A The etcd port number. PPS_WORKER_GRPC_PORT 80 The GRPs port number. PORT 650 The pachd port number. HTTP_PORT 652 The HTTP port number. PEER_PORT 653 The port for pachd-to-pachd communication. NAMESPACE deafult The namespace in which Pachyderm is deployed. PachD Configuration # Environment Variable Default Value Description NUM_SHARDS 32 The max number of pachd pods that can run in a single cluster. STORAGE_BACKEND \u0026quot;\u0026quot; The storage backend defined for the Pachyderm cluster. STORAGE_HOST_PATH \u0026quot;\u0026quot; The host path to storage. KUBERNETES_PORT_443_TCP_ADDR none An IP address that Kubernetes exports automatically for your code to communicate with the Kubernetes API. Read access only. Most variables that have use the PORT_ADDRESS_TCP_ADDR pattern are Kubernetes environment variables. For more information,\nsee Kubernetes environment variables. METRICS true Defines whether anonymous Pachyderm metrics are being collected or not. BLOCK_CACHE_BYTES 1G The size of the block cache in pachd. WORKER_IMAGE \u0026quot;\u0026quot; The base Docker image that is used to run your pipeline. WORKER_SIDECAR_IMAGE \u0026quot;\u0026quot; The pachd image that is used as a worker sidecar. WORKER_IMAGE_PULL_POLICY IfNotPresent The pull policy that defines how Docker images are pulled. You can set a Kubernetes image pull policy as needed. LOG_LEVEL info Verbosity of the log output. If you want to disable logging, set this variable to 0. Viable Options debug info error\nFor more information, see Go logrus log levels. IAM_ROLE \u0026quot;\u0026quot; The role that defines permissions for Pachyderm in AWS. IMAGE_PULL_SECRET \u0026quot;\u0026quot; The Kubernetes secret for image pull credentials. EXPOSE_OBJECT_API false Controls access to internal Pachyderm API. WORKER_USES_ROOT true Controls root access in the worker container. S3GATEWAY_PORT 600 The S3 gateway port number DISABLE_COMMIT_PROGRESS_COUNTER false A feature flag that disables commit propagation progress counter. If you have a large DAG, setting this parameter to true might help improve etcd performance. You only need to set this parameter on the pachd pod. Pachyderm passes this parameter to worker containers automatically. Storage Configuration # Environment Variable Default Value Description STORAGE_MEMORY_THRESHOLD N/A Defines the storage memory threshold. STORAGE_SHARD_THRESHOLD N/A Defines the storage shard threshold. Pipeline Worker Environment Variables # Pachyderm defines many environment variables for each Pachyderm worker that runs your pipeline code. You can print the list of environment variables into your Pachyderm logs by including the env command into your pipeline specification. For example, if you have an images repository, you can configure your pipeline specification like this:\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;env\u0026#34; }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;images\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [\u0026#34;sh\u0026#34; ], \u0026#34;stdin\u0026#34;: [\u0026#34;env\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;ubuntu:14.04\u0026#34; } } Run this pipeline and, upon completion, you can view the log with variables by running the following command:\npachctl logs --pipeline=env PPS_WORKER_IP=172.17.0.7 DASH_PORT_8081_TCP_PROTO=tcp PACHD_PORT_600_TCP_PORT=600 KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 ... You should see a lengthy list of variables. Many of them define internal networking parameters that most probably you will not need to use.\nMost users find the following environment variables particularly useful:\nEnvironment Variable Description AWS_ACCESS_KEY_ID The ID that contains your AWS access key; requires pfs.s3: true or s3_out:true in your pipeline spec. AWS_SECRET_ACCESS_KEY The name of the secret which contains your AWS access key; requires pfs.s3: true or s3_out:true in your pipeline spec. PACH_JOB_ID The ID of the current job. For example, PACH_JOB_ID=8991d6e811554b2a8eccaff10ebfb341. PACH_DATUM_ID The ID of the current Datum. PACH_DATUM_\u0026lt;input.name\u0026gt;_JOIN_ON Exposes the join_on match to the pipeline\u0026rsquo;s job. PACH_DATUM_\u0026lt;input.name\u0026gt;_GROUP_BY Expose the group_by match to the pipeline\u0026rsquo;s job. PACH_OUTPUT_COMMIT_ID The ID of the commit in the output repo for the current job. For example, PACH_OUTPUT_COMMIT_ID=a974991ad44d4d37ba5cf33b9ff77394. PPS_NAMESPACE The PPS namespace. For example, PPS_NAMESPACE=default. PPS_SPEC_COMMIT The hash of the pipeline specification commit.\nThis value is tied to the pipeline version. Therefore, jobs that use the same version of the same pipeline have the same spec commit. For example, PPS_SPEC_COMMIT=3596627865b24c4caea9565fcde29e7d. PPS_POD_NAME The name of the pipeline pod. For example, pipeline-env-v1-zbwm2. PPS_PIPELINE_NAME The name of the pipeline that this pod runs. For example, env. PIPELINE_SERVICE_PORT_PROMETHEUS_METRICS The port that you can use to exposed metrics to Prometheus from within your pipeline. The default value is 9090. HOME The path to the home directory. The default value is /root \u0026lt;input-repo\u0026gt;=\u0026lt;path/to/input/repo\u0026gt; The path to the filesystem that is defined in the input in your pipeline specification. Pachyderm defines such a variable for each input. The path is defined by the glob pattern in the spec. For example, if you have an input images and a glob pattern of /, Pachyderm defines the images=/pfs/images variable. If you have a glob pattern of /*, Pachyderm matches the files in the images repository and, therefore, the path is images=/pfs/images/liberty.png. input_COMMIT The ID of the commit that is used for the input. For example, images_COMMIT=fa765b5454e3475f902eadebf83eac34. S3_ENDPOINT A Pachyderm S3 gateway sidecar container endpoint. If you have an S3 enabled pipeline, this parameter specifies a URL that you can use to access the pipeline\u0026rsquo;s repositories state when a particular job was run. The URL has the following format: http://\u0026lt;job-ID\u0026gt;-s3:600. An example of accessing the data by using AWS CLI looks like this: `echo foo_data In addition to these environment variables, Kubernetes injects others for Services that run inside the cluster. These variables enable you to connect to those outside services, which can be powerful but might also result in processing being retried multiple times.\nFor example, if your code writes a row to a database, that row might be written multiple times because of retries. Interaction with outside services must be idempotent to prevent unexpected behavior. Furthermore, one of the running services that your code can connect to is Pachyderm itself. This is generally not recommended as very little of the Pachyderm API is idempotent, but in some specific cases it can be a viable approach.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["configuration"],
    "id": "8119651404fd1e6256dec24063317254"
  },
  {
    "title": "Kubernetes RBAC",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn how our platform supports Kubernetes' Role-Base Access Controls (RBAC).",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/rbac/",
    "relURI": "/latest/set-up/rbac/",
    "body": "Pachyderm has support for Kubernetes Role-Based Access Controls (RBAC), which is a default part of all Pachyderm deployments. In most use cases, Pachyderm sets all the RBAC permissions automatically. However, if you are deploying Pachyderm on a cluster that your company owns, security policies might not allow certain RBAC permissions by default. Therefore, you need to contact your Kubernetes administrator and provide the following list of required permissions:\nRules: []rbacv1.PolicyRule{{ APIGroups: []string{\u0026#34;\u0026#34;}, Verbs: []string{\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;}, Resources: []string{\u0026#34;nodes\u0026#34;, \u0026#34;pods\u0026#34;, \u0026#34;pods/log\u0026#34;, \u0026#34;endpoints\u0026#34;}, }, { APIGroups: []string{\u0026#34;\u0026#34;}, Verbs: []string{\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;delete\u0026#34;}, Resources: []string{\u0026#34;replicationcontrollers\u0026#34;, \u0026#34;services\u0026#34;}, }, { APIGroups: []string{\u0026#34;\u0026#34;}, Verbs: []string{\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;delete\u0026#34;}, Resources: []string{\u0026#34;secrets\u0026#34;}, ResourceNames: []string{client.StorageSecretName}, }}, The following table explains how Pachyderm uses those permissions:\nPermission Description Access to nodes Used for metrics reporting, disabling should not affect Pachyderm\u0026rsquo;s operation. Access to pods, replica controllers, and services Pachyderm uses this permission to monitor the created pipelines. The permissions related to replicationcontrollers and services are used in the setup and deletion of pipelines. Each pipeline has its own RC and service in addition to the pods. Access to secrets Required to give various kinds of credentials to pipelines, including storage credentials to access S3 or other object storage backends, Docker credentials to pull from a private registry, and others. RBAC and DNS # In older Kubernetes versions, kube-dns did not work properly with RBAC. To check if your cluster is affected by this issue, run:\nkubectl get all --namespace=kube-system System response:\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 1 1 1 0 3m NAME DESIRED CURRENT READY AGE rs/kube-dns-86f6f55dd5 1 1 0 3m NAME READY STATUS RESTARTS AGE po/kube-addon-manager-oryx 1/1 Running 0 3m po/kube-dns-86f6f55dd5-xksnb 2/3 Running 4 3m po/kubernetes-console-bzjjh 1/1 Running 0 3m po/storage-provisioner 1/1 Running 0 3m NAME DESIRED CURRENT READY AGE rc/kubernetes-console 1 1 1 3m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 3m svc/kubernetes-console NodePort 10.97.194.16 \u0026lt;none\u0026gt; 80:30000/TCP 3m In the output above, po/kubernetes-console-bzjjh has only two out of three pods ready and has restarted four times. To fix this issue, run:\nkubectl -n kube-system create sa kube-dns kubectl -n kube-system patch deploy/kube-dns -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;template\u0026#34;: {\u0026#34;spec\u0026#34;: {\u0026#34;serviceAccountName\u0026#34;: \u0026#34;kube-dns\u0026#34;}}}}\u0026#39; These commands enforce kube-dns to use the appropriate ServiceAccount. Kubernetes has created the ServiceAccount, but does not use it until you run the above commands.\nResolving RBAC Permissions on GKE # When you deploy Pachyderm on GKE, you might see the following error:\nError from server (Forbidden): error when creating \u0026#34;STDIN\u0026#34;: clusterroles.rbac.authorization.k8s.io \u0026#34;pachyderm\u0026#34; is forbidden: attempt to grant extra privileges: To fix this issue, run the following command and redeploy Pachyderm:\nkubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account) ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["configuration", "permissions"],
    "id": "ac0da25ee07e99a2e6408675b146789b"
  },
  {
    "title": "Import a Kubernetes Context",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn how to import and embed a Kubernetes Context into a Pachyderm context.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/import-kubernetes-context/",
    "relURI": "/latest/set-up/import-kubernetes-context/",
    "body": "After you have deployed Pachyderm, the Pachyderm context is not created. Therefore, you need to manually create a new Pachyderm context with the embedded current Kubernetes context and activate that context.\nTo import a Kubernetes context, complete the following steps:\nVerify that the cluster was successfully deployed:\nkubectl get pods You should see a pod for pachd running (alongside etcd, pg-bouncer or postgres, console, depending on your installation).\nSystem Response:\nNAME READY STATUS RESTARTS AGE console-6c989c8d56-ftxk7 1/1 Running 0 3d18h etcd-0 1/1 Running 0 3d18h pachd-f9fd5b6fc-8d774 1/1 Running 0 3d18h pg-bouncer-794d8f68f-sjbbh 1/1 Running 0 3d18h Create a new Pachyderm context with the embedded Kubernetes context:\npachctl config import-kube \u0026lt;new-pachyderm-context-name\u0026gt; -k `kubectl config current-context` Verify that the context was successfully created and view the context parameters:\nExample:\npachctl config get context \u0026lt;new-pachyderm-context-name\u0026gt; System Response:\n{ \u0026#34;source\u0026#34;: \u0026#34;IMPORTED\u0026#34;, \u0026#34;cluster_name\u0026#34;: \u0026#34;minikube\u0026#34;, \u0026#34;auth_info\u0026#34;: \u0026#34;minikube\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34; } Activate the new Pachyderm context:\npachctl config set active-context \u0026lt;new-pachyderm-context-name\u0026gt; Verify that the new context has been activated:\npachctl config get active-context ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["kubernetes"],
    "id": "e11660f446346f0da2fe5f877ee844c8"
  },
  {
    "title": "Log Aggregation (Loki)",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn how to enable log aggregation with Loki through the Promtail agent service.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/loki/",
    "relURI": "/latest/set-up/loki/",
    "body": " Shipping logs to Loki # Loki retrieves logs from pods in Kubernetes through an agent service called Promtail. Promtail runs on each node and sends logs from Kubernetes pods to the Loki API Server, tagging each log entry with information about the pod that produced it.\nYou need to configure Promtail for your environment to ship logs to your Loki instance. If you are running multiple nodes, then you will need to install and configure Promtail for each node shipping logs to Loki.\nFetching logs # While installing Loki will enable the collection of logs, commands such as pachctl logs will not fetch logs directly from Loki until the LOKI_LOGGING environment variable on the pachd container is true.\nThis is controlled by the helm value pachd.lokiLogging, which can be set by adding the following to your values.yaml file:\npachd: lokiLogging: true Pachyderm reads logs from the Loki API Server with a particular set of tags. The URI at which Pachyderm reads from the Loki API Server is determined by the LOKI_SERVICE_HOST and LOKI_SERVICE_PORT environment values automatically added by Loki Kubernetes service.\nIf Loki is deployed after the pachd container, the pachd container will need to be redeployed to receive these connection parameters.\n‚ÑπÔ∏è If you are not running Promtail on the node where your Pachyderm pods are located, you will be unable to get logs for pipelines running on that node via pachctl logs -p pipelineName.\nDefault Loki Bundle # Per default, Pachyderm ships with an embedded version of Loki that can be deployed by adding the lokiDeploy: true next to the existing lokiLogging: true.\npachd: lokiDeploy: true lokiLogging: true In such case, add the following section to your value.yaml:\nloki-stack: loki: persistence: enabled: true accessModes: - ReadWriteOnce size: 5Gi storageClassName: standard annotations: {} grafana: enabled: true ‚ÑπÔ∏è Grafana Users:\nTo use Grafana, deploy with loki-stack.grafana.enabled: true.\nTo access Grafana, run port-forward with kubectl port-forward svc/pachyderm-grafana 4001:80. Change the port 4001 to what suits you best.\nLogin to localhost:4001 with the username admin, and the password found with running kubectl get secret pachyderm-grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 -d. If enterprise is activated, you will be able to inspect containers logs in your console.\nUsing Loki in Another Namespace # Instead of deploying a local loki instance in your pachyderm namespace, you can configure pachyderm to use a loki running in another namespace. To do so, you must set lokiHost and lokiPort. You should also set lokiDeploy: false to prevent the chart from deploying a local loki instance.:\npachd: lokiDeploy: false lokiHost: \u0026#34;\u0026lt;loki-namespace\u0026gt;.\u0026lt;loki-service-name\u0026gt;.svc.cluster.local.\u0026#34; lokiPort: 3100 References # Loki Documentation - https://grafana.com/docs/loki/latest/ Promtail Documentation - https://grafana.com/docs/loki/latest/clients/promtail/ Operating Loki - https://grafana.com/docs/loki/latest/operations/ ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["loki", "logs"],
    "id": "84e7dad185e80cab0e38a30e8774e911"
  },
  {
    "title": "Non-Default Namespaces",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn how to deploy to a non-default namespace for easier admin management.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/namespaces/",
    "relURI": "/latest/set-up/namespaces/",
    "body": "Often, production deploys of Pachyderm involve deploying Pachyderm to a non-default namespace. This helps administrators of the cluster more easily manage Pachyderm components alongside other things that might be running inside of Kubernetes (DataDog, TensorFlow Serving, etc.).\nTo deploy Pachyderm to a non-default namespace, you need to add the -n or --namespace flag when deploying. If the namespace does not already exist, you can have Helm create it with --create-namespace.\nhelm install \u0026lt;args\u0026gt; --namespace pachyderm --create-namespace To talk to your Pachyderm cluster:\nYou can either modify an existing pachctl context\npachctl config update context --namespace pachyderm or import one from Kubernetes:\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["deployment"],
    "id": "1a08388ab76465eeb2d2d3009bf6dd07"
  },
  {
    "title": "Enterprise Edition",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Set Up",
    "description": "Learn about the unique features and settings specific to Enterprise edition.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise/",
    "relURI": "/latest/set-up/enterprise/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "bb80d701aae63fa0c0f8ad5fb5c7339d"
  },
  {
    "title": "Activate Enterprise via Helm",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Enterprise Edition",
    "description": "Learn how to deploy the Enterprise edition using Helm.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise/activate-via-helm/",
    "relURI": "/latest/set-up/enterprise/activate-via-helm/",
    "body": " Before You Start # You must have a Pachyderm Enterprise License Key. You must have pachctl and Pachyderm installed. You must have the Pachyderm Helm repo downloaded. How to Activate Enterprise Pachyderm via Helm # Activation Method: License License Secret Open your Helm values.yml file. Find the the pachd.enterpriseLicenseKey attribute. Input your enterprise key. Upgrade your cluster by running the following command: helm upgrade pachyderm pachyderm/pachyderm -f values.yml Once deployed, Pachyderm stores your provided Enterprise license as the platform secret pachyderm-license in the key enterprise-license-key.\nCreate a secret for your Enterprise license. Open your Helm values.yml file. Find the the pachd.enterpriseLicenseKeySecretName attribute. Input your license\u0026rsquo;s secret name. Upgrade your cluster by running the following command: helm upgrade pachyderm pachyderm/pachyderm -f values.yml ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["enterprise"],
    "id": "3273926180b46e83b252b8a9984775db"
  },
  {
    "title": "Activate Enterprise via PachCTL",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Enterprise Edition",
    "description": "Learn how to deploy the Enterprise edition using the PachCTL CLI for an existing cluster.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise/activate-via-pachctl/",
    "relURI": "/latest/set-up/enterprise/activate-via-pachctl/",
    "body": " Before You Start # You must have a Pachyderm Enterprise License Key. You must have pachctl and Pachyderm installed. You must have the Pachyderm Helm repo downloaded. How to Activate Enterprise Pachyderm via Pachctl # Open your terminal. Input the following command: echo \u0026lt;your-activation-token\u0026gt; | pachctl license activate Verify the status of the enterprise activation: pachctl enterprise get-state # ACTIVE You have unlocked Pachyderm\u0026rsquo;s enterprise features.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["enterprise"],
    "id": "6707755a1b7f6a2a430b7463472c1ef4"
  },
  {
    "title": "Features Overview",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Enterprise Edition",
    "description": "Learn about the main features unique Enterprise edition.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise/overview/",
    "relURI": "/latest/set-up/enterprise/overview/",
    "body": "Enterprise helps you scale and manage Pachyderm data pipelines by removing all scaling limits and providing you with additional features not available in the Community Edition.\n‚ÑπÔ∏è Want to try Enterprise, or simply have a few questions? Get in touch with us at sales@pachyderm.io or on our Slack.\nAdditional Features # Authentication: Authenticate against your favorite OIDC providers. Role-Based Access Control (RBAC): Use RBAC on pachyderm resources (clusters, projects, repos), silo data, and prevent unintended changes on production pipelines. Enterprise Server: Simplify licensing and Identity Provider management by using one Enterprise server to register many Pachyderm clusters. Additionally, you have access to a PachCTL command that pauses (pachctl enterprise pause) and unpauses (pachctl enterprise unpause) your cluster for a backup and restore. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["enterprise"],
    "id": "d7166163c885e822369a8a3bb7200ac1"
  },
  {
    "title": "Enterprise Server (ES)",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Set Up",
    "description": "Learn how to spin up and manage a Enterprise server.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise-server/",
    "relURI": "/latest/set-up/enterprise-server/",
    "body": "You can manage your enterprise licensing and identity provider (IdP) integrations through the Enterprise Server. A Enterprise Server can have multiple Pachyderm clustered registered to it.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "1cfaaa86d75657099df42c3ea8c17d96"
  },
  {
    "title": "Activate ES for Multi-Cluster",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Enterprise Server (ES)",
    "description": "Learn how to set up a Enterprise server as a standalone cluster within a multi-cluster deployment.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise-server/multi-cluster-setup/",
    "relURI": "/latest/set-up/enterprise-server/multi-cluster-setup/",
    "body": "This guide deploys Enterprise Server as a standalone cluster within a multi-cluster deployment.\nBefore You Start # There are a few minor differences to note when deploying an Enterprise Server when compared to a standard Pachyderm cluster:\nNo deployment target is necessary in your Helm chart since there is no object store The Enterprise Server cluster contains the dex database Each registered cluster requires its own PostgresSQL pachyderm database How to Activate Enterprise for Multi-Cluster # Create a separate Kubernetes namespace dedicated to your enterprise server: kubectl create namespace enterprise-server kubectl config set-context --current --namespace=enterprise-server Create a Helm chart enterprise-server-values.yml file for your enterprise server (see Helm Chart Reference Guide). Deploy the Enterprise Server cluster: helm install enterprise-server pachyderm/pachyderm --f enterprise-server-values.yml Verify deployment: kubectl get all --namespace enterprise-server Reference Diagram # The following diagram gives you a quick overview of an organization with multiple Pachyderm clusters behind a single Enterprise Server. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["enterprise", "deployment", "helm"],
    "id": "6388671ea77ef498042715b890e1756b"
  },
  {
    "title": "Activate ES for Single-Cluster",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Enterprise Server (ES)",
    "description": "Learn how to set up a Enterprise server for a single-cluster environment embedded in pachd.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise-server/single-cluster-setup/",
    "relURI": "/latest/set-up/enterprise-server/single-cluster-setup/",
    "body": "You can register an existing single-cluster Pachyderm instance to the embedded Enterprise Server that comes included with pachd using the steps in this guide. Doing so enables you to also activate authentication and set up IdP connectors.\nBefore You Start # You must have an Enterprise license key You must have an active Pachyderm cluster How to Activate Enterprise Server # Open your terminal. Activate Enterprise Server: echo \u0026lt;enterprise-license-key-value\u0026gt; | pachctl license activate Activate Authentication: pachctl auth activate --enterprise Set up your Identity Provider (IdP). ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["enterprise", "deployment", "helm"],
    "id": "5e7d39f1eaf917dab44dab081d9087a3"
  },
  {
    "title": "Register a Cluster via Helm",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Enterprise Server (ES)",
    "description": "Learn how to register a pachd cluster to your Enterprise Server using Helm.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise-server/register-cluster-via-helm/",
    "relURI": "/latest/set-up/enterprise-server/register-cluster-via-helm/",
    "body": " Before You Start # You must have an Enterprise license key You must have the Pachyderm Helm repo downloaded. How to Register a Cluster # Open your Helm values.yml file. Update the pachd section with the following attributes: pachd: activateEnterpriseMember: true enterpriseServerAddress: \u0026#34;grpc://\u0026lt;ENTERPRISE_SERVER_ADDRESS\u0026gt;\u0026#34; enterpriseCallbackAddress: \u0026#34;grpc://\u0026lt;PACHD_ADDRESS\u0026gt;\u0026#34; enterpriseServerToken: \u0026#34;\u0026lt;ENTERPRISE-SERVER-TOKEN\u0026gt;\u0026#34; # the same root token of the enterprise cluster # Alternatively, use a secret enterpriseServerTokenSecretName: \u0026#34;\u0026lt;Name of you secret containing enterpriseServerToken\u0026gt;\u0026#34; Upgrade the cluster: helm upgrade pachyderm pachyderm/pachyderm -f values.yml ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["enterprise", "deployment", "helm"],
    "id": "ac73ff7b76f3f3cc08e66c02c1003698"
  },
  {
    "title": "Register a Cluster via PachCTL",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Enterprise Server (ES)",
    "description": "Learn how to register a pachd cluster to your Enterprise Server using PachCTL.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise-server/register-cluster-via-pachctl/",
    "relURI": "/latest/set-up/enterprise-server/register-cluster-via-pachctl/",
    "body": " Before You Start # You must have an Enterprise license key You must have an active Pachyderm cluster You must have the Pachyderm Helm repo downloaded. How to Register a Cluster # Open your terminal. Run the following command: pachctl enterprise register --id \u0026lt;my-pachd-config-name\u0026gt; --enterprise-server-address \u0026lt;pach-enterprise-IP\u0026gt;:650 --pachd-address \u0026lt;pachd-IP\u0026gt;:650 Attribute Description --id the name of the context pointing to your cluster in ~/.pachyderm/config.json. --enterprise-server-address the host and port where pachd can reach the enterprise server. --pachd-address the host and port where the enterprise server can reach pachd. This may be internal to the kubernetes cluster, or over the internet. View all registered clusters with your enterprise server: pachctl license list-clusters # Using enterprise context: my-enterprise-context-name # id: john # address: ae1ba915f8b5b477c98cd26c67d7563b-66539067.us-west-2.elb.amazonaws.com:650 # version: 2.0.0 # auth_enabled: true # last_heartbeat: 2021-05-21 18:37:36.072156 +0000 UTC # --- # id: doe # address: 34.71.247.191:650 # version: 2.0.0 # auth_enabled: true # last_heartbeat: 2021-05-21 18:43:42.157027 +0000 UTC # --- Activate Authentication: pachctl auth activate --enterprise Set up your Identity Provider (IdP). ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["enterprise", "deployment", "pachctl"],
    "id": "af64fc131f776823d70598a26b65a6f4"
  },
  {
    "title": "Server Management",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Enterprise Server (ES)",
    "description": "Learn how to manage your Enterprise server.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise-server/manage/",
    "relURI": "/latest/set-up/enterprise-server/manage/",
    "body": " Contexts # The enterprise server has a separate context in the pachctl config file (~/.pachyderm/config.json).\nPachctl has an active pachd context (the cluster it is binded to), and separately an active enterprise context.\nTo check the active enterprise context, run:\npachctl config get active-enterprise-context ‚ö†Ô∏è In a single-cluster deployment, the active enterprise context will be the same as the enterprise context. The pachctl license and pachctl idp commands run against the enterprise context. pachctl auth commands accept an --enterprise flag to run against the enterprise context. Configuring IDPs # To configure IDP integrations, use pachctl idp create-connector as documented in the Pachyderm Integration with Identity Providers page.\nManage your Enterprise Server # Add Users As Administrators # By default, only the root token (Root User) can administer the Enterprise Server. Run the following command to add more ClusterAdmin to your Enterprise Server:\npachctl auth set enterprise clusterAdmin user:\u0026lt;email\u0026gt; List All Registered Clusters # pachctl license list-clusters The output includes the pachd version, whether auth is enabled, and the last heartbeat:\nid: pach-2 address: 34.71.247.191:650 version: 2.0.0 auth_enabled: true last_heartbeat: 2021-05-21 18:43:42.157027 +0000 UTC Synchronize all available contexts in your ~/.pachyderm/config.json file # In the case where the enterprise server of your organization has multiple pachd instances, you can use the following command to ‚Äúdiscover‚Äù other pachd instances. It will automatically update your ~/.pachyderm/config.json file with all the contexts you can connect to.\npachctl enterprise sync-contexts Update The Enterprise License # To apply a new license and have it picked up by all clusters, run:\npachctl license activate --no-register Unregister A Cluster # To unregister a given cluster from your Enterprise Server, run:\npachctl license delete-cluster --id \u0026lt;cluster id\u0026gt; Undeploy # To undeploy a Cluster registered with an Enterprise Server: Unregister the cluster as mentioned above (pachctl license delete-cluster) Then, undeploy it: helm uninstall ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["enterprise", "management"],
    "id": "25ffa62ef1b670b8888377fba9489481"
  },
  {
    "title": "Server Setup",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Enterprise Server (ES)",
    "description": "Learn how to set up a Enterprise server.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/enterprise-server/setup/",
    "relURI": "/latest/set-up/enterprise-server/setup/",
    "body": " ‚ÑπÔ∏è For POCs and smaller organizations with one single Pachyderm cluster, the Enterprise Server services can be run embedded in pachd. A separate deployment is not necessary. An organization with a single Pachyderm cluster can run the Enterprise Server services embedded within pachd.\nThe setup of an Enterprise Server requires to:\nDeploy it. Activate your Enterprise Key and enable Auth. Register your newly created or existing Pachyderm clusters with your enterprise server. Optional: Enable Auth on each cluster. 1. Deploy An Enterprise Server # Deploying and configuring an enterprise server can be done in one of two ways:\nProvide all licensing and authentication configurations as a part of the Helm deployment. Use pachctl commands to set up licensing and authentication. As Part Of A Regular Pachyderm Helm Deployment # Update your values.yaml with your enterprise license key and auth configurations (for an example on localhost, see the example values.yaml here) or check our minimal example below to your values.yaml.\n‚ö†Ô∏è If a pachyderm cluster will also be installed in the same kubernetes cluster, they should be installed in different namespaces: kubectl create namespace enterprise helm install ... --set enterpriseServer.enabled=true --namespace enterprise This command deploys postgres, etcd and a deployment and service called pach-enterprise. pach-enterprise uses the same docker image and pachd binary, but it listens on a different set of ports (31650, 31657, 31658) to avoid conflicts with pachd.\nCheck the state of your deployment by running: kubectl get all --namespace enterprise System Response\nNAME READY STATUS RESTARTS AGE pod/etcd-5fd7c675b6-46kz7 1/1 Running 0 113m pod/pach-enterprise-6dc9cb8f66-rs44t 1/1 Running 0 105m pod/postgres-6bfd7bfc47-9mz28 1/1 Running 0 113m values.yaml for a standalone Enterprise Server as part of a multi-cluster deployment # Deploying a standalone enterprise server requires setting the helm parameter enterpriseServer.enabled to true and the pachd.enabled to false.\nenterpriseServer: enabled: true pachd: enabled: false enterpriseLicenseKey: \u0026#34;\u0026lt;ENTERPRISE-LICENSE-KEY\u0026gt;\u0026#34; # Alternatively, you can pass your license in a secret enterpriseLicenseKeySecretName: \u0026#34;\u0026lt;enterprise License key secret name\u0026gt;\u0026#34; oauthClientID: \u0026#34;pachd\u0026#34; oauthRedirectURI: \u0026#34;http://\u0026lt;PACHD-IP\u0026gt;:30657/authorization-code/callback\u0026#34; ## if a secret name is not provided in `oauthClientSecretSecretName`, a secret containing `oauthClientSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret \u0026#39;pachyderm-auth` under the key `auth-config\u0026#39; oauthClientSecret: \u0026#34;\u0026#34; oauthClientSecretSecretName: \u0026#34;\u0026#34; ## if a secret name is not provided in `enterpriseSecretSecretName`, a secret containing `enterpriseSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret \u0026#39;pachyderm-enterprise` under the key `enterprise-secret\u0026#39; enterpriseSecretSecretName: \u0026#34;\u0026#34; enterpriseSecret: \u0026#34;\u0026#34; activateAuth: true ## if a secret name is not provided in `rootTokenSecretName`, a secret containing `rootToken` (or a randomly generated value if empty) will be created on install and stored in the k8s secret \u0026#39;pachyderm-auth` under the key `rootToken\u0026#39; rootTokenSecretName: \u0026#34;\u0026#34; rootToken: \u0026#34;\u0026#34; externalService: enabled: true oidc: issuerURI: \u0026#34;http://\u0026lt;PACHD-IP\u0026gt;:30658/\u0026#34; ## userAccessibleOauthIssuerHost is necessary in localhost settings or anytime the registered Issuer address isn\u0026#39;t accessible outside the cluster # userAccessibleOauthIssuerHost: \u0026#34;localhost:30658\u0026#34; ## if `mockIDP` is set to true, `pachd.upstreamIDPs` will be ignored in favor of a testing placeholder IDP with username/password: admin/password mockIDP: false ## to set up upstream IDPs, set pachd.mockIDP to false, ## and populate the pachd.upstreamIDPs with an array of Dex Connector configurations. ## See the example below or https://dexidp.io/docs/connectors/ upstreamIDPs: - id: idpConnector jsonConfig: \u0026gt;- { \u0026#34;issuer\u0026#34;: \u0026#34;\u0026lt;ISSUER\u0026gt;\u0026#34;, \u0026#34;clientID\u0026#34;: \u0026#34;\u0026lt;CLIENT-ID\u0026gt;\u0026#34;, \u0026#34;clientSecret\u0026#34;: \u0026#34;\u0026lt;CLIENT-SECRET\u0026gt;\u0026#34;, \u0026#34;redirectURI\u0026#34;: \u0026#34;http://\u0026lt;PACHD-IP\u0026gt;:30658/callback\u0026#34;, \u0026#34;insecureEnableGroups\u0026#34;: true, \u0026#34;insecureSkipEmailVerified\u0026#34;: true, \u0026#34;insecureSkipIssuerCallbackDomainCheck\u0026#34;: true, \u0026#34;forwardedLoginParams\u0026#34;: [\u0026#34;login_hint\u0026#34;] } name: idpConnector type: oidc values.yaml for an embedded single-cluster deployment # pachd: enterpriseLicenseKey: \u0026#34;\u0026lt;ENTERPRISE-LICENSE-KEY\u0026gt;\u0026#34; # Alternatively, you can pass your license in a secret enterpriseLicenseKeySecretName: \u0026#34;\u0026lt;enterprise License key secret name\u0026gt;\u0026#34; oauthClientID: \u0026#34;pachd\u0026#34; oauthRedirectURI: \u0026#34;http://\u0026lt;PACHD-IP\u0026gt;:30657/authorization-code/callback\u0026#34; ## if a secret name is not provided in `oauthClientSecretSecretName`, a secret containing `oauthClientSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret \u0026#39;pachyderm-auth` under the key `auth-config\u0026#39; oauthClientSecret: \u0026#34;\u0026#34; oauthClientSecretSecretName: \u0026#34;\u0026#34; ## if a secret name is not provided in `enterpriseSecretSecretName`, a secret containing `enterpriseSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret \u0026#39;pachyderm-enterprise` under the key `enterprise-secret\u0026#39; enterpriseSecretSecretName: \u0026#34;\u0026#34; enterpriseSecret: \u0026#34;\u0026#34; activateAuth: true ## if a secret name is not provided in `rootTokenSecretName`, a secret containing `rootToken` (or a randomly generated value if empty) will be created on install and stored in the k8s secret \u0026#39;pachyderm-auth` under the key `rootToken\u0026#39; rootTokenSecretName: \u0026#34;\u0026#34; rootToken: \u0026#34;\u0026#34; externalService: enabled: true oidc: issuerURI: \u0026#34;http://\u0026lt;PACHD-IP\u0026gt;:30658/\u0026#34; ## userAccessibleOauthIssuerHost is necessary in localhost settings or anytime the registered Issuer address isn\u0026#39;t accessible outside the cluster # userAccessibleOauthIssuerHost: \u0026#34;localhost:30658\u0026#34; ## if `mockIDP` is set to true, `pachd.upstreamIDPs` will be ignored in favor of a testing placeholder IDP with username/password: admin/password mockIDP: false ## to set up upstream IDPs, set pachd.mockIDP to false, ## and populate the pachd.upstreamIDPs with an array of Dex Connector configurations. ## See the example below or https://dexidp.io/docs/connectors/ upstreamIDPs: - id: idpConnector jsonConfig: \u0026gt;- { \u0026#34;issuer\u0026#34;: \u0026#34;\u0026lt;ISSUER\u0026gt;\u0026#34;, \u0026#34;clientID\u0026#34;: \u0026#34;\u0026lt;CLIENT-ID\u0026gt;\u0026#34;, \u0026#34;clientSecret\u0026#34;: \u0026#34;\u0026lt;CLIENT-SECRET\u0026gt;\u0026#34;, \u0026#34;redirectURI\u0026#34;: \u0026#34;http://\u0026lt;PACHD-IP\u0026gt;:30658/callback\u0026#34;, \u0026#34;insecureEnableGroups\u0026#34;: true, \u0026#34;insecureSkipEmailVerified\u0026#34;: true, \u0026#34;insecureSkipIssuerCallbackDomainCheck\u0026#34;: true, \u0026#34;forwardedLoginParams\u0026#34;: [\u0026#34;login_hint\u0026#34;] } name: idpConnector type: oidc This results in a single pachd pod, with authentication enabled, and an IDP integration configured.\n‚ÑπÔ∏è Update the following values as follows:\nPACHD-IP: The address of Pachyderm\u0026rsquo;s IP. Retrieve Pachyderm external IP address if necessary. ISSUER, CLIENT-ID, CLIENT-SECRET.\nCheck the list of all available helm values at your disposal in our reference documentation or on Github.\n‚ö†Ô∏è When enterprise is enabled through Helm, auth is automatically activated (i.e., you do not need to run pachctl auth activate) and a pachyderm-auth k8s secret is created containing a rootToken key. Use {{\u0026quot;kubectl get secret pachyderm-auth -o go-template='{{.data.rootToken | base64decode }}'\u0026quot;}} to retrieve it and save it where you see fit. However, this secret is only used when configuring through helm:\nIf you run pachctl auth activate, the secret is not updated. Instead, the rootToken is printed in your STDOUT for you to save. Set the helm value pachd.activateAuth to false to prevent the automatic bootstrap of auth on the cluster. On An Existing Pachyderm Cluster # To enable the Enterprise Server on an existing cluster:\nActivate your enterprise key and authentication then proceed to configuring IDP integrations. 2. Activate Enterprise Licensing And Enable Authentication # Use your enterprise key to activate your enterprise server: echo \u0026lt;your-activation-token\u0026gt; | pachctl license activate Then enable Authentication at the Enterprise Server level: pachctl auth activate --enterprise ‚ö†Ô∏è Enabling Auth will return a root token for the enterprise server. This is separate from the root tokens for each pachd (cluster). They should all be stored securely.\nOnce the enterprise server is deployed, deploy your cluster(s) and register it(them) with the enterprise server.\n3. Register Your Cluster With The Enterprise Server # Similarly to the enterprise server, we can configure our pachyderm clusters to leverage Helm for licensing and authentication in one of two flavors:\nProvide enterprise registration information as a part of the Helm deployment of a cluster. Register a cluster with the Enterprise Server using pachctl commands. Register Clusters With Helm # Add the enterprise server\u0026rsquo;s root token, and network addresses to the values.yaml of each cluster you plan to deploy and register, for the cluster and enterprise server to communicate (for an example on localhost, see the example values.yaml here), or insert our minimal example below to your values.yaml.\nvalues.yaml with activation of an enterprise license and authentication # pachd: activateEnterpriseMember: true enterpriseServerAddress: \u0026#34;grpc://\u0026lt;ENTERPRISE_SERVER_ADDRESS\u0026gt;\u0026#34; enterpriseCallbackAddress: \u0026#34;grpc://\u0026lt;PACHD_ADDRESS\u0026gt;\u0026#34; enterpriseServerToken: \u0026#34;\u0026lt;ENTERPRISE-SERVER-TOKEN\u0026gt;\u0026#34; # the same root token of the enterprise cluster # Alternatively, use a secret enterpriseServerTokenSecretName: \u0026#34;\u0026lt;Name of you secret containing enterpriseServerToken\u0026gt;\u0026#34; ‚ö†Ô∏è When setting your enterprise server info as part of the Helm deployment of a cluster, auth is automatically activated unless the helm value pachd.activateAuth was intentionally set to false. (i.e., you can skip step 4).\nIn this case, a pachyderm-auth k8s secret is automatically created containing an entry for your rootToken in the key rootToken. Use the following to retrieve it and save it where you see fit:\n{{\u0026#34;kubectl get secret pachyderm-auth -o go-template=\u0026#39;{{.data.rootToken | base64decode }}\u0026#39;\u0026#34;}} Register Clusters With pachctl # Run this command for each of the clusters you wish to register using pachctl:\npachctl enterprise register --id \u0026lt;my-pachd-config-name\u0026gt; --enterprise-server-address \u0026lt;pach-enterprise-IP\u0026gt;:650 --pachd-address \u0026lt;pachd-IP\u0026gt;:650 --id is the name of the context pointing to your cluster in ~/.pachyderm/config.json.\n--enterprise-server-address is the host and port where pachd can reach the enterprise server. In production, the enterprise server may be exposed on the internet.\n--pachd-address is the host and port where the enterprise server can reach pachd. This may be internal to the kubernetes cluster, or over the internet.\nDisplay the list of all registered clusters with your enterprise server:\npachctl license list-clusters Using enterprise context: my-enterprise-context-name id: john address: ae1ba915f8b5b477c98cd26c67d7563b-66539067.us-west-2.elb.amazonaws.com:650 version: 2.0.0 auth_enabled: true last_heartbeat: 2021-05-21 18:37:36.072156 +0000 UTC --- id: doe address: 34.71.247.191:650 version: 2.0.0 auth_enabled: true last_heartbeat: 2021-05-21 18:43:42.157027 +0000 UTC --- 4. Enable Auth On Each Cluster # Finally, if your clusters were registered with the Enterprise Server using pachctl, you might choose to activate auth on each (or some) of them. This is an optional step. Clusters can be registered with the enterprise server without authentication being enabled.\nBefore enabling authentication, set up the issuer in the idp config between the enterprise server and your cluster:\necho \u0026#34;issuer: http://\u0026lt;enterprise-server-IP\u0026gt;:658\u0026#34; | pachctl idp set-config --config - Check that your config has been updated properly: pachctl idp get-config\nFor each registered cluster you want to enable auth on:\npachctl auth activate --client-id \u0026lt;my-pachd-config-name\u0026gt; --redirect http://\u0026lt;pachd-IP\u0026gt;:657/authorization-code/callback ‚ÑπÔ∏è Note the /authorization-code/callback appended after \u0026lt;pachd-IP\u0026gt;:657 in --redirect. --client-id is to pachctl auth activate what --id is to pachctl enterprise register: In both cases, enter \u0026lt;my-pachd-config-name\u0026gt;. Make sure than your enterprise context is set up properly: pachctl config get active-enterprise-context If not:\npachctl config set active-enterprise-context \u0026lt;my-enterprise-context-name\u0026gt; ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["enterprise", "deployment", "helm"],
    "id": "eb7bf830acd9d3666c7b8e984a0df976"
  },
  {
    "title": "S3 Gateway API",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn about the operations exposed by the S3 Gateway API.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/s3gateway-api/",
    "relURI": "/latest/set-up/s3gateway-api/",
    "body": "This section outlines the operations exposed by Pachyderm\u0026rsquo;s HTTP API S3 Gateway.\nüìñ Since 1.13.3, all operations mentioning \u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt; also accept the syntax \u0026lt;commit\u0026gt;.\u0026lt;repo\u0026gt; and \u0026lt;commit\u0026gt;.\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;.\nListBuckets # Route: GET /.\nLists all of the branches across all of the repos as S3 buckets.\nDeleteBucket # Route: DELETE /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;/.\nDeletes the branch. If it is the last branch in the repo, the repo is also deleted. Unlike S3, you can delete non-empty branches.\nListObjects # Route: GET /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;/\nOnly S3\u0026rsquo;s list objects v1 is supported.\nPFS directories are represented via CommonPrefixes. This largely mirrors how S3 is used in practice, but leads to a couple of differences:\nIf you set the delimiter parameter, it must be /. Empty directories are included in listed results. With regard to listed results:\nDue to PFS peculiarities, the LastModified field references when the most recent commit to the branch happened, which may or may not have modified the specific object listed. The HTTP ETag field does not use MD5, but is a cryptographically secure hash of the file contents. The S3 StorageClass and Owner fields always have the same filler value. GetBucketLocation # Route: GET /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;/?location\nThis will always serve the same location for every bucket, but the endpoint is implemented to provide better compatibility with S3 clients.\nGetBucketVersioning # Route: GET /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;/?versioning\nThis will get whether versioning is enabled, which is always true.\nListMultipartUploads # Route: GET /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;/?uploads\nLists the in-progress multipart uploads in the given branch. The delimiter query parameter is not supported.\nCreateBucket # Route: PUT /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;/.\nIf the repo does not exist, it is created. If the branch does not exist, it is likewise created. As per S3\u0026rsquo;s behavior in some regions (but not all), trying to create the same bucket twice will return a BucketAlreadyOwnedByYou error.\nDeleteObjects # Route: POST /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;/?delete.\nDeletes multiple files specified in the request payload.\nDeleteObject # Route: DELETE /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;/\u0026lt;filepath\u0026gt;.\nDeletes the PFS file filepath in an atomic commit on the HEAD of branch.\nGetObject # Route: GET /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;/\u0026lt;filepath\u0026gt;.\nBy default, this request gets the HEAD version of the file. You can use s3\u0026rsquo;s versioning API to get the object at a non-HEAD commit by specifying either a specific commit ID, or by using the caret syntax \u0026ndash; for example, HEAD^.\nThere is support for range queries and conditional requests, however error response bodies for bad requests using these headers are not standard S3 XML.\nWith regard to HTTP response headers:\nDue to PFS peculiarities, the HTTP Last-Modified header references when the most recent commit to the branch happened, which may or may not have modified this specific object. The HTTP ETag does not use MD5, but is a cryptographically secure hash of the file contents. PutObject # Route: PUT /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;/\u0026lt;filepath\u0026gt;.\nWrites the PFS file at filepath in an atomic commit on the HEAD of branch.\nAny existing file content is overwritten. Unlike S3, there is no limit to upload size.\nUnlike s3, a 64mb max size is not enforced on this endpoint. Especially, as the file upload size gets larger, we recommend setting the Content-MD5 request header to ensure data integrity.\nAbortMultipartUpload # Route: DELETE /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;?uploadId=\u0026lt;uploadId\u0026gt;\nAborts an in-progress multipart upload.\nCompleteMultipartUpload # Route: POST /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;?uploadId=\u0026lt;uploadId\u0026gt;\nCompletes a multipart upload. If ETags are included in the request payload, they must be of the same format as returned by the S3 gateway when the multipart chunks are included. If they are md5 hashes or any other hash algorithm, they are ignored.\nCreateMultipartUpload # Route: POST /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;?uploads\nInitiates a multipart upload.\nListParts # Route: GET /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;?uploadId=\u0026lt;uploadId\u0026gt;\nLists the parts of an in-progress multipart upload.\nUploadPart # Route: PUT /\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;?uploadId=\u0026lt;uploadId\u0026gt;\u0026amp;partNumber=\u0026lt;partNumber\u0026gt;\nUploads a chunk of a multipart upload.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["s3"],
    "id": "5b1f90ea11780cf254dd16bc2f5b4e21"
  },
  {
    "title": "TLS (SSL, HTTPS)",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn how to deploy a cluster with Transport Layer Security (TLS).",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/tls/",
    "relURI": "/latest/set-up/tls/",
    "body": "Secure internet browser connections and transactions via data encryption by deploying Pachyderm with Transport Layer Security (TLS).\nBefore You Start # You must have admin control over the domain you wish to use. You must obtain a certificate from a trusted Certificate Authority (CA) such as: Let\u0026rsquo;s Encrypt HashiCorp Vault Venafi The .crt file you are using must contain the full certificate chain (root, intermediates, and leaf). üí° You can simplify this process by using a tool like Cert-Manager, which is a certificate controller for Kubernetes that obtains certificates from Issuers, ensures the certificates are valid, and attempts to renew certificates at a configured time before expiry.\nYou can install Cert-Manager using the following command:\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.10.1/cert-manager.yaml How to Deploy With TLS # 1. Create a TLS Secret # Open a terminal and navigate to the location of your generated .key and .crt files. Run the following command: kubectl create secret tls \u0026lt;name\u0026gt; --key=tls.key --cert=tls.crt Verify your certificate: kubectl get certificate 2. Enable TLS in Your Helm Chart Values. # Reference the certificate object in your helm chart by setting your TLS secret name in the proper TLS section.\nSetup Type: With Proxy Without Proxy proxy: tls: enabled: true secretName: \u0026#34;\u0026lt;the-secret-name-in-your-certificate-resource\u0026gt;\u0026#34; pachd: tls: enabled: true secretName: \u0026#34;\u0026lt;the-secret-name-in-your-certificate-resource\u0026gt;\u0026#34; For the Cert Manager users, the secret name should match the name set in your certificate resource.\nSelf-Signed \u0026amp; Custom Certificates # When using self signed certificates or custom certificate authority (instead of Lets Encrypt, HashiCorp Vault, or Venafi), you must set global.customCaCerts to true to add Pachyderm\u0026rsquo;s certificate and CA to the list of trusted authorities for console and enterprise.\nIf you are using a custom ca-signed cert, you must include the full certificate chain in the root.crt file.\n3. Connect to Pachyderm via SSL # After you deploy Pachyderm, to connect through pachctl by using a trusted certificate, you will need to set the pachd_address in the Pachyderm context with the cluster IP address that starts with grpcs://. You can do so by running the following command:\nSetup Type: With Proxy Without Proxy pachctl connect https://localhost:80 pachd: tls: enabled: true secretName: \u0026#34;\u0026lt;the-secret-name-in-your-certificate-resource\u0026gt;\u0026#34; ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["deployment"],
    "id": "8f5fdf0d5ad1080c85a3b42dec2d6eed"
  },
  {
    "title": "Tracing (Jaeger)",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Set Up",
    "description": "Learn how to trace requests with Jaeger when diagnosing slow cluster performance.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/tracing/",
    "relURI": "/latest/set-up/tracing/",
    "body": "Pachyderm has the ability to trace requests using Jaeger. This can be useful when diagnosing slow clusters.\nCollecting Traces # To use tracing in Pachyderm, complete the following steps:\nRun Jaeger in Kubernetes\nkubectl apply -f https://raw.githubusercontent.com/pachyderm/pachyderm/v2.6.1/etc/deploy/tracing/jaeger-all-in-one.yaml Point Pachyderm at Jaeger\nFor pachctl, run:\nexport JAEGER_ENDPOINT=localhost:14268 kubectl port-forward svc/jaeger-collector 14268 \u0026amp; # Collector service For pachd, run:\nkubectl delete po -l suite=pachyderm,app=pachd The port-forward command is necessary because pachctl sends traces to Jaeger (it actually initiates every trace), and reads the JAEGER_ENDPOINT environment variable for the address to which it will send the trace info.\nRestarting the pachd pod is necessary because pachd also sends trace information to Jaeger, but it reads the environment variables corresponding to the Jaeger service[1] on startup to find Jaeger (the Jaeger service is created by the jaeger-all-in-one.yaml manifest). Killing the pods restarts them, which causes them to connect to Jaeger.\nSend Pachyderm a traced request by setting the PACH_TRACE environment variable to \u0026ldquo;true\u0026rdquo; before running any pachctl command (note that JAEGER_ENDPOINT must also be set/exported):\nPACH_TRACE=true pachctl list job # for example Pachyderm does not recommend exporting PACH_TRACE because tracing calls can slow them down and make interesting traces hard to find in Jaeger. Therefore, you might want to set this variable for the specific calls you want to trace.\nHowever, Pachyderm\u0026rsquo;s client library reads this variable and implements the relevant tracing, so any binary that uses Pachyderm\u0026rsquo;s go client library can trace calls if these variables are set.\nView Traces # To view traces, run:\nkubectl port-forward svc/jaeger-query 16686:80 \u0026amp; # UI service Then, connect to localhost:16686 in your browser, and you should see all collected traces.\n‚ÑπÔ∏è See Also: Kubernetes Service Environment Variables\nTroubleshooting # If you see \u0026lt;trace-without-root-span\u0026gt;, this likely means that pachd has connected to Jaeger, but pachctl has not. Make sure that the JAEGER_ENDPOINT environment variable is set on your local machine, and that kubectl port-forward \u0026quot;po/${jaeger_pod}\u0026quot; 14268 is running.\nIf you see a trace appear in Jaeger with no subtraces, like so:\nThis might mean that pachd has not connected to Jaeger, but pachctl has. Restart the pachd pods after creating the Jaeger service in Kubernetes.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["logs", "jaeger"],
    "id": "fa4556712df4b4946d06b30ce65bba9e"
  },
  {
    "title": "Local Deploy",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Set Up",
    "description": "Learn how to install locally using your favorite container solution.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/local-deploy/",
    "relURI": "/latest/set-up/local-deploy/",
    "body": " What is a Local Installation? # A local installation means that you will allocate resources from your local machine (e.g., your laptop) to spin up a Kubernetes cluster to run Pachyderm. This installation method is not for a production setup, but is great for personal use, testing, and product exploration.\nWhich Guide Should I Use? # Both the Docker Desktop and Minikube installation guides support MacOS, Windows, and Linux. If this is your first time using Kubernetes, try Docker Desktop \u0026mdash; if you are experienced with Kubernetes, you can deploy using a variety of solutions not listed here (KinD, Rancher Desktop, Podman, etc.).\nüí° Binary Files (Advanced Users)\nYou can download the latest binary files from GitHub for a direct installation of pachctl and the mount-server.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["deployment"],
    "id": "229f733b178c19e8c7421278e65c6228"
  },
  {
    "title": "Docker Desktop",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Local Deploy",
    "description": "Learn how to install locally with Docker Desktop.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/local-deploy/docker/",
    "relURI": "/latest/set-up/local-deploy/docker/",
    "body": " Before You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:\nOpen a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each of the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; All installation steps after 1. Install Docker Desktop must be run through the WSL terminal (Ubuntu) and not in Powershell.\nYou are now ready to continue to Step 1.\nYou must have Homebrew installed. /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; 1. Install Docker Desktop # Install Docker Desktop for your machine. Navigate to Settings for Mac, Windows, or Linux. Adjust your resources (~4 CPUs and ~12GB Memory) Enable Kubernetes Select Apply \u0026amp; Restart. 2. Install Pachctl CLI # Operating System: MacOs, Windows, \u0026amp; Darwin Debian brew tap pachyderm/tap \u0026amp;\u0026amp; brew install pachyderm/tap/pachctl@2.6 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.6.1/pachctl_2.6.1_amd64.deb \u0026amp;\u0026amp; sudo dpkg -i /tmp/pachctl.deb 3. Install \u0026amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install PachD: Version: Community Edition Enterprise helm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:\nhelm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --set pachd.enterpriseLicenseKey=$(cat license.txt) --set ingress.host=localhost A mock user is created by default to get you started, with the username: admin and password: password.\nThis may take several minutes to complete.\n4. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 5. Connect to Cluster # pachctl connect http://localhost:80 ‚ÑπÔ∏è If the connection commands did not work together, run each separately.\nOptionally open your browser and navigate to the Console UI.\nüí° You can check your Pachyderm version and connection to pachd at any time with the following command:\npachctl version COMPONENT VERSION pachctl 2.6.1 pachd 2.6.1 ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [
      "docker",
      "linux",
      "mac",
      "windows",
      "getting-started",
      "local-deploy"
    ],
    "id": "c74b7de66e798dcfa9d3d02bafd85133"
  },
  {
    "title": "Minikube",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Local Deploy",
    "description": "Learn how to install locally with Minikube.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/local-deploy/minikube/",
    "relURI": "/latest/set-up/local-deploy/minikube/",
    "body": "Minikube is a tool that quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. It\u0026rsquo;s a great solution for trying out Pachyderm locally.\nBefore You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; You must have Docker Desktop installed with Kubernetes enabled. You must have Docker Desktop installed with Kubernetes enabled. You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:\nOpen a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; All installation steps after this point must be run through the WSL terminal (Ubuntu) and not in Powershell.\nYou are now ready to continue to Step 1.\nYou must have Homebrew installed. /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; You must have Docker Desktop installed. 1. Install Docker # brew install docker See the official Docker getting started guide for the most up-to-date installation steps.\n2. Install \u0026amp; Start Minikube # Install # brew install minikube See the official Minikube getting started guide for the most up-to-date installation steps.\nStart # Launch Docker Desktop. Start Minikube: minikube start 3. Install Pachctl CLI # brew tap pachyderm/tap \u0026amp;\u0026amp; brew install pachyderm/tap/pachctl@2.6 4. Install \u0026amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install Pachyderm: Version: Community Edition Enterprise helm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:\nhelm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --set pachd.enterpriseLicenseKey=$(cat license.txt) --set ingress.host=localhost This unlocks Enterprise features but also requires user authentication to access Console. A mock user is created by default to get you started, with the username: admin and password: password.\nThis may take several minutes to complete.\n5. Verify Installation # Run the following command in a new terminal to check the status of your pods: kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE default console-6b9bb8766d-f2gm4 1/1 Running 0 41m default etcd-0 1/1 Running 0 41m default pachd-76896d6b5d-kmfvw 1/1 Running 0 41m default pachd-loki-0 1/1 Running 0 41m default pachd-promtail-rm5ss 1/1 Running 0 41m default pachyderm-kube-event-tail-b9b554fb6-dpcsr 1/1 Running 0 41m default pg-bouncer-5c9494c678-z57qh 1/1 Running 0 41m default postgres-0 1/1 Running 0 41m kube-system coredns-6d4b75cb6d-jnl5f 1/1 Running 3 (42m ago) 97d kube-system etcd-minikube 1/1 Running 4 (42m ago) 97d kube-system kube-apiserver-minikube 1/1 Running 3 (42m ago) 97d kube-system kube-controller-manager-minikube 1/1 Running 4 (42m ago) 97d kube-system kube-proxy-bngzv 1/1 Running 3 (42m ago) 97d kube-system kube-scheduler-minikube 1/1 Running 3 (42m ago) 97d kube-system storage-provisioner 1/1 Running 5 (42m ago) 97d kubernetes-dashboard dashboard-metrics-scraper-78dbd9dbf5-swttf 1/1 Running 3 (42m ago) 97d kubernetes-dashboard kubernetes-dashboard-5fd5574d9f-c7ptx 1/1 Running 4 (42m ago) 97d Re-run this command after a few minutes if pachd is not ready. 6. Connect to Cluster # pachctl connect http://localhost:80 ‚ÑπÔ∏è If the connection commands did not work together, run each separately.\nOptionally open your browser and navigate to the Console UI.\nüí° You can check your Pachyderm version and connection to pachd at any time with the following command:\npachctl version COMPONENT VERSION pachctl 2.6.1 pachd 2.6.1 ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [
      "minikube",
      "getting-started",
      "local-deploy",
      "linux",
      "mac",
      "windows"
    ],
    "id": "91b4aedda821f2202d0649c99fd9ec6b"
  },
  {
    "title": "On-Prem Deploy",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Set Up",
    "description": "Learn how to install on your premises.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/set-up/on-prem/",
    "relURI": "/latest/set-up/on-prem/",
    "body": " Before you start # Before you can deploy Pachyderm, you will need to perform the following actions:\nInstall kubectl Install Helm Deploy Kubernetes on-premises. Deploy two Kubernetes persistent volumes for Pachyderm metadata storage. Deploy an on-premises object store using a storage provider like MinIO, EMC\u0026rsquo;s ECS, or SwiftStack to provide S3-compatible access to your data storage. How to Deploy Pachyderm On-Premises # 1. Install Pachyderm via Helm # helm repo add pachyderm https://helm.pachyderm.com helm repo update 2. Add Storage classes to Helm Values # Update your Helm values file to include the storage classes you are going to use:\netcd: storageClass: MyStorageClass size: 10Gi postgresql: persistence: storageClass: MyStorageClass size: 10Gi 3. Size \u0026amp; Configure Object Store # Determine the endpoint of your object store, for example minio-server:9000. Choose a unique name for the bucket you will dedicate to Pachyderm. Create a new access key ID and secret key for Pachyderm to use when accessing the object store. Update the Pachyderm Helm values file with the endpoint, bucket name, access key ID, and secret key. pachd: storage: backend: minio minio: endpoint: minio-server:9000 bucket: pachyderm-bucket id: pachyderm-access-key secret: pachyderm-secret-key secure: false üí° You can update your Helm values file using the following command:\nhelm upgrade pachyderm pachyderm/pachyderm -f values.yml 4. Install PachCTL # Install PachCTL and PachCTL Auto-completion.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["deployment"],
    "id": "848da01f077dfda9db8d1d34e3d59b03"
  },
  {
    "title": "Manage",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Manage your instance, its GPUs, backups, and more.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/",
    "relURI": "/latest/manage/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "47120c47e2a031b8a8db31dc2f3fa462"
  },
  {
    "title": "Helm Chart Values (HCVs)",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Manage",
    "description": "Learn about the configurable helm chart attributes available.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/",
    "relURI": "/latest/manage/helm-values/",
    "body": "Explore all of the available Helm chart values for Pachyderm in this section.\n‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["configuration", "helm", "helm chart"],
    "id": "586257f10592d26975472d6ad35f37d6"
  },
  {
    "title": "Deploy Target HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Choose where you're deploying (Local, Cloud).",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/deploy-target/",
    "relURI": "/latest/manage/helm-values/deploy-target/",
    "body": " About # The Deploy Target section defines where you\u0026rsquo;re deploying Pachyderm; this is typically located at the top of your values.yaml file.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Amazon Custom Google Local Microsoft Minio # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: \u0026#34;AMAZON\u0026#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: \u0026#34;CUSTOM\u0026#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: \u0026#34;GOOGLE\u0026#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: \u0026#34;LOCAL\u0026#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: \u0026#34;MICROSOFT\u0026#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: \u0026#34;MINIO\u0026#34; ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "1dd93627f5e01ff949bf20959c08cf95"
  },
  {
    "title": "Global HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Configure the postgresql database connection.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/global/",
    "relURI": "/latest/manage/helm-values/global/",
    "body": " About # The Global section configures the connection to the PostgreSQL database. By default, it uses the included Postgres service.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: With Secrets Without Secrets global: postgresql: postgresqlAuthType: \u0026#34;md5\u0026#34; # sets the auth type used with postgres \u0026amp; pg-bounder; options include \u0026#34;md5\u0026#34; and \u0026#34;scram-sha-256\u0026#34; postgresqlUsername: \u0026#34;pachyderm\u0026#34; # defines the username to access the pachyderm and dex databases postgresqlExistingSecretName: \u0026#34;\u0026#34; # leave blank if using password postgresqlExistingSecretKey: \u0026#34;\u0026#34; # leave blank if using password postgresqlDatabase: \u0026#34;pachyderm\u0026#34; # defines the database name where pachyderm data will be stored postgresqlHost: \u0026#34;postgres\u0026#34; # defines the postgresql database host to connect to postgresqlPort: \u0026#34;5432\u0026#34; # defines he postgresql database port to connect to postgresqlSSL: \u0026#34;disable\u0026#34; # defines the SSL mode used to connect pg-bouncer to postgrs postgresqlSSLCACert: \u0026#34;\u0026#34; # defines the CA Certificate required to connect to Postgres postgresqlSSLSecret: \u0026#34;\u0026#34; # defines the TLS Secret with cert/key to connect to Postgres identityDatabaseFullNameOverride: \u0026#34;\u0026#34; # defines the DB name that dex connects to; defaults to \u0026#34;Dex\u0026#34; imagePullSecrets: [] # allows you to pull images from private repositories; also added to pipeline workers # Example: # imagePullSecrets: # - regcred customCaCerts: false # loads the cert file in pachd-tls-cert as the root cert for pachd, console, and enterprise-server proxy: \u0026#34;\u0026#34; # sets server address for outbound cluster traffic noProxy: \u0026#34;\u0026#34; # if proxy is set, allows a comma-separated list of destinations that bypass the proxy securityContexts: # set security context runAs users. If running on openshift, set enabled to false as openshift creates its own contexts. enabled: true global: postgresql: postgresqlAuthType: \u0026#34;md5\u0026#34; # sets the auth type used with postgres \u0026amp; pg-bounder; options include \u0026#34;md5\u0026#34; and \u0026#34;scram-sha-256\u0026#34; postgresqlUsername: \u0026#34;pachyderm\u0026#34; # defines the username to access the pachyderm and dex databases postgresqlPostgresPassword: \u0026#34;insecure-root-password\u0026#34; # leave blank if using a secret postgresqlDatabase: \u0026#34;pachyderm\u0026#34; # defines the database name where pachyderm data will be stored postgresqlHost: \u0026#34;postgres\u0026#34; # defines the postgresql database host to connect to postgresqlPort: \u0026#34;5432\u0026#34; # defines he postgresql database port to connect to postgresqlSSL: \u0026#34;disable\u0026#34; # defines the SSL mode used to connect pg-bouncer to postgrs postgresqlSSLCACert: \u0026#34;\u0026#34; # defines the CA Certificate required to connect to Postgres postgresqlSSLSecret: \u0026#34;\u0026#34; # defines the TLS Secret with cert/key to connect to Postgres identityDatabaseFullNameOverride: \u0026#34;\u0026#34; # defines the DB name that dex connects to; defaults to \u0026#34;Dex\u0026#34; imagePullSecrets: [] # allows you to pull images from private repositories; also added to pipeline workers customCaCerts: false # loads the cert file in pachd-tls-cert as the root cert for pachd, console, and enterprise-server proxy: \u0026#34;\u0026#34; # sets server address for outbound cluster traffic noProxy: \u0026#34;\u0026#34; # if proxy is set, allows a comma-separated list of destinations that bypass the proxy # Set security context runAs users. If running on openshift, set enabled to false as openshift creates its own contexts securityContexts: enabled: true ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "a6d8d52aa8b7f91c5b389b04e3aed62d"
  },
  {
    "title": "Console HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Configure the platform's UI.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/console/",
    "relURI": "/latest/manage/helm-values/console/",
    "body": " About # Console is the Graphical User Interface (GUI) for Pachyderm. Users that would prefer to navigate and manage through their project resources visually can connect to Console by authenticating against your configured OIDC. For personal-machine installations of Pachyderm, a user may access Console without authentication via localhost.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Enabled No Metrics Enabled With Metrics Disabled console: enabled: true # deploys Console UI annotations: {} image: # defines which image to use for the console; replicates the --console-image \u0026amp; --registry arguments to pachctl repository: \u0026#34;pachyderm/haberdashery\u0026#34; # defines image repo location pullPolicy: \u0026#34;IfNotPresent\u0026#34; tag: \u0026#34;2.3.3-1\u0026#34; # defines the image repo to pull from priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} tolerations: [] podLabels: {} # specifies labels to add to the console pod. resources: # specifies the resource request and limits; unset by default. {} #limits: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; #requests: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; config: # defines primary configuration settings, including authentication. reactAppRuntimeIssuerURI: \u0026#34;\u0026#34; # defines the pachd oauth address accessible to outside clients. oauthRedirectURI: \u0026#34;\u0026#34; # defines the oauth callback address within console that the pachd oauth service would redirect to. oauthClientID: \u0026#34;console\u0026#34; # defines the client identifier for the Console with pachd oauthClientSecret: \u0026#34;\u0026#34; # defines the secret configured for the client with pachd; if blank, autogenerated. oauthClientSecretSecretName: \u0026#34;\u0026#34; # uses the value of an existing k8s secret by pulling from the `OAUTH_CLIENT_SECRET` key. graphqlPort: 4000 # defines the http port that the console service will be accessible on. pachdAddress: \u0026#34;pachd-peer:30653\u0026#34; disableTelemetry: false # disables analytics and error data collection service: annotations: {} labels: {} # specifies labels to add to the console service. type: ClusterIP # specifies the Kubernetes type of the console service; default is `ClusterIP`. console: enabled: true # deploys Console UI annotations: {} image: # defines which image to use for the console; replicates the --console-image \u0026amp; --registry arguments to pachctl repository: \u0026#34;pachyderm/haberdashery\u0026#34; # defines image repo location pullPolicy: \u0026#34;IfNotPresent\u0026#34; tag: \u0026#34;2.3.3-1\u0026#34; # defines the image repo to pull from priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} tolerations: [] podLabels: {} # specifies labels to add to the console pod. resources: # specifies the resource request and limits; unset by default. {} #limits: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; #requests: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; config: # defines primary configuration settings, including authentication. reactAppRuntimeIssuerURI: \u0026#34;\u0026#34; # defines the pachd oauth address accessible to outside clients. oauthRedirectURI: \u0026#34;\u0026#34; # defines the oauth callback address within console that the pachd oauth service would redirect to. oauthClientID: \u0026#34;console\u0026#34; # defines the client identifier for the Console with pachd oauthClientSecret: \u0026#34;\u0026#34; # defines the secret configured for the client with pachd; if blank, autogenerated. oauthClientSecretSecretName: \u0026#34;\u0026#34; # uses the value of an existing k8s secret by pulling from the `OAUTH_CLIENT_SECRET` key. graphqlPort: 4000 # defines the http port that the console service will be accessible on. pachdAddress: \u0026#34;pachd-peer:30653\u0026#34; disableTelemetry: true # disables analytics and error data collection service: annotations: {} labels: {} # specifies labels to add to the console service. type: ClusterIP # specifies the Kubernetes type of the console service; default is `ClusterIP`. console: enabled: false ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "a58f4c940b4ac9c429b47a300142c49d"
  },
  {
    "title": "Enterprise Server HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Configure the Enterprise Server for production deployments.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/enterprise-server/",
    "relURI": "/latest/manage/helm-values/enterprise-server/",
    "body": " About # Enterprise Server is a production management layer that centralizes the licensing registration of multiple Pachyderm clusters for Enterprise use and the setup of user authorization/authentication via OIDC.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: TLS Disabled TLS New Secret TLS Existing Secret ES Disabled enterpriseServer: enabled: true affinity: {} annotations: {} tolerations: [] priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} service: type: ClusterIP apiGRPCPort: 31650 prometheusPort: 31656 oidcPort: 31657 identityPort: 31658 s3GatewayPort: 31600 tls: enabled: false resources: {} #limits: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; #requests: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; podLabels: {} # specifies labels to add to the pachd pod. clusterDeploymentID: \u0026#34;\u0026#34; image: repository: \u0026#34;pachyderm/pachd\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; tag: \u0026#34;\u0026#34; # defaults to the chart‚Äôs specified appVersion. enterpriseServer: enabled: true affinity: {} annotations: {} tolerations: [] priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} service: type: ClusterIP apiGRPCPort: 31650 prometheusPort: 31656 oidcPort: 31657 identityPort: 31658 s3GatewayPort: 31600 tls: enabled: true newSecret: create: true crt: \u0026#34;\u0026#34; key: \u0026#34;\u0026#34; resources: {} #limits: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; #requests: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; podLabels: {} # specifies labels to add to the pachd pod. clusterDeploymentID: \u0026#34;\u0026#34; image: repository: \u0026#34;pachyderm/pachd\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; tag: \u0026#34;\u0026#34; # defaults to the chart‚Äôs specified appVersion. enterpriseServer: enabled: true affinity: {} annotations: {} tolerations: [] priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} service: type: ClusterIP apiGRPCPort: 31650 prometheusPort: 31656 oidcPort: 31657 identityPort: 31658 s3GatewayPort: 31600 tls: enabled: true secretName: \u0026#34;\u0026#34; resources: {} #limits: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; #requests: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; podLabels: {} # specifies labels to add to the pachd pod. clusterDeploymentID: \u0026#34;\u0026#34; image: repository: \u0026#34;pachyderm/pachd\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; tag: \u0026#34;\u0026#34; # defaults to the chart‚Äôs specified appVersion. enterpriseServer: enabled: false ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "a8489a97563a0515c49ab63a3ed61884"
  },
  {
    "title": "ETCD HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Configure your ETCD key-value storage cluster.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/etcd/",
    "relURI": "/latest/manage/helm-values/etcd/",
    "body": " About # The ETCD section configures the ETCD cluster in the deployment.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\netcd: affinity: {} annotations: {} dynamicNodes: 1 # sets the number of nodes in the etcd StatefulSet; analogous to the --dynamic-etcd-nodes argument to pachctl image: repository: \u0026#34;pachyderm/etcd\u0026#34; tag: \u0026#34;v3.5.1\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; maxTxnOps: 10000 # sets the --max-txn-ops in the container args priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} podLabels: {} # specifies labels to add to the etcd pod. resources: # specifies the resource request and limits {} #limits: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; #requests: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; storageClass: \u0026#34;\u0026#34; # defines what existing storage class to use; analogous to --etcd-storage-class argument to pachctl storageSize: 10Gi # specifies the size of the volume to use for etcd. service: annotations: {} # specifies annotations to add to the etcd service. labels: {} # specifies labels to add to the etcd service. type: ClusterIP # specifies the Kubernetes type of the etcd service. tolerations: [] ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "b33df9816c2b483b5ab1e37aad44a5cc"
  },
  {
    "title": "Ingress HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "This section is being deprecated; use proxy instead.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/ingress/",
    "relURI": "/latest/manage/helm-values/ingress/",
    "body": " About # ‚ö†Ô∏è ingress will be removed from the helm chart once the deployment of Pachyderm with a proxy becomes mandatory.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: TLS Existing Secret TLS New Secret TLS Disabled ingress: enabled: true annotations: {} host: \u0026#34;\u0026#34; uriHttpsProtoOverride: false # if true, adds the https protocol to the ingress URI routes without configuring certs tls: enabled: true secretName: \u0026#34;\u0026#34; ingress: enabled: true annotations: {} host: \u0026#34;\u0026#34; uriHttpsProtoOverride: false # if true, adds the https protocol to the ingress URI routes without configuring certs tls: enabled: true newSecret: create: true crt: \u0026#34;\u0026#34; key: \u0026#34;\u0026#34; ingress: enabled: true annotations: {} host: \u0026#34;\u0026#34; uriHttpsProtoOverride: false # if true, adds the https protocol to the ingress URI routes without configuring certs tls: enabled: false ",
    "beta": "<no value>",
    "hidden": "true",
    "categories": [],
    "tags": ["helm"],
    "id": "1a713319c511366b7901de134c77c8b1"
  },
  {
    "title": "Loki HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Set up logs with Loki, Grafana, and Promtail.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/loki/",
    "relURI": "/latest/manage/helm-values/loki/",
    "body": " About # Loki Stack contains values that are passed to the loki-stack subchart. For more details on each service, see their official documentation:\nLoki storage documentation Grafana documentation Promtail documentation Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nloki-stack: loki: serviceAccount: automountServiceAccountToken: false persistence: enabled: true accessModes: - ReadWriteOnce size: 10Gi # More info for setting up storage classes on various cloud providers: # AWS: https://docs.aws.amazon.com/eks/latest/userguide/storage-classes.html # GCP: https://cloud.google.com/compute/docs/disks/performance#disk_types # Azure: https://docs.microsoft.com/en-us/azure/aks/concepts-storage#storage-classes storageClassName: \u0026#34;\u0026#34; annotations: {} priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} tolerations: [] config: limits_config: retention_period: 24h retention_stream: - selector: \u0026#39;{suite=\u0026#34;pachyderm\u0026#34;}\u0026#39; priority: 1 period: 168h # = 1 week grafana: enabled: false promtail: config: clients: - url: \u0026#34;http://{{ .Release.Name }}-loki:3100/loki/api/v1/push\u0026#34; snippets: # The scrapeConfigs section is copied from loki-stack-2.6.4 # The pipeline_stages.match stanza has been added to prevent multiple lokis in a cluster from mixing their logs. scrapeConfigs: | - job_name: kubernetes-pods pipeline_stages: {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }} - match: selector: \u0026#39;{namespace!=\u0026#34;{{ .Release.Namespace }}\u0026#34;}\u0026#39; action: drop kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: - __meta_kubernetes_pod_controller_name regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})? action: replace target_label: __tmp_controller_name - source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_name - __meta_kubernetes_pod_label_app - __tmp_controller_name - __meta_kubernetes_pod_name regex: ^;*([^;]+)(;.*)?$ action: replace target_label: app - source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_instance - __meta_kubernetes_pod_label_release regex: ^;*([^;]+)(;.*)?$ action: replace target_label: instance - source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_component - __meta_kubernetes_pod_label_component regex: ^;*([^;]+)(;.*)?$ action: replace target_label: component {{- if .Values.config.snippets.addScrapeJobLabel }} - replacement: kubernetes-pods target_label: scrape_job {{- end }} {{- toYaml .Values.config.snippets.common | nindent 4 }} {{- with .Values.config.snippets.extraRelabelConfigs }} {{- toYaml . | nindent 4 }} {{- end }} pipelineStages: - cri: {} common: # This is copy and paste of existing actions, so we don\u0026#39;t lose them. # Cf. https://github.com/grafana/loki/issues/3519#issuecomment-1125998705 - action: replace source_labels: - __meta_kubernetes_pod_node_name target_label: node_name - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace replacement: $1 separator: / source_labels: - namespace - app target_label: job - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - action: replace replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - action: replace regex: true/(.*) replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash - __meta_kubernetes_pod_container_name target_label: __path__ - action: keep regex: pachyderm source_labels: - __meta_kubernetes_pod_label_suite # this gets all kubernetes labels as well - action: labelmap regex: __meta_kubernetes_pod_label_(.+) livenessProbe: failureThreshold: 5 tcpSocket: port: http-metrics initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "46cbe906527cbeaf91e526b7a4d54294"
  },
  {
    "title": "PachD HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Configure the core settings.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/pachd/",
    "relURI": "/latest/manage/helm-values/pachd/",
    "body": " Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: With Secrets Without Secrets pachd: enabled: true preflightChecks: enabled: true # runs kube validation preflight checks. affinity: {} annotations: {} clusterDeploymentID: \u0026#34;\u0026#34; # sets Pachyderm cluster ID. configJob: annotations: {} goMaxProcs: 0 # passed as GOMAXPROCS to the pachd container. image: repository: \u0026#34;pachyderm/pachd\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; tag: \u0026#34;\u0026#34; # sets worker image tag; defaults to appVersion. logFormat: \u0026#34;json\u0026#34; logLevel: \u0026#34;info\u0026#34; lokiDeploy: true lokiLogging: true metrics: enabled: true endpoint: \u0026#34;\u0026#34; # provide the URL of the metrics endpoint. priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} podLabels: {} # adds labels to the pachd pod. replicas: 1 # sets the number of pachd running pods resources: # specifies the resource requests \u0026amp; limits {} #limits: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; #requests: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; requireCriticalServersOnly: false externalService: enabled: false # Creates a service that\u0026#39;s safe to expose. loadBalancerIP: \u0026#34;\u0026#34; apiGRPCPort: 30650 s3GatewayPort: 30600 annotations: {} service: labels: {} # adds labels to the pachd service. type: \u0026#34;ClusterIP\u0026#34; # specifies pachd service\u0026#39;s Kubernetes type annotations: {} apiGRPCPort: 30650 prometheusPort: 30656 oidcPort: 30657 identityPort: 30658 s3GatewayPort: 30600 #apiGrpcPort: # expose: true # port: 30650 activateEnterpriseMember: false # connects to an existing enterprise server. activateAuth: true # bootstraps auth via the config job. enterpriseLicenseKey: \u0026#34;\u0026#34; # activates enterprise if provided. enterpriseLicenseKeySecretName: \u0026#34;\u0026#34; # pulls value from k8s secret key \u0026#34;enterprise-license-key\u0026#34; rootToken: \u0026#34;\u0026#34; # autogenerated if not provided; stored in k8s secret \u0026#34;pachyderm-bootstrap-config.rootToken\u0026#34; rootTokenSecretName: \u0026#34;\u0026#34; # passes rooToken value from k8s secret key \u0026#34;root-token\u0026#34; enterpriseSecret: \u0026#34;\u0026#34; # autogenerated if not provided; stored in k8s secret \u0026#34;pachyderm-bootstrap-config.enterpriseSecret\u0026#34; enterpriseSecretSecretName: \u0026#34;\u0026#34; # passes value from k8s secret key \u0026#34;enterprise-secret\u0026#34; oauthClientID: pachd oauthClientSecret: \u0026#34;\u0026#34; # autogenerated if not provided; stored in k8s secret \u0026#34;pachyderm-bootstrap-config.authConfig.clientSecret\u0026#34; oauthClientSecretSecretName: \u0026#34;\u0026#34; # passes value from k8s secret key \u0026#34;pachd-oauth-client-secret\u0026#34; oauthRedirectURI: \u0026#34;\u0026#34; enterpriseServerToken: \u0026#34;\u0026#34; # authenticates to a enterprise server \u0026amp; registers this cluster as a member if activateEnterpriseMember is true. enterpriseServerTokenSecretName: \u0026#34;\u0026#34; # passes value from k8s secret key \u0026#34;enterprise-server-token\u0026#34; if activateEnterpriseMember is true. enterpriseServerAddress: \u0026#34;\u0026#34; enterpriseCallbackAddress: \u0026#34;\u0026#34; localhostIssuer: \u0026#34;\u0026#34; # Indicates to pachd whether dex is embedded in its process; \u0026#34;true\u0026#34;, \u0026#34;false\u0026#34;, or \u0026#34;\u0026#34; pachAuthClusterRoleBindings: {} # map initial users to their list of roles. # robot:wallie: # - repoReader # robot:eve: # - repoWriter additionalTrustedPeers: [] # configures identity service to recognize trusted peers. # - example-app serviceAccount: create: true additionalAnnotations: {} name: \u0026#34;pachyderm\u0026#34; storage: backend: \u0026#34;\u0026#34; # options: GOOGLE, AMAZON, MINIO, MICROSOFT or LOCAL amazon: bucket: \u0026#34;\u0026#34; # sets the S3 bucket to use. cloudFrontDistribution: \u0026#34;\u0026#34; # sets the CloudFront distribution in the storage secrets. customEndpoint: \u0026#34;\u0026#34; disableSSL: false id: \u0026#34;\u0026#34; # sets the Amazon access key ID logOptions: \u0026#34;\u0026#34; # case-sensitive comma-separated list: \u0026#39;Debug\u0026#39;, \u0026#39;Signing\u0026#39;, \u0026#39;HTTPBody\u0026#39;, \u0026#39;RequestRetries\u0026#39;, \u0026#39;EventStreamBody\u0026#39;, or \u0026#39;all\u0026#39; maxUploadParts: 10000 verifySSL: true partSize: \u0026#34;5242880\u0026#34; # sets part size for object storage uploads; must be a string. region: \u0026#34;\u0026#34; # sets AWS region retries: 10 reverse: true secret: \u0026#34;\u0026#34; # sets the Amazon secret access key to use. timeout: \u0026#34;5m\u0026#34; # sets the timeout for object storage requests. token: \u0026#34;\u0026#34; # sets the Amazon token to use. uploadACL: \u0026#34;bucket-owner-full-control\u0026#34; google: bucket: \u0026#34;\u0026#34; cred: \u0026#34;\u0026#34; # sets GCP service account private key as string. # cred: | # { # \u0026#34;type\u0026#34;: \u0026#34;service_account\u0026#34;, # \u0026#34;project_id\u0026#34;: \u0026#34;‚Ä¶\u0026#34;, # \u0026#34;private_key_id\u0026#34;: \u0026#34;‚Ä¶\u0026#34;, # \u0026#34;private_key\u0026#34;: \u0026#34;-----BEGIN PRIVATE KEY-----\\n‚Ä¶\\n-----END PRIVATE KEY-----\\n\u0026#34;, # \u0026#34;client_email\u0026#34;: \u0026#34;‚Ä¶@‚Ä¶.iam.gserviceaccount.com\u0026#34;, # \u0026#34;client_id\u0026#34;: \u0026#34;‚Ä¶\u0026#34;, # \u0026#34;auth_uri\u0026#34;: \u0026#34;https://accounts.google.com/o/oauth2/auth\u0026#34;, # \u0026#34;token_uri\u0026#34;: \u0026#34;https://oauth2.googleapis.com/token\u0026#34;, # \u0026#34;auth_provider_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/oauth2/v1/certs\u0026#34;, # \u0026#34;client_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/robot/v1/metadata/x509/‚Ä¶%40‚Ä¶.iam.gserviceaccount.com\u0026#34; # } local: hostPath: \u0026#34;\u0026#34; # path where PFS metadata is stored; must end with \u0026#34;/\u0026#34;. requireRoot: true # root required for hostpath, but we run rootless in CI microsoft: container: \u0026#34;\u0026#34; id: \u0026#34;\u0026#34; secret: \u0026#34;\u0026#34; minio: bucket: \u0026#34;\u0026#34; # sets bucket name. endpoint: \u0026#34;\u0026#34; # format: hostname:port id: \u0026#34;\u0026#34; # username/id with readwrite access to the bucket. secret: \u0026#34;\u0026#34; # the secret/password of the user with readwrite access to the bucket. secure: \u0026#34;false\u0026#34; # enables https for minio if \u0026#34;true\u0026#34; signature: \u0026#34;\u0026#34; # Enables S3v2 support by setting signature to \u0026#34;1\u0026#34;; being deprecated. putFileConcurrencyLimit: 100 # sets the maximum number of files to upload or fetch from remote sources uploadConcurrencyLimit sets the maximum number of concurrent; analogous to --put-file-concurrency-limit argument to pachctl uploadConcurrencyLimit: 100 # object storage uploads per Pachd instance; analogous to --upload-concurrency-limit argument to pachctl compactionShardSizeThreshold: 0 # the total size of the files in a shard. compactionShardCountThreshold: 0 # the total number of files in a shard. memoryThreshold: 0 levelFactor: 0 maxFanIn: 10 maxOpenFileSets: 50 # diskCacheSize and memoryCacheSize are defined in units of 8 Mb chunks. The default is 100 chunks which is 800 Mb. diskCacheSize: 100 memoryCacheSize: 100 ppsWorkerGRPCPort: 1080 storageGCPeriod: 0 # the number of seconds between PFS\u0026#39;s garbage collection cycles; \u0026lt;0 disables garbage collection; 0 defaults to pachyderm\u0026#39;s internal config. storageChunkGCPeriod: 0 # the number of seconds between chunk garbage collection cycles; \u0026lt;0 disables chunk garbage collection; 0 defaults to pachyderm\u0026#39;s internal config. # There are three options for TLS: # 1. Disabled # 2. Enabled, existingSecret, specify secret name # 3. Enabled, newSecret, must specify cert, key and name tls: enabled: false secretName: \u0026#34;\u0026#34; newSecret: create: false crt: \u0026#34;\u0026#34; key: \u0026#34;\u0026#34; tolerations: [] worker: image: repository: \u0026#34;pachyderm/worker\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; # Worker tag is set under pachd.image.tag (they should be kept in lock step) serviceAccount: create: true additionalAnnotations: {} name: \u0026#34;pachyderm-worker\u0026#34; # sets the name of the worker service account; analogous to --worker-service-account argument to pachctl. rbac: create: true # indicates whether RBAC resources should be created; analogous to --no-rbac to pachctl # Set up default resources for pipelines that don\u0026#39;t include any requests or limits. The values # are k8s resource quantities, so \u0026#34;1Gi\u0026#34;, \u0026#34;2\u0026#34;, etc. Set to \u0026#34;0\u0026#34; to disable setting any defaults. defaultPipelineCPURequest: \u0026#34;\u0026#34; defaultPipelineMemoryRequest: \u0026#34;\u0026#34; defaultPipelineStorageRequest: \u0026#34;\u0026#34; defaultSidecarCPURequest: \u0026#34;\u0026#34; defaultSidecarMemoryRequest: \u0026#34;\u0026#34; defaultSidecarStorageRequest: \u0026#34;\u0026#34; pachd: enabled: true preflightChecks: # if enabled runs kube validation preflight checks. enabled: true affinity: {} annotations: {} # clusterDeploymentID sets the Pachyderm cluster ID. clusterDeploymentID: \u0026#34;\u0026#34; configJob: annotations: {} # goMaxProcs is passed as GOMAXPROCS to the pachd container. goMaxProcs: 0 image: repository: \u0026#34;pachyderm/pachd\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; # tag defaults to the chart‚Äôs specified appVersion. # This sets the worker image tag as well (they should be kept in lock step) tag: \u0026#34;\u0026#34; logFormat: \u0026#34;json\u0026#34; logLevel: \u0026#34;info\u0026#34; # If lokiDeploy is true, a Pachyderm-specific instance of Loki will # be deployed. lokiDeploy: true # lokiLogging enables Loki logging if set. lokiLogging: true metrics: # enabled sets the METRICS environment variable if set. enabled: true # endpoint should be the URL of the metrics endpoint. endpoint: \u0026#34;\u0026#34; priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} # podLabels specifies labels to add to the pachd pod. podLabels: {} # resources specifies the resource requests and limits # replicas sets the number of pachd running pods replicas: 1 resources: {} #limits: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; #requests: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; # requireCriticalServersOnly only requires the critical pachd # servers to startup and run without errors. It is analogous to the # --require-critical-servers-only argument to pachctl deploy. requireCriticalServersOnly: false # If enabled, External service creates a service which is safe to # be exposed externally externalService: enabled: false # (Optional) specify the existing IP Address of the load balancer loadBalancerIP: \u0026#34;\u0026#34; apiGRPCPort: 30650 s3GatewayPort: 30600 annotations: {} service: # labels specifies labels to add to the pachd service. labels: {} # type specifies the Kubernetes type of the pachd service. type: \u0026#34;ClusterIP\u0026#34; annotations: {} apiGRPCPort: 30650 prometheusPort: 30656 oidcPort: 30657 identityPort: 30658 s3GatewayPort: 30600 #apiGrpcPort: # expose: true # port: 30650 # DEPRECATED: activateEnterprise is no longer used. activateEnterprise: false ## if pachd.activateEnterpriseMember is set, enterprise will be activated and connected to an existing enterprise server. ## if pachd.enterpriseLicenseKey is set, enterprise will be activated. activateEnterpriseMember: false ## if pachd.activateAuth is set, auth will be bootstrapped by the config-job. activateAuth: true ## the license key used to activate enterprise features enterpriseLicenseKey: \u0026#34;\u0026#34; rootToken: \u0026#34;\u0026#34; enterpriseSecret: \u0026#34;\u0026#34; # enterpriseSecretSecretName is used to pass the enterprise secret value via an existing k8s secret. # The value is pulled from the key, \u0026#34;enterprise-secret\u0026#34;. enterpriseSecretSecretName: \u0026#34;\u0026#34; # if a secret is not provided, a secret will be autogenerated on install and stored in the k8s secret \u0026#39;pachyderm-bootstrap-config.authConfig.clientSecret\u0026#39; oauthClientID: pachd oauthClientSecret: \u0026#34;\u0026#34; # oauthClientSecretSecretName is used to set the OAuth Client Secret via an existing k8s secret. # The value is pulled from the key, \u0026#34;pachd-oauth-client-secret\u0026#34;. oauthClientSecretSecretName: \u0026#34;\u0026#34; oauthRedirectURI: \u0026#34;\u0026#34; # DEPRECATED: enterpriseRootToken is deprecated, in favor of enterpriseServerToken # NOTE only used if pachd.activateEnterpriseMember == true enterpriseRootToken: \u0026#34;\u0026#34; # enterpriseServerToken represents a token that can authenticate to a separate Enterprise server, # and is used to complete the enterprise member registration process for this pachyderm cluster. # The user backing this token should have either the licenseAdmin \u0026amp; identityAdmin roles assigned, or # the clusterAdmin role. # NOTE: only used if pachd.activateEnterpriseMember == true enterpriseServerToken: \u0026#34;\u0026#34; # enterpriseServerTokenSecretName is used to pass the enterpriseServerToken value via an existing k8s secret. # The value is pulled from the key, \u0026#34;enterprise-server-token\u0026#34;. enterpriseServerTokenSecretName: \u0026#34;\u0026#34; # only used if pachd.activateEnterpriseMember == true enterpriseServerAddress: \u0026#34;\u0026#34; enterpriseCallbackAddress: \u0026#34;\u0026#34; # Indicates to pachd whether dex is embedded in its process. localhostIssuer: \u0026#34;\u0026#34; # \u0026#34;true\u0026#34;, \u0026#34;false\u0026#34;, or \u0026#34;\u0026#34; (used string as bool doesn\u0026#39;t support empty value) # set the initial pachyderm cluster role bindings, mapping a user to their list of roles # ex. # pachAuthClusterRoleBindings: # robot:wallie: # - repoReader # robot:eve: # - repoWriter pachAuthClusterRoleBindings: {} # additionalTrustedPeers is used to configure the identity service to recognize additional OIDC clients as trusted peers of pachd. # For example, see the following example or the dex docs (https://dexidp.io/docs/custom-scopes-claims-clients/#cross-client-trust-and-authorized-party). # additionalTrustedPeers: # - example-app additionalTrustedPeers: [] serviceAccount: create: true additionalAnnotations: {} name: \u0026#34;pachyderm\u0026#34; #TODO Set default in helpers / Wire up in templates storage: # backend configures the storage backend to use. It must be one # of GOOGLE, AMAZON, MINIO, MICROSOFT or LOCAL. This is set automatically # if deployTarget is GOOGLE, AMAZON, MICROSOFT, or LOCAL backend: \u0026#34;\u0026#34; amazon: # bucket sets the S3 bucket to use. bucket: \u0026#34;\u0026#34; # cloudFrontDistribution sets the CloudFront distribution in the # storage secrets. It is analogous to the # --cloudfront-distribution argument to pachctl deploy. cloudFrontDistribution: \u0026#34;\u0026#34; customEndpoint: \u0026#34;\u0026#34; # disableSSL disables SSL. It is analogous to the --disable-ssl # argument to pachctl deploy. disableSSL: false # id sets the Amazon access key ID to use. Together with secret # and token, it implements the functionality of the # --credentials argument to pachctl deploy. id: \u0026#34;\u0026#34; # logOptions sets various log options in Pachyderm‚Äôs internal S3 # client. Comma-separated list containing zero or more of: # \u0026#39;Debug\u0026#39;, \u0026#39;Signing\u0026#39;, \u0026#39;HTTPBody\u0026#39;, \u0026#39;RequestRetries\u0026#39;, # \u0026#39;RequestErrors\u0026#39;, \u0026#39;EventStreamBody\u0026#39;, or \u0026#39;all\u0026#39; # (case-insensitive). See \u0026#39;AWS SDK for Go\u0026#39; docs for details. # logOptions is analogous to the --obj-log-options argument to # pachctl deploy. logOptions: \u0026#34;\u0026#34; # maxUploadParts sets the maximum number of upload parts. It is # analogous to the --max-upload-parts argument to pachctl # deploy. maxUploadParts: 10000 # verifySSL performs SSL certificate verification. It is the # inverse of the --no-verify-ssl argument to pachctl deploy. verifySSL: true # partSize sets the part size for object storage uploads. It is # analogous to the --part-size argument to pachctl deploy. It # has to be a string due to Helm and YAML parsing integers as # floats. Cf. https://github.com/helm/helm/issues/1707 partSize: \u0026#34;5242880\u0026#34; # region sets the AWS region to use. region: \u0026#34;\u0026#34; # retries sets the number of retries for object storage # requests. It is analogous to the --retries argument to # pachctl deploy. retries: 10 # reverse reverses object storage paths. It is analogous to the # --reverse argument to pachctl deploy. reverse: true # secret sets the Amazon secret access key to use. Together with id # and token, it implements the functionality of the # --credentials argument to pachctl deploy. secret: \u0026#34;\u0026#34; # timeout sets the timeout for object storage requests. It is # analogous to the --timeout argument to pachctl deploy. timeout: \u0026#34;5m\u0026#34; # token optionally sets the Amazon token to use. Together with # id and secret, it implements the functionality of the # --credentials argument to pachctl deploy. token: \u0026#34;\u0026#34; # uploadACL sets the upload ACL for object storage uploads. It # is analogous to the --upload-acl argument to pachctl deploy. uploadACL: \u0026#34;bucket-owner-full-control\u0026#34; google: bucket: \u0026#34;\u0026#34; # cred is a string containing a GCP service account private key, # in object (JSON or YAML) form. A simple way to pass this on # the command line is with the set-file flag, e.g.: # # helm install pachd -f my-values.yaml --set-file storage.google.cred=creds.json pachyderm/pachyderm cred: \u0026#34;\u0026#34; # Example: # cred: | # { # \u0026#34;type\u0026#34;: \u0026#34;service_account\u0026#34;, # \u0026#34;project_id\u0026#34;: \u0026#34;‚Ä¶\u0026#34;, # \u0026#34;private_key_id\u0026#34;: \u0026#34;‚Ä¶\u0026#34;, # \u0026#34;private_key\u0026#34;: \u0026#34;-----BEGIN PRIVATE KEY-----\\n‚Ä¶\\n-----END PRIVATE KEY-----\\n\u0026#34;, # \u0026#34;client_email\u0026#34;: \u0026#34;‚Ä¶@‚Ä¶.iam.gserviceaccount.com\u0026#34;, # \u0026#34;client_id\u0026#34;: \u0026#34;‚Ä¶\u0026#34;, # \u0026#34;auth_uri\u0026#34;: \u0026#34;https://accounts.google.com/o/oauth2/auth\u0026#34;, # \u0026#34;token_uri\u0026#34;: \u0026#34;https://oauth2.googleapis.com/token\u0026#34;, # \u0026#34;auth_provider_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/oauth2/v1/certs\u0026#34;, # \u0026#34;client_x509_cert_url\u0026#34;: \u0026#34;https://www.googleapis.com/robot/v1/metadata/x509/‚Ä¶%40‚Ä¶.iam.gserviceaccount.com\u0026#34; # } local: # hostPath indicates the path on the host where the PFS metadata # will be stored. It must end in /. It is analogous to the # --host-path argument to pachctl deploy. hostPath: \u0026#34;\u0026#34; requireRoot: true #Root required for hostpath, but we run rootless in CI microsoft: container: \u0026#34;\u0026#34; id: \u0026#34;\u0026#34; secret: \u0026#34;\u0026#34; minio: # minio bucket name bucket: \u0026#34;\u0026#34; # the minio endpoint. Should only be the hostname:port, no http/https. endpoint: \u0026#34;\u0026#34; # the username/id with readwrite access to the bucket. id: \u0026#34;\u0026#34; # the secret/password of the user with readwrite access to the bucket. secret: \u0026#34;\u0026#34; # enable https for minio with \u0026#34;true\u0026#34; defaults to \u0026#34;false\u0026#34; secure: \u0026#34;\u0026#34; # Enable S3v2 support by setting signature to \u0026#34;1\u0026#34;. This feature is being deprecated signature: \u0026#34;\u0026#34; # putFileConcurrencyLimit sets the maximum number of files to # upload or fetch from remote sources (HTTP, blob storage) using # PutFile concurrently. It is analogous to the # --put-file-concurrency-limit argument to pachctl deploy. putFileConcurrencyLimit: 100 # uploadConcurrencyLimit sets the maximum number of concurrent # object storage uploads per Pachd instance. It is analogous to # the --upload-concurrency-limit argument to pachctl deploy. uploadConcurrencyLimit: 100 # The shard size corresponds to the total size of the files in a shard. # The shard count corresponds to the total number of files in a shard. # If either criteria is met, a shard will be created. compactionShardSizeThreshold: 0 compactionShardCountThreshold: 0 memoryThreshold: 0 levelFactor: 0 maxFanIn: 10 maxOpenFileSets: 50 # diskCacheSize and memoryCacheSize are defined in units of 8 Mb chunks. The default is 100 chunks which is 800 Mb. diskCacheSize: 100 ppsWorkerGRPCPort: 1080 # the number of seconds between PFS\u0026#39;s garbage collection cycles. # if this value is set to 0, it will default to pachyderm\u0026#39;s internal configuration. # if this value is less than 0, it will turn off garbage collection. storageGCPeriod: 0 # the number of seconds between chunk garbage collection cycles. # if this value is set to 0, it will default to pachyderm\u0026#39;s internal configuration. # if this value is less than 0, it will turn off chunk garbage collection. storageChunkGCPeriod: 0 # There are three options for TLS: # 1. Disabled # 2. Enabled, existingSecret, specify secret name # 3. Enabled, newSecret, must specify cert, key and name tls: enabled: false secretName: \u0026#34;\u0026#34; newSecret: create: false crt: \u0026#34;\u0026#34; key: \u0026#34;\u0026#34; tolerations: [] worker: image: repository: \u0026#34;pachyderm/worker\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; # Worker tag is set under pachd.image.tag (they should be kept in lock step) serviceAccount: create: true additionalAnnotations: {} # name sets the name of the worker service account. Analogous to # the --worker-service-account argument to pachctl deploy. name: \u0026#34;pachyderm-worker\u0026#34; #TODO Set default in helpers / Wire up in templates rbac: # create indicates whether RBAC resources should be created. # Setting it to false is analogous to passing --no-rbac to pachctl # deploy. create: true # Set up default resources for pipelines that don\u0026#39;t include any requests or limits. The values # are k8s resource quantities, so \u0026#34;1Gi\u0026#34;, \u0026#34;2\u0026#34;, etc. Set to \u0026#34;0\u0026#34; to disable setting any defaults. defaultPipelineCPURequest: \u0026#34;\u0026#34; defaultPipelineMemoryRequest: \u0026#34;\u0026#34; defaultPipelineStorageRequest: \u0026#34;\u0026#34; defaultSidecarCPURequest: \u0026#34;\u0026#34; defaultSidecarMemoryRequest: \u0026#34;\u0026#34; defaultSidecarStorageRequest: \u0026#34;\u0026#34; ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "71f294939ff0e4eb0c3153ddcd56fe3d"
  },
  {
    "title": "PachW HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Create a pool of pachd instances that dynamically scale storage task handling.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/pachw/",
    "relURI": "/latest/manage/helm-values/pachw/",
    "body": " About # PachW enables fine-grained control of where compaction and object-storage interaction occur by running storage tasks in a dedicated Kubernetes deployment. Users can configure PachW\u0026rsquo;s min and max replicas as well as define nodeSelectors, tolerations, and resource requests. Using PachW allows power users to save on costs by claiming fewer resources and running storage tasks on less expensive nodes.\n‚ö†Ô∏è If you are upgrading to 2.5.0+ for the first time and you wish to use PachW, you must calculate how many maxReplicas you need. By default, PachW is set to maxReplicas:1 \u0026mdash; however, that is not sufficient for production runs.\nmaxReplicas # You should set the maxReplicas value to at least match the number of pipeline replicas that you have. For high performance, we suggest taking the following approach:\nnumber of pipelines * highest parallelism spec * 1.5 = maxReplicas\nLet\u0026rsquo;s say you have 6 pipelines. One of these pipelines has a parallelism spec value of 6, and the rest are 5 or fewer.\n6 * 6 * 1.5 = 54\nminReplicas # Workloads that constantly process storage and compaction tasks because they are committing rapidly may want to increase minReplicas to have instances on standby.\nnodeSelectors # Workloads that utilize GPUs and other expensive resources may want to add a node selector to scope PachW instances to less expensive nodes.\nValues # Options: Enabled With Minimum With Specific Resources As Sidecars (Legacy) pachw: inheritFromPachd: true # defaults below configuration options like \u0026#39;resources\u0026#39; and \u0026#39;tolerations\u0026#39; to values from pachd maxReplicas: 1 minReplicas: 0 inSidecars: false #tolerations: [] #affinity: {} #nodeSelector: {} pachw: inheritFromPachd: true # defaults below configuration options like \u0026#39;resources\u0026#39; and \u0026#39;tolerations\u0026#39; to values from pachd maxReplicas: 6 # set to match the number of pipeline replicas you have; sample formula: pipeline count * parallelism = target maxReplicas minReplicas: 1 #tolerations: [] #affinity: {} #nodeSelector: {} #resources: # sets kubernetes resource configuration for pachw pods. If not defined, config from pachd is reused. We recommend defining resources when running pachw with a high value of maxReplicas (when formula is: target maxReplicas * 1.5). #limits: #cpu: \u0026#34;1\u0026#34; #memory: \u0026#34;2G\u0026#34; #requests: #cpu: \u0026#34;1\u0026#34; #memory: \u0026#34;2G\u0026#34; pachw: inheritFromPachd: false # defaults below configuration options like \u0026#39;resources\u0026#39; and \u0026#39;tolerations\u0026#39; to values from pachd maxReplicas: 6 # set to match the number of pipeline replicas you have; sample formula: pipeline count * parallelism = target maxReplicas minReplicas: 1 #tolerations: [] #affinity: {} #nodeSelector: {} resources: # sets kubernetes resource configuration for pachw pods. If not defined, config from pachd is reused. We recommend defining resources when running pachw with a high value of maxReplicas (when formula is: target maxReplicas * 1.5). limits: cpu: \u0026#34;1\u0026#34; memory: \u0026#34;2G\u0026#34; requests: cpu: \u0026#34;1\u0026#34; memory: \u0026#34;2G\u0026#34; pachw: inheritFromPachd: true # defaults below configuration options like \u0026#39;resources\u0026#39; and \u0026#39;tolerations\u0026#39; to values from pachd inSidecars: true # processes storage related tasks in pipeline storage sidecars like version 2.4.2 or less. maxReplicas: 1 #tolerations: [] #affinity: {} #nodeSelector: {} ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "e147ef3b0cb23cbdb197236d8997fbac"
  },
  {
    "title": "Kube Event Tail HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Deploy lightweight logging for Kubernetes events.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/kube-event-tail/",
    "relURI": "/latest/manage/helm-values/kube-event-tail/",
    "body": " About # Kube Event Tail deploys a lightweight app that watches Kubernetes events and echoes them into logs.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: All Events Namespace Events Disabled kubeEventTail: enabled: true clusterScope: false # if true, watches just events in its namespace image: repository: pachyderm/kube-event-tail pullPolicy: \u0026#34;IfNotPresent\u0026#34; tag: \u0026#34;v0.0.6\u0026#34; resources: limits: cpu: \u0026#34;1\u0026#34; memory: 100Mi requests: cpu: 100m memory: 45Mi kubeEventTail: enabled: true clusterScope: true # if true, watches just events in its namespace image: repository: pachyderm/kube-event-tail pullPolicy: \u0026#34;IfNotPresent\u0026#34; tag: \u0026#34;v0.0.6\u0026#34; resources: limits: cpu: \u0026#34;1\u0026#34; memory: 100Mi requests: cpu: 100m memory: 45Mi kubeEventTail: enabled: false ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "f8f07658fb58866424cae82db0c45c1b"
  },
  {
    "title": "PGBouncer HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Deploy a lightweight connection pooler for PostgreSQL.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/pgbouncer/",
    "relURI": "/latest/manage/helm-values/pgbouncer/",
    "body": " About # The PGBouncer section configures a PGBouncer Postgres connection pooler.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\npgbouncer: service: type: ClusterIP # defines the Kubernetes service type. annotations: {} priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} tolerations: [] image: repository: pachyderm/pgbouncer tag: 1.16.1-debian-10-r82 resources: # defines resources in standard kubernetes format; unset by default. {} #limits: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; #requests: # cpu: \u0026#34;1\u0026#34; # memory: \u0026#34;2G\u0026#34; maxConnections: 1000 # defines the maximum number of concurrent connections into pgbouncer. defaultPoolSize: 20 # specifies the maximum number of concurrent connections from pgbouncer to the postgresql database. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "abfac03a8fb0079aa0405af1b3fd400e"
  },
  {
    "title": "PostgreSQL Subchart HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Use the bundled version of PostgreSQL (metadata storage) for testing on your personal machine.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/postgressql/",
    "relURI": "/latest/manage/helm-values/postgressql/",
    "body": " About # The PostgresQL section controls the Bitnami PostgreSQL subchart. Pachyderm runs on Kubernetes, is backed by an object store of your choice, and comes with a bundled version of PostgreSQL (metadata storage) by default.\nWe recommended disabling this bundled PostgreSQL and using a managed database instance (such as RDS, CloudSQL, or PostgreSQL Server) for production environments.\nSee storage class details for your provider:\nAWS | Min: 500Gi (GP2) / 1,500 IOP GCP | Min: 50Gi / 1,500 IOPS Azure | Min: 256Gi / 1,100 IOPS Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Production Personal Machine postgresql: enabled: false # if false, you must specify a PostgreSQL database server connection @ global.postgresql postgresql: enabled: true # if false, you must specify a PostgreSQL database server connection @ global.postgresql image: tag: \u0026#34;13.3.0\u0026#34; # DEPRECATED from pachyderm 2.1.5 initdbScripts: dex.sh: | #!/bin/bash set -e psql -v ON_ERROR_STOP=1 --username \u0026#34;$POSTGRES_USER\u0026#34; --dbname \u0026#34;$POSTGRES_DB\u0026#34; \u0026lt;\u0026lt;-EOSQL CREATE DATABASE dex; GRANT ALL PRIVILEGES ON DATABASE dex TO \u0026#34;$POSTGRES_USER\u0026#34;; EOSQL fullnameOverride: postgres persistence: # Specify the storage class for the postgresql Persistent Volume (PV) storageClass: \u0026#34;\u0026#34; # specifies the size of the volume to use for postgresql size: 10Gi labels: suite: pachyderm primary: priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} tolerations: [] readReplicas: priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} tolerations: [] ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "fdb02efdd76913b66fae7b1426606a3b"
  },
  {
    "title": "CloudSQL Auth Proxy HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Deploy on GCP with CloudSQL.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/cloudsql-auth-proxy/",
    "relURI": "/latest/manage/helm-values/cloudsql-auth-proxy/",
    "body": " About # The CloudSQL Auth Proxy section configures the CloudSQL Auth Proxy for deploying Pachyderm on GCP with CloudSQL.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Enabled Disabled cloudsqlAuthProxy: connectionName: \u0026#34;\u0026#34; # may be found by running `gcloud sql instances describe INSTANCE_NAME --project PROJECT_ID` serviceAccount: \u0026#34;\u0026#34; # defines the account used to connect to the cloudSql instance iamLogin: false port: 5432 # the cloudql database port to expose. The default is `5432` enabled: true # controls whether to deploy the cloudsqlAuthProxy. Default is false. image: repository: \u0026#34;gcr.io/cloudsql-docker/gce-proxy\u0026#34; # the image repo to pull from; replicates --registry to pachctl pullPolicy: \u0026#34;IfNotPresent\u0026#34; tag: \u0026#34;1.23.0\u0026#34; # the image repo to pull from; replicates the --dash-image argument to pachctl deploy. priorityClassName: \u0026#34;\u0026#34; nodeSelector: {} tolerations: [] podLabels: {} # specifies labels to add to the dash pod. resources: {} # specifies the resource request and limits. # requests: # # The proxy\u0026#39;s memory use scales linearly with the number of active # # connections. Fewer open connections will use less memory. Adjust # # this value based on your application\u0026#39;s requirements. # memory: \u0026#34;\u0026#34; # # The proxy\u0026#39;s CPU use scales linearly with the amount of IO between # # the database and the application. Adjust this value based on your # # application\u0026#39;s requirements. # cpu: \u0026#34;\u0026#34; service: labels: {} # specifies labels to add to the cloudsql auth proxy service. type: ClusterIP # specifies the Kubernetes type of the cloudsql auth proxy service. The default is `ClusterIP`. cloudsqlAuthProxy: enabled: false ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm", "gcp"],
    "id": "08eb1e5abf1c6a8d59ef12ab40d12606"
  },
  {
    "title": "OpenID Connect HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Set up your OIDC authentication and connect to IDPs.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/oidc/",
    "relURI": "/latest/manage/helm-values/oidc/",
    "body": " About # The OIDC section of the helm chart enables you to set up authentication through upstream IDPs. To use authentication, you must have an Enterprise license.\nWe recommend setting up this section alongside the Enterprise Server section of your Helm chart so that you can easily scale multiple clusters using the same authentication configurations.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Mock IDP Upstream IDP Additional Clients oidc: issuerURI: \u0026#34;\u0026#34; # inferred if running locally or using proxy requireVerifiedEmail: false # if true, email verification is required to authenticate IDTokenExpiry: 24h # if set, specifies the duration where OIDC ID Tokens are valid; parsed into golang\u0026#39;s time.Duration: https://pkg.go.dev/time#example-ParseDuration RotationTokenExpiry: 48h # If set, enables OIDC rotation tokens, and specifies the duration where they are valid. userAccessibleOauthIssuerHost: \u0026#34;\u0026#34; # (Optional) Only set in cases where the issuerURI is not user accessible (ie. localhost install) mockIDP: true # if true, ignores upstreamIDPs in favor of a placeholder IDP with the username/password of \u0026#34;admin\u0026#34;/\u0026#34;password\u0026#34; oidc: issuerURI: \u0026#34;\u0026#34; # inferred if running locally or using proxy requireVerifiedEmail: false # if true, email verification is required to authenticate IDTokenExpiry: 24h # if set, specifies the duration where OIDC ID Tokens are valid; parsed into golang\u0026#39;s time.Duration: https://pkg.go.dev/time#example-ParseDuration RotationTokenExpiry: 48h # If set, enables OIDC rotation tokens, and specifies the duration where they are valid. userAccessibleOauthIssuerHost: \u0026#34;\u0026#34; # (Optional) Only set in cases where the issuerURI is not user accessible (ie. localhost install) upstreamIDPs: # defines a list of Identity Providers to use for authentication. https://dexidp.io/docs/connectors/ - id: idpConnector config: issuer: \u0026#34;\u0026#34; clientID: \u0026#34;\u0026#34; clientSecret: \u0026#34;\u0026#34; redirectURI: \u0026#34;http://localhost:30658/callback\u0026#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: idpConnector type: oidc - id: okta config: issuer: \u0026#34;https://dev-84362674.okta.com\u0026#34; clientID: \u0026#34;client_id\u0026#34; clientSecret: \u0026#34;notsecret\u0026#34; redirectURI: \u0026#34;http://localhost:30658/callback\u0026#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: okta type: oidc upstreamIDPsSecretName: \u0026#34;\u0026#34; # passes the upstreamIDPs value via an existing k8s secret (key: `upstream-idps`) dexCredentialSecretName: \u0026#34;\u0026#34; # mounts a credential file to the pachd pod at /dexcreds/ (e.g., serviceAccountFilePath: /dexcreds/googleAuth.json); required for some dex configs like Google. mockIDP: false # if true, ignores upstreamIDPs in favor of a placeholder IDP with the username/password of \u0026#34;admin\u0026#34;/\u0026#34;password\u0026#34; oidc: issuerURI: \u0026#34;\u0026#34; # inferred if running locally or using proxy requireVerifiedEmail: false # if true, email verification is required to authenticate IDTokenExpiry: 24h # if set, specifies the duration where OIDC ID Tokens are valid; parsed into golang\u0026#39;s time.Duration: https://pkg.go.dev/time#example-ParseDuration RotationTokenExpiry: 48h # If set, enables OIDC rotation tokens, and specifies the duration where they are valid. userAccessibleOauthIssuerHost: \u0026#34;\u0026#34; # (Optional) Only set in cases where the issuerURI is not user accessible (ie. localhost install) upstreamIDPs: # defines a list of Identity Providers to use for authentication. https://dexidp.io/docs/connectors/ - id: idpConnector config: issuer: \u0026#34;\u0026#34; clientID: \u0026#34;\u0026#34; clientSecret: \u0026#34;\u0026#34; redirectURI: \u0026#34;http://localhost:30658/callback\u0026#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: idpConnector type: oidc - id: okta config: issuer: \u0026#34;https://dev-84362674.okta.com\u0026#34; clientID: \u0026#34;client_id\u0026#34; clientSecret: \u0026#34;notsecret\u0026#34; redirectURI: \u0026#34;http://localhost:30658/callback\u0026#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: okta type: oidc upstreamIDPsSecretName: \u0026#34;\u0026#34; # passes the upstreamIDPs value via an existing k8s secret (key: `upstream-idps`) dexCredentialSecretName: \u0026#34;\u0026#34; # mounts a credential file to the pachd pod at /dexcreds/ (e.g., serviceAccountFilePath: /dexcreds/googleAuth.json); required for some dex configs like Google. mockIDP: false # if true, ignores upstreamIDPs in favor of a placeholder IDP with the username/password of \u0026#34;admin\u0026#34;/\u0026#34;password\u0026#34; additionalOIDCClient: - id: example-app secret: example-app-secret name: \u0026#39;Example App\u0026#39; redirectURIs: - \u0026#39;http://127.0.0.1:5555/callback\u0026#39; additionalClientsSecretName: \u0026#34;\u0026#34; ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "631cda3769260ff6a4c0d996961b097c"
  },
  {
    "title": "Test Connection HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Used by certain orgs to test connection during installation.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/test-connection/",
    "relURI": "/latest/manage/helm-values/test-connection/",
    "body": " About # The Test Connection section is used by Pachyderm to test the connection during installation. This config is used by organizations that do not have permission to pull Docker images directly from the Internet, and instead need to mirror locally.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\ntestConnection: image: repository: alpine tag: latest ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["configuration", "helm"],
    "id": "9d602dfee724fc8147987c09ba05493d"
  },
  {
    "title": "Proxy HCVs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Helm Chart Values (HCVs)",
    "description": "Centralize all traffic on a single port that's safe to expose to the Internet.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/helm-values/proxy/",
    "relURI": "/latest/manage/helm-values/proxy/",
    "body": " About # Proxy is a service that handles all Pachyderm traffic (S3, Console, OIDC, Dex, GRPC) on a single port; It\u0026rsquo;s great for exposing directly to the Internet.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Load Balancer With Legacy Ports Node Port proxy: enabled: true host: \u0026#34;\u0026#34; # the external hostname (including port if nonstandard) that the proxy will be reachable at. replicas: 1 # each replica can handle 50,000 concurrent connections. There is an affinity rule to prefer scheduling the proxy pods on the same node as pachd, so a number here that matches the number of pachd replicas is a fine configuration. (Note that we don\u0026#39;t guarantee to keep the proxy\u0026lt;-\u0026gt;pachd traffic on-node or even in-region.) image: repository: \u0026#34;envoyproxy/envoy\u0026#34; tag: \u0026#34;v1.22.0\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; resources: requests: cpu: 100m memory: 512Mi limits: memory: 512Mi # proxy sheds traffic before using 500MB of RAM. labels: {} annotations: {} service: # configures the service that routes traffic to the proxy. type: LoadBalancer # can be ClusterIP, NodePort, or LoadBalancer. loadBalancerIP: \u0026#34;\u0026#34; # If the service is a LoadBalancer, you can specify the IP address to use; defaults to 80. httpPort: 80 # The port to serve plain HTTP traffic on. httpsPort: 443 # The port to serve HTTPS traffic on, if enabled below. annotations: {} labels: {} # adds labels to the service itself (not the selector!). tls: # Incompatible with legacy ports. enabled: false secretName: \u0026#34;\u0026#34; # must contain \u0026#34;tls.key\u0026#34; and \u0026#34;tls.crt\u0026#34; keys; generate with kubectl create secret tls \u0026lt;name\u0026gt; --key=tls.key --cert=tls.cert\u0026#34; secret: {} # generate the secret from values here. This is intended only for unit tests. proxy: enabled: true host: \u0026#34;\u0026#34; # the external hostname (including port if nonstandard) that the proxy will be reachable at. replicas: 1 # each replica can handle 50,000 concurrent connections. There is an affinity rule to prefer scheduling the proxy pods on the same node as pachd, so a number here that matches the number of pachd replicas is a fine configuration. (Note that we don\u0026#39;t guarantee to keep the proxy\u0026lt;-\u0026gt;pachd traffic on-node or even in-region.) image: repository: \u0026#34;envoyproxy/envoy\u0026#34; tag: \u0026#34;v1.22.0\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; resources: requests: cpu: 100m memory: 512Mi limits: memory: 512Mi # proxy sheds traffic before using 500MB of RAM. labels: {} annotations: {} service: # configures the service that routes traffic to the proxy. type: LoadBalancer # can be ClusterIP, NodePort, or LoadBalancer. loadBalancerIP: \u0026#34;\u0026#34; # If the service is a LoadBalancer, you can specify the IP address to use; defaults to 80. httpPort: 80 # The port to serve plain HTTP traffic on. httpsPort: 443 # The port to serve HTTPS traffic on, if enabled below. annotations: {} labels: {} # adds labels to the service itself (not the selector!). legacyPorts: # proxy can serve backend services on a numbered port if not set to 0. If this service is of type NodePort, the port numbers here will be used for the node port, and will need to be in the node port range. console: 0 # legacy 30080, conflicts with default httpNodePort grpc: 0 # legacy 30650 s3Gateway: 0 # legacy 30600 oidc: 0 # legacy 30657 identity: 0 # legacy 30658 metrics: 0 # legacy 30656 proxy: enabled: true host: \u0026#34;\u0026#34; # the external hostname (including port if nonstandard) that the proxy will be reachable at. replicas: 1 # each replica can handle 50,000 concurrent connections. There is an affinity rule to prefer scheduling the proxy pods on the same node as pachd, so a number here that matches the number of pachd replicas is a fine configuration. (Note that we don\u0026#39;t guarantee to keep the proxy\u0026lt;-\u0026gt;pachd traffic on-node or even in-region.) image: repository: \u0026#34;envoyproxy/envoy\u0026#34; tag: \u0026#34;v1.22.0\u0026#34; pullPolicy: \u0026#34;IfNotPresent\u0026#34; resources: requests: cpu: 100m memory: 512Mi limits: memory: 512Mi # proxy sheds traffic before using 500MB of RAM. labels: {} annotations: {} service: # configures the service that routes traffic to the proxy. type: NodePort # can be ClusterIP, NodePort, or LoadBalancer. httpPort: 80 # The port to serve plain HTTP traffic on. httpsPort: 443 # The port to serve HTTPS traffic on, if enabled below. httpNodePort: 30080 # If the service is a NodePort, you can specify the port to receive HTTP traffic on. httpsNodePort: 30443 annotations: {} labels: {} # adds labels to the service itself (not the selector!). legacyPorts: # proxy can serve backend services on a numbered port if not set to 0. If this service is of type NodePort, the port numbers here will be used for the node port, and will need to be in the node port range. console: 0 # legacy 30080, conflicts with default httpNodePort grpc: 0 # legacy 30650 s3Gateway: 0 # legacy 30600 oidc: 0 # legacy 30657 identity: 0 # legacy 30658 metrics: 0 # legacy 30656 ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["helm"],
    "id": "bb545431a174e5ecfd4127b901873e76"
  },
  {
    "title": "S3 Gateway",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Manage",
    "description": "Learn about the embedded S3 gateway, which is compatible with MinIO, AWS S3 CLI, and boto3.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/s3gateway/",
    "relURI": "/latest/manage/s3gateway/",
    "body": "Use the embedded S3 Gateway to send or receive data through the S3 protocol using object storage tooling such as Minio, boto3, or AWS s3 CLI. Operations available are similar to those officially documented for S3.\nS3 Gateway Syntax # The S3 gateway presents each branch from every Pachyderm repository as an S3 bucket. Buckets are represented via [\u0026lt;commit\u0026gt;.]\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;.\u0026lt;project\u0026gt;, with the commit being optional.\nThe master.foo.bar bucket corresponds to the master branch of the repo foo within the bar project. The be97b64f110643389f171eb64697d4e1.master.foo.bar bucket corresponds to the commit be97b64f110643389f171eb64697d4e1 on the master branch of the foo repo within the bar project. If auth is enabled, credentials must be passed with each S3 gateway endpoint as mentioned in S3 Client configuration.\nCommand Examples # The following command examples assume that you have upgraded to use the embedded proxy, which will become mandatory in future releases.\nPut Data Into Pachyderm Repo # Tool: S3 Client Pachctl CLI aws --endpoint-url \u0026lt;pachyderm-address\u0026gt; s3 cp myfile.csv s3://master.foo.bar pachctl put file data@master:/ -f myfile.csv --project bar Retrieve Data From Pachyderm Repo # Tool: S3 Client Pachctl CLI aws --endpoint-url \u0026lt;pachyderm-address\u0026gt; s3 cp s3://master.foo.bar/myfile.csv pachctl get file data@master:/myfile.csv --project bar Port Forwarding # You can pachctl port-forward to access the s3 gateway through the localhost:30600 endpoint, however, the Kubernetes port forwarder incurs substantial overhead and does not recover well from broken connections.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "5ec606a25c68cf601294ac9f46432d9e"
  },
  {
    "title": "AWS CLI",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway",
    "description": "Learn how to configure AWS CLI for the S3 Gateway",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/s3gateway/aws/",
    "relURI": "/latest/manage/s3gateway/aws/",
    "body": " Before You Start # You must have the AWS CLI installed Configuration Steps # Install the AWS CLI as described in the AWS documentation.\nVerify that the AWS CLI is installed:\naws --version Configure AWS CLI. Use the aws configure command to configure your credentials file: aws configure --profile \u0026lt;name-your-profile\u0026gt; # AWS Access Key ID: YOUR-PACHYDERM-AUTH-TOKEN # AWS Secret Access Key: YOUR-PACHYDERM-AUTH-TOKEN # Default region name: # Default output format [None]: ‚ÑπÔ∏è Note that the --profile flag (named profiles) is optional. If not used, your access information will be stored in the default profile.\nTo reference a given profile when using the S3 client, append --profile \u0026lt;name-your-profile\u0026gt; at the end of your command.\nVerify Setup # To verify your setup, you can check the list of filesystem objects on the master branch of your repository.\naws --endpoint-url http://\u0026lt;localhost_or_externalIP\u0026gt;:30600/ s3 ls s3://master.\u0026lt;repo\u0026gt;.\u0026lt;project\u0026gt; ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "fe396833b1ddfb23bee0c390eb277a09"
  },
  {
    "title": "Boto3",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway",
    "description": "Learn how to configure Boto for the S3 Gateway",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/s3gateway/boto3/",
    "relURI": "/latest/manage/s3gateway/boto3/",
    "body": " Before You Start # Before using Boto3, you need to set up authentication credentials for your AWS accountn using the AWS CLI.\nConfiguration Steps # Then follow the Using boto documentation starting with importing boto3 in your python file and creating your S3 resources.\nüìñ Find boto3 full documentation here.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "2085bfd364bcea3b3b8d95b4a6d40a11"
  },
  {
    "title": "Credentials",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway",
    "description": "Learn how to configure an S3 client.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/s3gateway/configure-s3client/",
    "relURI": "/latest/manage/s3gateway/configure-s3client/",
    "body": " Before You Start # You must configure an S3 Client (Boto3, AWS, MinIO) You must have authentication enabled. How to Set Your Credentials # Auth: Enabled Disabled Run the following command: more ~/.pachyderm/config.json Search for your session token: \u0026quot;session_token\u0026quot;: \u0026quot;your-session-token-value\u0026quot;. Make sure to fill both fields Access Key ID and Secret Access Key with that same value. You must set ACCESS_KEY_ID and SECRET_ACCESS_KEY to any matching, non-empty string.\nFor example, you could set both values to \u0026quot;x\u0026quot;.\nRobot Users # Depending on your use case, it might make sense to pass the credentials of a robot-user or another type of user altogether. Refer to the RBAC for more information.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "ab07126f203bac4480c53c7335d2ba37"
  },
  {
    "title": "MinIO",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway",
    "description": "Learn how to configure MinIO for the S3 Gateway",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/s3gateway/minio/",
    "relURI": "/latest/manage/s3gateway/minio/",
    "body": " Before You Start # Configuration Steps # Install the MinIO client as described on the MinIO download page.\nVerify that MinIO components are successfully installed by running the following command:\nminio version mc version System Response:\nVersion: 2019-07-11T19:31:28Z Release-tag: RELEASE.2019-07-11T19-31-28Z Commit-id: 31e5ac02bdbdbaf20a87683925041f406307cfb9 Set up the MinIO configuration file to use the S3 Gateway port 30600 for your host:\nvi ~/.mc/config.json You should see a configuration similar to the following. For a minikube deployment, verify the local configuration:\n\u0026#34;local\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://localhost:30600\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;YOUR-PACHYDERM-AUTH-TOKEN\u0026#34;, \u0026#34;secretKey\u0026#34;: \u0026#34;YOUR-PACHYDERM-AUTH-TOKEN\u0026#34;, \u0026#34;api\u0026#34;: \u0026#34;S3v4\u0026#34;, \u0026#34;lookup\u0026#34;: \u0026#34;auto\u0026#34; }, Verify Setup # Check the list of filesystem objects on the master branch of the repository raw_data.\nmc ls local/master.\u0026lt;repo\u0026gt;.\u0026lt;project\u0026gt; ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "05ecce84f077c8cd883dd540bd0d6ff4"
  },
  {
    "title": "Unsupported Operations",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway",
    "description": "Learn which S3 operations are not supported.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/s3gateway/unsupported-operations/",
    "relURI": "/latest/manage/s3gateway/unsupported-operations/",
    "body": "Some of the S3 operations are not yet supported by Pachyderm. If you run any of these operations, Pachyderm returns a standard S3 NotImplemented error.\nThe S3 Gateway does not support the following S3 operations:\nAccelerate Analytics Object copying. PFS supports this functionality through gRPC. CORS configuration Encryption HTML form uploads Inventory Legal holds Lifecycles Logging Metrics Notifications Object locks Payment requests Policies Public access blocks Regions Replication Retention policies Tagging Torrents Website configuration ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b07878eccad21c47e3e273eeed5c73aa"
  },
  {
    "title": "Backup & Restore",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Manage",
    "description": "Learn how to back-up and restore a cluster or standalone Enterprise Server.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/backup-restore/",
    "relURI": "/latest/manage/backup-restore/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["management", "backups"],
    "id": "f048e9151d85e074d9b7edf29b23671b"
  },
  {
    "title": "Cluster Backup",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Backup & Restore",
    "description": "Learn how to back-up and restore the state of a production cluster.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/backup-restore/cluster/",
    "relURI": "/latest/manage/backup-restore/cluster/",
    "body": "This page will walk you through the main steps required to manually back up and restore the state of a Pachyderm cluster in production. Details on how to perform those steps might vary depending on your infrastructure and cloud provider / on-premises setup. Refer to your provider\u0026rsquo;s documentation.\nOverview # Pachyderm state is stored in two main places:\nAn object-store holding Pachyderm\u0026rsquo;s data. A PostgreSQL instance made up of one or two databases: pachyderm holding Pachyderm\u0026rsquo;s metadata and dex holding authentication data. Backing up a Pachyderm cluster involves snapshotting both the object store and the PostgreSQL database(s), in a consistent state, at a given point in time.\nRestoring it involves re-populating the database(s) and the object store using those backups, then recreating a Pachyderm cluster.\n‚ÑπÔ∏è Make sure that you have a bucket for backup use, separate from the object store used by your cluster. Depending on the reasons behind your cluster recovery, you might choose to use an existing vs. a new instance of PostgreSQL and/or the object store. Manual Back Up Of A Pachyderm Cluster # Before any manual backup:\nMake sure to retain a copy of the Helm values used to deploy your cluster. Then, suspend any state-mutating operations. ‚ÑπÔ∏è Backups incur downtime until operations are resumed. Operational best practices include notifying Pachyderm users of the outage and providing an estimated time when downtime will cease. Downtime duration is a function of the size of the data be to backed up and the networks involved; Testing before going into production and monitoring backup times on an ongoing basis might help make accurate predictions. Suspend Operations # Pause any external automated process ingressing data to Pachyderm input repos, or queue/divert those as they will fail to connect to the cluster while the backup occurs.\nSuspend all mutation of state by scaling pachd and the worker pods down:\n‚ö†Ô∏è Before starting, make sure that your context points to the server you want to pause by running pachctl config get active-context.\nTo pause Pachyderm:\nIf you are an Enterprise user: Run the pachctl enterprise pause command.\nAlternatively, you can use kubectl:\nBefore starting, make sure that kubectl points to the right cluster.\nRun kubectl config get-contexts to list all available clusters and contexts (the current context is marked with a *), then kubectl config use-context \u0026lt;your-context-name\u0026gt; to set the proper active context.\nkubectl scale deployment pachd --replicas 0 kubectl scale rc --replicas 0 -l suite=pachyderm,component=worker Note that it takes some time for scaling down to take effect;\nRun the watch command to monitor the state of pachd and worker pods terminating:\nwatch -n 5 kubectl get pods Back Up The Databases And The Object Store # This step is specific to your database and object store hosting.\nIf your PostgreSQL instance is solely dedicated to Pachyderm, you can use PostgreSQL\u0026rsquo;s tools, like pg_dumpall, to dump your entire PostgreSQL state.\nAlternatively, you can use targeted pg_dump commands to dump the pachyderm and dex databases, or use your Cloud Provider\u0026rsquo;s backup product. In any case, make sure to use TLS. Note that if you are using a cloud provider, you might choose to use the provider‚Äôs method of making PostgreSQL backups.\n‚ö†Ô∏è A production setting of Pachyderm implies that you are running a managed PostgreSQL instance.\nüìñ PostgreSQL on AWS RDS backup GCP Cloud SQL backup Azure Database for PostgreSQL backup For on-premises Kubernetes deployments, check the vendor documentation for your on-premises PostgreSQL for details on backing up and restoring your databases.\nTo back up the object store, you can either download all objects or use the object store provider‚Äôs backup method.\nThe latter is preferable since it will typically not incur egress costs. üìñ AWS backup for S3 GCP Cloud storage bucket backup Azure blob backup For on-premises Kubernetes deployments, check the vendor documentation for your on-premises object store for details on backing up and restoring a bucket.\nResuming operations # Once your backup is completed, resume your normal operations by scaling pachd back up. It will take care of restoring the worker pods:\nEnterprise users: run pachctl enterprise unpause.\nAlternatively, if you used kubectl:\nkubectl scale deployment pachd --replicas 1 Restore Pachyderm # There are two primary use cases for restoring a cluster:\nYour data have been corrupted, preventing your cluster from functioning correctly. You want the same version of Pachyderm re-installed on the latest uncorrupted data set. You have upgraded a cluster and are encountering problems. You decide to uninstall the current version and restore the latest backup of a previous version of Pachyderm. Depending on your scenario, pick all or a subset of the following steps:\nPopulate new pachyderm and dex (if required) databases on your PostgreSQL instance Populate a new bucket or use the backed-up object-store (note that, in that case, it will no longer be a backup) Create a new empty Kubernetes cluster and give it access to your databases and bucket Deploy Pachyderm into your new cluster Restore The Databases And Objects # Restore PostgreSQL backups into your new databases using the appropriate method (this is most straightforward when using a cloud provider). Copy the objects from the backed-up object store to your new bucket or re-use your backup. Deploy Pachyderm Into The New Cluster # Finally, update the copy of your original Helm values to point Pachyderm to the new databases and the new object store, then use Helm to install Pachyderm into the new cluster.\nConnect \u0026lsquo;pachctl\u0026rsquo; To Your Restored Cluster # And check that your cluster is up and running.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["management", "backups"],
    "id": "351e7b45f104ab2fb87f6efbd214aa06"
  },
  {
    "title": "Enterprise Server Backup",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Backup & Restore",
    "description": "Learn how to back-up and restore the state of a production Enterprise Server.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/backup-restore/enterprise-server/",
    "relURI": "/latest/manage/backup-restore/enterprise-server/",
    "body": "This page will walk you through the main steps required to manually back up and restore the state of a Pachyderm cluster in production. Details on how to perform those steps might vary depending on your infrastructure and cloud provider / on-premises setup. Refer to your provider\u0026rsquo;s documentation.\nOverview # Pachyderm state is stored in two main places:\nAn object-store holding Pachyderm\u0026rsquo;s data. A PostgreSQL instance made up of one or two databases: pachyderm holding Pachyderm\u0026rsquo;s metadata and dex holding authentication data. Backing up a Pachyderm cluster involves snapshotting both the object store and the PostgreSQL database(s), in a consistent state, at a given point in time.\nRestoring it involves re-populating the database(s) and the object store using those backups, then recreating a Pachyderm cluster.\n‚ÑπÔ∏è Make sure that you have a bucket for backup use, separate from the object store used by your cluster. Depending on the reasons behind your cluster recovery, you might choose to use an existing vs. a new instance of PostgreSQL and/or the object store. Backup A Standalone Enterprise Server # Backing up and restoring an Enterprise Server is similar to the backing up and restoring of a regular cluster, with three slight variations:\nThe name of its Kubernetes deployment is pach-enterprise versus pachd in the case of a regular cluster. The Enterprise Server does not use an Object Store. An Enterprise server only requires a dex database. ‚ö†Ô∏è Make sure that pachctl and kubectl are pointing to the right cluster. Check your Enterprise Server context: pachctl config get active-enterprise-context, or pachctl config set active-enterprise-context \u0026lt;my-enterprise-context-name\u0026gt; --overwrite to set it.\nPause the Enterprise Server like you would pause a regular cluster by running pachctl enterprise pause (Enterprise users), or using kubectl. ‚ÑπÔ∏è kubectl users: There is a difference with the pause of a regular cluster. The deployment of the enterprise server is named pach-enterprise; therefore, the first command should be:\nkubectl scale deployment pach-enterprise --replicas 0 There is no need to pause all the Pachyderm clusters registered to the Enterprise Server to backup the enterprise server; however, pausing the Enterprise server will result in your clusters becoming unavailable.\nAs a reminder, the Enterprise Server does not use any object-store. Therefore, the backup of the Enterprise Server only consists in backing up the database dex.\nResume the operations on your Enterprise Server by running pachctl enterprise unpause (Enterprise users) to scale the pach-enterprise deployment back up. Alternatively, if you used kubectl, run:\nkubectl scale deployment pach-enterprise --replicas 1 Restore An Enterprise Server # Follow the cluster restoration steps while skipping all tasks related to creating and populating a new object-store.\nOnce your cluster is up and running, check that all your clusters are automatically registered with your new Enterprise Server.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["management", "backups"],
    "id": "7377efa0fcba14e398f4cf7249b4b22c"
  },
  {
    "title": "Upgrade",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to upgrade PachCTLand \u0026 PachD.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/upgrades/",
    "relURI": "/latest/manage/upgrades/",
    "body": "Learn how to upgrade Pachyderm to access new features and performance enhancements.\nBefore You Start # Check the release notes before upgrading Back up your cluster Update your Helm chart values if applicable How to Upgrade Pachyderm # Run the following brew command or download \u0026amp; install the latest release assets: brew tap pachyderm/tap \u0026amp;\u0026amp; brew install pachyderm/tap/pachctl@2.6 Upgrade Helm. Deploy Method: Production Local (Personal Machine) Note that the repo name input (pachd) must match the name you provided upon first install. You can also pass in a specific version (e.g., --version x.x.0-rc.1) if you are testing a pre-released version of Pachyderm.\nhelm repo update helm upgrade pachyderm pachyderm/pachyderm -f my_pachyderm_values.yaml --set proxy.enabled=true --set proxy.service.type=LoadBalancer Note that the repo name input (pachd) must match the name you provided upon first install. You can also pass in a specific version (e.g., --version x.x.0-rc.1) if you are testing a pre-released version of Pachyderm.\nhelm repo update helm upgrade pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Verify that the installation was successful by running pachctl version: pachctl version # COMPONENT VERSION # pachctl 2.6.1 # pachd 2.6.1 ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["management", "upgrades", "pachctl", "pachd"],
    "id": "ae82491900bac26f951a48d4422e1a60"
  },
  {
    "title": "PachCTL Shell",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to use the PachCTL shell.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/pachctl-shell/",
    "relURI": "/latest/manage/pachctl-shell/",
    "body": "The Pachyderm Shell is a special-purpose shell for Pachyderm that provides auto-suggesting as you type. New Pachyderm users will find this user-friendly shell especially appealing as it helps to learn pachctl, type commands faster, and displays useful information about the objects you are interacting with. This new shell does not supersede the classic use of pachctl shell in your standard terminal, but is a compelling convenience for power users and beginners alike. If you prefer to use just pachctl, you can continue to do so.\nTo enter the Pachyderm Shell, type:\npachctl shell When you enter pachctl shell, your prompt changes to display your current Pachyderm context, as well as displays a list of available commands in a drop-down list.\nTo scroll through the list, press TAB and then use arrows to move up or down. Press SPACE to select a command.\nWhen in the Pachyderm Shell, you do not need to prepend your commands with pachctl because Pachyderm does that for you automatically behind the scenes. For example, instead of running pachctl list repo, run list repo:\nWith nested commands, pachctl shell can do even more. For example, if you type list file \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;/, you can preview and select files from that branch:\nSimilarly, you can select a commit:\nExit the Pachyderm Shell # To exit the Pachyderm Shell, press CTRL-D or type exit.\nClearing Cached Completions # To optimize performance and achieve faster response time, the Pachyderm Shell caches completion results. You can clear this cache by pressing F5 forcing the Pachyderm Shell to send requests to the server for new completions.\nClearing the screen # To clear the screen, press CTRL-L.\nLimitations # The Pachyderm Shell does not support standard UNIX commands or kubectl commands. To run them, exit the Pachyderm Shell or run the commands in a different terminal window.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pachctl", "cli"],
    "id": "1a10c4786b22e78148509829d3854c1e"
  },
  {
    "title": "Check IdP User",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to check which IdP user you are logged in as using pachctl.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/check-user/",
    "relURI": "/latest/manage/check-user/",
    "body": " Before You Start # Your organization must have an active Enterprise license key. You must have PachCTL installed. How to Check Your Current User # Open a terminal. Run the following command: pachctl auth whoami # You are \u0026#34;user:one-pachyderm-user@gmail.com\u0026#34; # session expires: 08 May 21 13:59 EDT ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["workflows", "permissions", "management"],
    "id": "86b7b3e8cfe18a52dd8e0e9e4c5e2419"
  },
  {
    "title": "Supported Releases & Features",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Check which release versions are actively supported using this guide.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/supported-releases/",
    "relURI": "/latest/manage/supported-releases/",
    "body": "Pachyderm lists the status for each release and feature, so that you can understand expectations for support and stability.\nSupported Releases # Pachyderm supports the latest Generally Available (GA) release and the previous two major and minor GA releases. Releases three or more major and minor versions back are considered End of Life (EOL).\nRelease Status by Version # Version Release Status Support 2.6.x Upcoming Upcoming 2.5.x GA Yes 2.4.x GA Yes 2.3.x GA No 2.2.x GA No 2.1.x GA No 2.0.x GA No 1.13.x GA No 1.12.x EOL No 1.11.x EOL No 1.10.x EOL No \u0026lt; 1.9.11 EOL No Releases Under Development # A release under development may undergo several pre-release stages before becoming Generally Available (GA). These pre-releases enable the Pachyderm team to do development and testing in partnership with our users before a release is considered ready for a Generally Availability (GA).\nalpha \u0026gt; beta \u0026gt; Release Candidate (RC) \u0026gt; Generally Available (GA)\nRelease Status # Alpha # alpha releases are a pre-release version of a product, intended for development and testing purposes only. alpha releases include many bugs and unfinished features, and are only suitable for early technical feedback. alpha releases should not be used in a production environment.\nBeta # beta releases are a pre-release version of a product, intended for development and testing purposes only, and include a wider range of users than an alpha release. beta releases should not be used in a production environment.\nRelease Candidate (RC) # Release Candidate or RC releases are a pre-release version of a product, intended for users to prepare for a GA release. RC releases should not be used in a production environment.\nGenerally Available (GA) # Generally Available or GA releases are considered stable and intended for production usage.\nContain new features, fixed defects, and patched security vulnerabilities. Support is available from Pachyderm. End of Life (EOL) # End of Life or EOL indicates the release will no longer receive support.\nDocumentation will be archived. Release artifacts will remain available. We keep release artifacts on Github and Docker Hub. Support is no longer available for End of Life (EOL) releases. Support can assist with upgrading to a newer version. Supported Features # Stable # stable indicates that the Pachyderm team believes the feature is ready for use in a production environment.\nThe feature\u0026rsquo;s API is stable and unlikely to change. There are no major defects for the feature. The Pachyderm team believes there is a sufficient amount of testing, including automated tests, community testing, and user production environments. Support is available from Pachyderm. Experimental # experimental indicates that a feature has not met the Pachyderm team\u0026rsquo;s criteria for production use. Therefore, these features should be used with caution in production environments. experimental features are likely to change, have outstanding defects, and/or missing documentation. Users considering using experimental features should contact Pachyderm for guidance.\nProduction use is not recommended without guidance from Pachyderm. These features may have missing documentation, lack of examples, and lack of content. Support is available from Pachyderm, which may be limited in scope based on our guidance. Deprecated # deprecated indicates that a feature is no longer developed. Users of deprecated features are encouraged to upgrade or migrate to newer versions or compatible features. deprecated features become End of Life (EOL) features after 6 months.\nUsers continuing to use deprecated features should contact support to migrate to features. Support is available from Pachyderm. End of Life (EOL) Features # End of Life or EOL indicates that a feature is no longer supported.\nDocumentation will be archived. Support is no longer available for End of Life (EOL) features. Support can assist upgrading to a newer version. Experimental Features # Feature Version Date Service Pipelines 1.9.9 2019-11-06 JupyterLab Extension 0.6.3 2022-09-21 Deprecated Features # Feature Version EOL Date End of Life (EOL) Features # Feature Version EOL Date Build Pipelines 2.0.0 2021-07-25 Git Inputs 2.0.0 2021-07-25 pachctl deploy 2.0.0 2021-07-25 Spouts: Named Pipes 2.0.0 2021-07-25 Vault Plugin 2.0.0 2021-07-25 pachctl put file --split 2.0.0 2021-07-25 MaxQueueSize 2.0.0 2021-07-25 S3v2 signatures 1.12.0 2021-01-05 atom inputs 1.9.0 2019-06-12 ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "ed774ee56a710afbf968b2339765b287"
  },
  {
    "title": "Cluster Access",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to manage Kubernetes cluster access using Contexts.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/cluster-access/",
    "relURI": "/latest/manage/cluster-access/",
    "body": "Pachyderm contexts enable you to store configuration parameters for multiple Pachyderm clusters in a single configuration file saved at ~/.pachyderm/config.json. This file stores the information about all Pachyderm clusters that you have deployed from your machine locally or on a remote server.\nFor example, if you have a cluster that is deployed locally in minikube and another one deployed on Amazon EKS, configurations for these clusters are stored in that config.json file. By default, all local cluster configurations have the local prefix. If you have multiple local clusters, Pachyderm adds a consecutive number to the local prefix of each cluster.\nThe following text is an example of a Pachyderm config.json file:\n{ \u0026#34;user_id\u0026#34;: \u0026#34;b4fe4317-be21-4836-824f-6661c68b8fba\u0026#34;, \u0026#34;v2\u0026#34;: { \u0026#34;active_context\u0026#34;: \u0026#34;local-1\u0026#34;, \u0026#34;contexts\u0026#34;: { \u0026#34;default\u0026#34;: {}, \u0026#34;local\u0026#34;: {}, \u0026#34;local-1\u0026#34;: {}, }, \u0026#34;metrics\u0026#34;: true } } View the Active Context # When you have multiple Pachyderm clusters, you can switch between them by setting the current context. The active context is the cluster that you interact with when you run pachctl commands.\nTo view active context, type:\nView the active context:\npachctl config get active-context System response:\nlocal-1 List all contexts and view the current context:\npachctl config list context System response:\nACTIVE NAME default local * local-1 The active context is marked with an asterisk.\nChange the Active Context # To change the active context, type pachctl config set active-context \u0026lt;name\u0026gt;.\nAlso, you can set the PACH_CONTEXT environmental variable that overrides the active context.\nExample:\nexport PACH_CONTEXT=local1 Create a New Context # When you deploy a new Pachyderm cluster, a new context that points to the new cluster is created automatically.\nIn addition, you can create a new context by providing your parameters through the standard input stream (stdin) in your terminal. Specify the parameters as a comma-separated list enclosed in curly brackets.\n‚ÑπÔ∏è By default, the pachd port is 30650.\nTo create a new context with specific parameters, complete the following steps:\nCreate a new Pachyderm context with a specific pachd IP address and a client certificate:\necho \u0026#39;{\u0026#34;pachd_address\u0026#34;:\u0026#34;10.10.10.130:650\u0026#34;, \u0026#34;server_cas\u0026#34;:\u0026#34;insert your base 64 encoded key.pem\u0026#34;}\u0026#39; | pachctl config set context new-local System response:\nReading from stdin Verify your configuration by running the following command:\npachctl config get context new-local { \u0026#34;pachd_address\u0026#34;: \u0026#34;10.10.10.130:650\u0026#34;, \u0026#34;server_cas\u0026#34;: \u0026#34;insert your base 64 encoded key.pem\u0026#34; } Update an Existing Context # You can update an existing context with new parameters, such as a Pachyderm IP address, certificate authority (CA), and others.\nTo update the Active Context, run the following commands:\nUpdate the context with a new pachd address:\npachctl config update context local-1 --pachd-address 10.10.10.131 The pachctl config update command supports the --pachd-address flag only.\nVerify that the context has been updated:\npachctl config get context local-1 System response:\n{ \u0026#34;pachd_address\u0026#34;: \u0026#34;10.10.10.131\u0026#34; } Alternatively, you can update multiple properties by using an echo script:\necho \u0026#39;{\u0026#34;pachd_address\u0026#34;:\u0026#34;10.10.10.132\u0026#34;, \u0026#34;server_cas\u0026#34;:\u0026#34;insert your base 64 encoded key.pem\u0026#34;}\u0026#39; | pachctl config set context local-1 --overwrite System response:\nReading from stdin. Verify that the changes were applied:\npachctl config get context local-1 System response:\n{ \u0026#34;pachd_address\u0026#34;: \u0026#34;10.10.10.132\u0026#34;, \u0026#34;server_cas\u0026#34;: \u0026#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVEakNDQXZhZ0F3SUJBZ0lERDkyc01BMEdDU3FHU0liM0RRRUJDd1VBTUVVeEN6QUpCZ05WQkFZVEFrUkYKTVJVd0V3WURWUVFLREF4RUxWUnlkWE4wSUVkdFlrZ3hIekFkQmdOVkJBTU1Ga1F0VkZKVlUxUWdVbTl2ZENCRApRU0F6SURJd01UTXdIaGNOTVRNd09USXdNRGd5TlRVeFdoY05Namd3T1RJd01EZ3lOVFV4V2pCRk1Rc3dDUVlEClZRUUdFd0pFUlRFVk1CTUdBMVVFQ2d3TVJDMVVjblZ6ZENCSGJXSklNUjh3SFFZRFZRUUREQlpFTFZSU1ZWTlUKSUZKdmIzUWdRMEVnTXlBeU1ERXpNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQQp4SHRDa29JZjdPMVVtSTRTd01vSjM1TnVPcE5jRytRUWQ1NU9hWWhzOXVGcDh2YWJvbUd4dlFjZ2RKaGw4WXdtCkNNMm9OY3FBTnRGamJlaEVlb0xEYkY3ZXUrZzIwc1JvTm95Zk1yMkVJdURjd3U0UVJqbHRyNU01cm9mbXc3d0oKeVN4cloxdlptM1oxVEF2Z3U4WFh2RDU1OGwrKzBaQlgrYTcyWmw4eHY5TnRqNmU2U3ZNalpidTM3Nk1sMXdycQpXTGJ2aVByNmViSlNXTlh3ckl5aFVYUXBsYXBSTzVBeUE1OGNjblNRM2ozdFlkTGw0LzFrUitXNXQwcXA5eCt1CmxvWUVyQy9qcElGM3Qxb1cvOWdQUC9hM2VNeWtyL3BiUEJKYnFGS0pjdStJODlWRWdZYVZJNTk3M2J6Wk5POTgKbER5cXdFSEM0NTFRR3NEa0dTTDhzd0lEQVFBQm80SUJCVENDQVFFd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVVQNURJZmNjVmIvTWtqNm5ETDB1aUR5R3lMK2N3RGdZRFZSMFBBUUgvQkFRREFnRUdNSUcrCkJnTlZIUjhFZ2JZd2diTXdkS0J5b0hDR2JteGtZWEE2THk5a2FYSmxZM1J2Y25rdVpDMTBjblZ6ZEM1dVpYUXYKUTA0OVJDMVVVbFZUVkNVeU1GSnZiM1FsTWpCRFFTVXlNRE1sTWpBeU1ERXpMRTg5UkMxVWNuVnpkQ1V5TUVkdApZa2dzUXoxRVJUOWpaWEowYVdacFkyRjBaWEpsZG05allYUnBiMjVzYVhOME1EdWdPYUEzaGpWb2RIUndPaTh2ClkzSnNMbVF0ZEhKMWMzUXVibVYwTDJOeWJDOWtMWFJ5ZFhOMFgzSnZiM1JmWTJGZk0xOHlNREV6TG1OeWJEQU4KQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBRGxrT1dPUjBTQ05FenpRaHRad1VHcTJhUzdlemlHMWNxUmR3OENxZgpqWHY1ZTRYNnh6bm9FQWl3TlN0Znp3TFMwNXpJQ3g3dUJWU3VONU1FQ1gxc2o4SjB2UGdjbEw0eEFVQXQ4eVFnCnQ0UlZMRnpJOVhSS0VCbUxvOGZ0TmRZSlNOTU93TG81cUxCR0FyRGJ4b2had3I3OGU3RXJ6MzVpaDFXV3pBRnYKbTJjaGxUV0wrQkQ4Y1J1M1N6ZHBwanZXN0l2dXdiRHpKY21Qa24yaDZzUEtSTDhtcFhTU25PTjA2NTEwMmN0TgpoOWo4dEdsc2k2QkRCMkI0bCtuWmszekNScnliTjFLajdZbzhFNmw3VTB0Sm1oRUZMQXR1VnF3ZkxvSnM0R2xuCnRRNXRMZG5rd0JYeFAvb1ljdUVWYlNkYkxUQW9LNTlJbW1Rcm1lL3lkVWxmWEE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\u0026#34; } ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["management", "permissions"],
    "id": "5d02eb1d2a9011edc81ec742c3847b55"
  },
  {
    "title": "Deactivate Authorization",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to deactivate Authorization (User Access Management).",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/deactivate-auth/",
    "relURI": "/latest/manage/deactivate-auth/",
    "body": "When you deactivate authorization, all permissions granted to users on Pachyderm resources are removed. Everyone that can connect to Pachyderm is back to being a clusterAdmin (can access and modify all data in all repos).\nBefore You Start # You must be logged in as a clusterAdmin. How to Deactivate Auth # pachctl auth deactivate ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "7c2a60a0c2b796627dcefe1b0cdd5104"
  },
  {
    "title": "GPUs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to access GPUs on a Kubernetes cluster for data transformations.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/gpus/",
    "relURI": "/latest/manage/gpus/",
    "body": " Set up a GPU enabled Kubernetes Cluster # Pachyderm leverages Kubernetes Device Plugins to let Kubernetes Pods access specialized hardware such as GPUs. For instructions on how to set up a GPU-enabled Kubernetes cluster through device plugins, see the Kubernetes documentation.\nPachyderm on NVIDIA DGX A100 # Let‚Äôs walk through the main steps allowing Pachyderm to leverage the AI performance of your DGX A100 GPUs.\nüìñ Read about NVIDIA DGX A100\u0026rsquo;s full user guide.\nüí° Support for scheduling GPU workloads in Kubernetes requires a fair amount of trial and effort. To ease the process:\nThis setup page will walk you through very detailed installation steps to prepare your Kubernetes cluster. Take advantage of a user\u0026rsquo;s past experience in this blog. Here is a quick recap of what will be needed:\nHave a working Kubernetes control plane and worker nodes attached to your cluster. Install the DGX system in a hosting environment. Add the DGX to your K8s API server as a worker node. Now that the DGX is added to your API server, you can then proceed to:\nEnable the GPU worker node in the Kubernetes cluster by installing NVIDIA\u0026rsquo;s dependencies:\nDependencies packages and deployment methods may vary. The following list is not exhaustive and is intended to serve as a general guideline.\nNVIDIA drivers\nFor complete instructions on setting up NVIDIA drivers, visit this quickstart guide or check this summary of the steps.\nNVIDIA Container Toolkit (nvidia-docker2)\nYou may need to use different packages depending on your container engine.\nNVIDIA Kubernetes Device Plugin\nTo use GPUs in Kubernetes, the NVIDIA Device Plugin is required. The NVIDIA Device Plugin is a daemonset that enumerates the number of GPUs on each node of the cluster and allows pods to be run on GPUs. Follow those steps to deploy the device plugin as a daemonset using helm.\nCheckpoint: Run NVIDIA System Management Interface (nvidia-smi) on the CLI. It should return the list of NVIDIA GPUs.\nTest a sample container with GPU:\nTo test whether CUDA jobs can be deployed, run a sample CUDA (vectorAdd) application.\nFor reference, find the pod spec below:\napiVersion: v1 kind: Pod metadata: name: gpu-test spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: \u0026#34;nvidia/samples:vectoradd-cuda10.2\u0026#34; resources: limits: nvidia.com/gpu: 1 Save it as gpu-pod.yaml then deploy the application:\nkubectl apply -f gpu-pod.yaml Check the logs to make sure that the app completed successfully:\nkubectl get pods gpu-test If the container above is scheduled successfully: install Pachyderm. You are ready to start leveraging NVIDIA\u0026rsquo;s GPUs in your Pachyderm pipelines.\nüí° Note that you have the option to use GPUs for compute-intensive workloads on:\nGoogle Container Engine (GKE). Amazon Elastic Kubernetes Service (EKS). Azure Kubermnetes Service (AKS). Configure GPUs in Pipelines # Once your GPU-enabled Kubernetes cluster is set, you can request a GPU tier in your pipeline specifications by setting up GPU resource limits, along with its type and number of GPUs.\nüí° By default, Pachyderm workers are spun up and wait for new input. That works great for pipelines that are processing a lot of new incoming commits. However, for lower volume of input commits, you could have your pipeline workers \u0026rsquo;taking\u0026rsquo; the GPU resource as far as k8s is concerned, but \u0026lsquo;idling\u0026rsquo; as far as you are concerned.\nMake sure to set the autoscaling field to true so that if your pipeline is not getting used, the worker pods get spun down and the GPU resource freed. Additionally, specify how much of GPU your pipeline worker will need via the resource_requests fields in your pipeline specification with resource_requests \u0026lt;= resource_limits. Below is an example of a pipeline spec for a GPU-enabled pipeline from our market sentiment analysis example:\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;train_model\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;Fine tune a BERT model for sentiment analysis on financial data.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;cross\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;dataset\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34; } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;language_model\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34; } } ] }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python\u0026#34;, \u0026#34;finbert_training.py\u0026#34;, \u0026#34;--lm_path\u0026#34;, \u0026#34;/pfs/language_model/\u0026#34;, \u0026#34;--cl_path\u0026#34;, \u0026#34;/pfs/out\u0026#34;, \u0026#34;--cl_data_path\u0026#34;, \u0026#34;/pfs/dataset/\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/market_sentiment:dev0.25\u0026#34; }, \u0026#34;resource_limits\u0026#34;: { \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nvidia.com/gpu\u0026#34;, \u0026#34;number\u0026#34;: 1 } }, \u0026#34;resource_requests\u0026#34;: { \u0026#34;memory\u0026#34;: \u0026#34;4G\u0026#34;, \u0026#34;cpu\u0026#34;: 1 } } ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["management", "deployment"],
    "id": "9aafcaa3618e0dca7b88786f951484c4"
  },
  {
    "title": "Log In via IdP",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to log in to a cluster as an IdP user.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/login/",
    "relURI": "/latest/manage/login/",
    "body": " How to Log in to a Cluster via IdP # Open a terminal. Input the following command: pachctl auth login Select the connector you wish to use. Provide your credentials ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["workflows", "permissions", "management"],
    "id": "ecee86d9a21425a03c5068fb087cd3f2"
  },
  {
    "title": "Revoke User Access",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to revoke user access to a cluster.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/revoke-user/",
    "relURI": "/latest/manage/revoke-user/",
    "body": " Before You Start # You must have clusterAdmin role permissions. How to Revoke User Access # ‚ö†Ô∏è You must remove the revoked user from your IdP user registry after completing the steps in this guide.\nRevoke a Specific Token # pachctl auth revoke --token=\u0026lt;pach token\u0026gt; Revoke All Tokens # pachctl auth revoke --user=idp:usernamen@pachyderm.io ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["workflows", "permissions", "management"],
    "id": "8a9e7ce4acf63e5ec58c60689208cbde"
  },
  {
    "title": "Scaling Limits (CE)",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn about the built-in scaling limitations of our Community Edition.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/scaling-limits/",
    "relURI": "/latest/manage/scaling-limits/",
    "body": "Our free Pachyderm Community Edition contains built-in scaling limitations and parallelism thresholds. To scale beyond these limits, request a Enterprise trial token and enjoy unlimited scaling, and more.\nüìñ You might qualify for a free Enterprise license.\nPachyderm offers activation keys for proofs-of-concept, startups, academic, nonprofit, or open-source projects. Tell us about your project!.\nScaling Limits # Number of concurrent pipelines deployed Number of workers for each pipeline Community Users can deploy up to 16 pipelines. Community Users can run up to 8 workers in parallel on each pipeline. What happens when you exceed those limits? # As a general rule, Pachyderm provides an error message in the STDERR whenever a limit is encountered that prevents you from successfully running a command. In that case, the alert message links to a free trial request form.\nLimit on the number of pipelines # When exceeding the number of pipelines:\npachctl create pipeline fails once the maximum number of pipelines is reached.\npachctl update pipeline and pachctl edit pipeline succeed on existing pipelines, fail when attempting to create pipelines beyond the limit.\n‚ÑπÔ∏è If update pipeline fails for any other reason, it does not log any message related to pipeline limits.\nAll of the commands listed above create a distinct message to STDERR and to the pachd logs. This message includes information such as the limit on the number of pipelines in the Community Edition, the total number of pipelines deployed, and provides a link to request an Enterprise key to lift those limitations.\nall other list, run, start, stop pipeline commands\u0026rsquo; behavior remains unchanged. Limit on the number of workers per pipeline # When constant parallelism \u0026gt; 8:\npachctl create pipeline and pachctl update pipeline fail. A message to STDERR and pachd logs is generated. You will need to update your pipeline specification file accordingly or activate an Enterprise license. What happens when your license expires? # If your Enterprise License has expired and you have more than 16 pipelines, all existing pipelines continue to work. However, you will not be able to create additional pipelines. Same behavior if you upgrade your cluster.\n‚ö†Ô∏è Restoring or installing Pachyderm with an expired license will fail.\n‚ÑπÔ∏è Pipelines automatically generated by the system (for example cron\u0026hellip;) are not considered when assessing the total number of pipelines deployed. The limit applies to user-created pipelines only.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["community-edition"],
    "id": "05239b1c5302637868bd7716031c2fc4"
  },
  {
    "title": "Secrets",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to create and manage Kubernetes secrets.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/secrets/",
    "relURI": "/latest/manage/secrets/",
    "body": "Pachyderm uses Kubernetes\u0026rsquo; Secrets to store and manage sensitive data, such as passwords, OAuth tokens, or ssh keys. You can use any of Kubernetes\u0026rsquo; types of Secrets that match your use case. Namely, generic (or Opaque), tls, or docker-registry.\nAbout Secrets # When you install or upgrade a cluster, you can provide values for the configuration fields in your Helm Chart values.yaml file. However, some of those values are sensitive and should not be stored in your values.yaml file.\nPachyderm provides a way to inject those values at the time of the deployment or upgrade. We call those values platform secrets.\nPachyderm Platform Secrets Map # If no Secret KEY name is provided for the Helm Chart\u0026rsquo;s Secret NAME Attribute, Pachyderm will use the Helm Chart\u0026rsquo;s RAW Attribute to populate its own platform secrets at the time of the installation/upgrade. Those that are not marked as required are automatically generated by the platform if not provided.\nRequired Secret KEY Name : Platform Secret Helm Chart\u0026rsquo;s Secret NAME Attribute Helm Chart\u0026rsquo;s RAW Attribute Yes enterprise-license-key : pachyderm-license pachd.enterpriseLicenseKeySecretName pachd.enterpriseLicenseKey Yes upstream-idps : pachyderm-identity oidc.upstreamIDPsSecretName oidc.upstreamIDPs No rootToken : pachyderm-auth pachd.rootTokenSecretName pachd.rootToken No auth-config : pachyderm-auth pachd.oauthClientSecretSecretName pachd.oauthClientSecret No cluster-role-bindings : pachyderm-auth Use plain text in your values.yaml pachd.pachAuthClusterRoleBindings No postgresql-password : postgres global.postgresql.postgresqlExistingSecretName global.postgresql.postgresqlPassword No OAUTH_CLIENT_SECRET : pachyderm-console-secret console.config.oauthClientSecretSecretName console.config.oauthClientSecret No N/A; passed into deployment manifest as plaintext. pachd.enterpriseServerTokenSecretName pachd.enterpriseServerToken No enterprise-secret : pachyderm-enterprise pachd.enterpriseSecretSecretName pachd.enterpriseSecret Create A Secret # The creation of a Secret in Pachyderm requires a JSON configuration file.\nA good way to create this file is:\nTo generate it by calling a dry-run of the kubectl create secret ... --dry-run=client --output=json \u0026gt; myfirstsecret.json command. Then call pachctl create secret -f myfirstsecret.json. ‚ö†Ô∏è Kubernetes Secrets are, by default, stored as unencrypted base64-encoded strings (i.e., the values for all keys in the data field have to be base64-encoded strings). When using the kubectl create secret command, the encoding is done for you. If you choose to manually create your JSON file, make sure to use your own base 64 encoder.\nGenerate Your Secret Config File # Let\u0026rsquo;s first generate your secret configuration file using the kubectl command. For example:\nfor a generic authentication secret: kubectl create secret generic mysecretname --from-literal=username=\u0026lt;myusername\u0026gt; --from-literal=password=\u0026lt;mypassword\u0026gt; --dry-run=client --output=json \u0026gt; myfirstsecret.json for a tls secret: kubectl create secret tls mysecretname --cert=\u0026lt;Path to your certificate\u0026gt; --key=\u0026lt;Path to your SSH key\u0026gt; --dry-run=client --output=json \u0026gt; myfirstsecret.json for a docker registry secret: kubectl create secret docker-registry mysecretname --dry-run=client --docker-server=\u0026lt;DOCKER_REGISTRY_SERVER\u0026gt; --docker-username=\u0026lt;DOCKER_USER\u0026gt; --docker-password=\u0026lt;DOCKER_PASSWORD\u0026gt; --output=json \u0026gt; myfirstsecret.json Generic Secret Example # { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;clearml\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;Opaque\u0026#34;, \u0026#34;stringData\u0026#34;: { \u0026#34;access\u0026#34;: \u0026#34;\u0026lt;CLEARML_API_ACCESS_KEY\u0026gt;\u0026#34;, \u0026#34;secret\u0026#34;: \u0026#34;\u0026lt;CLEARML_API_SECRET_KEY\u0026gt;\u0026#34; } } Find more detailed information on the creation of Secrets in Kubernetes documentation.\nCreate your Secret in Pachyderm # Next, run the following to actually create the secret in the Pachyderm Kubernetes cluster:\npachctl create secret -f myfirstsecret.json You can run pachctl list secret to verify that your secret has been properly created. You should see an output that looks like the following:\nNAME TYPE CREATED mysecret kubernetes.io/dockerconfigjson 11 seconds ago ‚ÑπÔ∏è Use pachctl delete secret to delete a secret given its name, pachctl inspect secret to list a secret given its name.\nYou can now edit your pipeline specification file as follow.\nReference a Secret in a Pipeline Spec # Now that your secret is created on Pachyderm cluster, you will need to notify your pipeline by updating your pipeline specification file. In Pachyderm, a Secret can be used in three different ways:\nAs a container environment variable:\nIn this case, in Pachyderm\u0026rsquo;s pipeline specification file, you need to reference Kubernetes\u0026rsquo; Secret by its:\nname and specify an environment variable named env_var that the value of your key should be bound to. This makes for easy access to your Secret\u0026rsquo;s data in your pipeline\u0026rsquo;s code. For example, this is useful for passing the password to a third-party system to your pipeline\u0026rsquo;s code.\n\u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;cmd\u0026#34;: [ string ], ... \u0026#34;secrets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;env_var\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;key\u0026#34;: string }] } Example # Example of a pipeline specification file assigning a Secret\u0026rsquo;s values to environment variables.\nLook at the pipeline specification in this example and see how we used the \u0026quot;env_var\u0026quot; to pass CLEARML API credentials to the pipeline code.\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mnist\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;MNIST example logging to ClearML\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;/bin/sh\u0026#34; ], \u0026#34;stdin\u0026#34;: [ \u0026#34;python pytorch_mnist.py --lr 0.2 --save-location /pfs/out\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/clearml_mnist:dev0.11\u0026#34;, \u0026#34;secrets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;clearml\u0026#34;, \u0026#34;env_var\u0026#34;: \u0026#34;CLEARML_API_ACCESS_KEY\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;access\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;clearml\u0026#34;, \u0026#34;env_var\u0026#34;: \u0026#34;CLEARML_API_SECRET_KEY\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;secret\u0026#34; } ] } } As a file in a volume mounted on a container:\nIn this case, in Pachyderm\u0026rsquo;s pipeline specification file, you need to reference Kubernetes\u0026rsquo; Secret by its:\nname and specify the mount point (mount_path) to the secret (ex: \u0026quot;/var/my-app-secret\u0026quot;). Pachyderm mounts all of the keys in the secret with file names corresponding to the keys. This is useful for secure configuration files.\n\u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;cmd\u0026#34;: [ string ], ... \u0026#34;secrets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;mount_path\u0026#34;: string }] } When pulling images:\nImage pull Secrets are a different kind of secret used to store access credentials to your private image registry.\nYou reference Image Pull Secrets (or Docker Registry Secrets) by setting the image_pull_secrets field of your pipeline specification file to the secret\u0026rsquo;s name you created (ex: \u0026quot;mysecretname\u0026quot;).\n\u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;cmd\u0026#34;: [ string ], ... \u0026#34;image_pull_secrets\u0026#34;: [ string ] } ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["secrets", "management", "kubernetes"],
    "id": "c2af003368fa8037606b6ecc5f9ada08"
  },
  {
    "title": "Sidecar S3 Gateway",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to use S3-protocol-enabled pipelines and interact with external input/output data.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/deploy-s3gateway-sidecar/",
    "relURI": "/latest/manage/deploy-s3gateway-sidecar/",
    "body": "You can interact with input/output data through the S3 protocol using Pachyderm\u0026rsquo;s S3-protocol-enabled pipelines.\nAbout # Pachyderm\u0026rsquo;s S3-protocol-enabled pipelines run a separate S3 gateway instance in a sidecar container within the pipeline-worker pod. Using this approach enables maintaining data provenance since the external code (e.g., within a Kubeflow pod) is executed in (and associated with) a Pachyderm job.\nWhen enabled, input and output repositories are exposed as S3 Buckets via the S3 gateway sidecar instance.\nInput Address: s3://\u0026lt;input_repo_name\u0026gt;. Output Address: s3://out Example with Kubeflow Pod # The following diagram shows communication between the S3 gateway deployed in a sidecar and the Kubeflow pod.\nConfigure an S3-enabled Pipeline # Open your pipeline spec. Add \u0026quot;s3\u0026quot;: true to input.pfs. Add \u0026quot;s3_out\u0026quot;: true to pipeline. Save your spec. Update your pipeline. Example Pipeline Spec # The following spec example reads files in the input bucket labresults and copies them in the pipeline\u0026rsquo;s output bucket:\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;s3_protocol_enabled_pipeline\u0026#34; }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;labresults\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;labresults\u0026#34;, \u0026#34;s3\u0026#34;: true } }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;sh\u0026#34; ], \u0026#34;stdin\u0026#34;: [ \u0026#34;set -x \u0026amp;\u0026amp; mkdir -p /tmp/result \u0026amp;\u0026amp; aws --endpoint-url $S3_ENDPOINT s3 ls \u0026amp;\u0026amp; aws --endpoint-url $S3_ENDPOINT s3 cp s3://labresults/ /tmp/result/ --recursive \u0026amp;\u0026amp; aws --endpoint-url $S3_ENDPOINT s3 cp /tmp/result/ s3://out --recursive\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/ubuntu-with-s3-clients:v0.0.1\u0026#34; }, \u0026#34;s3_out\u0026#34;: true } User Code Requirements # Your user code is responsible for:\nProviding its own S3 client package as part of the image (boto3) reading and writing in the S3 Buckets exposed to the pipeline Accessing the Sidecar # Use the S3_ENDPOINT environment variable to access the sidecar. No authentication is needed; you can only read the input bucket and write in the output bucket.\naws --endpoint-url $S3_ENDPOINT s3 cp /tmp/result/ s3://out --recursive Triggering External Pipelines # If Authentication is enabled, you can access the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY env vars in your pipeline user code to forward your pipeline\u0026rsquo;s auth credentials to third-party tools like Spark.\nConstraints # All files are processed as a single datum, meaning: The glob field in the pipeline must be set to \u0026quot;glob\u0026quot;: \u0026quot;/\u0026quot;. Already processed datums are not skipped. Only cross inputs are supported; join, group, and union are not supported. You can create a cross of an S3-enabled input with a non-S3 input; For a non-S3 input in such a cross, you can still specify a glob pattern. Input bucket(s) are read-only, and the output bucket is initially empty and writable. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "faa3044ce2135025ee7ba472c4efc8d0"
  },
  {
    "title": "Storage Optimization",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to optimize your data storage to increase data-processing performance.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/data-management/",
    "relURI": "/latest/manage/data-management/",
    "body": "This section discusses best practices for minimizing the space needed to store your Pachyderm data, increasing the performance of your data processing as related to data organization, and general good ideas when you are using Pachyderm to version/process your data.\nSetting a root volume size # When planning and configuring your Pachyderm deployment, you need to make sure that each node\u0026rsquo;s root volume is big enough to accommodate your total processing bandwidth. Specifically, you should calculate the bandwidth for your expected running jobs as follows:\n(storage needed per datum) x (number of datums being processed simultaneously) / (number of nodes) Here, the storage needed per datum must be the storage needed for the largest datum you expect to process anywhere on your DAG plus the size of the output files that will be written for that datum. If your root volume size is not large enough, pipelines might fail when downloading the input. The pod would get evicted and rescheduled to a different node, where the same thing might happen (assuming that node had a similar volume).\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["management", "storage"],
    "id": "cf195688a7b85f8e74eba4b0f3e3114b"
  },
  {
    "title": "Usage Metrics",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to disable automatically collected usage metrics.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/disable-metrics/",
    "relURI": "/latest/manage/disable-metrics/",
    "body": "Pachyderm automatically collects and reports anonymous usage metrics. These metrics help the Pachyderm team understand how people use Pachyderm to make it better. If you want opt out of anonymous metrics collection, disable them by setting the METRICS environment variable to false in the pachd container.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "61b51deb8c1fb30c8822bfe88259c125"
  },
  {
    "title": "Monitor with Prometheus",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Manage",
    "description": "Learn how to monitor a cluster using Prometheus.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/prometheus/",
    "relURI": "/latest/manage/prometheus/",
    "body": " ‚ÑπÔ∏è To monitor a Pachyderm cluster with Prometheus, a Enterprise License is required.\nPachyderm\u0026rsquo;s deployment manifest exposes Prometheus metrics, allowing an easy set up of the monitoring of your cluster. Only available for self-managed deployments today.\n‚ö†Ô∏è These installation steps are for Informational Purposes ONLY. Please refer to your full Prometheus documentation for further installation details and any troubleshooting advice.\nPrometheus installation and Service Monitor creation # Helm install kube-prometheus-stack, Prometheus\u0026rsquo; Kubernetes cluster monitoring using the Prometheus Operator:\nGet Repo Info helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Install the Prometheus-operator helm chart helm install \u0026lt;a-release-name\u0026gt; prometheus-community/kube-prometheus-stack Create a ServiceMonitor for Pachyderm in Kubernetes:\nCreate a myprometheusservice.yaml\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: pachyderm-scraper labels: release: \u0026lt;a-release-name\u0026gt; spec: selector: matchLabels: suite: pachyderm namespaceSelector: matchNames: - default endpoints: - port: prom-metrics interval: 30s Create a ServiceMonitor looking to scrape metrics from suite: pachyderm:\nkubectl create -f myprometheusservice.yaml The prometheus-operator will search for the pods based on the label selector \u0026lt;a-release-name\u0026gt; and creates a prometheus target so prometheus will scrape the metrics endpoint prom-metrics.\nIn this case, it looks for anything with the label suite: pachyderm - which is by default associated with all Pachyderm resources.\n‚ÑπÔ∏è Our Service Monitor pachyderm-scraper above maps the endpoint port prom-metrics to a corresponding prom-metrics port described in Pachyderm\u0026rsquo;s deployment manifest. Let\u0026rsquo;s take a quick look at this file:\nkubectl -o json get service/pachd In the json file, find:\n{ \u0026#34;name\u0026#34;: \u0026#34;prom-metrics\u0026#34;, \u0026#34;port\u0026#34;: 1656, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34;, \u0026#34;targetPort\u0026#34;: \u0026#34;prom-metrics\u0026#34; } Port-Forward # One last step before you can collect your metrics: If you followed the instruction above, you can connect to Prometheus by using kubectl port-forward.\nkubectl port-forward pod/prometheus-\u0026lt;a-release-name\u0026gt;-kube-prometheus-stack-prometheus-0 9090 If you have an existing Prometheus deployment, please navigate to your Prometheus GUI.\nBrowse # You can now browse your targets (http://localhost:9090/targets). Run a pipeline of your choice. The pachyderm-scraper should be visible:\nIn the ClassicUI tab, you should be able to see the new Pachydermmetrics.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "c316fabb6180b00a0269901156dbad2d"
  },
  {
    "title": "Job Metrics",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Monitor with Prometheus",
    "description": "Learn about the job metrics available.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/prometheus/job-metrics/",
    "relURI": "/latest/manage/prometheus/job-metrics/",
    "body": "pachyderm_worker_datum_count pachyderm_worker_datum_download_bytes_count pachyderm_worker_datum_download_seconds_count pachyderm_worker_datum_download_size_bucket pachyderm_worker_datum_download_size_count pachyderm_worker_datum_download_size_sum pachyderm_worker_datum_download_time_bucket pachyderm_worker_datum_download_time_count pachyderm_worker_datum_download_time_sum pachyderm_worker_datum_proc_seconds_count pachyderm_worker_datum_proc_time_bucket pachyderm_worker_datum_proc_time_count pachyderm_worker_datum_proc_time_sum pachyderm_worker_datum_upload_bytes_count pachyderm_worker_datum_upload_seconds_count pachyderm_worker_datum_upload_size_bucket pachyderm_worker_datum_upload_size_count pachyderm_worker_datum_upload_size_sum pachyderm_worker_datum_upload_time_bucket pachyderm_worker_datum_upload_time_count pachyderm_worker_datum_upload_time_sum\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "daea9f3728f9245e2fb693209e91275a"
  },
  {
    "title": "Pachd Metrics",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Monitor with Prometheus",
    "description": "Learn about the pachd metrics available.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/prometheus/pachd-metrics/",
    "relURI": "/latest/manage/prometheus/pachd-metrics/",
    "body": "pachyderm_pachd_auth_log_req_seconds_count pachyderm_pachd_cache_object_cache_hits_gauge pachyderm_pachd_cache_object_gets_gauge pachyderm_pachd_cache_object_info_cache_hits_gauge pachyderm_pachd_cache_object_info_gets_gauge pachyderm_pachd_cache_object_info_loads_deduped_gauge pachyderm_pachd_cache_object_info_loads_gauge pachyderm_pachd_cache_object_info_local_load_errs_gauge pachyderm_pachd_cache_object_info_local_loads_gauge pachyderm_pachd_cache_object_info_peer_errors_gauge pachyderm_pachd_cache_object_info_peer_loads_gauge pachyderm_pachd_cache_object_info_server_requests_gauge pachyderm_pachd_cache_object_loads_deduped_gauge pachyderm_pachd_cache_object_loads_gauge pachyderm_pachd_cache_object_local_load_errs_gauge pachyderm_pachd_cache_object_local_loads_gauge pachyderm_pachd_cache_object_peer_errors_gauge pachyderm_pachd_cache_object_peer_loads_gauge pachyderm_pachd_cache_object_server_requests_gauge pachyderm_pachd_cache_tag_cache_hits_gauge pachyderm_pachd_cache_tag_gets_gauge pachyderm_pachd_cache_tag_loads_deduped_gauge pachyderm_pachd_cache_tag_loads_gauge pachyderm_pachd_cache_tag_local_load_errs_gauge pachyderm_pachd_cache_tag_local_loads_gauge pachyderm_pachd_cache_tag_peer_errors_gauge pachyderm_pachd_cache_tag_peer_loads_gauge pachyderm_pachd_cache_tag_server_requests_gauge pachyderm_pachd_enterprise_activate_seconds_count pachyderm_pachd_enterprise_activate_time_bucket pachyderm_pachd_enterprise_activate_time_count pachyderm_pachd_enterprise_activate_time_sum pachyderm_pachd_enterprise_log_req_seconds_count pachyderm_pachd_pfs_check_object_seconds_count pachyderm_pachd_pfs_check_object_time_bucket pachyderm_pachd_pfs_check_object_time_count pachyderm_pachd_pfs_check_object_time_sum pachyderm_pachd_pfs_create_repo_seconds_count pachyderm_pachd_pfs_create_repo_time_bucket pachyderm_pachd_pfs_create_repo_time_count pachyderm_pachd_pfs_create_repo_time_sum pachyderm_pachd_pfs_delete_all_seconds_count pachyderm_pachd_pfs_delete_all_time_bucket pachyderm_pachd_pfs_delete_all_time_count pachyderm_pachd_pfs_delete_all_time_sum pachyderm_pachd_pfs_delete_branch_seconds_count pachyderm_pachd_pfs_delete_branch_time_bucket pachyderm_pachd_pfs_delete_branch_time_count pachyderm_pachd_pfs_delete_branch_time_sum pachyderm_pachd_pfs_delete_repo_seconds_count pachyderm_pachd_pfs_delete_repo_time_bucket pachyderm_pachd_pfs_delete_repo_time_count pachyderm_pachd_pfs_delete_repo_time_sum pachyderm_pachd_pfs_finish_commit_seconds_count pachyderm_pachd_pfs_finish_commit_time_bucket pachyderm_pachd_pfs_finish_commit_time_count pachyderm_pachd_pfs_finish_commit_time_sum pachyderm_pachd_pfs_func_1_seconds_count pachyderm_pachd_pfs_get_file_seconds_count pachyderm_pachd_pfs_get_file_time_bucket pachyderm_pachd_pfs_get_file_time_count pachyderm_pachd_pfs_get_file_time_sum pachyderm_pachd_pfs_get_object_seconds_count pachyderm_pachd_pfs_get_object_time_bucket pachyderm_pachd_pfs_get_object_time_count pachyderm_pachd_pfs_get_object_time_sum pachyderm_pachd_pfs_get_objects_seconds_count pachyderm_pachd_pfs_get_objects_time_bucket pachyderm_pachd_pfs_get_objects_time_count pachyderm_pachd_pfs_get_objects_time_sum pachyderm_pachd_pfs_inspect_branch_seconds_count pachyderm_pachd_pfs_inspect_branch_time_bucket pachyderm_pachd_pfs_inspect_branch_time_count pachyderm_pachd_pfs_inspect_branch_time_sum pachyderm_pachd_pfs_inspect_object_seconds_count pachyderm_pachd_pfs_inspect_object_time_bucket pachyderm_pachd_pfs_inspect_object_time_count pachyderm_pachd_pfs_inspect_object_time_sum pachyderm_pachd_pfs_inspect_repo_seconds_count pachyderm_pachd_pfs_inspect_repo_time_bucket pachyderm_pachd_pfs_inspect_repo_time_count pachyderm_pachd_pfs_inspect_repo_time_sum pachyderm_pachd_pfs_list_file_stream_seconds_count pachyderm_pachd_pfs_list_file_stream_time_bucket pachyderm_pachd_pfs_list_file_stream_time_count pachyderm_pachd_pfs_list_file_stream_time_sum pachyderm_pachd_pfs_list_repo_seconds_count pachyderm_pachd_pfs_list_repo_time_bucket pachyderm_pachd_pfs_list_repo_time_count pachyderm_pachd_pfs_list_repo_time_sum pachyderm_pachd_pfs_put_file_seconds_count pachyderm_pachd_pfs_put_file_time_bucket pachyderm_pachd_pfs_put_file_time_count pachyderm_pachd_pfs_put_file_time_sum pachyderm_pachd_pfs_put_object_seconds_count pachyderm_pachd_pfs_put_object_split_seconds_count pachyderm_pachd_pfs_put_object_split_time_bucket pachyderm_pachd_pfs_put_object_split_time_count pachyderm_pachd_pfs_put_object_split_time_sum pachyderm_pachd_pfs_put_object_time_bucket pachyderm_pachd_pfs_put_object_time_count pachyderm_pachd_pfs_put_object_time_sum pachyderm_pachd_pfs_start_commit_seconds_count pachyderm_pachd_pfs_start_commit_time_bucket pachyderm_pachd_pfs_start_commit_time_count pachyderm_pachd_pfs_start_commit_time_sum pachyderm_pachd_pps_create_pipeline_seconds_count pachyderm_pachd_pps_create_pipeline_time_bucket pachyderm_pachd_pps_create_pipeline_time_count pachyderm_pachd_pps_create_pipeline_time_sum pachyderm_pachd_pps_delete_all_seconds_count pachyderm_pachd_pps_delete_all_time_bucket pachyderm_pachd_pps_delete_all_time_count pachyderm_pachd_pps_delete_all_time_sum pachyderm_pachd_pps_delete_job_seconds_count pachyderm_pachd_pps_delete_job_time_bucket pachyderm_pachd_pps_delete_job_time_count pachyderm_pachd_pps_delete_job_time_sum pachyderm_pachd_pps_delete_pipeline_seconds_count pachyderm_pachd_pps_delete_pipeline_time_bucket pachyderm_pachd_pps_delete_pipeline_time_count pachyderm_pachd_pps_delete_pipeline_time_sum pachyderm_pachd_pps_func_1_seconds_count pachyderm_pachd_pps_list_pipeline_seconds_count pachyderm_pachd_pps_list_pipeline_time_bucket pachyderm_pachd_pps_list_pipeline_time_count pachyderm_pachd_pps_list_pipeline_time_sum pachyderm_pachd_report_metric pachyderm_pachd_transaction_delete_all_seconds_count pachyderm_pachd_transaction_delete_all_time_bucket pachyderm_pachd_transaction_delete_all_time_count pachyderm_pachd_transaction_delete_all_time_sum pachyderm_pachd_transaction_func_1_seconds_count\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "064aa9a12b0e846729c5e3753ea6d302"
  },
  {
    "title": "Uninstall",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Manage",
    "description": "Learn how to uninstall our platform.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/manage/uninstall/",
    "relURI": "/latest/manage/uninstall/",
    "body": " Uninstall Pachyderm # helm uninstall pachd kubectl delete pvc -l suite=pachyderm Uninstall Pachctl # brew uninstall @\u0026lt;major\u0026gt;.\u0026lt;minor\u0026gt; ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e77bc743ead01661876be371bc798caf"
  },
  {
    "title": "Prepare Data",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Prepare your data for transformation.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/prepare-data/",
    "relURI": "/latest/prepare-data/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "872d7861878d992cd04c02f168436bb6"
  },
  {
    "title": "Datum Batching",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Prepare Data",
    "description": "Learn how to batch datums to optimize performance.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/prepare-data/datum-batching/",
    "relURI": "/latest/prepare-data/datum-batching/",
    "body": "By default, Pachyderm processes each datum independently. This means that your user code is called once for each datum. This can be inefficient and costly if you have a large number of small datums or if your user code is slow to start.\nWhen you have a large number of datums, you can batch them to optimize performance. Pachyderm provides a next datum command that you can use to batch datums.\nFlow Diagram # flowchart LR user_code(User Code) ndsuccess(NextDatum) nderror(\u0026#34;NextDatum(error)\u0026#34;) response(NextDatumResponse) process_datum{process datum} cmd_err(Run cmd_err) kill[Kill User Code] datum?{datum exists?} retry?{retry?} cmd_err?{cmd_err defined} user_code ==\u0026gt;ndsuccess ndsuccess =====\u0026gt; datum? datum? ==\u0026gt;|yes| process_datum process_datum ==\u0026gt;|success| response response ==\u0026gt; user_code datum? --\u0026gt;|no| kill process_datum --\u0026gt;|fail| nderror nderror --\u0026gt; cmd_err? cmd_err? --\u0026gt;|yes| cmd_err cmd_err? --\u0026gt;|no|kill cmd_err --\u0026gt; retry? retry? --\u0026gt;|yes| response retry? --\u0026gt;|no| kill How to Batch Datums # Via PachCTL # Define your user code and build a docker image. Your user code must call pachctl next datum to get the next datum to process.\nLanguage: Bash transformation() { # Your transformation code goes here echo \u0026#34;Transformation function executed\u0026#34; } echo \u0026#34;Starting while loop\u0026#34; while true; do pachctl next datum echo \u0026#34;Next datum called\u0026#34; transformation done Create a repo (e.g., pachctl create repo repoName).\nDefine a pipeline spec in YAML or JSON that references your Docker image and repo.\nAdd the following to the transform section of your pipeline spec:\ndatum_batching: true pipeline: name: p_datum_batching_example input: pfs: repo: repoName glob: \u0026#34;/*\u0026#34; transform: datum_batching: true image: user/docker-image:tag Create the pipeline (e.g., pachctl update pipeline -f pipeline.yaml).\nMonitor the pipeline\u0026rsquo;s state either via Console or via pachctl list pipeline.\nüí° You can view the printed confirmation of \u0026ldquo;Next datum called\u0026rdquo; in the logs your pipeline\u0026rsquo;s job.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["datums", "data-operations"],
    "id": "8e39de3fe15770b30c9d698b63215b9f"
  },
  {
    "title": "Defer Processing via Staging Branch",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Prepare Data",
    "description": "Learn how to defer processing of data by using a staging branch in an input repository.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/prepare-data/dp-staging-branch/",
    "relURI": "/latest/prepare-data/dp-staging-branch/",
    "body": "When you want to load data into Pachyderm without triggering a pipeline, you can upload it to a staging branch and then submit accumulated changes in one batch by re-pointing the HEAD of your master branch to a commit in the staging branch. Let\u0026rsquo;s see how this works.\nHow to Use a Staging Branch # Create a repository. For example, data.\npachctl create repo data Create a master branch.\npachctl create branch data@master View the created branch:\npachctl list commit data REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION data master 8090bfb4d4fe44158eac12199c37a591 About a minute ago 0B AUTO Pachyderm automatically created an empty HEAD commit on the new branch, as you can see from the 0B (zero-byte) size and AUTO commit origin.\nCommit a file to a staging branch:\npachctl put file data@staging -f \u0026lt;file\u0026gt; Pachyderm automatically creates the staging branch. Your repo now has 2 branches, staging and master. In this example, the staging name is used, but you can name the branch as you want \u0026ndash; and have as many staging branches as you need.\nVerify that the branches were created:\npachctl list branch data BRANCH HEAD TRIGGER staging f3506f0fab6e483e8338754081109e69 - master 8090bfb4d4fe44158eac12199c37a591 - The master branch still has the same HEAD commit. No jobs have started to process the new file, because there are no pipelines that take staging as inputs. You can continue to commit to staging to add new data to the branch, and the pipeline will not process anything.\nWhen you are ready to process the data, update the master branch to point it to the head of the staging branch:\npachctl create branch data@master --head staging List your branches to verify that the master branch\u0026rsquo;s HEAD commit has changed:\npachctl list branch data staging f3506f0fab6e483e8338754081109e69 master f3506f0fab6e483e8338754081109e69 The master and staging branches now have the same HEAD commit. This means that your pipeline has data to process.\nVerify that the pipeline has new jobs:\npachctl list job data@f3506f0fab6e483e8338754081109e69 ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE f3506f0fab6e483e8338754081109e69 data 32 seconds ago Less than a second 0 6 + 0 / 6 108B 24B success You should see one job that Pachyderm created for all the changes you have submitted to the staging branch, with the same ID. While the commits to the staging branch are ancestors of the current HEAD in master, they were never the actual HEAD of master themselves, so they do not get processed. This behavior works for most of the use cases because commits in Pachyderm are generally additive, so processing the HEAD commit also processes data from previous commits.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["datums", "data-operations"],
    "id": "7d9c137aa0b00cfb3b6e122fce00c830"
  },
  {
    "title": "Ingest Data",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Prepare Data",
    "description": "Learn how to ingest data using the pachctl put command.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/prepare-data/ingest-data/",
    "relURI": "/latest/prepare-data/ingest-data/",
    "body": " pachctl put file # ‚ÑπÔ∏è At any time, run pachctl put file --help for the complete list of flags available to you.\nLoad your data into Pachyderm by using pachctl requires that one or several input repositories have been created.\npachctl create repo \u0026lt;repo name\u0026gt; Use the pachctl put file command to put your data into the created repository. Select from the following options:\nAtomic commit: no open commit exists in your input repo. Pachyderm automatically starts a new commit, adds your data, and finishes the commit. pachctl put file \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;:\u0026lt;/path/to/file1\u0026gt; -f \u0026lt;file1\u0026gt; Alternatively, you can manually start a new commit, add your data in multiple put file calls, and close the commit by running pachctl finish commit.\nStart a commit: pachctl start commit \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; Put your data: pachctl put file \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;:\u0026lt;/path/to/file1\u0026gt; -f \u0026lt;file1\u0026gt; Put more data: pachctl put file \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;:\u0026lt;/path/to/file2\u0026gt; -f \u0026lt;file2\u0026gt; Close the commit: pachctl finish commit \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; Filepath Formats # üí° Pachyderm uses *?[]{}!()@+^ as reserved characters for glob patterns. Because of this, you cannot use these characters in your filepath.\nIn Pachyderm, you specify the path to file by using the -f option. A path to file can be a local path or a URL to an external resource. You can add multiple files or directories by using the -i option. To add contents of a directory, use the -r flag.\nThe following table provides examples of pachctl put file commands with various filepaths and data sources:\nPut data from a URL:\npachctl put file \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;:\u0026lt;/path/to/file\u0026gt; -f http://url_path Put data from an object store. You can use s3://, gcs://, or as:// in your filepath:\nchctl put file \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;:\u0026lt;/path/to/file\u0026gt; -f s3://object_store_url ‚ÑπÔ∏è If you are configuring a local cluster to access an external bucket, make sure that Pachyderm has been given the proper access.\nAdd multiple files at once by using the -i option or multiple -f flags. In the case of -i, the target file must be a list of files, paths, or URLs that you want to input all at once:\nchctl put file \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; -i \u0026lt;file containing list of files, paths, or URLs\u0026gt; Add an entire directory or all of the contents at a particular URL, either HTTP(S) or object store URL, s3://, gcs://, and as://, by using the recursive flag, -r:\npachctl put file \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; -r -f \u0026lt;dir\u0026gt; Loading Your Data Partially # Depending on your use case and the volume of your data, you might decide to keep your dataset in its original source and process only a subset in Pachyderm.\nAdd a metadata file containing a list of URL/path to your external data to your repo.\nYour pipeline code will retrieve the data following their path without the need to preload it all. In this case, Pachyderm will not keep versions of the source file, but it will keep track and provenance of the resulting output commits.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["ingest"],
    "id": "d615129a6dc4f62f20cebb711a82f0b1"
  },
  {
    "title": "Mount Volumes",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Prepare Data",
    "description": "Learn how to mount local or network-attached storage and use its data in pipelines.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/prepare-data/mount-volume/",
    "relURI": "/latest/prepare-data/mount-volume/",
    "body": "You may have a local or a network-attached storage that you want your pipeline to write files to. You can mount that folder as a volume in Kubernetes and make it available in your pipeline worker by using the pod_patch pipeline parameter. The pod_patch parameter takes a string that specifies the changes that you want to add to your existing manifest. To create a patch, you need to generate a diff of the original ReplicationController and the one with your changes. You can use one of the online JSON patch utilities, such as JSON Patch Generator to create a diff. A diff for mounting a volume might look like this:\n[ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/volumes/-\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;task-pv-storage\u0026#34;, \u0026#34;persistentVolumeClaim\u0026#34;: { \u0026#34;claimName\u0026#34;: \u0026#34;task-pv-claim\u0026#34; } } }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/containers/0/volumeMounts/-\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;mountPath\u0026#34;: \u0026#34;/data\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;task-pv-volume\u0026#34; } } ] This output needs to be converted into a one-liner and added to the pipeline spec.\nWe will use the OpenCV example. to demonstrate this functionality.\nTo mount a volume, complete the following steps:\nCreate a PersistentVolume and a PersistentVolumeClaim as described in Configure a Pod to Use a PersistentVolume for Storage. Modify mountPath and path as needed.\nFor testing purposes, you might want to add an index.html file as described in Create an index.html file.\nGet the ReplicationController (RC) manifest from your pipeline:\nkubectl get rc \u0026lt;rc-pipeline\u0026gt; -o json \u0026gt; \u0026lt;filename\u0026gt;.yaml Example:\nkubectl get rc pipeline-edges-v7 -o json \u0026gt; test-rc.yaml Open the generated RC manifest for editing.\nUnder spec, find the volumeMounts section.\nAdd your volume in the list of mounts.\nExample:\n{ \u0026#34;mountPath\u0026#34;: \u0026#34;/data\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;task-pv-storage\u0026#34; } mountPath is where your volume will be mounted inside of the container.\nFind the volumes section.\nAdd the information about the volume.\nExample:\n{ \u0026#34;name\u0026#34;: \u0026#34;task-pv-storage\u0026#34;, \u0026#34;persistentVolumeClaim\u0026#34;: { \u0026#34;claimName\u0026#34;: \u0026#34;task-pv-claim\u0026#34; } } In this section, you need to specify the PersistentVolumeClaim you have created in Step 1.\nSave these changes to a new file.\nCopy the contents of the original RC to the clipboard.\nGo to a JSON patch generator, such as JSON Patch Generator, and paste the contents of the original RC manifest to the Source JSON field.\nCopy the contents of the modified RC manifest to clipboard as described above.\nPaste the contents of the modified RC manifest to the Target JSON field.\nCopy the generated JSON Patch.\nGo to your terminal and open the pipeline manifest for editing.\nFor example, if you are modifying the edges pipeline, open the edges.json file.\nAdd the patch as a one-liner under the pod_patch parameter.\nExample:\n\u0026#34;pod_patch\u0026#34;: \u0026#34;[{\\\u0026#34;op\\\u0026#34;: \\\u0026#34;add\\\u0026#34;,\\\u0026#34;path\\\u0026#34;: \\\u0026#34;/volumes/-\\\u0026#34;,\\\u0026#34;value\\\u0026#34;: {\\\u0026#34;name\\\u0026#34;: \\\u0026#34;task-pv-storage\\\u0026#34;,\\\u0026#34;persistentVolumeClaim\\\u0026#34;: {\\\u0026#34;claimName\\\u0026#34;: \\\u0026#34;task-pv-claim\\\u0026#34;}}}, {\\\u0026#34;op\\\u0026#34;: \\\u0026#34;add\\\u0026#34;,\\\u0026#34;path\\\u0026#34;: \\\u0026#34;/containers/0/volumeMounts/-\\\u0026#34;,\\\u0026#34;value\\\u0026#34;: {\\\u0026#34;mountPath\\\u0026#34;: \\\u0026#34;/data\\\u0026#34;,\\\u0026#34;name\\\u0026#34;: \\\u0026#34;task-pv-storage\\\u0026#34;}}]\u0026#34; You need to add a backslash () before every quote (\u0026quot;) sign that is enclosed in square brackets ([]). Also, you might need to modify the path to volumeMounts and volumes by removing the /spec/template/spec/ prefix and replacing the assigned volume number with a dash (-). For example, if a path in the JSON patch is /spec/template/spec/volumes/5, you might need to replace it with /volumes/-. See the example above for details.\nAfter modifying the pipeline spec, update the pipeline:\npachctl update pipeline -f \u0026lt;pipeline-spec.yaml\u0026gt; A new pod and new replication controller should be created with your modified changes.\nVerify that your file was mounted by connecting to your pod and listing the directory that you have specified as a mountpoint. In this example, it is /data.\nExample:\nkubectl exec -it \u0026lt;pipeline-pod\u0026gt; -- /bin/bash ls /data If you have added the index.html file for testing as described in Step 1, you should see that file in the mounted directory.\nYou might want to adjust your pipeline code to read from or write to the mounted directory. For example, in the aforementioned OpenCV example, the code reads from the /pfs/images directory and writes to the /pfs/out directory. If you want to read or write to the /data directory, you need to change those to /data.\nüìñ Pachyderm has no notion of the files stored in the mounted directory before it is mounted to Pachyderm. Moreover, if you have mounted a network share to which you write files from other than Pachyderm sources, Pachyderm does not guarantee the provenance of those changes.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["data-operations"],
    "id": "b79ef65915885a552d52e8c72c4a280f"
  },
  {
    "title": "Skip Failed Datums",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Prepare Data",
    "description": "Learn how to skip failed datums to prevent jobs from fully failing.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/prepare-data/err-cmd/",
    "relURI": "/latest/prepare-data/err-cmd/",
    "body": " ‚ÑπÔ∏è The err_cmd parameter enables you to fail a datum without failing the whole job.\nüí° Before you read this section, make sure that you understand such concepts as Datum and Pipeline.\nWhen Pachyderm processes your data, it breaks it up into units of computation called datums. Each datum is processed separately. In a basic pipeline configuration, a failed datum results in a failed job. However, in some cases, you might not need all datums to consider a job successful. If your downstream pipelines can be run on only the successful datums instead of needing all the datums to be successful, Pachyderm can mark some datums as recovered which means that they failed with a non-critical error, but the successful datums will be processed.\nTo configure a condition under which you want your failed datums not to fail the whole job, you can add your custom error code in err_cmd and err_stdin fields in your pipeline specification.\nFor example, your DAG consists of two pipelines:\nThe pipeline 1 cleans the data. The pipeline 2 trains your model by using the data from the first pipeline. That means that the second pipeline takes the results of the first pipeline from its output repository and uses that data to train a model. In some cases, you might not need all the datums in the first pipeline to be successful to run the second pipeline.\nThe following diagram describes how Pachyderm transformation and error code work:\nHere is what is happening in the diagram above:\nPachyderm executes the transformation code that you defined in the cmd field against your datums. If a datum is processed without errors, Pachyderm marks it as processed. If a datum fails, Pachyderm executes your error code (err_cmd) on that datum. If the code in err_cmd successfully runs on the skipped datum, Pachyderm marks the skipped datum as recovered. The datum is in a failed state and, therefore, the pipeline does not put it into the output repository, but successful datums continue onto the next step in your DAG. If the err_cmd code fails on the skipped datum, the datum is marked as failed, and, consequently, the job is marked as failed. You can view the processed, skipped, and recovered datums in the PROGRESS field in the output of the pachctl list job -p \u0026lt;pipeline name\u0026gt; command:\nPachyderm writes only processed datums of successful jobs to the output commit so that these datums can be processed by downstream pipelines. For example, in your first pipeline, Pachyderm processes three datums. If one of the datums is marked as recovered and two others are successfully processed, only these two successful datums are used in the next pipeline.\nIf you want to let the job proceed with only the successful datums being written to the output, set \u0026quot;err_cmd\u0026quot; : [\u0026quot;true\u0026quot;]. The failed datums, which are \u0026ldquo;recovered\u0026rdquo; by err_cmd in this way, will be retried on the next job, just as failed datums.\n‚ÑπÔ∏è See Also: Example err_cmd pipeline\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["datums", "data-operations"],
    "id": "7d64be327c19fc41e80b2b6a32a8928f"
  },
  {
    "title": "SQL Ingest",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Prepare Data",
    "description": "Learn how to set up the SQL Ingest Tool to import data.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/prepare-data/sql-ingest/",
    "relURI": "/latest/prepare-data/sql-ingest/",
    "body": " ‚ö†Ô∏è SQL Ingest is an experimental feature.\nYou can inject database content, collected by your data warehouse, by pulling the result of a given query into Pachyderm and saving it as a CSV or JSON file.\nBefore You Start # You should be familiar with Jsonnet. You should be familiar with creating Jsonnet pipeline specs in Pachyderm. You should be familiar with managing Kubernetes secrets. How to Set Up SQL Ingest # 1. Create \u0026amp; Upload a Secret # You must generate a secret that contains the password granting user access to the database; you will pass the username details through the database connection string in step 2.\nCopy the following: kubectl create secret generic yourSecretName --from-literal=PACHYDERM_SQL_PASSWORD=yourDatabaseUserPassword --dry-run=client --output=json \u0026gt; yourSecretFile.json Swap out yourSecretName, yourDatabaseUserPassword, and yourSecretFile with relevant inputs. Open a terminal and run the command. Copy the following: pachctl create secret -f yourSecretFile.json Swap out yourSecretfile with relevant filename. Run the command. Confirm secret by running pachctl list secret. ‚ÑπÔ∏è Not all secret formats are the same. For a full walkthrough on how to create, edit, and view different types of secrets, see Create and Manage Secrets in Pachyderm.\n2. Create a Database Connection String # Pachyderm\u0026rsquo;s SQL Ingest requires a connection string defined as a Jsonnet URL parameter to connect to your database; the URL is structured as follows:\n\u0026lt;protocol\u0026gt;://\u0026lt;username\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;database\u0026gt;?\u0026lt;param1\u0026gt;=\u0026lt;value1\u0026gt;\u0026amp;\u0026lt;param2\u0026gt;=\u0026lt;value2\u0026gt; 3. Create a Pipeline Spec # Pachyderm provides a default Jsonnet template that has key parameters built in. To use it, you must pass an argument for each parameter.\nCopy the following: pachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.6.x/src/templates/sql_ingest_cron.jsonnet \\ --arg name=\u0026lt;pipelineName\u0026gt; \\ --arg url=\u0026#34;\u0026lt;connectionStringToDdatabase\u0026gt;\u0026#34; \\ --arg query=\u0026#34;\u0026lt;query\u0026gt;\u0026#34; \\ --arg hasHeader=\u0026lt;boolean\u0026gt; \\ --arg cronSpec=\u0026#34;\u0026lt;pullInterval\u0026gt;\u0026#34; \\ --arg secretName=\u0026#34;\u0026lt;youSecretName\u0026gt;\u0026#34; \\ --arg format=\u0026lt;CsvOrJson\u0026gt; --arg outputFile=\u0026#39;\u0026lt;fileName\u0026gt;\u0026#39; Swap out all of the parameter values with relevant inputs. Open terminal. Run the command. 4. View Query \u0026amp; Results # To View Query String: pachctl inspect pipeline \u0026lt;pipelineName\u0026gt; To View Output File Name: pachctl list file \u0026lt;pipelineName\u0026gt;@master To View Output File Contents: pachctl get file \u0026lt;pipelineName\u0026gt;@master:/0000 Example: Snowflake # In this example, we are leveraging Snowflake\u0026rsquo;s support for queries traversing semi-structured data (here, JSON).\nCreate a secret with your password named snowflakeSecret. Create a Snowflake specific database connection URL using the following details: Protocol: snowflake Username: username Host: VCNYTW-MH64356 (account name or locator) Database: SNOWFLAKE_SAMPLE_DATA Schema: WEATHER Warehouse: COMPUTE_WH snowflake://username@VCNYTW-MH64356/SNOWFLAKE_SAMPLE_DATA/WEATHER?warehouse=COMPUTE_WH Build query for the table DAILY_14_TOTAL using information from column V. select T, V:city.name, V:data[0].weather[0].description as morning, V:data[12].weather[0].description as pm FROM DAILY_14_TOTAL LIMIT 1 Define the pipeline spec by populating all of the parameter values: pachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.6.x/src/templates/sql_ingest_cron.jsonnet \\ --arg name=mysnowflakeingest \\ --arg url=\u0026#34;snowflake://username@VCNYTW-MH64356/SNOWFLAKE_SAMPLE_DATA/WEATHER?warehouse=COMPUTE_WH\u0026#34; \\ --arg query=\u0026#34;select T, V:city.name, V:data[0].weather[0].description as morning, V:data[12].weather[0].description as pm FROM DAILY_14_TOTAL LIMIT 1\u0026#34; \\ --arg hasHeader=true \\ --arg cronSpec=\u0026#34;@every 30s\u0026#34; \\ --arg secretName=\u0026#34;snowflakeSecret\u0026#34; \\ --arg format=json Run the command. How Does This Work? # SQL Ingest\u0026rsquo;s Jsonnet pipeline spec, sql_ingest_cron.jsonnet, creates all of the following:\n1 Input Data Repo: Used to store timestamp files at the cronSpec\u0026rsquo;s set interval rate (--arg cronSpec=\u0026quot;pullInterval\u0026quot; \\) to trigger the pipeline. 1 Cron Pipeline: Houses the spec details that define the input type and settings and data transformation. 1 Output Repo: Used to store the data transformed by the cron pipeline; set by the pipeline spec\u0026rsquo;s pipeline.name attribute, which you can define through the Jsonnet parameter --arg name=outputRepoName \\. 1 Output File: Used to save the query results (JSON or CSV) and potentially be used as input for a following pipeline. In the default Jsonnet template, the file generated is obtainable from the output repo, outputRepoName@master:/0000. The filename is hardcoded, however you could paramaterize this as well using a custom Jsonnet pipeline spec and passing --arg outputFile='0000'. The file\u0026rsquo;s contents are the result of the query(--arg query=\u0026quot;query\u0026quot;) being ran against the database--arg url=\u0026quot;connectionStringToDdatabase\u0026quot; ; both are defined in the transform.cmd attribute.\nAbout SQL Ingest Pipeline Specs # To create an SQL Ingest Jsonnet Pipeline spec, you must have a .jsonnet file and several parameters:\npachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.6.x/src/templates/sql_ingest_cron.jsonnet \\ --arg name=\u0026lt;pipelineName\u0026gt; \\ --arg url=\u0026#34;\u0026lt;connectionStringToDdatabase\u0026gt;\u0026#34; \\ --arg query=\u0026#34;\u0026lt;query\u0026gt;\u0026#34; \\ --arg hasHeader=\u0026lt;boolean\u0026gt; \\ --arg cronSpec=\u0026#34;\u0026lt;pullInterval\u0026gt;\u0026#34; \\ --arg secretName=\u0026#34;\u0026lt;secretName\u0026gt;\u0026#34; \\ --arg format=\u0026lt;CsvOrJson\u0026gt; The name of each pipeline (and their related input/output repos) are derived from the name parameter (--arg name=\u0026lt;pipelineName\u0026gt;). Parameters # Parameter Description name The name of output repo where query results will materialize. url The connection string to the database. query The SQL query to be run against the connected database. hasHeader Adds a header to your CSV file if set to true. Ignored if format=\u0026quot;json\u0026quot; (JSON files always display (header,value) pairs for each returned row). Defaults to false. Pachyderm creates the header after each element of the comma separated list of your SELECT clause. For example country.country_name_eng will have country.country_name_eng as header while country.country_name_eng as country_name will have country_name. cronSpec How often to run the query. For example \u0026quot;@every 60s\u0026quot;. format The type of your output file containing the results of your query (either json or csv). secretName The Kubernetes secret name that contains the password to the database. outputFile The name of the file created by your pipeline and stored in your output repo; default 0000 URL Parameter Details # \u0026lt;protocol\u0026gt;://\u0026lt;username\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;database\u0026gt;?\u0026lt;param1\u0026gt;=\u0026lt;value1\u0026gt;\u0026amp;\u0026lt;param2\u0026gt;=\u0026lt;value2\u0026gt; Passwords are not included in the URL; they are retrieved from a secret. The additional parameters after ? are optional and needed on a case-by-case bases (for example, Snowflake). Parameter Description protocol The name of the database protocol. As of today, we support: - postgres and postgresql : connect to Postgresql or compatible (for example Redshift).\n- mysql : connect to MySQL or compatible (for example MariaDB). - snowflake : connect to Snowflake. username The user used to access the database. host The hostname of your database instance. port The port number your instance is listening on. database The name of the database to connect to. Snowflake # Pachyderm supports two connection URL patterns to query Snowflake:\nsnowflake://username@\u0026lt;account_identifier\u0026gt;/\u0026lt;db_name\u0026gt;/\u0026lt;schema_name\u0026gt;?warehouse=\u0026lt;warehouse_name\u0026gt; snowflake://username@hostname:port/\u0026lt;db_name\u0026gt;/\u0026lt;schema_name\u0026gt;?account=\u0026lt;account_identifier\u0026gt;\u0026amp;warehouse=\u0026lt;warehouse_name\u0026gt; The account_identifier takes one of the following forms for most URLs:\nOption 1 - Account Name:organization_name-account_name. Option 2 - Account Locator: account_locator.region.cloud. Formats \u0026amp; SQL Data Types # The following comments on formatting reflect the state of this release and are subject to change.\nFormats # Numeric # All numeric values are converted into strings in your CSV and JSON.\nDatabase CSV JSON 12345 12345 \u0026ldquo;12345\u0026rdquo; 123.45 123.45 \u0026ldquo;123.45\u0026rdquo; ‚ö†Ô∏è Note that infinite (Inf) and not a number (NaN) values will also be stored as strings in JSON files. Use this format #.# for all decimals that you plan to egress back to a database. Date/Timestamps # Type Database CSV JSON Date 2022-05-09 2022-05-09T00:00:00 \u0026ldquo;2022-05-09T00:00:00\u0026rdquo; Timestamp ntz 2022-05-09 16:43:00 2022-05-09T16:43:00 \u0026ldquo;2022-05-09T16:43:00\u0026rdquo; Timestamp tz 2022-05-09 16:43:00-05:00 2022-05-09T16:43:00-05:00 \u0026ldquo;2022-05-09T16:43:00-05:00\u0026rdquo; Strings # Database CSV \u0026ldquo;null\u0026rdquo; null `\u0026quot;\u0026quot;` \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; nil \u0026quot;my string\u0026quot; \u0026ldquo;\u0026ldquo;\u0026ldquo;my string\u0026rdquo;\u0026rdquo;\u0026rdquo; \u0026ldquo;this will be enclosed in quotes because it has a ,\u0026rdquo; \u0026ldquo;this will be enclosed in quotes because it has a ,\u0026rdquo; üí° When parsing your CSVs in your user code, remember to escape \u0026quot; with \u0026quot;\u0026quot;.\nSupported Data Types # Some of the Data Types listed in this section are specific to a particular database.\nDates/Timestamps Varchars Numerics Booleans DATE TIME\nTIMESTAMP\nTIMESTAMP_LTZ\nTIMESTAMP_NTZ\nTIMESTAMP_TZ\nTIMESTAMPTZ\nTIMESTAMP WITH TIME ZONE\nTIMESTAMP WITHOUT TIME ZONE VARCHAR\nTEXT\nCHARACTER VARYING SMALLINT\nINT2\nINTEGER\nINT\nINT4\nBIGINT\nINT8\nFLOAT\nFLOAT4\nFLOAT8\nREAL\nDOUBLE PRECISION\nNUMERIC\nDECIMAL\nNUMBER BOOL\nBOOLEAN ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["sql", "ingest", "data-operations"],
    "id": "6e91842f59b9bf8df7667be085067a04"
  },
  {
    "title": "Time-Windowed Data",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Prepare Data",
    "description": "Learn how to process historical data using time windows such as last 24 hours or the past 7 days.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/prepare-data/time-windows/",
    "relURI": "/latest/prepare-data/time-windows/",
    "body": " üìñ Before you read this section, make sure that you understand the concepts described in the following sections:\nDatum Distributed Computing Developer Workflow If you are analyzing data that is changing over time, you might need to analyze historical data. For example, you might need to examine the last two weeks of data, January\u0026rsquo;s data, or some other moving or static time window of data.\nPachyderm provides the following approaches to this task:\nFixed time windows - for rigid, fixed time windows, such as months (Jan, Feb, and so on) or days‚Äî01-01-17, 01-02-17, and so on.\nMoving time windows\nfor rolling time windows of data, such as three-day windows or two-week windows. Fixed Time Windows # Datum is the basic unit of data partitioning in Pachyderm. The glob pattern property in the pipeline specification defines a datum. When you analyze data within fixed time windows, such as the data that corresponds to fixed calendar dates, Pachyderm recommends that you organize your data repositories so that each of the time windows that you plan to analyze corresponds to a separate file or directory in your repository, and therefore, Pachyderm processes it as a separate datum.\nOrganizing your repository as described above, enables you to do the following:\nAnalyze each time window in parallel. Only re-process data within a time window when that data, or a corresponding data pipeline, changes. For example, if you have monthly time windows of sales data stored in JSON format that needs to be analyzed, you can create a sales data repository with the following data:\nsales ‚îú‚îÄ‚îÄ January | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ February | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îî‚îÄ‚îÄ ... ‚îî‚îÄ‚îÄ March ‚îú‚îÄ‚îÄ 01-01-17.json ‚îú‚îÄ‚îÄ 01-02-17.json ‚îî‚îÄ‚îÄ ... When you run a pipeline with sales as an input repository and a glob pattern of /*, Pachyderm processes each month\u0026rsquo;s worth of sales data in parallel if workers are available. When you add new data into a subset of the months or add data into a new month, for example, May, Pachyderm processes only these updated datums.\nMore generally, this structure enables you to create the following types of pipelines:\nPipelines that aggregate or otherwise process daily data on a monthly basis by using the /* glob pattern. Pipelines that only analyze a particular month\u0026rsquo;s data by using a /subdir/* or /subdir/ glob pattern. For example, /January/* or /January/. Pipelines that process data on daily by using the /*/* glob pattern. Any combination of the above. Moving Time Windows # In some cases, you need to run analyses for moving or rolling time windows that do not correspond to certain calendar months or days. For example, you might need to analyze the last three days of data, the three days of data before that, or similar. In other words, you need to run an analysis for every rolling length of time.\nFor rolling or moving time windows, there are a couple of recommended patterns:\nBin your data in repository folders for each of the moving time windows.\nMaintain a time-windowed set of data that corresponds to the latest of the moving time windows.\nBin Data into Moving Time Windows # In this method of processing rolling time windows, you create the following two-pipeline DAGs to analyze time windows efficiently:\nPipeline Description Pipeline 1 Reads in data, determines to which bins the data corresponds, and writes the data into those bins. Pipeline 2 Read in and analyze the binned data. By splitting this analysis into two pipelines, you can benefit from using parallelism at the file level. In other words, Pipeline 1 can be easily parallelized for each file, and Pipeline 2 can be parallelized per bin. This structure enables easy pipeline scaling as the number of files increases.\nFor example, you have three-day moving time windows, and you want to analyze three-day moving windows of sales data. In the first repo, called sales, you commit data for the first day of sales:\nsales ‚îî‚îÄ‚îÄ 01-01-17.json In the first pipeline, you specify to bin this data into a directory that corresponds to the first rolling time window from 01-01-17 to 01-03-17:\nbinned_sales ‚îî‚îÄ‚îÄ 01-01-17_to_01-03-17 ‚îî‚îÄ‚îÄ 01-01-17.json When the next day\u0026rsquo;s worth of sales is committed, that data lands in the sales repository:\nsales ‚îú‚îÄ‚îÄ 01-01-17.json ‚îî‚îÄ‚îÄ 01-02-17.json Then, the first pipeline executes again to bin the 01-02-17 data into relevant bins. In this case, the data is placed in the previously created bin named 01-01-17 to 01-03-17. However, the data also goes to the bin that stores the data that is received starting on 01-02-17:\nbinned_sales ‚îú‚îÄ‚îÄ 01-01-17_to_01-03-17 | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îî‚îÄ‚îÄ 01-02-17.json ‚îî‚îÄ‚îÄ 01-02-17_to_01-04-17 ‚îî‚îÄ‚îÄ 01-02-17.json As more and more daily data is added, your repository structure starting to looks as follows:\nbinned_sales ‚îú‚îÄ‚îÄ 01-01-17_to_01-03-17 | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îî‚îÄ‚îÄ 01-03-17.json ‚îú‚îÄ‚îÄ 01-02-17_to_01-04-17 | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îú‚îÄ‚îÄ 01-03-17.json | ‚îî‚îÄ‚îÄ 01-04-17.json ‚îú‚îÄ‚îÄ 01-03-17_to_01-05-17 | ‚îú‚îÄ‚îÄ 01-03-17.json | ‚îú‚îÄ‚îÄ 01-04-17.json | ‚îî‚îÄ‚îÄ 01-05-17.json ‚îî‚îÄ‚îÄ ... The following diagram describes how data accumulates in the repository over time:\nYour second pipeline can then process these bins in parallel according to the glob pattern of /* or as described further. Both pipelines can be easily parallelized.\nIn the above directory structure, it might seem that data is duplicated. However, under the hood, Pachyderm deduplicates all of these files and maintains a space-efficient representation of your data. The binning of the data is merely a structural re-arrangement to enable you to process these types of moving time windows.\nIt might also seem as if Pachyderm performs unnecessary data transfers over the network to bin files. However, Pachyderm ensures that these data operations do not require transferring data over the network.\nMaintaining a Single Time-Windowed Data Set # The advantage of the binning pattern above is that any of the moving time windows are available for processing. They can be compared, aggregated, and combined in any way, and any results or aggregations are kept in sync with updates to the bins. However, you do need to create a process to maintain the binning directory structure.\nThere is another pattern for moving time windows that avoids the binning of the above approach and maintains an up-to-date version of a moving time-windowed data set. This approach involves the creation of the following pipelines:\nPipeline Description Pipeline 1 Reads in data, determines which files belong in your moving time window, and writes the relevant files into an updated\nversion of the moving time-windowed data set. Pipeline 2 Reads in and analyzes the moving time-windowed data set. For example, you have three-day moving time windows, and you want to analyze three-day moving windows of sales data. The input data is stored in the sales repository:\nsales ‚îú‚îÄ‚îÄ 01-01-17.json ‚îú‚îÄ‚îÄ 01-02-17.json ‚îú‚îÄ‚îÄ 01-03-17.json ‚îî‚îÄ‚îÄ 01-04-17.json When the January 4th file, 01-04-17.json, is committed, the first pipeline pulls out the last three days of data and arranges it in the following order:\nlast_three_days ‚îú‚îÄ‚îÄ 01-02-17.json ‚îú‚îÄ‚îÄ 01-03-17.json ‚îî‚îÄ‚îÄ 01-04-17.json When the January 5th file, 01-05-17.json, is committed into the sales repository:\nsales ‚îú‚îÄ‚îÄ 01-01-17.json ‚îú‚îÄ‚îÄ 01-02-17.json ‚îú‚îÄ‚îÄ 01-03-17.json ‚îú‚îÄ‚îÄ 01-04-17.json ‚îî‚îÄ‚îÄ 01-05-17.json the first pipeline updates the moving window:\nlast_three_days ‚îú‚îÄ‚îÄ 01-03-17.json ‚îú‚îÄ‚îÄ 01-04-17.json ‚îî‚îÄ‚îÄ 01-05-17.json The analysis that you need to run on the moving windowed dataset in moving_sales_window can use the / or /* glob pattern, depending on whether you need to process all of the time-windowed files together or if they can be processed in parallel.\n‚ö†Ô∏è When you create this type of moving time-windowed data set, the concept of now or today is relative. You must define the time based on your use case. For example, by configuring to use UTC. Do not use functions such as time.now() to determine the current time. The actual time when this pipeline runs might vary.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["data-operations"],
    "id": "169b25da89c16785cf9004cd60d57241"
  },
  {
    "title": "Transactions",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Prepare Data",
    "description": "Learn how to use transactions to create collections of PachCTL commands that are executed concurrently.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/prepare-data/transactions/",
    "relURI": "/latest/prepare-data/transactions/",
    "body": " ‚ÑπÔ∏è TL;DR:Use transactions to run multiple Pachyderm commands simultaneously in one job run.\nA transaction is a Pachyderm operation that enables you to create a collection of Pachyderm commands and execute them concurrently. Regular Pachyderm operations, that are not in a transaction, are executed one after another. However, when you need to run multiple commands at the same time, you can use transactions. This functionality is useful in particular for pipelines with multiple inputs. If you need to update two or more input repos, you might not want pipeline jobs for each state change. You can issue a transaction to start commits in each of the input repos, which puts them both in the same global commit, creating a single downstream commit in the pipeline repo. After the transaction, you can put files and finish the commits at will, and the pipeline job will run once all the input commits have been finished.\nStart and Finish Transaction Demarcations # A transaction demarcation initializes some transactional behavior before the demarcated area begins, then ends that transactional behavior when the demarcated area ends. You should see those demarcations as a declaration of the group of commands that will be treated together as a single coherent operation.\nTo start a transaction demarcation, run the following command:\npachctl start transaction System Response:\nStarted new transaction: 7a81eab5e6c6430aa5c01deb06852ca5 This command generates a transaction object in the cluster and saves its ID in the local Pachyderm configuration file. By default, this file is stored at ~/.pachyderm/config.json.\nExample # { \u0026#34;user_id\u0026#34;: \u0026#34;b4fe4317-be21-4836-824f-6661c68b8fba\u0026#34;, \u0026#34;v2\u0026#34;: { \u0026#34;active_context\u0026#34;: \u0026#34;local-2\u0026#34;, \u0026#34;contexts\u0026#34;: { \u0026#34;default\u0026#34;: {}, \u0026#34;local-2\u0026#34;: { \u0026#34;source\u0026#34;: 3, \u0026#34;active_transaction\u0026#34;: \u0026#34;7a81eab5e6c6430aa5c01deb06852ca5\u0026#34;, \u0026#34;cluster_name\u0026#34;: \u0026#34;minikube\u0026#34;, \u0026#34;auth_info\u0026#34;: \u0026#34;minikube\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34; }, After you start a transaction demarcation, you can add supported commands (i.e., transactional commands), such as pachctl start commit, pachctl create branch \u0026hellip;, to the transaction.\nAll commands that are performed in a transaction are queued up and not executed against the actual cluster until you finish the transaction. When you finish the transaction, all queued command are executed atomically.\nTo finish a transaction, run:\npachctl finish transaction System Response:\nCompleted transaction with 1 requests: 7a81eab5e6c6430aa5c01deb06852ca5 üí° As soon as a commit is started (whether through start commit or put file without an open commit, or finishing a transaction that contains a start commit), a new global commit as well as a global job is created. All open commits are in a started state, each of the pipeline jobs created is running, and the workers waiting for the commit(s) to be closed to process the data. In other words, your changes will only be applied when you close the commits.\nIn the case of a transaction, the workers will wait until all of the input commits are finished to process them in one batch. All of those commits and jobs will be part of the same global commit/job and share the same globalID (Transaction ID). Without a transaction, each commit would trigger its own separate job.\nWe have used the inner join pipeline in our joins example to illustrate the difference between no transaction and the use a transaction, all other things being equal. Make sure to follow the example README if you want to run those pachctl commands yourself.\n‚ÑπÔ∏è Note that in the case with the transaction, the put file and following finish commit are happening after the finish transaction instruction.\nYou must finish your transaction before putting files in the corresponding repo for the data to be part of the same batch. Running a \u0026lsquo;put file\u0026rsquo; before closing the transaction would result in a commit being created independently from the transaction itself and a job to run on that commit.\nSupported Operations # While there is a transaction object in the Pachyderm configuration file, all supported API requests append the request to the transaction instead of running directly. These supported commands include:\ncreate repo delete repo update repo start commit finish commit squash commit create branch delete branch create pipeline update pipeline edit pipeline Each time you add a command to a transaction, Pachyderm validates the transaction against the current state of the cluster metadata and obtains any return values, which is important for such commands as start commit. If validation fails for any reason, Pachyderm does not add the operation to the transaction. If the transaction has been invalidated by changing the cluster state, you must delete the transaction and start over, taking into account the new state of the cluster. From a command-line perspective, these commands work identically within a transaction as without. The only differences are that you do not apply your changes until you run finish transaction, and Pachyderm logs a message to stderr to indicate that the command was placed in a transaction rather than run directly.\nOther Transaction Commands # Other supported commands for transactions include:\nCommand Description pachctl list transaction List all unfinished transactions available in the Pachyderm cluster. pachctl stop transaction Remove the currently active transaction from the local Pachyderm config file. The transaction remains in the Pachyderm cluster and can be resumed later. pachctl resume transaction Set an already-existing transaction as the active transaction in the local Pachyderm config file. pachctl delete transaction Deletes a transaction from the Pachyderm cluster. pachctl inspect transaction Provides detailed information about an existing transaction, including which operations it will perform. By default, displays information about the current transaction. If you specify a transaction ID, displays information about the corresponding transaction. Multiple Opened Transactions # Some systems have a notion of nested transactions. That is when you open transactions within an already opened transaction. In such systems, the operations added to the subsequent transactions are not executed until all the nested transactions and the main transaction are finished.\nPachyderm does not support such behavior. Instead, when you open a transaction, the transaction ID is written to the Pachyderm configuration file. If you begin another transaction while the first one is open, Pachyderm returns an error.\nEvery time you add a command to a transaction, Pachyderm creates a blueprint of the commit and verifies that the command is valid. However, one transaction can invalidate another. In this case, a transaction that is closed first takes precedence over the other. For example, if two transactions create a repository with the same name, the one that is executed first results in the creation of the repository, and the other results in error.\nUse Cases # Pachyderm users implement transactions to their own workflows finding unique ways to benefit from this feature, whether it is a small research team or an enterprise-grade machine learning workflow.\nBelow are examples of the most commonly employed ways of using transactions.\nCommit to Separate Repositories Simultaneously # For example, you have a Pachyderm pipeline with two input repositories. One repository includes training data and the other parameters for your machine learning pipeline. If you need to run specific data against specific parameters, you need to run your pipeline against specific commits in both repositories. To achieve this, you need to commit to these repositories simultaneously.\nIf you use a regular Pachyderm workflow, the data is uploaded sequentially, each time triggering a separate job instead of one job with both commits of new data. One put file operation commits changes to the data repository and the other updates the parameters repository. The following animation shows the standard Pachyderm workflow without a transaction:\nIn Pachyderm, a pipeline starts as soon as a new commit lands in a repository. In the diagram above, as soon as commit 1 is added to the data repository, Pachyderm runs a job for commit 1 and commit 0 in the parameters repository. You can also see that Pachyderm runs the second job and processes commit 1 from the data repository with the commit 1 in the parameters repository. In some cases, this is perfectly acceptable solution. But if your job takes many hours and you are only interested in the result of the pipeline run with commit 1 from both repositories, this approach does not work.\nWith transactions, you can ensure that only one job triggers with both the new data and parameters. The following animation demonstrates how transactions work:\nThe transaction ensures that a single job runs for the two commits that were started within the transaction. While Pachyderm supports some workflows where you can get the same effect by having both data and parameters in the same repo, often separating them and using transactions is much more efficient for organizational and performance reasons.\nSwitching from Staging to Master Simultaneously # If you are using deferred processing in your repositories because you want to commit your changes frequently without triggering jobs every time, then transactions can help you manage deferred processing with multiple inputs. You commit your changes to the staging branch and when needed, switch the HEAD of your master branch to a commit in the staging branch. To do this simultaneously, you can use transactions.\nFor example, you have two repositories data and parameters, both of which have a master and staging branch. You commit your changes to the staging branch while your pipeline is subscribed to the master branch. To switch to these branches simultaneously, you can use transactions like this:\npachctl start transaction System Response:\nStarted new transaction: 0d6f0bc337a0493696e382034a2a2055 pachctl pachctl create branch data@master --head staging Added to transaction: 0d6f0bc337a0493696e382034a2a2055 pachctl create branch parameters@master --head staging Added to transaction: 0d6f0bc337a0493696e382034a2a2055 pachctl finish transaction Completed transaction with 2 requests: 0d6f0bc337a0493696e382034a2a2055 When you finish the transaction, both repositories switch to the master branch at the same time which triggers one job to process those commits together.\nUpdating Multiple Pipelines Simultaneously # If you want to change logic or intermediate data formats in your DAG, you may need to change multiple pipelines. Performing these changes together in a transaction can avoid creating jobs with mismatched pipeline versions and potentially wasting work.\nTo get a better understanding of how transactions work in practice, try Use Transactions with Hyperparameter Tuning.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["data-operations"],
    "id": "bf263a88be574acc58d64de5e01a8bed"
  },
  {
    "title": "Build Pipelines & DAGs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Build pipelines \u0026 DAGs for every use case.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/",
    "relURI": "/latest/build-dags/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "2f210ad8da99579a08889f52bcd34e1a"
  },
  {
    "title": "Pipeline Specification (PPS)",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Build Pipelines & DAGs",
    "description": "Learn about the different attributes of a pipeline spec.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/",
    "relURI": "/latest/build-dags/pipeline-spec/",
    "body": "This document discusses each of the fields present in a pipeline specification.\nBefore You Start # Pachyderm\u0026rsquo;s pipeline specifications can be written in JSON or YAML. Pachyderm uses its json parser if the first character is {. A pipeline specification file can contain multiple pipeline declarations at once. Minimal Spec # Generally speaking, the only attributes that are strictly required for all scenarios are pipeline.name and transform. Beyond those, other attributes are conditionally required based on your pipeline\u0026rsquo;s use case. The following are a few examples of common use cases along with their minimally required attributes.\nUse Case: Cron Egress (DB) Egress (Storage) Input Service Spout S3 Full { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;wordcount\u0026#34;, \u0026#34;project\u0026#34;: { name: \u0026#34;projectName\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;wordcount-image\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;/binary\u0026#34;, \u0026#34;/pfs/data\u0026#34;, \u0026#34;/pfs/out\u0026#34;] }, \u0026#34;input\u0026#34;: { \u0026#34;cron\u0026#34;: { { \u0026#34;name\u0026#34;: string, \u0026#34;spec\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;start\u0026#34;: time, \u0026#34;overwrite\u0026#34;: bool } } } } { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;wordcount\u0026#34;, \u0026#34;project\u0026#34;: { name: \u0026#34;projectName\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;wordcount-image\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;/binary\u0026#34;, \u0026#34;/pfs/data\u0026#34;, \u0026#34;/pfs/out\u0026#34;] }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } }, \u0026#34;egress\u0026#34;: { \u0026#34;sql_database\u0026#34;: { \u0026#34;url\u0026#34;: string, \u0026#34;file_format\u0026#34;: { \u0026#34;type\u0026#34;: string, \u0026#34;columns\u0026#34;: [string] }, \u0026#34;secret\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;key\u0026#34;: \u0026#34;PACHYDERM_SQL_PASSWORD\u0026#34; } } }, } { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;wordcount\u0026#34;, \u0026#34;project\u0026#34;: { name: \u0026#34;projectName\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;wordcount-image\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;/binary\u0026#34;, \u0026#34;/pfs/data\u0026#34;, \u0026#34;/pfs/out\u0026#34;] }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } }, \u0026#34;egress\u0026#34;: { \u0026#34;URL\u0026#34;: \u0026#34;s3://bucket/dir\u0026#34; }, } { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;wordcount\u0026#34;, \u0026#34;project\u0026#34;: { name: \u0026#34;projectName\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;wordcount-image\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;/binary\u0026#34;, \u0026#34;/pfs/data\u0026#34;, \u0026#34;/pfs/out\u0026#34;] }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } } } { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;wordcount\u0026#34;, \u0026#34;project\u0026#34;: { name: \u0026#34;projectName\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;wordcount-image\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;/binary\u0026#34;, \u0026#34;/pfs/data\u0026#34;, \u0026#34;/pfs/out\u0026#34;] }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } }, \u0026#34;service\u0026#34;: { \u0026#34;internal_port\u0026#34;: int, \u0026#34;external_port\u0026#34;: int }, } { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;wordcount\u0026#34;, \u0026#34;project\u0026#34;: { name: \u0026#34;projectName\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;wordcount-image\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;/binary\u0026#34;, \u0026#34;/pfs/data\u0026#34;, \u0026#34;/pfs/out\u0026#34;] }, \u0026#34;spout\u0026#34;: { }, } { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;wordcount\u0026#34;, \u0026#34;project\u0026#34;: { name: \u0026#34;projectName\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;wordcount-image\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;/binary\u0026#34;, \u0026#34;/pfs/data\u0026#34;, \u0026#34;/pfs/out\u0026#34;] }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } }, \u0026#34;s3_out\u0026#34;: true, } { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;project\u0026#34;: { name: \u0026#34;projectName\u0026#34; }, }, \u0026#34;description\u0026#34;: string, \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;annotation\u0026#34;: string }, \u0026#34;labels\u0026#34;: { \u0026#34;label\u0026#34;: string } }, \u0026#34;tf_job\u0026#34;: { \u0026#34;tf_job\u0026#34;: string, }, \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: string, \u0026#34;cmd\u0026#34;: [ string ], \u0026#34;err_cmd\u0026#34;: [ string ], \u0026#34;env\u0026#34;: { string: string }, \u0026#34;secrets\u0026#34;: [ { \u0026#34;name\u0026#34;: string, \u0026#34;mount_path\u0026#34;: string }, { \u0026#34;name\u0026#34;: string, \u0026#34;env_var\u0026#34;: string, \u0026#34;key\u0026#34;: string } ], \u0026#34;image_pull_secrets\u0026#34;: [ string ], \u0026#34;stdin\u0026#34;: [ string ], \u0026#34;err_stdin\u0026#34;: [ string ], \u0026#34;accept_return_code\u0026#34;: [ int ], \u0026#34;debug\u0026#34;: bool, \u0026#34;user\u0026#34;: string, \u0026#34;working_dir\u0026#34;: string, \u0026#34;dockerfile\u0026#34;: string, \u0026#34;memory_volume\u0026#34;: bool, }, \u0026#34;parallelism_spec\u0026#34;: { \u0026#34;constant\u0026#34;: int }, \u0026#34;egress\u0026#34;: { // Egress to an object store \u0026#34;URL\u0026#34;: \u0026#34;s3://bucket/dir\u0026#34; // Egress to a database \u0026#34;sql_database\u0026#34;: { \u0026#34;url\u0026#34;: string, \u0026#34;file_format\u0026#34;: { \u0026#34;type\u0026#34;: string, \u0026#34;columns\u0026#34;: [string] }, \u0026#34;secret\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;key\u0026#34;: \u0026#34;PACHYDERM_SQL_PASSWORD\u0026#34; } } }, \u0026#34;update\u0026#34;: bool, \u0026#34;output_branch\u0026#34;: string, [ { \u0026#34;worker_id\u0026#34;: string, \u0026#34;job_id\u0026#34;: string, \u0026#34;datum_status\u0026#34; : { \u0026#34;started\u0026#34;: timestamp, \u0026#34;data\u0026#34;: [] } } ], \u0026#34;s3_out\u0026#34;: bool, \u0026#34;resource_requests\u0026#34;: { \u0026#34;cpu\u0026#34;: number, \u0026#34;memory\u0026#34;: string, \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: string, \u0026#34;number\u0026#34;: int } \u0026#34;disk\u0026#34;: string, }, \u0026#34;resource_limits\u0026#34;: { \u0026#34;cpu\u0026#34;: number, \u0026#34;memory\u0026#34;: string, \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: string, \u0026#34;number\u0026#34;: int } \u0026#34;disk\u0026#34;: string, }, \u0026#34;sidecar_resource_limits\u0026#34;: { \u0026#34;cpu\u0026#34;: number, \u0026#34;memory\u0026#34;: string, \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: string, \u0026#34;number\u0026#34;: int } \u0026#34;disk\u0026#34;: string, }, \u0026#34;input\u0026#34;: { \u0026lt;\u0026#34;pfs\u0026#34;, \u0026#34;cross\u0026#34;, \u0026#34;union\u0026#34;, \u0026#34;join\u0026#34;, \u0026#34;group\u0026#34; or \u0026#34;cron\u0026#34; see below\u0026gt; }, \u0026#34;description\u0026#34;: string, \u0026#34;reprocess\u0026#34;: bool, \u0026#34;service\u0026#34;: { \u0026#34;internal_port\u0026#34;: int, \u0026#34;external_port\u0026#34;: int }, \u0026#34;spout\u0026#34;: { \\\\ Optionally, you can combine a spout with a service: \u0026#34;service\u0026#34;: { \u0026#34;internal_port\u0026#34;: int, \u0026#34;external_port\u0026#34;: int } }, \u0026#34;datum_set_spec\u0026#34;: { \u0026#34;number\u0026#34;: int, \u0026#34;size_bytes\u0026#34;: int, \u0026#34;per_worker\u0026#34;: int, } \u0026#34;datum_timeout\u0026#34;: string, \u0026#34;job_timeout\u0026#34;: string, \u0026#34;salt\u0026#34;: string, \u0026#34;datum_tries\u0026#34;: int, \u0026#34;scheduling_spec\u0026#34;: { \u0026#34;node_selector\u0026#34;: {string: string}, \u0026#34;priority_class_name\u0026#34;: string }, \u0026#34;pod_spec\u0026#34;: string, \u0026#34;pod_patch\u0026#34;: string, \u0026#34;spec_commit\u0026#34;: { \u0026#34;option\u0026#34;: false, \u0026#34;branch\u0026#34;: { \u0026#34;option\u0026#34;: false, \u0026#34;repo\u0026#34;: { \u0026#34;option\u0026#34;: false, \u0026#34;name\u0026#34;: string, \u0026#34;type\u0026#34;: string, \u0026#34;project\u0026#34;:{ \u0026#34;option\u0026#34;: false, \u0026#34;name\u0026#34;: string, }, }, \u0026#34;name\u0026#34;: string }, \u0026#34;id\u0026#34;: string, } \u0026#34;metadata\u0026#34;: { }, \u0026#34;reprocess_spec\u0026#34;: string, \u0026#34;autoscaling\u0026#34;: bool } ------------------------------------ \u0026#34;pfs\u0026#34; input ------------------------------------ \u0026#34;pfs\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;repo_type\u0026#34;:string, \u0026#34;branch\u0026#34;: string, \u0026#34;commit\u0026#34;:string, \u0026#34;glob\u0026#34;: string, \u0026#34;join_on\u0026#34;:string, \u0026#34;outer_join\u0026#34;: bool, \u0026#34;group_by\u0026#34;: string, \u0026#34;lazy\u0026#34; bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool, \u0026#34;trigger\u0026#34;: { \u0026#34;branch\u0026#34;: string, \u0026#34;all\u0026#34;: bool, \u0026#34;cron_spec\u0026#34;: string, }, } ------------------------------------ \u0026#34;cross\u0026#34; or \u0026#34;union\u0026#34; input ------------------------------------ \u0026#34;cross\u0026#34; or \u0026#34;union\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;lazy\u0026#34; bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;lazy\u0026#34; bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } } ... ] ------------------------------------ \u0026#34;join\u0026#34; input ------------------------------------ \u0026#34;join\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;join_on\u0026#34;: string, \u0026#34;outer_join\u0026#34;: bool, \u0026#34;lazy\u0026#34;: bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;join_on\u0026#34;: string, \u0026#34;outer_join\u0026#34;: bool, \u0026#34;lazy\u0026#34;: bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } } ] ------------------------------------ \u0026#34;group\u0026#34; input ------------------------------------ \u0026#34;group\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;group_by\u0026#34;: string, \u0026#34;lazy\u0026#34;: bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;group_by\u0026#34;: string, \u0026#34;lazy\u0026#34;: bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } } ] ------------------------------------ \u0026#34;cron\u0026#34; input ------------------------------------ \u0026#34;cron\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;spec\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;start\u0026#34;: time, \u0026#34;overwrite\u0026#34;: bool } ‚ÑπÔ∏è For a single-page view of all PPS options, go to the PPS series page.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipeline", "pipelines", "pipeline specification", "pps"],
    "id": "6c207d2f3ac2c28e2737bad2f2fbe05e"
  },
  {
    "title": "Autoscaling PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Enable autoscaling of the worker pool based on datums in queue.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/autoscaling/",
    "relURI": "/latest/build-dags/pipeline-spec/autoscaling/",
    "body": " Spec # \u0026#34;autoscaling\u0026#34;: false Behavior # The autoscaling attribute in a Pachyderm Pipeline Spec is used to specify whether the pipeline should automatically scale up or down based on the processing load.\nIf the autoscaling attribute is set to true, Pachyderm will monitor the processing load of the pipeline, and automatically scale up or down the number of worker nodes as needed to keep up with the demand. This can help to ensure that the pipeline is always running at optimal efficiency, without wasting resources when the load is low.\nautocaling is set to false by default. The maximum number of workers is controlled by the parallelism_spec. A pipeline with no outstanding jobs will go into standby. A pipeline in a standby state consumes no resources. When to Use # You should consider using the autoscaling attribute in a Pachyderm Pipeline Spec when you have a workload that has variable processing requirements or when the processing load of your pipeline is difficult to predict.\nExample scenarios:\nProcessing unpredictable workloads: If you have a workload that has variable processing requirements, it can be difficult to predict the number of worker nodes that will be needed to keep up with the demand. In this case, you could use the autoscaling attribute to automatically scale the number of worker nodes up or down based on the processing load.\nProcessing large datasets: If you have a pipeline that is processing a large dataset, it can be difficult to predict the processing requirements for the pipeline. In this case, you could use the autoscaling attribute to automatically scale the number of worker nodes based on the processing load, in order to keep up with the demand.\nHandling bursty workloads: If you have a workload that has periods of high demand followed by periods of low demand, it can be difficult to predict the processing requirements for the pipeline. In this case, you could use the autoscaling attribute to automatically scale the number of worker nodes up or down based on the processing load, in order to handle the bursty demand.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "cd69099dac9a464ef2e476413506b14e"
  },
  {
    "title": "Datum Set Spec PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Define how a pipeline should group its datums.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/datum-set-spec/",
    "relURI": "/latest/build-dags/pipeline-spec/datum-set-spec/",
    "body": " Spec # \u0026#34;datum_set_spec\u0026#34;: { \u0026#34;number\u0026#34;: 0, \u0026#34;size_bytes\u0026#34;: 0, \u0026#34;per_worker\u0026#34;: 0, } Attributes # Attribute Description number The desired number of datums in each datum set. If specified, each datum set will contain the specified number of datums. If the total number of input datums is not evenly divisible by the number of datums per set, the last datum set may contain fewer datums than the others. size_bytes The desired target size of each datum set in bytes. If specified, Pachyderm will attempt to create datum sets with the specified size, though the actual size may vary due to the size of the input files. per_worker The desired number of datum sets that each worker should process at a time. This field is similar to number, but specifies the number of sets per worker instead of the number of datums per set. Behavior # The datum_set_spec attribute in a Pachyderm Pipeline Spec is used to control how the input data is partitioned into individual datum sets for processing. Datum sets are the unit of work that workers claim, and each worker can claim 1 or more datums. Once done processing, it commits a full datum set.\nnumber if nonzero, specifies that each datum set should contain number datums. Sets may contain fewer if the total number of datums don\u0026rsquo;t divide evenly. If you lower the number to 1 it\u0026rsquo;ll update after every datum,the cost is extra load on etcd which can slow other stuff down. Default is 0.\nsize_bytes , if nonzero, specifies a target size for each set of datums. Sets may be larger or smaller than size_bytes, but will usually be pretty close to size_bytes in size. Default is 0.\nper_worker, if nonzero, specifies how many datum sets should be created for each worker. It can\u0026rsquo;t be set with number or size_bytes. Default is 0.\nWhen to Use # You should consider using the datum_set_spec attribute in your Pachyderm pipeline when you are experiencing stragglers, which are situations where most of the workers are idle but a few are still processing jobs. This can happen when the work is not divided up in a balanced way, which can cause some workers to be overloaded with work while others are idle.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "f503851fcadcb2d0209399ae5a74e6e6"
  },
  {
    "title": "Datum Timeout PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Set the maximum execution time allowed for each datum.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/datum-timeout/",
    "relURI": "/latest/build-dags/pipeline-spec/datum-timeout/",
    "body": " Spec # \u0026#34;datum_timeout\u0026#34;: string, Behavior # The datum_timeout attribute in a Pachyderm pipeline is used to specify the maximum amount of time that a worker is allowed to process a single datum in the pipeline.\nWhen a worker begins processing a datum, Pachyderm starts a timer that tracks the elapsed time since the datum was first assigned to the worker. If the worker has not finished processing the datum before the datum_timeout period has elapsed, Pachyderm will automatically mark the datum as failed and reassign it to another worker to retry. This helps to ensure that slow or problematic datums do not hold up the processing of the entire pipeline.\nOther considerations:\nNot set by default, allowing a datum to process for as long as needed. Takes precedence over the parallelism or number of datums; no single datum is allowed to exceed this value. The value must be a string that represents a time value, such as 1s, 5m, or 15h. When to Use # You should consider using the datum_timeout attribute in your Pachyderm pipeline when you are processing large or complex datums that may take a long time to process, and you want to avoid having individual datums hold up the processing of the entire pipeline.\nFor example, if you are processing images or videos that are particularly large, or if your pipeline is doing complex machine learning or deep learning operations that can take a long time to run on individual datums, setting a reasonable datum_timeout can help ensure that your pipeline continues to make progress even if some datums are slow or problematic.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "447b2339589a89d66ac5932e4506daed"
  },
  {
    "title": "Datum Tries PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Define the number of job attempts to run on a datum when a failure occurs.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/datum-tries/",
    "relURI": "/latest/build-dags/pipeline-spec/datum-tries/",
    "body": " Spec # \u0026#34;datum_tries\u0026#34;: int, Behavior # The datum_tries attribute in a Pachyderm pipeline specifies the maximum number of times a datum can be retried if it fails to process. When a datum fails to process, either because of an error in the processing logic or because it exceeds the datum_timeout value, Pachyderm will automatically retry the datum, up to the number of times specified in datum_tries.\nEach retry of a datum is treated as a new attempt, and the datum is added back to the job queue for processing. The retry process is transparent to the user and happens automatically within the Pachyderm system.\nOther considerations:\ndatum_tries is set to 0 by default. Setting to 1 attempts a job once with no retries. If the operation succeeds in retry attempts, then the job is marked as successful. Otherwise, the job is marked as failed. When to Use # You should consider setting a higher datum_tries count if your pipeline has a large number of datums that are prone to errors or timeouts, or if the datums you are working with have to be imported or fetched (via data ingress) from an external source.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "cc657dd16100a367a1205b82af07c6b8"
  },
  {
    "title": "Description PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Display meaningful information about your pipeline.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/description/",
    "relURI": "/latest/build-dags/pipeline-spec/description/",
    "body": " Spec # \u0026#34;description\u0026#34;: string, Behavior # description is displayed in your pipeline details when viewed from pachCTL or console.\nWhen to Use # It\u0026rsquo;s recommended to always provide meaningful descriptions to your Pachyderm resources.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "015864b54cb187fc6858d69735631c0c"
  },
  {
    "title": "Egress PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Push the results of a Pipeline to an external data store or an SQL Database.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/egress/",
    "relURI": "/latest/build-dags/pipeline-spec/egress/",
    "body": " Spec # \u0026#34;egress\u0026#34;: { // Egress to an object store \u0026#34;URL\u0026#34;: \u0026#34;s3://bucket/dir\u0026#34; // Egress to a database \u0026#34;sql_database\u0026#34;: { \u0026#34;url\u0026#34;: string, \u0026#34;file_format\u0026#34;: { \u0026#34;type\u0026#34;: string, \u0026#34;columns\u0026#34;: [string] }, \u0026#34;secret\u0026#34;: { \u0026#34;name\u0026#34;: string, \u0026#34;key\u0026#34;: \u0026#34;PACHYDERM_SQL_PASSWORD\u0026#34; } } }, Attributes # Attribute Description URL The URL of the object store where the pipeline\u0026rsquo;s output data should be written. sql_database An optional field that is used to specify how the pipeline should write output data to a SQL database. url The URL of the SQL database, in the format postgresql://user:password@host:port/database. file_format The file format of the output data, which can be specified as csv or tsv. This field also includes the column names that should be included in the output. secret The name and key of the Kubernetes secret that contains the password for the SQL database. Behavior # The egress field in a Pachyderm Pipeline Spec is used to specify how the pipeline should write the output data. The egress field supports two types of outputs: writing to an object store and writing to a SQL database.\nData is pushed after the user code finishes running but before the job is marked as successful. For more information, see Egress Data to an object store or Egress Data to a database.\nThis is required if the pipeline needs to write output data to an external storage system.\nWhen to Use # You should use the egress field in a Pachyderm Pipeline Spec when you need to write the output data from your pipeline to an external storage system, such as an object store or a SQL database.\nExample scenarios:\nLong-term data storage: If you need to store the output data from your pipeline for a long time, you can use the egress field to write the data to an object store, such as Amazon S3 or Google Cloud Storage.\nData sharing: If you need to share the output data from your pipeline with external users or systems, you can use the egress field to write the data to an object store that is accessible to those users or systems.\nAnalytics and reporting: If you need to perform further analytics or reporting on the output data from your pipeline, you can use the egress field to write the data to a SQL database that can be used for those purposes.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "ec1c39fa36da4c2008fe7197d78ebbd5"
  },
  {
    "title": "Input Cron PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Trigger pipelines based on time.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/input-cron/",
    "relURI": "/latest/build-dags/pipeline-spec/input-cron/",
    "body": " Spec # \u0026#34;input\u0026#34;: { \u0026#34;cron\u0026#34;: { { \u0026#34;name\u0026#34;: string, \u0026#34;spec\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;start\u0026#34;: time, \u0026#34;overwrite\u0026#34;: bool } } } Attributes # Attribute Required? Description name Yes The name of the cron job, which should be unique within the Pachyderm cluster. spec Yes The cron schedule for the job, specified using the standard cron format or macros. See schedule macros for examples. Pachyderm also supports non-standard schedules, such as \u0026quot;@daily\u0026quot;. repo No The name of the input repository that the cron job should read data from; default:\u0026lt;pipeline-name\u0026gt;_\u0026lt;input-name\u0026gt; start No Specifies the start time for the cron job. This is useful for running the job on a specific date in the future. If not specified, starts immediately. Specifying a time enables you to run on matching times from the past or skip times from the present and only start running on matching times in the future. Format the time value according to RFC3339. overwrite No Defines whether you want the timestamp file to be overwritten on each tick; defaults to simply writing new files on each tick. By default, when \u0026quot;overwrite\u0026quot; is disabled, ticks accumulate in the cron input repo. When \u0026quot;overwrite\u0026quot; is enabled, Pachyderm erases the old ticks and adds new ticks with each commit. If you do not add any manual ticks or run pachctl run cron, only one tick file per commit (for the latest tick) is added to the input repo. Behavior # The input field in a Pachyderm Pipeline Spec is used to specify the inputs to the pipeline, which are the Pachyderm repositories that the pipeline should read data from. The input field can include both static and dynamic inputs.\nThe cron field within the input field is used to specify a dynamic input that is based on a cron schedule. This is useful for pipelines that need to process data on a regular schedule, such as daily or hourly.\nA repo is created for each cron input. When a Cron input triggers, pachd commits a single file, named by the current RFC3339 timestamp to the repo which contains the time which satisfied the spec.\nCallouts # Avoid using intervals faster than 1-5 minutes You can use never during development and manually trigger the pipeline If using jsonnet, you can pass arguments like: --arg cronSpec=\u0026quot;@every 5m\u0026quot; You cannot update a cron pipeline after it has been created; instead, you must delete the pipeline and build a new one. When to Use # You should use a cron input in a Pachyderm Pipeline Spec when you need to process data on a regular schedule, such as hourly or daily. A cron input allows you to specify a schedule for the pipeline to run, and Pachyderm will automatically trigger the pipeline at the specified times.\nExample scenarios:\nBatch processing: If you have a large volume of data that needs to be processed on a regular schedule, a cron input can be used to trigger the processing automatically, without the need for manual intervention.\nData aggregation: If you need to collect data from different sources and process it on a regular schedule, a cron input can be used to automate the data collection and processing.\nReport generation: If you need to generate reports on a regular schedule, a cron input can be used to trigger the report generation process automatically.\nExamples # Examples: Every 60s Daily With Overwrites SQL Ingest \u0026#34;input\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tick\u0026#34;, \u0026#34;spec\u0026#34;: \u0026#34;@every 60s\u0026#34; } } \u0026#34;input\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tick\u0026#34;, \u0026#34;spec\u0026#34;: \u0026#34;@daily\u0026#34;, \u0026#34;overwrite\u0026#34;: true } } pachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.6.x/src/templates/sql_ingest_cron.jsonnet \\ --arg name=myingest \\ --arg url=\u0026#34;mysql://root@mysql:3306/test_db\u0026#34; \\ --arg query=\u0026#34;SELECT * FROM test_data\u0026#34; \\ --arg hasHeader=false \\ --arg cronSpec=\u0026#34;@every 60s\u0026#34; \\ --arg secretName=\u0026#34;mysql-creds\u0026#34; \\ --arg format=json ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "7288e0c3ede41b63cf085d69eb1d36d3"
  },
  {
    "title": "Input Cross PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Create a cross product of other inputs.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/input-cross/",
    "relURI": "/latest/build-dags/pipeline-spec/input-cross/",
    "body": " Spec # \u0026#34;input\u0026#34;: { \u0026#34;cross\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;project\u0026#34;: string, \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;lazy\u0026#34; bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;project\u0026#34;: string, \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;lazy\u0026#34; bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } } ... ] } Attributes # Attribute Description name The name of the PFS input that appears in the INPUT field when you run the pachctl list pipeline command. If an input name is not specified, it defaults to the name of the repo. repo Specifies the name of the Pachyderm repository that contains the input data. branch The branch to watch for commits. If left blank, Pachyderm sets this value to master. glob A wildcard pattern that defines how a dataset is broken up into datums for further processing. When you use a glob pattern in a group input, it creates a naming convention that Pachyderm uses to group the files. lazy Controls how the data is exposed to jobs. The default is false which means the job eagerly downloads the data it needs to process and exposes it as normal files on disk. If lazy is set to true, data is exposed as named pipes instead, and no data is downloaded until the job opens the pipe and reads it. If the pipe is never opened, then no data is downloaded. empty_files Controls how files are exposed to jobs. If set to true, it causes files from this PFS to be presented as empty files. This is useful in shuffle pipelines where you want to read the names of files and reorganize them by using symlinks. s3 Indicates whether the input data is stored in an S3 object store. Behavior # input.cross is an array of inputs to cross. The inputs do not have to be pfs inputs. They can also be union and cross inputs.\nA cross input creates tuples of the datums in the inputs. In the example below, each input includes individual datums, such as if foo and bar were in the same repository with the glob pattern set to /*. Alternatively, each of these datums might have come from separate repositories with the glob pattern set to / and being the only file system objects in these repositories.\n| inputA | inputB | inputA ‚®Ø inputB | | ------ | ------ | --------------- | | foo | fizz | (foo, fizz) | | bar | buzz | (foo, buzz) | | | | (bar, fizz) | | | | (bar, buzz) | The cross inputs above do not take a name and maintain the names of the sub-inputs. In the example above, you would see files under /pfs/inputA/... and /pfs/inputB/....\nWhen to Use # You should use a cross input in a Pachyderm Pipeline Spec when you need to perform operations on combinations of data from multiple Pachyderm repositories. The cross input allows you to generate a set of combinations of files between two or more repositories, which can be used as the input to your pipeline.\nExample scenarios:\nData analysis: If you have data from multiple sources that you need to combine and analyze, a cross input can be used to generate a set of combinations of data that can be used as the input to your analysis.\nMachine learning: If you need to train a machine learning model on combinations of data from multiple sources, a cross input can be used to generate a set of combinations of data that can be used as the input to your model.\nReport generation: If you need to generate reports that combine data from multiple sources, a cross input can be used to generate a set of combinations of data that can be used as the input to your report generation process.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "9fd240b1a1cda0d1cc2128f4f7e4a4c3"
  },
  {
    "title": "Input Group PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Group files stored in one or multiple repos.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/input-group/",
    "relURI": "/latest/build-dags/pipeline-spec/input-group/",
    "body": " Spec # \u0026#34;input\u0026#34;: { \u0026#34;group\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;project\u0026#34;: string, \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;group_by\u0026#34;: string, \u0026#34;lazy\u0026#34;: bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;project\u0026#34;: string, \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;group_by\u0026#34;: string, \u0026#34;lazy\u0026#34;: bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } } ] } Attributes # Attribute Description name The name of the PFS input that appears in the INPUT field when you run the pachctl list pipeline command. If an input name is not specified, it defaults to the name of the repo. repo Specifies the name of the Pachyderm repository that contains the input data. branch The branch to watch for commits. If left blank, Pachyderm sets this value to master. glob A wildcard pattern that defines how a dataset is broken up into datums for further processing. When you use a glob pattern in a group input, it creates a naming convention that Pachyderm uses to group the files. group_by A parameter that is used to group input files by a specific pattern. lazy Controls how the data is exposed to jobs. The default is false which means the job eagerly downloads the data it needs to process and exposes it as normal files on disk. If lazy is set to true, data is exposed as named pipes instead, and no data is downloaded until the job opens the pipe and reads it. If the pipe is never opened, then no data is downloaded. empty_files Controls how files are exposed to jobs. If set to true, it causes files from this PFS to be presented as empty files. This is useful in shuffle pipelines where you want to read the names of files and reorganize them by using symlinks. s3 Indicates whether the input data is stored in an S3 object store. Behavior # The group input in a Pachyderm Pipeline Spec allows you to group input files by a specific pattern.\nTo use the group input, you specify one or more PFS inputs with a group_by parameter. This parameter specifies a pattern or field to use for grouping the input files. The resulting groups are then passed to your pipeline as a series of grouped datums, where each datum is a single group of files.\nYou can specify multiple group input fields in a Pachyderm Pipeline Spec, each with their own group_by parameter. This allows you to group files by multiple fields or patterns, and pass each group to your pipeline as a separate datum.\nThe glob and group_by parameters must be configured.\nWhen to Use # You should consider using the group input in a Pachyderm Pipeline Spec when you have large datasets with multiple files that you want to partition or group by a specific field or pattern. This can be useful in a variety of scenarios, such as when you need to perform complex data analysis on a large dataset, or when you need to group files by some attribute or characteristic in order to facilitate further processing.\nExample scenarios:\nPartitioning data by time: If you have a large dataset that spans a long period of time, you might want to partition it by day, week, or month in order to perform time-based analysis or modeling. In this case, you could use the group input field to group files by date or time, and then process each group separately.\nGrouping data by user or account: If you have a dataset that includes data from multiple users or accounts, you might want to group the data by user or account in order to perform user-based analysis or modeling. In this case, you could use the group input field to group files by user or account, and then process each group separately.\nPartitioning data by geography: If you have a dataset that includes data from multiple geographic regions, you might want to partition it by region in order to perform location-based analysis or modeling. In this case, you could use the group input field to group files by region, and then process each group separately.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "af87d1757cc3d8883e362d73c9b179bd"
  },
  {
    "title": "Input Join PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Join files that are stored in separate repositories.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/input-join/",
    "relURI": "/latest/build-dags/pipeline-spec/input-join/",
    "body": " Spec # \u0026#34;input\u0026#34;: { \u0026#34;join\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;project\u0026#34;: string, \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;join_on\u0026#34;: string, \u0026#34;outer_join\u0026#34;: bool, \u0026#34;lazy\u0026#34;: bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;project\u0026#34;: string, \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;join_on\u0026#34;: string, \u0026#34;outer_join\u0026#34;: bool, \u0026#34;lazy\u0026#34;: bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } } ] } Behavior # A join input must have the glob and join_on parameters configured to work properly. A join can combine multiple PFS inputs. You can optionally add \u0026quot;outer_join\u0026quot;: true to your PFS input. In that case, you will alter the join\u0026rsquo;s behavior from a default \u0026ldquo;inner-join\u0026rdquo; (creates a datum if there is a match only) to a \u0026ldquo;outer-join\u0026rdquo; (the repos marked as \u0026quot;outer_join\u0026quot;: true will see a datum even if there is no match). You can set 0 to many PFS input to \u0026quot;outer_join\u0026quot;: true within your join. Capture Groups # When you configure a join input (inner or outer), you must specify a glob pattern that includes a capture group. The capture group defines the specific string in the file path that is used to match files in other joined repos. Capture groups work analogously to the regex capture group. You define the capture group inside parenthesis. Capture groups are numbered from left to right and can also be nested within each other. Numbering for nested capture groups is based on their opening parenthesis.\nBelow you can find a few examples of applying a glob pattern with a capture group to a file path. For example, if you have the following file path:\n/foo/bar-123/ABC.txt The following glob patterns in a joint input create the following capture groups:\nRegular expression Capture groups /(*) foo /*/bar-(*) 123 /(*)/*/(??)*.txt Capture group 1: foo, capture group 2: AB. /*/(bar-(123))/* Capture group 1: bar-123, capture group 2: 123. Also, joins require you to specify a replacement group in the join_on parameter to define which capture groups you want to tryto match.\nFor example, $1 indicates that you want Pachyderm to match based on capture group 1. Similarly, $2 matches the capture group 2. $1$2 means that it must match both capture groups 1 and 2.\nSee the full join input configuration in the pipeline specification.\nYou can test your glob pattern and capture groups by using the pachctl list datum -f \u0026lt;your_pipeline_spec.json\u0026gt; command.\nüí° The content of the capture group defined in the join_on parameter is available to your pipeline\u0026rsquo;s code in an environment variable: PACH_DATUM_\u0026lt;input.name\u0026gt;_JOIN_ON.\nExamples # Inner Join # Per default, a join input has an inner-join behavior.\nFor example, you have two repositories. One with sensor readings and the other with parameters. The repositories have the following structures:\nreadings repo:\n‚îú‚îÄ‚îÄ ID1234 ‚îú‚îÄ‚îÄ file1.txt ‚îú‚îÄ‚îÄ file2.txt ‚îú‚îÄ‚îÄ file3.txt ‚îú‚îÄ‚îÄ file4.txt ‚îú‚îÄ‚îÄ file5.txt parameters repo:\n‚îú‚îÄ‚îÄ file1.txt ‚îú‚îÄ‚îÄ file2.txt ‚îú‚îÄ‚îÄ file3.txt ‚îú‚îÄ‚îÄ file4.txt ‚îú‚îÄ‚îÄ file5.txt ‚îú‚îÄ‚îÄ file6.txt ‚îú‚îÄ‚îÄ file7.txt ‚îú‚îÄ‚îÄ file8.txt Pachyderm runs your code only on the pairs of files that match the glob pattern and capture groups.\nThe following example shows how you can use joins to group matching IDs:\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;joins\u0026#34; }, \u0026#34;input\u0026#34;: { \u0026#34;join\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;readings\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*/(*).txt\u0026#34;, \u0026#34;join_on\u0026#34;: \u0026#34;$1\u0026#34; } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;parameters\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/(*).txt\u0026#34;, \u0026#34;join_on\u0026#34;: \u0026#34;$1\u0026#34; } } ] }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python3\u0026#34;, \u0026#34;/joins.py\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;joins-example\u0026#34; } } The glob pattern for the readings repository, /*/(*).txt, indicates all matching files in the ID sub-directory. In the parameters repository, the glob pattern /(*).txt selects all the matching files in the root directory. All files with indices from 1 to 5 match. The files with indices from 6 to 8 do not match. Therefore, you only get five datums for this job.\nTo experiment further, see the full joins example.\nOuter Join # Pachyderm also supports outer joins. Outer joins include everything an inner join does plus the files that didn\u0026rsquo;t match anything. Inputs can be set to outer semantics independently. So while there isn\u0026rsquo;t an explicit notion of \u0026ldquo;left\u0026rdquo; or \u0026ldquo;right\u0026rdquo; outer joins, you can still get those semantics, and even extend them to multiway joins.\nBuilding off the previous example, notice that there are 3 files in the parameters repo, file6.txt, file7.txt and file8.txt, which don\u0026rsquo;t match any files in the readings repo. In an inner join, those files are omitted. If you still want to see the files without a match, you can use an outer join. The change to the pipeline spec is simple:\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;joins\u0026#34; }, \u0026#34;input\u0026#34;: { \u0026#34;join\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;readings\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*/(*).txt\u0026#34;, \u0026#34;join_on\u0026#34;: \u0026#34;$1\u0026#34; } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;parameters\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/(*).txt\u0026#34;, \u0026#34;join_on\u0026#34;: \u0026#34;$1\u0026#34;, \u0026#34;outer_join\u0026#34;: true } } ] }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python3\u0026#34;, \u0026#34;/joins.py\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;joins-example\u0026#34; } } Your code will see the joined pairs that it saw before. In addition to those five datums, your code will also see three new ones: one for each of the files in parameters that didn\u0026rsquo;t match. Note that this means that your code needs to handle (not crash) the case where input files are missing from /pfs/readings.\nTo experiment further, see the full join example.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "4412ffe879cb2f2f6d0cfa39d2f183a7"
  },
  {
    "title": "Input PFS PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Add data to an input repo.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/input-pfs/",
    "relURI": "/latest/build-dags/pipeline-spec/input-pfs/",
    "body": " Spec # \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;project\u0026#34;: string, \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;repo_type\u0026#34;:string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;join_on\u0026#34;:string, \u0026#34;outer_join\u0026#34;: bool, \u0026#34;group_by\u0026#34;: string, \u0026#34;lazy\u0026#34; bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool, \u0026#34;trigger\u0026#34;: { \u0026#34;branch\u0026#34;: string, \u0026#34;all\u0026#34;: bool, \u0026#34;cron_spec\u0026#34;: string, }, } } Behavior # input.pfs.name is the name of the input. An input with the name XXX is visible under the path /pfs/XXX when a job runs. Input names must be unique if the inputs are crossed, but they may be duplicated between PFSInputs that are combined by using the union operator. This is because when PFSInputs are combined, you only ever see a datum from one input at a time. Overlapping the names of combined inputs allows you to write simpler code since you no longer need to consider which input directory a particular datum comes from. If an input\u0026rsquo;s name is not specified, it defaults to the name of the repo. Therefore, if you have two crossed inputs from the same repo, you must give at least one of them a unique name.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "a28f553daeea0dd056e9ac1c1c2bf212"
  },
  {
    "title": "Input Union PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Create a union of pfs, cross, or other union inputs.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/input-union/",
    "relURI": "/latest/build-dags/pipeline-spec/input-union/",
    "body": " Spec # \u0026#34;input\u0026#34;: { \u0026#34;union\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;project\u0026#34;: string, \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;lazy\u0026#34; bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;project\u0026#34;: string, \u0026#34;name\u0026#34;: string, \u0026#34;repo\u0026#34;: string, \u0026#34;branch\u0026#34;: string, \u0026#34;glob\u0026#34;: string, \u0026#34;lazy\u0026#34; bool, \u0026#34;empty_files\u0026#34;: bool, \u0026#34;s3\u0026#34;: bool } } ... ] } Behavior # input.union is an array of inputs to combine. The inputs do not have to be pfs inputs. They can also be union and cross inputs.\nUnion inputs take the union of other inputs. In the example below, each input includes individual datums, such as if foo and bar were in the same repository with the glob pattern set to /*. Alternatively, each of these datums might have come from separate repositories with the glob pattern set to / and being the only file system objects in these repositories.\n| inputA | inputB | inputA ‚à™ inputB | | ------ | ------ | --------------- | | foo | fizz | foo | | bar | buzz | fizz | | | | bar | | | | buzz | The union inputs do not take a name and maintain the names of the sub-inputs. In the example above, you would see files under /pfs/inputA/... or /pfs/inputB/..., but never both at the same time. When you write code to address this behavior, make sure that your code first determines which input directory is present. Starting with Pachyderm 1.5.3, we recommend that you give your inputs the same Name. That way your code only needs to handle data being present in that directory. This only works if your code does not need to be aware of which of the underlying inputs the data comes from.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "0d13138b7676f7a553b771b4a26abdc8"
  },
  {
    "title": "Job Timeout PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Set the maximum execution time allowed for a job.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/job-timeout/",
    "relURI": "/latest/build-dags/pipeline-spec/job-timeout/",
    "body": " Spec # \u0026#34;job_timeout\u0026#34;: string, Behavior # Work that is not complete by set timeout is interrupted. Value must be a string that represents a time value, such as 1s, 5m, or 15h. Differs from datum_timeout in that the limit is applied across all workers and all datums. If not set, a job will run indefinitely until it succeeds or fails. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "74aa5fbf49742bcc02b885c9b86e3964"
  },
  {
    "title": "Metadata PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Add metadata to your pipeline pods using Kubernetes' labels and annotations.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/metadata/",
    "relURI": "/latest/build-dags/pipeline-spec/metadata/",
    "body": " Spec # \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;annotation\u0026#34;: string }, \u0026#34;labels\u0026#34;: { \u0026#34;label\u0026#34;: string } }, Behavior # Labels help organize and track cluster objects by creating groups of pods based on a given dimension.\nAnnotations enable you to specify any arbitrary metadata.\nBoth parameters require a key-value pair. Do not confuse this parameter with pod_patch, which adds metadata to the user container of the pipeline pod. For more information, see Labels and Selectors and Kubernetes Annotations in the Kubernetes documentation.\nWhen to Use # Use metadata for operation ergonomics and to simplify the querying of Kubernetes objects.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "46a43b35b353f90d027de977ec1bddce"
  },
  {
    "title": "Output Branch PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Define the branch where the pipeline outputs new commits.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/output-branch/",
    "relURI": "/latest/build-dags/pipeline-spec/output-branch/",
    "body": " Spec # \u0026#34;output_branch\u0026#34;: string, Behavior # Set to master by default. When to Use # Use this setting to output commits to dev or testing branches.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "68a68b806b2f22aad0cd0e42726859dd"
  },
  {
    "title": "Parallelism Spec PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Define the number of workers used in parallel.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/parallelism/",
    "relURI": "/latest/build-dags/pipeline-spec/parallelism/",
    "body": " Spec # \u0026#34;parallelism_spec\u0026#34;: { \u0026#34;constant\u0026#34;: int }, Behavior # Pachyderm starts the number of workers that you specify. For example, set \u0026quot;constant\u0026quot;:10 to use 10 workers.\nThe default value is 1 When to Use # ‚ö†Ô∏è Because spouts and services are designed to be single instances, do not modify the default parallism_spec value for these pipelines.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "c53698e0e03a6528e83f62c74542ffaa"
  },
  {
    "title": "Pod Patch PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Patch a Pod Spec.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/pod-patch/",
    "relURI": "/latest/build-dags/pipeline-spec/pod-patch/",
    "body": " Spec # \u0026#34;pod_patch\u0026#34;: string, Behavior # pod_patch is similar to pod_spec but is applied as a JSON Patch. Note, this means that the process outlined above of modifying an existing pod spec and then manually blanking unchanged fields won\u0026rsquo;t work, you\u0026rsquo;ll need to create a correctly formatted patch by diffing the two pod specs.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "577a7d552e38b75c454d8d3d638cdc35"
  },
  {
    "title": "Pod Spec PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Set fields in the Pod Spec that aren't explicitly exposed.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/pod-spec/",
    "relURI": "/latest/build-dags/pipeline-spec/pod-spec/",
    "body": " Spec # \u0026#34;pod_spec\u0026#34;: string, Behavior # pod_spec is an advanced option that allows you to set fields in the pod spec that haven\u0026rsquo;t been explicitly exposed in the rest of the pipeline spec. A good way to figure out what JSON you should pass is to create a pod in Kubernetes with the proper settings, then do:\nkubectl get po/\u0026lt;pod-name\u0026gt; -o json | jq .spec this will give you a correctly formatted piece of JSON, you should then remove the extraneous fields that Kubernetes injects or that can be set else where.\nThe JSON is applied after the other parameters for the pod_spec have already been set as a JSON Merge Patch. This means that you can modify things such as the storage and user containers.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "c391c2e6f1385f0916c835e7ac2ab9f1"
  },
  {
    "title": "Reprocess Spec PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Define the reprocessing behavior of a repo upon receiving new or modified. data.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/reprocess-spec/",
    "relURI": "/latest/build-dags/pipeline-spec/reprocess-spec/",
    "body": " Spec # \u0026#34;reprocess_spec\u0026#34;: string, Behavior # \u0026quot;reprocess_spec\u0026quot;: \u0026quot;until_success\u0026quot; is the default behavior. To mitigate datums failing for transient connection reasons, Pachyderm automatically retries user code three (3) times before marking a datum as failed. Additionally, you can set the datum_tries field to determine the number of times a job attempts to run on a datum when a failure occurs.\nLet\u0026rsquo;s compare \u0026quot;until_success\u0026quot; and \u0026quot;every_job\u0026quot;:\nSay we have 2 identical pipelines (reprocess_until_success.json and reprocess_at_every_job.json) but for the \u0026quot;reprocess_spec\u0026quot; field set to \u0026quot;every_job\u0026quot; in reprocess_at_every_job.json.\nBoth use the same input repo and have a glob pattern set to /*.\nWhen adding 3 text files to the input repo (file1.txt, file2.txt, file3.txt), the 2 pipelines (reprocess_until_success and reprocess_at_every_job) will process the 3 datums (here, the glob pattern /* creates one datum per file). Now, let\u0026rsquo;s add a 4th file file4.txt to our input repo or modify the content of file2.txt for example. Case of our default reprocess_until_success.json pipeline: A quick check at the list datum on the job id shows 4 datums, of which 3 were skipped. (Only the changed file was processed) Case of reprocess_at_every_job.json: A quick check at the list datum on the job id shows that all 4 datums were reprocessed, none were skipped. ‚ö†Ô∏è \u0026quot;reprocess_spec\u0026quot;: \u0026quot;every_job will not take advantage of Pachyderm\u0026rsquo;s default de-duplication. In effect, this can lead to slower pipeline performance. Before using this setting, consider other options such as including metadata in your file, naming your files with a timestamp, UUID, or other unique identifiers in order to take advantage of de-duplication.\nWhen to Use # Per default, Pachyderm avoids repeated processing of unchanged datums (i.e., it processes only the datums that have changed and skip the unchanged datums). This incremental behavior ensures efficient resource utilization. However, you might need to alter this behavior for specific use cases and force the reprocessing of all of your datums systematically. This is especially useful when your pipeline makes an external call to other resources, such as a deployment or triggering an external pipeline system. Set \u0026quot;reprocess_spec\u0026quot;: \u0026quot;every_job\u0026quot; in order to enable this behavior.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "f69a8cbc4e4d265f77c239508d863d4f"
  },
  {
    "title": "Resource Limits PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Set the upper threshold of allowed resources the user container can consume.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/resource-limits/",
    "relURI": "/latest/build-dags/pipeline-spec/resource-limits/",
    "body": " Spec # \u0026#34;resource_limits\u0026#34;: { \u0026#34;cpu\u0026#34;: number, \u0026#34;memory\u0026#34;: string, \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: string, \u0026#34;number\u0026#34;: int } \u0026#34;disk\u0026#34;: string, }, Behavior # resource_limits describes the upper threshold of allowed resources a given worker can consume. If a worker exceeds this value, it will be evicted.\nThe gpu field is a number that describes how many GPUs each worker needs. Only whole number are supported, Kubernetes does not allow multiplexing of GPUs. Unlike the other resource fields, GPUs only have meaning in Limits, by requesting a GPU the worker will have sole access to that GPU while it is running. It\u0026rsquo;s recommended to enable autoscaling if you are using GPUs so other processes in the cluster will have access to the GPUs while the pipeline has nothing to process. For more information about scheduling GPUs see the Kubernetes docs on the subject.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "544e4a72b905fa6e74f77f156be9410f"
  },
  {
    "title": "Resource Requests PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Set the minimum amount of resources that the user container will reserve.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/resource-request/",
    "relURI": "/latest/build-dags/pipeline-spec/resource-request/",
    "body": " Spec # \u0026#34;resource_requests\u0026#34;: { \u0026#34;cpu\u0026#34;: number, \u0026#34;memory\u0026#34;: string, \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: string, \u0026#34;number\u0026#34;: int } \u0026#34;disk\u0026#34;: string, }, Behavior # resource_requests describes the amount of resources that the pipeline workers will consume. Knowing this in advance enables Pachyderm to schedule big jobs on separate machines, so that they do not conflict, slow down, or terminate.\nThis parameter is optional, and if you do not explicitly add it in the pipeline spec, Pachyderm creates Kubernetes containers with the following default resources:\nThe user and storage containers request 1 CPU, 0 disk space, and 256MB of memory. The init container requests the same amount of CPU, memory, and disk space that is set for the user container. The resource_requests parameter enables you to overwrite these default values.\nThe memory field is a string that describes the amount of memory, in bytes, that each worker needs. Allowed SI suffixes include M, K, G, Mi, Ki, Gi, and other.\nFor example, a worker that needs to read a 1GB file into memory might set \u0026quot;memory\u0026quot;: \u0026quot;1.2G\u0026quot; with a little extra for the code to use in addition to the file. Workers for this pipeline will be placed on machines with at least 1.2GB of free memory, and other large workers will be prevented from using it, if they also set their resource_requests.\nThe cpu field is a number that describes the amount of CPU time in cpu seconds/real seconds that each worker needs. Setting \u0026quot;cpu\u0026quot;: 0.5 indicates that the worker should get 500ms of CPU time per second. Setting \u0026quot;cpu\u0026quot;: 2 indicates that the worker gets 2000ms of CPU time per second. In other words, it is using 2 CPUs, though worker threads might spend 500ms on four physical CPUs instead of one second on two physical CPUs.\nThe disk field is a string that describes the amount of ephemeral disk space, in bytes, that each worker needs. Allowed SI suffixes include M, K, G, Mi, Ki, Gi, and other.\nIn both cases, the resource requests are not upper bounds. If the worker uses more memory than it is requested, it does not mean that it will be shut down. However, if the whole node runs out of memory, Kubernetes starts deleting pods that have been placed on it and exceeded their memory request, to reclaim memory. To prevent deletion of your worker node, you must set your memory request to a sufficiently large value. However, if the total memory requested by all workers in the system is too large, Kubernetes cannot schedule new workers because no machine has enough unclaimed memory. cpu works similarly, but for CPU time.\nFor more information about resource requests and limits see the Kubernetes docs on the subject.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "0615db6626998887ede4eec7a92b6f1e"
  },
  {
    "title": "s3 Out PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Write results out to an S3 gateway endpoint.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/s3-out/",
    "relURI": "/latest/build-dags/pipeline-spec/s3-out/",
    "body": " Spec # \u0026#34;s3_out\u0026#34;: bool, Behavior # s3_out allows your pipeline code to write results out to an S3 gateway endpoint instead of the typical pfs/out directory. When this parameter is set to true, Pachyderm includes a sidecar S3 gateway instance container in the same pod as the pipeline container. The address of the output repository will be s3://\u0026lt;output_repo\u0026gt;.\nIf you want to expose an input repository through an S3 gateway, see input.pfs.s3 in PFS Input.\nWhen to Use # You should use the s3 Out attribute when you\u0026rsquo;d like to access and store the results of your Pachyderm transformations externally.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "10ab31aa06930377b5e52ac6fa7400a4"
  },
  {
    "title": "Scheduling Spec PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Define how the pipeline pods should be scheduled.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/scheduling-spec/",
    "relURI": "/latest/build-dags/pipeline-spec/scheduling-spec/",
    "body": " Spec # \u0026#34;scheduling_spec\u0026#34;: { \u0026#34;node_selector\u0026#34;: {string: string}, \u0026#34;priority_class_name\u0026#34;: string }, Attributes # Attribute Description node_selector Allows you to select which nodes your pipeline will run on. Refer to the Kubernetes docs on node selectors for more information about how this works. priority_class_name Allows you to select the priority class for the pipeline, which is how Kubernetes chooses to schedule and de-schedule the pipeline. Refer to the Kubernetes docs on priority and preemption for more information about how this works. Behavior # When you include a node_selector in the scheduling_spec, it tells Kubernetes to schedule the pipeline\u0026rsquo;s Pods on nodes that match the specified key-value pairs. For example, if you specify {\u0026quot;gpu\u0026quot;: \u0026quot;true\u0026quot;} in the node_selector, Kubernetes will only schedule the pipeline\u0026rsquo;s Pods on nodes that have a label gpu=true. This is useful when you have specific hardware or other node-specific requirements for your pipeline.\nWhen you specify a priority_class_name in the scheduling_spec, it tells Kubernetes to assign the specified priority class to the pipeline\u0026rsquo;s Pods. The priority class determines the priority of the Pods relative to other Pods in the cluster, and can affect the order in which Pods are scheduled and the resources they are allocated. For example, if you have a high-priority pipeline that needs to complete as quickly as possible, you can assign it a higher priority class than other Pods in the cluster to ensure that it gets scheduled and allocated resources first.\nWhen to Use # You should use the scheduling_spec field in a Pachyderm Pipeline Spec when you have specific requirements for where and when your pipeline runs. This can include requirements related to hardware, node labels, scheduling priority, and other factors.\nExample requirements:\nHardware requirements: If your pipeline requires specific hardware, such as GPUs, you can use the node_selector field to ensure that your pipeline runs on nodes that have the necessary hardware.\nNode labels: If you have specific requirements for node labels, such as data locality, you can use the node_selector field to schedule your pipeline on nodes with the appropriate labels.\nPriority: If you have a high-priority pipeline that needs to complete as quickly as possible, you can use the priority_class_name field to assign a higher priority class to your pipeline\u0026rsquo;s Pods.\nResource constraints: If your pipeline requires a large amount of resources, such as CPU or memory, you can use the node_selector field to ensure that your pipeline runs on nodes with sufficient resources.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "55617b3203c7e12015f7fa255951846d"
  },
  {
    "title": "Service PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Enable a pipeline to be treated as a long-running service.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/service/",
    "relURI": "/latest/build-dags/pipeline-spec/service/",
    "body": " Spec # \u0026#34;service\u0026#34;: { \u0026#34;internal_port\u0026#34;: int, \u0026#34;external_port\u0026#34;: int }, Attributes # Attribute Description internal_port The port that the user code binds to inside the container. external_port The port on which it is exposed through the NodePorts functionality of Kubernetes services. Behavior # When enabled, transform.cmd is not expected to exit and will restart if it does. The service becomes exposed outside the container using a Kubernetes service. You can access the service at http://\u0026lt;kubernetes-host\u0026gt;:\u0026lt;external_port\u0026gt;. The Service starts running at the first commit in the input repo. When to Use # You should use the service field in a Pachyderm Pipeline Spec when you want to expose your pipeline as a Kubernetes service, and allow other Kubernetes services or external clients to connect to it.\nExample scenarios:\nMicroservices architecture: If you are building a microservices architecture, you may want to expose individual pipelines as services that can be accessed by other services in the cluster. By using the service field to expose your pipeline as a Kubernetes service, you can easily connect it to other services in the cluster.\nClient access: If you want to allow external clients to access the output of your pipeline, you can use the service field to expose your pipeline as a Kubernetes service and provide clients with the service\u0026rsquo;s IP address and external_port.\nLoad balancing: By exposing your pipeline as a Kubernetes service, you can take advantage of Kubernetes\u0026rsquo; built-in load balancing capabilities. Kubernetes automatically load balances traffic to the service\u0026rsquo;s IP address and external_port across all the replicas of the pipeline\u0026rsquo;s container.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "8e77b32ea4adecd6c9bdbf31a58932b2"
  },
  {
    "title": "Sidecar Resource Limits PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Set the upper threshold of resources allocated to the storage container.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/sidecar-resource-limits/",
    "relURI": "/latest/build-dags/pipeline-spec/sidecar-resource-limits/",
    "body": " Spec # \u0026#34;sidecar_resource_limits\u0026#34;: { \u0026#34;cpu\u0026#34;: number, \u0026#34;memory\u0026#34;: string, \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: string, \u0026#34;number\u0026#34;: int } \u0026#34;disk\u0026#34;: string, }, Attributes # Attribute Description cpu The maximum number of CPU cores that the sidecar container can use. memory The maximum amount of memory that the sidecar container can use. This can be specified in bytes, or with a unit such as \u0026ldquo;Mi\u0026rdquo; or \u0026ldquo;Gi\u0026rdquo;. gpu An optional field that specifies the number and type of GPUs that the sidecar container can use. type The type of GPU to use, such as \u0026ldquo;nvidia\u0026rdquo; or \u0026ldquo;amd\u0026rdquo;. number The number of GPUs that the sidecar container can use. disk The maximum amount of disk space that the sidecar container can use. This can be specified in bytes, or with a unit such as \u0026ldquo;Mi\u0026rdquo; or \u0026ldquo;Gi\u0026rdquo;. Behavior # The sidecar_resource_limits field in a Pachyderm Pipeline Spec is used to specify the resource limits for any sidecar containers that are run alongside the main pipeline container.\nIn a Pachyderm Pipeline, sidecar containers can be used to perform additional tasks alongside the main pipeline container, such as logging, monitoring, or handling external dependencies. By specifying resource limits for these sidecar containers, you can ensure that they don\u0026rsquo;t consume too many resources and impact the performance of the main pipeline container.\nThis field can also be useful in deployments where Kubernetes automatically applies resource limits to containers, which might conflict with Pachyderm pipelines\u0026rsquo; resource requests. Such a deployment might fail if Pachyderm requests more than the default Kubernetes limit. The sidecar_resource_limits enables you to explicitly specify these resources to fix the issue.\nWhen to Use # You should use the sidecar_resource_limits field in a Pachyderm Pipeline Spec when you have sidecar containers that perform additional tasks alongside the main pipeline container, and you want to set resource limits for those sidecar containers.\nExample scenarios:\nLogging: If you have a sidecar container that is responsible for logging, you may want to limit its CPU and memory usage to prevent it from consuming too many resources and impacting the performance of the main pipeline container.\nMonitoring: If you have a sidecar container that is responsible for monitoring the pipeline, you may want to limit its CPU and memory usage to prevent it from competing with the main pipeline container for resources.\nExternal dependencies: If you have a sidecar container that provides external dependencies, such as a database, you may want to limit its CPU and memory usage to ensure that the main pipeline container has sufficient resources to perform its task.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "7ef263902261ed9826d96cec3921d89e"
  },
  {
    "title": "Sidecar Resource Requests PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Set the minimum amount of resources that the storage container will reserve.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/sidecar-resource-requests/",
    "relURI": "/latest/build-dags/pipeline-spec/sidecar-resource-requests/",
    "body": " Spec # \u0026#34;sidecar_resource_requests\u0026#34;: { \u0026#34;cpu\u0026#34;: number, \u0026#34;memory\u0026#34;: string, \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: string, \u0026#34;number\u0026#34;: int } \u0026#34;disk\u0026#34;: string, }, Attributes # Attribute Description cpu The minimum number of CPU cores that the storage container will reserve. memory The minimum amount of memory that the storage container will reserve. This can be specified in bytes, or with a unit such as \u0026ldquo;Mi\u0026rdquo; or \u0026ldquo;Gi\u0026rdquo;. gpu An optional field that specifies the number and type of GPUs that the storage container will reserve. type The type of GPU to use, such as \u0026ldquo;nvidia\u0026rdquo; or \u0026ldquo;amd\u0026rdquo;. number The number of GPUs that the storage container will reserve. disk The minimum amount of disk space that the storage container will reserve. This can be specified in bytes, or with a unit such as \u0026ldquo;Mi\u0026rdquo; or \u0026ldquo;Gi\u0026rdquo;. Behavior # The sidecar_resource_requests field in a Pachyderm Pipeline Spec is used to specify the resource requests for the storage container that runs alongside the user container.\nIn a Pachyderm Pipeline, the storage container is used to perform additional tasks alongside the user pipeline container, such as logging, monitoring, or handling external dependencies. By specifying resource requests for this sidecar container, you can ensure that the storage container has enough resources reserved as to not impact the performance of the user container.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "18c39d682bf0d6a1bd309d2f87dffe1d"
  },
  {
    "title": "Spec Commit PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "This attribute is auto-generated and is not configurable.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/spec-commit/",
    "relURI": "/latest/build-dags/pipeline-spec/spec-commit/",
    "body": " Spec # \u0026#34;spec_commit\u0026#34;: { \u0026#34;option\u0026#34;: false, \u0026#34;branch\u0026#34;: { \u0026#34;option\u0026#34;: false, \u0026#34;repo\u0026#34;: { \u0026#34;option\u0026#34;: false, \u0026#34;name\u0026#34;: string, \u0026#34;type\u0026#34;: string, \u0026#34;project\u0026#34;:{ \u0026#34;option\u0026#34;: false, \u0026#34;name\u0026#34;: string, }, }, \u0026#34;name\u0026#34;: string }, \u0026#34;id\u0026#34;: string, } When to Use # You do not need to ever configure this attribute; its details are auto-generated.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "2eb83eb11b8b3878b2afeff0502ce23c"
  },
  {
    "title": "Spout PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Ingest streaming data using a spout pipeline.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/spout/",
    "relURI": "/latest/build-dags/pipeline-spec/spout/",
    "body": " Spec # \u0026#34;spout\u0026#34;: { \\\\ Optionally, you can combine a spout with a service: \u0026#34;service\u0026#34;: { \u0026#34;internal_port\u0026#34;: int, \u0026#34;external_port\u0026#34;: int } }, Attributes # Attribute Description service An optional field that is used to specify how to expose the spout as a Kubernetes service. internal_port Used for the spout\u0026rsquo;s container. external_port Used for the Kubernetes service that exposes the spout. Behavior # Does not have a PFS input; instead, it consumes data from an outside source. Can have a service added to it. See Service. Its code runs continuously, waiting for new events. The output repo, pfs/out is not directly accessible. To write into the output repo, you must to use the put file API call via any of the following: pachctl put file A Pachyderm SDK (for golang or Python ) Your own API client. Pachyderm CLI (PachCTL) is packaged in the base image of your spout as well as your authentication information. As a result, the authentication is seamless when using PachCTL. Diagram # When to Use # You should use the spout field in a Pachyderm Pipeline Spec when you want to read data from an external source that is not stored in a Pachyderm repository. This can be useful in situations where you need to read data from a service that is not integrated with Pachyderm, such as an external API or a message queue.\nExample scenarios:\nData ingestion: If you have an external data source, such as a web service, that you want to read data from and process with Pachyderm, you can use the spout field to read the data into Pachyderm.\nReal-time data processing: If you need to process data in real-time and want to continuously read data from an external source, you can use the spout field to read the data into Pachyderm and process it as it arrives.\nData integration: If you have data stored in an external system, such as a message queue or a streaming service, and you want to integrate it with data stored in Pachyderm, you can use the spout field to read the data from the external system and process it in Pachyderm.\nExample # { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-spout\u0026#34; }, \u0026#34;spout\u0026#34;: { }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;go\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;./main.go\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;myaccount/myimage:0.1\u0026#34;, \u0026#34;env\u0026#34;: { \u0026#34;HOST\u0026#34;: \u0026#34;kafkahost\u0026#34;, \u0026#34;TOPIC\u0026#34;: \u0026#34;mytopic\u0026#34;, \u0026#34;PORT\u0026#34;: \u0026#34;9092\u0026#34; } } } üí° For a first overview of how spouts work, see our spout101 example.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "565289ed67af0f206bb217ba7b1fa6a5"
  },
  {
    "title": "Transform PPS",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Specification (PPS)",
    "description": "Set the name of the Docker image that your jobs use.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-spec/transform/",
    "relURI": "/latest/build-dags/pipeline-spec/transform/",
    "body": " Spec # \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: string, \u0026#34;cmd\u0026#34;: [ string ], \u0026#34;datum_batching\u0026#34;: bool, \u0026#34;err_cmd\u0026#34;: [ string ], \u0026#34;env\u0026#34;: { string: string }, \u0026#34;secrets\u0026#34;: [ { \u0026#34;name\u0026#34;: string, \u0026#34;mount_path\u0026#34;: string }, { \u0026#34;name\u0026#34;: string, \u0026#34;env_var\u0026#34;: string, \u0026#34;key\u0026#34;: string } ], \u0026#34;image_pull_secrets\u0026#34;: [ string ], \u0026#34;stdin\u0026#34;: [ string ], \u0026#34;err_stdin\u0026#34;: [ string ], \u0026#34;accept_return_code\u0026#34;: [ int ], \u0026#34;debug\u0026#34;: bool, \u0026#34;user\u0026#34;: string, \u0026#34;working_dir\u0026#34;: string, \u0026#34;dockerfile\u0026#34;: string, \u0026#34;memory_volume\u0026#34;: bool, }, Attributes # Attribute Description cmd Passes a command to the Docker run invocation. datum_batching Enables you to call your user code once for a batch of datums versus calling it per each datum. stdin Passes an array of lines to your command on stdin. err_cmd Passes a command executed on failed datums. err_stdin Passes an array of lines to your error command on stdin. env Enables a key-value map of environment variables that Pachyderm injects into the container. secrets Passes an array of secrets to embed sensitive data. image_pull_secrets Passes an array of secrets that are mounted before the containers are created. accept_return_code Passes an array of return codes that are considered acceptable when your Docker command exits. debug Enables debug logging for the pipeline user Sets the user that your code runs as. working_dir Sets the directory that your command runs from. memory_volume Sets pachyderm-worker\u0026rsquo;s emptyDir.Medium to Memory, allowing Kubernetes to mount a memory-backed volume (tmpfs). Behavior # cmd is not run inside a shell which means that wildcard globbing (*), pipes (|), and file redirects (\u0026gt; and \u0026gt;\u0026gt;) do not work. To specify these settings, you can set cmd to be a shell of your choice, such as sh and pass a shell script to stdin. err_cmd can be used to ignore failed datums while still writing successful datums to the output repo, instead of failing the whole job when some datums fail. The transform.err_cmd command has the same limitations as transform.cmd. stdin lines do not have to end in newline characters. The following environment variables are automatically injected into the container: PACH_JOB_ID ‚Äì the ID of the current job. PACH_OUTPUT_COMMIT_ID ‚Äì the ID of the commit in the output repo for the current job. \u0026lt;input\u0026gt;_COMMIT - the ID of the input commit. For example, if your input is the images repo, this will be images_COMMIT. secrets reference Kubernetes secrets by name and specify a path to map the secrets or an environment variable (env_var) that the value should be bound to. 0 is always considered a successful exit code. tmpfs is cleared on node reboot and any files you write count against your container\u0026rsquo;s memory limit. This may be useful for workloads that are IO heavy or use memory caches. When to Use # You must always use the transform attribute when making a pipeline.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "pps"],
    "id": "816944b2a3400ac5730e84843e3b06a8"
  },
  {
    "title": "Pipeline Ops",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Build Pipelines & DAGs",
    "description": "Learn how to create, delete, and update pipelines using PachCTL and jsonnet templating.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-operations/",
    "relURI": "/latest/build-dags/pipeline-operations/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e071175d2ef885183ad643f6c8bc327e"
  },
  {
    "title": "Create a Pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Ops",
    "description": "Learn how to create a pipeline using the pachctl create command.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-operations/create-pipeline/",
    "relURI": "/latest/build-dags/pipeline-operations/create-pipeline/",
    "body": "To create a pipeline, you need to define a pipeline specification in YAML, JSON, or Jsonnet.\nBefore You Start # A basic pipeline must have all of the following:\npipeline.name: The name of your pipeline. transform.cmd: The command that executes your user code. transform.img: The image that contains your user code. input.pfs.repo: The output repository for the transformed data. input.pfs.glob: The glob pattern used to identify the shape of datums. How to Create a Pipeline # Via Local File # Define a pipeline specification in YAML, JSON, or Jsonnet.\nPass the pipeline configuration to Pachyderm:\npachctl create pipeline -f \u0026lt;pipeline_spec\u0026gt; Via URL # Find a pipeline specification hosted in a public or internal repository. Pass the pipeline configuration to Pachyderm: pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/2.6.x/examples/opencv/edges.json Via Jsonnet # Jsonnet Pipeline specs let you create pipelines while passing a set of parameters dynamically, allowing you to reuse the baseline of a given pipeline while changing the values of chosen fields. You can, for example, create multiple pipelines out of the same jsonnet pipeline spec file while pointing each of them at different input repositories, parameterize a command line in the transform field of your pipelines, or dynamically pass various docker images to train different models on the same dataset.\nFor illustration purposes, in the following example, we are creating a pipeline named edges-1 and pointing its input repository at the repo \u0026lsquo;images\u0026rsquo;:\npachctl create pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg src=images üìñ You can define multiple pipeline specifications in one file by separating the specs with the following separator: ---. This works in both JSON and YAML files.\nExamples # JSON # { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;edges\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;A pipeline that performs image edge detection by using the OpenCV library.\u0026#34;, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python3\u0026#34;, \u0026#34;/edges.py\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/opencv\u0026#34; }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;images\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } } } YAML # pipeline: name: edges description: A pipeline that performs image edge detection by using the OpenCV library. transform: cmd: - python3 - \u0026#34;/edges.py\u0026#34; image: pachyderm/opencv input: pfs: repo: images glob: \u0026#34;/*\u0026#34; Considerations # When you create a pipeline, Pachyderm automatically creates an eponymous output repository. However, if such a repo already exists, your pipeline will take over the master branch. The files that were stored in the repo before will still be in the HEAD of the branch. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines"],
    "id": "e985dc0a1f8d1149231d630d28db9a69"
  },
  {
    "title": "Delete a Pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Ops",
    "description": "Learn how to delete a pipeline using the pachctl delete command.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-operations/delete-pipeline/",
    "relURI": "/latest/build-dags/pipeline-operations/delete-pipeline/",
    "body": "You can delete a pipeline by running:\npachctl delete pipeline \u0026lt;pipeline_name\u0026gt; To delete all of your pipelines (be careful with this feature), use the additional --all flag.\nWhen you delete a pipeline:\nKubernetes deletes all resources associated with the pipeline - pods (if any), services, and replication controllers. Pachyderm deletes the user output repository with all its data as well as the system meta (stats) and spec (historical versions of the pipeline specification file) repositories. ‚ÑπÔ∏è If you are using Pachyderm authorization features, only authorized users will be able to delete a given pipeline. In particular, they will have to be repoOwner of the output repo of the pipeline (i.e., have created the pipeline) or clusterAdmin.\nYou can use the --keep-repo flag to preserve the output repo with all its branches. However, important job metadata will still be deleted (including all historical versions of the pipeline specification file). As a result, you will not be able to recreate the deleted pipeline with the same name unless that repo is deleted first.\nExample # For example, if a pipeline \u0026ldquo;xyz\u0026rdquo; exists, then there is an output repo \u0026ldquo;xyz\u0026rdquo;. If a user deletes the pipeline with --keep-repo, the output repo \u0026ldquo;xyz\u0026rdquo; will remain, but the pipeline will be gone. If the user tries to create a new pipeline called \u0026ldquo;xyz\u0026rdquo;, it will fail (there is already an output repo with that name). For the pipeline creation to be successful, the user would have to delete repo \u0026ldquo;xyz\u0026rdquo; first.\n‚ÑπÔ∏è You can use the output repo of a pipeline deleted with --keep-repo as an input repo and add more data.\nWhen Pachyderm cannot delete a pipeline with the standard command, you might need to enforce deletion using the --force flag. Because this option can break dependent components in your DAG, use this option withextreme caution.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines"],
    "id": "8101e14cee8de4e8cd30462d6357370a"
  },
  {
    "title": "Jsonnet Pipeline Specifications",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Ops",
    "description": "Learn how to configure a pipeline using the Jsonnet templating language.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-operations/jsonnet-pipeline-specs/",
    "relURI": "/latest/build-dags/pipeline-operations/jsonnet-pipeline-specs/",
    "body": " ‚ö†Ô∏è Jsonnet pipeline specifications is an experimental feature.\nPachyderm pipeline\u0026rsquo;s specification files are intuitive, simple, and language agnostic. They are, however, very static.\nA jsonnet pipeline specification file is a thin wrapping layer atop of your JSON file, allowing you to parameterize a pipeline specification file, thus adding a dynamic component to the creation and update of pipelines.\nWith jsonnet pipeline specs, you can easily reuse the baseline of a given pipeline spec while experimenting with various values of given fields.\nJsonnet Specs # Pachyderm\u0026rsquo;s Jsonnet pipeline specs are written in the open-source templating language jsonnet. Jsonnet wraps the baseline of a JSON file into a function, allowing the injection of parameters to a pipeline specification file.\nAll jsonnet pipeline specs have a .jsonnet extension.\nAs an example, check the file edges.jsonnet below. It is a parameterized version of the edges pipeline spec edges.json in the opencv example, used to inject a name modifier and an input repository name into the original pipeline specifications.\nExample 1 # In this snippet of edges.jsonnet, the parameter src sits in place of what would have been the value of the field repo, as a placeholder for any parameter that will be passed to the Jsonnet pipeline spec.\ninput: { pfs: { name: \u0026#34;images\u0026#34;, glob: \u0026#34;/*\u0026#34;, repo: src, } }, See the full edges.jsonnet here:\n//// // Template arguments: // // suffix : An arbitrary suffix appended to the name of this pipeline, for // disambiguation when multiple instances are created. // src : the repo from which this pipeline will read the images to which // it applies edge detection. //// function(suffix, src) { pipeline: { name: \u0026#34;edges-\u0026#34;+suffix }, description: \u0026#34;OpenCV edge detection on \u0026#34;+src, input: { pfs: { name: \u0026#34;images\u0026#34;, glob: \u0026#34;/*\u0026#34;, repo: src, } }, transform: { cmd: [ \u0026#34;python3\u0026#34;, \u0026#34;/edges.py\u0026#34; ], image: \u0026#34;pachyderm/opencv:0.0.1\u0026#34; } } Or check our full \u0026ldquo;jsonnet-ed\u0026rdquo; opencv example.\nTo create or update a pipeline using a jsonnet pipeline specification file:\nadd the --jsonnet flag to your pipeline create or pipeline update commands, followed by a local path to your jsonnet file or an url. add --arg \u0026lt;parameter-name\u0026gt;=value for each variable. Example 2 # pachctl create pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg src=images The command above will generate a JSON file named edges-1.json then create a pipeline of the same name taking the repository images as its input.\nüìñ Read Jsonnet\u0026rsquo;s complete standard library documentation to learn about all the variables types, string manipulation and mathematical functions, or assertions available to you.\nAt the minimum, your function should always have a parameter that acts as a name modifier. Pachyderm\u0026rsquo;s pipeline names are unique. You can quickly generate several pipelines from the same jsonnet pipeline specification file by adding a prefix or a suffix to its generic name.\nüìñ Your .jsonnet file can create multiple pipelines at once as illustrated in our group example.\nUse Cases # Using jsonnet pipeline specifications, you could pass different images to the transform section of an otherwise identical JSON specification file to train multiple models on the same dataset, or switch between one input repo holding test data to another holding production data by parameterizing the input repo field.\nDuring the development phase of a pipeline, it can be helpful to pass the tag of an image as a parameter: each re-build of the pipeline\u0026rsquo;s code requires you to increment your tag value; passing it as a parameter will save you the time to update your JSON specifications. You could also consider preparing a library of ready-made jsonnet pipeline specs for data science teams to instantiate, according to their own set of parameters.\nWe will let you imagine more use cases in which those jsonnet specs can be helpful to you.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines", "jsonnet"],
    "id": "9e964d15ec3c5df4b8b07d05d34dfa9d"
  },
  {
    "title": "Update a Pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Pipeline Ops",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/pipeline-operations/updating-pipelines/",
    "relURI": "/latest/build-dags/pipeline-operations/updating-pipelines/",
    "body": "While working with your data, you often need to modify an existing pipeline with new transformation code or pipeline parameters.\nUse the pachctl update pipeline command to make changes to a pipeline, whether you have re-built a docker image after a code change and/or need to update pipeline parameters in the pipeline specification file.\nAlternatively, you can update a pipeline using jsonnet pipeline specification files.\nAfter You Changed Your Specification File # Run the pachctl update pipeline command to apply any change to your pipeline specification JSON file, such as change to the parallelism settings, change of an image tag, change of an input repository, etc\u0026hellip;\nBy default, a pipeline update does not trigger the reprocessing of the data that has already been processed. Instead, it processes only the new data you submit to the input repo. If you want to run the changes in your pipeline against the data in your input repo\u0026rsquo;s HEAD commit, use the --reprocess flag. The updated pipeline will then continue to process new input data only. Previous results remain accessible through the corresponding commit IDs.\nTo update a pipeline, run the following command after you have updated your pipeline specification JSON file.\npachctl update pipeline -f pipeline.json ‚ÑπÔ∏è Similar to create pipeline, update pipeline with the -f flag can take a URL if your JSON manifest is hosted on GitHub or other remote location.\nUsing Jsonnet Pipeline Specification Files # Jsonnet pipeline specs allow you to bypass the \u0026ldquo;update-your -specification-file\u0026rdquo; step and apply your changes at once by running:\npachctl update pipeline --jsonnet \u0026lt;your jsonnet pipeline specs path or URL\u0026gt; --arg \u0026lt;param 1\u0026gt;=\u0026lt;value 1\u0026gt; --arg \u0026lt;param 2\u0026gt;=\u0026lt;value 2\u0026gt; Example # pachctl update pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg tag=1.0.2 Update the Code in a Pipeline # To update the code in your pipeline, complete the following steps:\nMake the code changes.\nVerify that the Docker daemon is running. Depending on your operating system and the Docker distribution that you use, steps for enabling it might vary:\ndocker ps If you get an error message similar to the following:\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? enable the Docker daemon (see the Docker documentation for your operating system and platform). For example, if you use minikube on macOS, run the following command:\neval $(minikube docker-env) Then build, tag, and push the new image to your image registry and update the pipeline. This step comes in 3 flavors:\nIf you prefer to use instructions from your image registry # Build, tag, and push a new image as described in your image registry documentation. For example, if you use DockerHub, see Docker Documentation.\nUpdate the transform.image field of your pipeline spec with your new tag.\nüí° Make sure to update your tag every time you re-build. Our pull policy is IfNotPresent (Only pull the image if it does not already exist on the node.). Failing to update your tag will result in your pipeline running on a previous version of your code.\nUpdate the pipeline:\npachctl update pipeline -f \u0026lt;pipeline.json\u0026gt; If you chose to use a jsonnet version of your pipeline specs # Pass the tag of your image to your jsonnet specs.\nAs an example, see the tag parameter in this jsonnet version of opencv\u0026rsquo;s edges pipeline (edges.jsonnet):\n//// // Template arguments: // // suffix : An arbitrary suffix appended to the name of this pipeline, for // disambiguation when multiple instances are created. // src : the repo from which this pipeline will read the images to which // it applies edge detection. //// function(suffix, src) { pipeline: { name: \u0026#34;edges-\u0026#34;+suffix }, description: \u0026#34;OpenCV edge detection on \u0026#34;+src, input: { pfs: { name: \u0026#34;images\u0026#34;, glob: \u0026#34;/*\u0026#34;, repo: src, } }, transform: { cmd: [ \u0026#34;python3\u0026#34;, \u0026#34;/edges.py\u0026#34; ], image: \u0026#34;pachyderm/opencv:0.0.1\u0026#34; } } Once your pipeline code is updated and your image is built, tagged, and pushed, update your pipeline using this command line. In this case, there is no need to edit the pipeline specification file to update the value of your new tag. This command will take care of it:\npachctl update pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg tag=1.0.2 If you use Pachyderm commands # Build your new image using docker build (for example, in a makefile: @docker build --platform linux/amd64 -t $(DOCKER_ACCOUNT)/$(CONTAINER_NAME) .). No tag needed, the following --push-images flag will take care of it.\nRun the following command:\npachctl update pipeline -f \u0026lt;pipeline name\u0026gt; --push-images --registry \u0026lt;registry\u0026gt; --username \u0026lt;registry user\u0026gt; If you use DockerHub, omit the --registry flag.\nExample:\npachctl update pipeline -f edges.json --push-images --username testuser When prompted, type your image registry password:\nExample:\nPassword for docker.io/testuser: Building pachyderm/opencv:f1e0239fce5441c483b09de425f06b40, this may take a while. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines"],
    "id": "0fdce58e314e7eb4fa4d9831c7386e79"
  },
  {
    "title": "Project Ops",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Build Pipelines & DAGs",
    "description": "Learn how to create, delete, and update projects",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/project-operations/",
    "relURI": "/latest/build-dags/project-operations/",
    "body": "Projects are logical collections of related work (such as repos and pipelines). Each Pachyderm cluster ships with an initial project named default. PachCTL supports all Project operations, such as adding/removing team members, resources, etc. Pachyderm Console can be used to view and access Projects. Pachyderm\u0026rsquo;s integrations with JupyterLab, Seldon, S3 Gateway, and DeterminedAI also support projects.\nBenefits of Projects # Logical Organization of DAGs: Similar to a file system, you can organize your work within a Pachyderm instance.\nStandardizable: Resources like repos can have the same name if they belong to different projects, making it easier to create and adhere to project templates in a collaborative environment. For example, ProjectA.Repo1 and ProjectB.Repo1.\nMulti-team Enablement: With Enterprise Pachyderm, You can grant access to projects based on roles; projects are hidden from users without access by default.\nExample # In the following example there are two projects: DOGS and CATS. They have similarly named repositories and pipelines. With Enterprise Pachyderm, you could scope access to each project by user or user group.\ngraph TD A[(Project DOGS)] --\u0026gt; B[Picture Repo] A[(Project DOGS)] --\u0026gt; C[Text Repo] A[(Project DOGS)] --\u0026gt; D[Audio Repo] B --\u0026gt; X(Cleanup Pipeline A) C --\u0026gt; Y(Cleanup Pipeline B) D --\u0026gt; Z(Cleanup Pipeline C) X -- from output repo --\u0026gt; 1(Grouping Pipeline) Y -- from output repo --\u0026gt; 1(Grouping Pipeline) Z -- from output repo --\u0026gt; 1(Grouping Pipeline) J[(Project CATS)] --\u0026gt; M[Picture Repo] J[(Project CATS)] --\u0026gt; N[Text Repo] J[(Project CATS)] --\u0026gt; O[Audio Repo] M --\u0026gt; P(Cleanup Pipeline A) N --\u0026gt; Q(Cleanup Pipeline B) O --\u0026gt; R(Cleanup Pipeline C) P -- from output repo --\u0026gt; 2(Grouping Pipeline) Q -- from output repo --\u0026gt; 2(Grouping Pipeline) R -- from output repo --\u0026gt; 2(Grouping Pipeline) Limitations # You currently cannot move existing repos or pipelines between different projects. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["projects"],
    "id": "051f013f21a155c0c928c6d1987b46ae"
  },
  {
    "title": "Create a Project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Project Ops",
    "description": "Learn how to create a new project.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/project-operations/create-project/",
    "relURI": "/latest/build-dags/project-operations/create-project/",
    "body": " Before You Start # Project names should be less than 51 characters long Project names cannot start with special characters and cannot contain periods (.) at all. Regex example: /^[a-zA-Z0-9-_]+$/. How to Create a Project # 1. Create a Project # Tool: Pachctl CLI Console pachctl create project foo Open Console. Click Create Project. Provide the following details: Name Description Click Create. 2. Verify Creation # You can verify that your project has been created by running pachctl list projects or by opening Console (localhost for non-production personal-machine installations) and viewing the home page.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["projects"],
    "id": "2a29c21067505da73e9b054f903e018f"
  },
  {
    "title": "Set a Project as Current",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Project Ops",
    "description": "Learn how to switch to a different project.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/project-operations/set-project/",
    "relURI": "/latest/build-dags/project-operations/set-project/",
    "body": " Before You Start # Pachyderm ships with an initial project named default that is automatically set to your active context. How to Set a Project to Your Current Context # Tool: Pachctl CLI Console In order to begin working on a project other than the default project, you must assign it to a pachCTL context. This enables you to safely add or update resources such as pipelines and repos without affecting other projects.\npachctl config update context --project foo You can check the details of your active context using the following commands:\npachctl config get active-context # returns contextName pachctl config get context \u0026lt;contextName\u0026gt; # { # \u0026#34;source\u0026#34;: \u0026#34;IMPORTED\u0026#34;, # \u0026#34;cluster_name\u0026#34;: \u0026#34;docker-desktop\u0026#34;, # \u0026#34;auth_info\u0026#34;: \u0026#34;docker-desktop\u0026#34;, # \u0026#34;cluster_deployment_id\u0026#34;: \u0026#34;dev\u0026#34;, # \u0026#34;project\u0026#34;: \u0026#34;foo\u0026#34; # } Open the Console UI. Navigate to the top-level Projects view. Scroll to a project you wish to work on. Select View Project. You can now work within the project from Console.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["projects"],
    "id": "b2aeb01f351d3f832f868198ea67aba5"
  },
  {
    "title": "Add a Project Resource",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Project Ops",
    "description": "Learn how to add a resource (like a repo) to a project.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/project-operations/add-project-resource/",
    "relURI": "/latest/build-dags/project-operations/add-project-resource/",
    "body": " Tool: Pachctl CLI Console There are two main ways to add a resource to a project, depending on whether or not the project has been set to your current pachyderm context.\nAdd Resource to Unset Project:\npachctl create repo bar --project foo Add Resource to Set Project:\npachctl create repo bar Open the Console UI. Scroll to the project you wish to work in. Select View Project. Select Create Repo. For more information about Repos.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["projects"],
    "id": "c5a2a029a031002a2e20893a6286fcea"
  },
  {
    "title": "Grant Project Access",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Project Ops",
    "description": "Learn how to grant a user access to a project.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/project-operations/grant-project-access/",
    "relURI": "/latest/build-dags/project-operations/grant-project-access/",
    "body": "Cluster Admins and Project Owners can grant or revoke user access to projects within a cluster (in addition to the individual resources within the project). View roles and permissions available.\nHow to Grant Project Access to a User # Tool: Pachctl CLI Console pachctl auth set project foo repoReader,repoWriter user:edna Open the Console UI. Navigate to the project you wish to grant user permissions to. Select Edit Project Permissions. Search for and select the user or user\u0026rsquo;s group. Choose which permissions to grant from the dropdown. Select Add. How to Check Project Access for a User # Tool: Pachctl CLI pachctl auth check project foo user:bob ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["projects"],
    "id": "8afde5058df652cc38284dcdb63569ea"
  },
  {
    "title": "Delete a Project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Project Ops",
    "description": "Learn how to delete a project.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/project-operations/delete-project/",
    "relURI": "/latest/build-dags/project-operations/delete-project/",
    "body": " How to Delete a Project # Tool: Pachctl CLI Console pachctl delete project \u0026lt;project\u0026gt; If the project you removed was set to your currently active context, make sure to assign a new one:\npachctl config update context --project foo Open Console. Navigate to Projects (the main page). Scroll to the project you want to delete. Select the ellipsis (\u0026hellip;) icon and click Delete Project. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["projects"],
    "id": "468679ef16e023c765648e8d6dc5fdac"
  },
  {
    "title": "Branch Ops",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Build Pipelines & DAGs",
    "description": "Learn how to create, delete, and update branches.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/branch-operations/",
    "relURI": "/latest/build-dags/branch-operations/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "edb591ff12cb1d5cda93aeacfba1d306"
  },
  {
    "title": "Copy Files",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Branch Ops",
    "description": "Learn how to copy files between branches.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/branch-operations/copy-files/",
    "relURI": "/latest/build-dags/branch-operations/copy-files/",
    "body": "Sometimes you need to be able to copy files between branches. This can be especially useful if you want to commit data in an ad-hoc, disorganized manner to a staging branch and then organize it later.\nWhen you run copy file, Pachyderm only copies references to the files and does not move the actual data for the files around.\nHow to Copy Files Between Branches # Start a commit:\npachctl start commit data@master Copy files:\npachctl copy file data@staging:file1 data@master:file1 pachctl copy file data@staging:file2 data@master:file2 ... Close the commit:\npachctl finish commit data@master ‚ÑπÔ∏è While the commit is open, you can run pachctl delete file if you want to remove something from the parent commit or pachctl put file if you want to upload something that is not in a repo yet.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["deferred processing", "branches"],
    "id": "c7be1e6e21d7fde49adc7a379c1dae34"
  },
  {
    "title": "Process Specific Commits",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Branch Ops",
    "description": "Learn how to process specific commits in a branch.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/branch-operations/process-specific-commits/",
    "relURI": "/latest/build-dags/branch-operations/process-specific-commits/",
    "body": "Pachyderm enables you to process specific commits in a branch that are not the HEAD commit, which can be useful in many scenarios. For example, you can use this feature for:\nTesting and debugging: By processing specific intermediary commits, you can identify issues, test changes, or debug the pipeline at different stages of the data processing flow. This helps you ensure the quality and correctness of your data processing tasks.\nSelective processing: In some cases, you might want to process only specific subsets of data instead of the entire dataset. By targeting specific commits, you can selectively process the data as needed.\nHistorical data reprocessing: If your pipeline has been updated or you need to re-process data with new parameters, you can target specific commits to re-run the pipeline on previously processed data.\nResource utilization: By processing only the necessary commits, you can optimize the use of resources and reduce the overall time and cost of data processing.\nHow to Process Specific Commits # To process a specific commit, you need to set the master branch of your repo to have your specified commit as HEAD.\nFor example, if you submitted ten commits in the staging branch and you want to process the seventh, third, and most recent commits, you need to run the following commands respectively:\npachctl create branch data@master --head staging^7 pachctl create branch data@master --head staging^3 pachctl create branch data@master --head staging When you run the commands above, Pachyderm creates a job for each of the commands one after another. Therefore, when one job is completed, Pachyderm starts the next one. To verify that Pachyderm created jobs for these commands, run the following:\npachctl list job -p \u0026lt;pipeline_name\u0026gt; --history all How to Change the Branch HEAD # You can move backward to previous commits as easily as advancing to the latest commits. For example, if you want to change the final output to be the result of processing staging^1, you can roll back your HEAD commit by running the following command:\npachctl create branch data@master --head staging^1 This command starts a new job to process staging^1. The HEAD commit on your output repo will be the result of processing staging^1 instead of staging.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "06d420c0d23c7bb2ca6a23b3ae2d8522"
  },
  {
    "title": "Set Branch Triggers",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Branch Ops",
    "description": "Learn how to use branch triggers to automate deferred processing.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/branch-operations/set-branch-triggers/",
    "relURI": "/latest/build-dags/branch-operations/set-branch-triggers/",
    "body": "You can automate re-pointing from one branch to another by using branch triggers. A branch trigger is a relationship between two branches, such as master and staging. When the head commit of staging meets a certain condition, it triggers master to update its head to that same commit. In other words, it does pachctl create branch data@master --head staging automatically when the trigger condition is met.\nYou can set branch triggers to fire when:\nA certain amount of time has passed (--trigger-cron) A specific number of commits have been made (--trigger-size) The amount of unprocessed data reaches a certain size (--trigger-commits) When more than one is specified, a branch repoint will be triggered when any of the conditions is met. To guarantee that they all must be met, add --trigger-all.\nHow to Automate Deferred Processing via Branch Triggers # Let\u0026rsquo;s make the master branch automatically trigger when there\u0026rsquo;s 1 Megabyte of new data on the staging branch.\nCreate a repo. pachctl create repo data Create a master branch with trigger settings (see pachctl create branch options) and a staging branch. pachctl create branch data@master --trigger staging --trigger-size 1MB pachctl create branch data@staging View your branches. pachctl list branch data BRANCH HEAD TRIGGER staging f35c5e5d6b4c499eaae0ab0c733ad7a6 - master 383c2acb298e4d6aa8327ea49aaeede6 staging on Size(1MB) üí° Triggers can point to branches that don\u0026rsquo;t exist yet. You can test your trigger using a command similar to the following:\ndd if=/dev/urandom bs=1M count=1 | pachctl put file data@staging:/file pachctl list branch data BRANCH HEAD TRIGGER staging 5e27464aab4c4857bfd5d402afe043c6 - master 5e27464aab4c4857bfd5d402afe043c6 staging on Size(1MB) How to Manually Trigger Master # Triggers automate deferred processing, but they don\u0026rsquo;t prevent manually updating the head of a branch. If you ever want to trigger master even though the trigger condition hasn\u0026rsquo;t been met, you can run:\npachctl create branch data@master --head staging Notice that you don\u0026rsquo;t need to re-specify the trigger when you call create branch to change the head. If you do want to clear the trigger delete the branch and recreate it.\nTo experiment further, see the full triggers example.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["deferred processing", "branches"],
    "id": "63e94c4d0750233ef07e8b76fdad2c25"
  },
  {
    "title": "Set Output Branch",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Branch Ops",
    "description": "Learn how to use an output branch to defer processing.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/branch-operations/set-output-branch/",
    "relURI": "/latest/build-dags/branch-operations/set-output-branch/",
    "body": "You can defer processing operations for data in output repositories by configuring an output branch field in your pipeline specification. This allows you to accumulate data in an output branch before processing it.\nHow to Defer Processing in an Output Branch # In the pipeline specification, add the output_branch field with the name of the branch in which you want to accumulate your data before processing:\n\u0026#34;output_branch\u0026#34;: \u0026#34;staging\u0026#34; When you want to process data, run:\npachctl create branch pipeline@master --head staging ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["deferred processing", "branches"],
    "id": "f8f0dc0a2a709cd86964560d231671d9"
  },
  {
    "title": "Datum Ops",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Build Pipelines & DAGs",
    "description": "Learn how to create, delete, and update datums.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/datum-operations/",
    "relURI": "/latest/build-dags/datum-operations/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "41c458b6af5c0f1923a7d7b605d76295"
  },
  {
    "title": "Get Metadata",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Datum Ops",
    "description": "Learn how to view additional datum metadata using the pachctl get file command.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/datum-operations/get-metadata/",
    "relURI": "/latest/build-dags/datum-operations/get-metadata/",
    "body": "Once a pipeline has finished a job, you can access additional execution metadata about the datums processed in the associated meta system repo. Note that all the inspect datum information above is stored in this meta repo, along with a couple more. For example, you can find the reason in meta/\u0026lt;datumID\u0026gt;/meta: the error message when the datum failed.\nSee the detail of the meta repo structure below.\nüìñ A meta repo contains 2 directories:\n/meta/: The meta directory holds datums\u0026rsquo; statistics /pfs: The pfs directory holds the input data of datums, and their resulting output data pachctl list file edges.meta@master System response:\nNAME TAG TYPE SIZE /meta/ dir 1.956KiB /pfs/ dir 371.9KiB Meta directory # The meta directory holds each datum\u0026rsquo;s JSON metadata, and can be accessed using a get file:\nExample # pachctl get file edges.meta@master:/meta/002f991aa9db9f0c44a92a30dff8ab22e788f86cc851bec80d5a74e05ad12868/meta | jq System response:\n{ \u0026#34;job\u0026#34;: { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;edges\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;efca9595bdde4c0ba46a444a5877fdfe\u0026#34; }, \u0026#34;inputs\u0026#34;: [ { \u0026#34;fileInfo\u0026#34;: { ... } ], \u0026#34;hash\u0026#34;: \u0026#34;28e6675faba53383ac84b899d853bb0781c6b13a90686758ce5b3644af28cb62f763\u0026#34;, \u0026#34;stats\u0026#34;: { \u0026#34;downloadTime\u0026#34;: \u0026#34;0.103591200s\u0026#34;, \u0026#34;processTime\u0026#34;: \u0026#34;0.374824700s\u0026#34;, \u0026#34;uploadTime\u0026#34;: \u0026#34;0.001807800s\u0026#34;, \u0026#34;downloadBytes\u0026#34;: \u0026#34;80588\u0026#34;, \u0026#34;uploadBytes\u0026#34;: \u0026#34;38046\u0026#34; }, \u0026#34;index\u0026#34;: \u0026#34;1\u0026#34; } Usepachctl list file edges.meta@master:/meta/ to list the files in the meta directory.\nPfs Directory # The pfs directory has both the input files of datums, and the resulting output files that were committed to the output repo:\nExample # pachctl list file montage.meta@master:/pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/ System response:\nNAME TAG TYPE SIZE /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/edges/ dir 133.6KiB /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/images/ dir 238.3KiB /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/out/ dir 1.292MiB Use pachctl list file montage.meta@master:/pfs/ to list the files in the pfs directory.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["datums"],
    "id": "ea3e99b96cc13a3377749a1c52565f64"
  },
  {
    "title": "Inspect Datum",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Datum Ops",
    "description": "Learn how to inspect a datum using the pachctl inspect datum command.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/datum-operations/inspect-datum/",
    "relURI": "/latest/build-dags/datum-operations/inspect-datum/",
    "body": "Pachyderm stores information about each datum that a pipeline processes, including timing information, size information, and /pfs snapshots. You can view these statistics by running the pachctl inspect datum command (or its language client equivalents).\nIn particular, Pachyderm provides the following information for each datum processed by your pipelines:\nThe amount of data that was uploaded and downloaded The time spend uploading and downloading data The total time spend processing Success/failure information, including any error encountered for failed datums The directory structure of input data that was seen by the job. Use the pachctl list datum \u0026lt;pipeline\u0026gt;@\u0026lt;job ID\u0026gt; to retrieve the list of datums processed by a given job, and pick the datum ID you want to inspect. That information can be useful when troubleshooting a failed job.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["datums"],
    "id": "8f68d8c899bd417c79531bad7736db5c"
  },
  {
    "title": "Provenance Ops",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Build Pipelines & DAGs",
    "description": "Learn how to track and view the provenance of your data.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/provenance-operations/",
    "relURI": "/latest/build-dags/provenance-operations/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["projects"],
    "id": "13949a2dbe9f3c02a16d1cec866936c4"
  },
  {
    "title": "List Global Commits & Jobs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Provenance Ops",
    "description": "Learn how to list global commits and jobs.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/provenance-operations/list-globals/",
    "relURI": "/latest/build-dags/provenance-operations/list-globals/",
    "body": " How to List Global Commits # You can list all global commits by running the following command:\npachctl list commit Each global commit displays how many (sub) commits it is made of.\nID SUBCOMMITS PROGRESS CREATED MODIFIED 1035715e796f45caae7a1d3ffd1f93ca 7 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 7 seconds ago 7 seconds ago 28363be08a8f4786b6dd0d3b142edd56 6 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 24 seconds ago 24 seconds ago e050771b5c6f4082aed48a059e1ac203 4 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 24 seconds ago 24 seconds ago How to List Global Jobs # Similarly, if you run the equivalent command for global jobs:\npachctl list job you will notice that the job IDs are shared with the global commit IDs.\nID SUBJOBS PROGRESS CREATED MODIFIED 1035715e796f45caae7a1d3ffd1f93ca 2 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 55 seconds ago 55 seconds ago 28363be08a8f4786b6dd0d3b142edd56 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá About a minute ago About a minute ago e050771b5c6f4082aed48a059e1ac203 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá About a minute ago About a minute ago For example, in this example, 7 commits and 2 jobs are involved in the changes occured in the global commit ID 1035715e796f45caae7a1d3ffd1f93ca.\n‚ÑπÔ∏è The progress bar is equally divided to the number of steps, or pipelines, you have in your DAG. In the example above,1035715e796f45caae7a1d3ffd1f93ca is two steps. If one of the sub-jobs fails, you will see the progress bar turn red for that pipeline step. To troubleshoot, look into that particular pipeline execution.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["provenance"],
    "id": "2e881de4cc300a1850e6a2d1341e2a51"
  },
  {
    "title": "List Global ID Sub Commits",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Provenance Ops",
    "description": "Learn how to list global commits and jobs.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/provenance-operations/list-sub-commits/",
    "relURI": "/latest/build-dags/provenance-operations/list-sub-commits/",
    "body": " How to List Commits via Global ID # To list all (sub) commits involved in a global commit:\npachctl list commit 1035715e796f45caae7a1d3ffd1f93ca REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION images master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 238.3KiB USER edges.spec master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 244B ALIAS montage.spec master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 405B ALIAS montage.meta master 1035715e796f45caae7a1d3ffd1f93ca 4 minutes ago 1.656MiB AUTO edges master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 133.6KiB AUTO edges.meta master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 373.9KiB AUTO montage master 1035715e796f45caae7a1d3ffd1f93ca 4 minutes ago 1.292MiB AUTO How to List Jobs via Global ID # Tto list all (sub) jobs linked to your global job ID:\npachctl list job 1035715e796f45caae7a1d3ffd1f93ca ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE 1035715e796f45caae7a1d3ffd1f93ca montage 5 minutes ago 4 seconds 0 1 + 0 / 1 79.49KiB 381.1KiB success 1035715e796f45caae7a1d3ffd1f93ca edges 5 minutes ago 2 seconds 0 1 + 0 / 1 57.27KiB 22.22KiB success For each pipeline execution (sub job) within this global job, Pachyderm shows the time since each sub job started and its duration, the number of datums in the PROGRESS section, and other information. The format of the progress column is DATUMS PROCESSED + DATUMS SKIPPED / TOTAL DATUMS.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["provenance"],
    "id": "a0d8cde8b63fecd52b0097095d872f5f"
  },
  {
    "title": "Track Downstream",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Provenance Ops",
    "description": "Learn how to track downstream commits and jobs.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/provenance-operations/track-downstream/",
    "relURI": "/latest/build-dags/provenance-operations/track-downstream/",
    "body": " Track Provenance Downstream # Pachyderm provides the wait commit \u0026lt;commitID\u0026gt; command that enables you to track your commits downstream as they are produced.\nUnlike the list commit \u0026lt;commitID\u0026gt;, each line is printed as soon as a new (sub) commit of your global commit finishes.\nChange commit in job to list the jobs related to your global job as they finish processing a commit.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["provenance"],
    "id": "8b536af2875ab9bd34386b6edce2511f"
  },
  {
    "title": "Delete Branch Head",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Provenance Ops",
    "description": "Learn how to delete the HEAD of a branch.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/provenance-operations/delete-head/",
    "relURI": "/latest/build-dags/provenance-operations/delete-head/",
    "body": "To fix a broken HEAD, run the following command:\npachctl delete commit \u0026lt;commit-ID\u0026gt; When you delete a HEAD commit, Pachyderm performs the following actions:\nChanges HEADs of all the branches that had the bad commit as their HEAD to their bad commit\u0026rsquo;s parent and deletes the commit. The data in the deleted commit is lost. If the bad commit does not have a parent, Pachyderm sets the branch\u0026rsquo;s HEAD to a new empty commit. Interrupts all running jobs, including not only the jobs that use the bad commit as a direct input but also the ones farther downstream in your DAG. Deletes the output commits from the deleted jobs. All the actions listed above are applied to those commits as well. ‚ö†Ô∏è This command will only succeed if the HEAD commit has no children on any branch. pachctl delete commit will error when attempting to delete a HEAD commit with children.\n‚ÑπÔ∏è Are you wondering how a HEAD commit can have children?\nA commit can be the head of a branch and still have children. For instance, given a master branch in a repository named repo, if you branch master by running pachctl create branch repo@staging --head repo@master, the master\u0026rsquo;s HEAD will have an alias child on staging.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["data-operations"],
    "id": "2752cf14d981313521eb8104bc9c8b1c"
  },
  {
    "title": "Squash Non-Head Commits",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Provenance Ops",
    "description": "Learn how to squash commits that are not the HEAD of a branch.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/provenance-operations/squash-nonhead/",
    "relURI": "/latest/build-dags/provenance-operations/squash-nonhead/",
    "body": "If your commit has children, you have the option to use the squash commit command. Squashing is a way to rewrite your commit history; this helps clean up and simplify your commit history before sharing your work with team members.\nSquashing a commit in Pachyderm means that you are combining all the file changes in the commits of a global commit into their children and then removing the global commit. This behavior is inspired by the squash option in git rebase. No data stored in PFS is removed since they remain in the child commits.\npachctl squash commit \u0026lt;commit-ID\u0026gt; Example # In the simple example below, we create three successive commits on the master branch of a repo repo:\nIn commit ID1, we added files A and B. In commit ID2, we added file C. In commit ID3, the latest commit, we altered the content of files A and C. We then run pachctl squash commit ID1, then pachctl squash commit ID2, and look at our branch and remaining commit(s).\nA‚Äô and C\u0026rsquo; are altered versions of files A and C. At any moment, pachctl list file repo@master invariably returns the same files A‚Äô, B, C‚Äô. pachctl list commit however, differs in each case, since, by squashing commits, we have deleted them from the branch.\nConsiderations # Squashing a global commit on the head of a branch (no children) will fail. Use pachctl delete commit instead. pachctl squash commit stops (but does not delete) associated jobs. Limitations # Squash commit only applies to user repositories. For example, you cannot squash a commit that updated a pipeline (Commit that lives in a spec repository). You cannot squash a commit set that contains a commit that is a dependency for another commit set. For example, if you have RepoB@master that depends on RepoA@master as an input, you cannot squash the commit set for RepoA@master. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["data-operations"],
    "id": "e93f406d4b33c57e674cb69b35e6e576"
  },
  {
    "title": "Delete File From History",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Provenance Ops",
    "description": "Learn how to delete a file from history.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/provenance-operations/delete-history/",
    "relURI": "/latest/build-dags/provenance-operations/delete-history/",
    "body": " üìñ It is important to note that this use case is limited to simple cases where the \u0026ldquo;bad\u0026rdquo; changes were made relatively recently, as any pipeline update since then will make it impossible.\nIn rare cases, you might need to delete a particular file from a given commit and further choose to delete its complete history. In such a case, you will need to:\nCreate a new commit in which you surgically remove the problematic file.\nStart a new commit:\npachctl start commit \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; Delete all corrupted files from the newly opened commit:\npachctl delete file \u0026lt;repo\u0026gt;@\u0026lt;branch or commitID\u0026gt;:/path/to/files Finish the commit:\npachctl finish commit \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; Optionally, wipe this file from your history by squashing the initial bad commit and all its children up to the newly finished commit.\nUnless the subsequent commits overwrote or deleted the bad data, the data might still be present in the children commits. Squashing those commits cleans up your commit history and ensures that the errant data is not available when non-HEAD versions of the data are read.\nExample # In the simple example below, we want to delete file C in commit 2. How would we do that?\nFor now, pachctl list file repo@master returns the files A‚Äô, B, C‚Äô, E, F.\nA‚Äô and C\u0026rsquo; are altered versions of files A and C. We create a new commit in which we surgically remove file C:\npachctl start commit repo@master pachctl delete file repo@master:path/to/C pachctl finish commit repo@master At this point, pachctl list file repo@master returns the files A‚Äô, B, E, F. We removed file C. However, it still exists in the commit history.\nTo remove C from the commit history, we squash the commits in which C appears, all the way down to the last commit.\npachctl squash commitID2 pachctl squash commitID3 It is as if C never existed.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["data-operations"],
    "id": "c05941ab7a5ce463407f7ef7632a85a1"
  },
  {
    "title": "Tutorials",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Build Pipelines & DAGs",
    "description": "Follow these tutorials to learn how to build ML, NLP, and other pipelines.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/tutorials/",
    "relURI": "/latest/build-dags/tutorials/",
    "body": "The following pipeline tutorials in this section walk you through how to build a variety of pipelines using Pachyderm. It is recommended to complete these tutorials in order from left to right, as each tutorial builds upon the skills and concepts learned in the previous tutorial.\nIf you are already familiar with how to build your own Docker images, you can start with the Standard ML Pipeline tutorial.\n",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "automl", "mljar"],
    "id": "8078fa7397ba6a384bb3f35b441694a9"
  },
  {
    "title": "Standard ML Pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Tutorials",
    "description": "Learn how to build a basic machine learning pipeline.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/tutorials/basic-ml/",
    "relURI": "/latest/build-dags/tutorials/basic-ml/",
    "body": "In this tutorial, we\u0026rsquo;ll build a simple machine learning pipeline in Pachyderm to train a regression model on housing market data to predict the value of homes in Boston.\nBefore You Start # You must have a Pachyderm cluster up and running You should have some basic familiarity with Pachyderm pipeline specs \u0026ndash; see the Transform and PFS Input sections in particular Tutorial # Our Docker image\u0026rsquo;s user code for this tutorial is built on top of the civisanalytics/datascience-python base image, which includes the necessary dependencies. It uses pandas to import the structured dataset and the scikit-learn library to train the model.\n1. Create a Project \u0026amp; Input Repo # Create a project named standard-ml-tutorial. pachctl create project standard-ml-tutorial Set the project as current. pachctl config update context --project standard-ml-tutorial Create a repo named housing_data. pachctl create repo housing_data 2. Create a Regression Pipeline # Create a file named regression.json with the following contents:\n# regression.json { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;regression\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;A pipeline that trains produces a regression model for housing prices.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;housing_data\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python\u0026#34;, \u0026#34;regression.py\u0026#34;, \u0026#34;--input\u0026#34;, \u0026#34;/pfs/housing_data/\u0026#34;, \u0026#34;--target-col\u0026#34;, \u0026#34;MEDV\u0026#34;, \u0026#34;--output\u0026#34;, \u0026#34;/pfs/out/\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/housing-prices:1.11.0\u0026#34; } } Save the file.\nRun the following command to create the pipeline:\npachctl create pipeline -f regression.json üí° The pipeline writes the output to a PFS repo (/pfs/out/) created with the same name as the pipeline.\n3. Upload the Housing Dataset # Download our first example data set, housing-simplified-1.csv.\nAdd the data to your repo. Processing begins automatically \u0026mdash; anytime you add new data, the pipeline will re-run.\npachctl put file housing_data@master:housing-simplified.csv -f /path/to/housing-simplified-1.csv Verify that the data is in the repository.\npachctl list file housing_data@master # NAME TYPE SIZE # /housing-simplified.csv file 2.482KiB Verify that the pipeline is running by looking at the status of the job(s).\npachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # e7dd14d201a64edc8bf61beed6085ae0 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 48 seconds ago 48 seconds ago # df117068124643299d46530859851a4b 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá About a minute ago About a minute ago 4. Download Output Files # Once the pipeline is completed, we can download the files that were created.\nView a list of the files in the output repo. pachctl list file regression@master # NAME TYPE SIZE # /housing-simplified_corr_matrix.png file 18.66KiB # /housing-simplified_cv_reg_output.png file 86.07KiB # /housing-simplified_model.sav file 798.5KiB # /housing-simplified_pairplot.png file 100.8KiB Download the files. pachctl get file regression@master:/ --recursive --output . When we inspect the learning curve, we can see that there is a large gap between the training score and the validation score. This typically indicates that our model could benefit from the addition of more data.\nNow let\u0026rsquo;s update our dataset with additional examples.\n5. Update the Dataset # Download our second example data set, housing-simplified-2.csv.\nAdd the data to your repo.\npachctl put file housing_data@master:housing-simplified.csv -f /path/to/housing-simplified-2.csv ‚ÑπÔ∏è We could also append new examples to the existing file, but in this tutorial we\u0026rsquo;re overwriting our previous file to one with more data.\nThis is where Pachyderm truly starts to shine. The new commit of data to the housing_data repository automatically kicks off a job on the regression pipeline without us having to do anything.\nWhen the job is complete we can download the new files and see that our model has improved, given the new learning curve.\n6. Inspect the Pipeline Lineage # Since the pipeline versions all of our input and output data automatically, we can continue to iterate on our data and code while Pachyderm tracks all of our experiments.\nFor any given output commit, Pachyderm can tell us exactly which input commit of data was run. In this tutorial, we have only run 2 experiments so far, but this becomes incredibly valuable as your experiments continue to evolve and scale.\nInspect the commits to your repo.\npachctl list commit # ID SUBCOMMITS PROGRESS CREATED MODIFIED # 3037785cc56c4387bbb897f1887b4a68 4 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 11 seconds ago 11 seconds ago # e7dd14d201a64edc8bf61beed6085ae0 4 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá About a minute ago About a minute ago # df117068124643299d46530859851a4b 4 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 2 minutes ago 2 minutes ago Use the commit ID to check for what dataset was used to create the model.\npachctl list file housing_data@3037785cc56c4387bbb897f1887b4a68 # NAME TYPE SIZE # /housing-simplified.csv file 12.14KiB Use the commit ID to check the commit\u0026rsquo;s details (such as parent commit, branch, size, etc.)\npachctl inspect commit housing_data@3037785cc56c4387bbb897f1887b4a68 # Commit: housing_data@3037785cc56c4387bbb897f1887b4a68 # Original Branch: master # Parent: e7dd14d201a64edc8bf61beed6085ae0 # Started: 2 minutes ago # Finished: 2 minutes ago # Size: 12.14KiB User Code Assets # The Docker image used in this tutorial was built with the following assets:\nAssets: Dockerfile Regression.py Utils.py FROM civisanalytics/datascience-python RUN pip install seaborn WORKDIR /workdir/ COPY *.py /workdir/ import argparse import os from os import path import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import joblib from utils import plot_learning_curve import subprocess from sklearn.model_selection import ShuffleSplit from sklearn import datasets, ensemble, linear_model from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import cross_val_score parser = argparse.ArgumentParser(description=\u0026#34;Structured data regression\u0026#34;) parser.add_argument(\u0026#34;--input\u0026#34;, type=str, help=\u0026#34;csv file with all examples\u0026#34;) parser.add_argument(\u0026#34;--target-col\u0026#34;, type=str, help=\u0026#34;column with target values\u0026#34;) parser.add_argument(\u0026#34;--output\u0026#34;, metavar=\u0026#34;DIR\u0026#34;, default=\u0026#39;./output\u0026#39;, help=\u0026#34;output directory\u0026#34;) def load_data(input_csv, target_col): # Load the Boston housing dataset data = pd.read_csv(input_csv, header=0) targets = data[target_col] features = data.drop(target_col, axis = 1) print(\u0026#34;Dataset has {} data points with {} variables each.\u0026#34;.format(*data.shape)) return data, features, targets def create_pairplot(data): plt.clf() # Calculate and show pairplot sns.pairplot(data, height=2.5) plt.tight_layout() def create_corr_matrix(data): plt.clf() # Calculate and show correlation matrix sns.set() corr = data.corr() # Generate a mask for the upper triangle mask = np.triu(np.ones_like(corr, dtype=np.bool)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio sns_plot = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, annot=True, cbar_kws={\u0026#34;shrink\u0026#34;: .5}) def train_model(features, targets): # Train a Random Forest Regression model reg = ensemble.RandomForestRegressor(random_state=1) scores = cross_val_score(reg, features, targets, cv=10) print(\u0026#34;Score: {:2f} (+/- {:2f})\u0026#34;.format(scores.mean(), scores.std() * 2)) reg.fit(features,targets) return reg def create_learning_curve(estimator, features, targets): plt.clf() title = \u0026#34;Learning Curves (Random Forest Regressor)\u0026#34; cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0) plot_learning_curve(estimator, title, features, targets, ylim=(0.5, 1.01), cv=cv, n_jobs=4) def main(): print(\u0026#34;User code is starting\u0026#34;) subprocess.run([\u0026#34;pachctl\u0026#34;, \u0026#34;connect\u0026#34;, \u0026#34;grpc://localhost:1650\u0026#34;]) print(\u0026#34;starting while loop\u0026#34;) while True: subprocess.run([\u0026#34;pachctl\u0026#34;, \u0026#34;next\u0026#34;, \u0026#34;datum\u0026#34;]) print(\u0026#34;next datum called\u0026#34;) args = parser.parse_args() if os.path.isfile(args.input): input_files = [args.input] else: # Directory for dirpath, dirs, files in os.walk(args.input): input_files = [ os.path.join(dirpath, filename) for filename in files if filename.endswith(\u0026#39;.csv\u0026#39;) ] print(\u0026#34;Datasets: {}\u0026#34;.format(input_files)) os.makedirs(args.output, exist_ok=True) for filename in input_files: experiment_name = os.path.basename(os.path.splitext(filename)[0]) # Data loading and Exploration data, features, targets = load_data(filename, args.target_col) create_pairplot(data) plt.savefig(path.join(args.output,experiment_name + \u0026#39;_pairplot.png\u0026#39;)) create_corr_matrix(data) plt.savefig(path.join(args.output, experiment_name + \u0026#39;_corr_matrix.png\u0026#39;)) # Fit model reg = train_model(features, targets) create_learning_curve(reg, features, targets) plt.savefig(path.join(args.output, experiment_name + \u0026#39;_cv_reg_output.png\u0026#39;)) # Save model joblib.dump(reg, path.join(args.output,experiment_name + \u0026#39;_model.sav\u0026#39;)) if __name__ == \u0026#34;__main__\u0026#34;: main() import numpy as np import matplotlib.pyplot as plt from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.datasets import load_digits from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)): \u0026#34;\u0026#34;\u0026#34; Generate 3 plots: the test and training learning curve, the training samples vs fit times curve, the fit times vs score curve. Parameters ---------- estimator : object type that implements the \u0026#34;fit\u0026#34; and \u0026#34;predict\u0026#34; methods An object of that type which is cloned for each validation. title : string Title for the chart. X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification or regression; None for unsupervised learning. axes : array of 3 axes, optional (default=None) Axes to use for plotting the curves. ylim : tuple, shape (ymin, ymax), optional Defines minimum and maximum yvalues plotted. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`StratifiedKFold` used. If the estimator is not a classifier or if ``y`` is neither binary nor multiclass, :class:`KFold` is used. Refer :ref:`User Guide \u0026lt;cross_validation\u0026gt;` for the various cross-validators that can be used here. n_jobs : int or None, optional (default=None) Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary \u0026lt;n_jobs\u0026gt;` for more details. train_sizes : array-like, shape (n_ticks,), dtype float or int Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5)) \u0026#34;\u0026#34;\u0026#34; if axes is None: _, axes = plt.subplots(1, 3, figsize=(20, 5)) axes[0].set_title(title) if ylim is not None: axes[0].set_ylim(*ylim) axes[0].set_xlabel(\u0026#34;Training examples\u0026#34;) axes[0].set_ylabel(\u0026#34;Score\u0026#34;) train_sizes, train_scores, test_scores, fit_times, _ = \\ learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, return_times=True) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) fit_times_mean = np.mean(fit_times, axis=1) fit_times_std = np.std(fit_times, axis=1) # Plot learning curve axes[0].grid() axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\u0026#34;r\u0026#34;) axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\u0026#34;g\u0026#34;) axes[0].plot(train_sizes, train_scores_mean, \u0026#39;o-\u0026#39;, color=\u0026#34;r\u0026#34;, label=\u0026#34;Training score\u0026#34;) axes[0].plot(train_sizes, test_scores_mean, \u0026#39;o-\u0026#39;, color=\u0026#34;g\u0026#34;, label=\u0026#34;Cross-validation score\u0026#34;) axes[0].legend(loc=\u0026#34;best\u0026#34;) # Plot n_samples vs fit_times axes[1].grid() axes[1].plot(train_sizes, fit_times_mean, \u0026#39;o-\u0026#39;) axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std, fit_times_mean + fit_times_std, alpha=0.1) axes[1].set_xlabel(\u0026#34;Training examples\u0026#34;) axes[1].set_ylabel(\u0026#34;fit_times\u0026#34;) axes[1].set_title(\u0026#34;Scalability of the model\u0026#34;) # Plot fit_time vs score axes[2].grid() axes[2].plot(fit_times_mean, test_scores_mean, \u0026#39;o-\u0026#39;) axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1) axes[2].set_xlabel(\u0026#34;fit_times\u0026#34;) axes[2].set_ylabel(\u0026#34;Score\u0026#34;) axes[2].set_title(\u0026#34;Performance of the model\u0026#34;) return plt ",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "automl", "mljar"],
    "id": "d09917d1d60d342a2a54e6628392a766"
  },
  {
    "title": "AutoML Pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Tutorials",
    "description": "Learn how to build an automated machine learning pipeline.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/tutorials/auto-ml/",
    "relURI": "/latest/build-dags/tutorials/auto-ml/",
    "body": "You can use Pachyderm to build an automated machine learning pipeline that trains a model on a CSV file.\nBefore You Start # You must have Pachyderm installed and running on your cluster You should have already completed the Standard ML Pipeline tutorial You must be familiar with jsonnet This tutorial assumes your active context is localhost:80 Tutorial # Our Docker image\u0026rsquo;s user code for this tutorial is built on top of the python:3.7-slim-buster base image. It also uses the mljar-supervised package to perform automated feature engineering, model selection, and hyperparameter tuning, making it easy to train high-quality machine learning models on structured data.\n1. Create a Project \u0026amp; Input Repo # Create a project named automl-tutorial. pachctl create project automl-tutorial Set the project as current. pachctl config update context --project automl-tutorial Create a new csv-data repo. pachctl create repo csv-data Upload the housing-simplified-1.csv file to the repo. pachctl put file csv_data@master:housing-simplified.csv -f /path/to/housing-simplified-1.csv 2. Create a Jsonnet Pipeline # Download or save our automl.jsonnet template.\n//// // Template arguments: // // name : The name of this pipeline, for disambiguation when // multiple instances are created. // input : the repo from which this pipeline will read the csv file to which // it applies automl. // target_col : the column of the csv to be used as the target // args : additional parameters to pass to the automl regressor (e.g. \u0026#34;--random_state 42\u0026#34;) //// function(name=\u0026#39;regression\u0026#39;, input, target_col, args=\u0026#39;\u0026#39;) { pipeline: { name: name}, input: { pfs: { glob: \u0026#34;/\u0026#34;, repo: input } }, transform: { cmd: [ \u0026#34;python\u0026#34;,\u0026#34;/workdir/automl.py\u0026#34;,\u0026#34;--input\u0026#34;,\u0026#34;/pfs/\u0026#34;+input+\u0026#34;/\u0026#34;, \u0026#34;--target-col\u0026#34;, target_col, \u0026#34;--output\u0026#34;,\u0026#34;/pfs/out/\u0026#34;]+ std.split(args, \u0026#39; \u0026#39;), image: \u0026#34;jimmywhitaker/automl:dev0.02\u0026#34; } } Create the AutoML pipeline by referencing and filling out the template\u0026rsquo;s arguments:\npachctl update pipeline --jsonnet /path/to/automl.jsonnet \\ --arg name=\u0026#34;regression\u0026#34; \\ --arg input=\u0026#34;csv_data\u0026#34; \\ --arg target_col=\u0026#34;MEDV\u0026#34; \\ --arg args=\u0026#34;--mode Explain --random_state 42\u0026#34; The model automatically starts training. Once complete, the trained model and evaluation metrics are output to the AutoML output repo.\n3. Upload the Dataset # Update the dataset using housing-simplified-2.csv; Pachyderm retrains the model automatically. pachctl put file csv_data@master:housing-simplified.csv -f /path/to/housing-simplified-2.csv Repeat the previous step as many times as you want. Each time, Pachyderm automatically retrains the model and outputs the new model and evaluation metrics to the AutoML output repo. User Code Assets # The Docker image used in this tutorial was built with the following assets:\nAssets: Dockerfile Automl.py Requirements.txt FROM python:3.7-slim-buster RUN apt-get update \u0026amp;\u0026amp; apt-get -y update RUN apt-get install -y build-essential python3-pip python3-dev RUN pip3 -q install pip --upgrade WORKDIR /workdir/ COPY requirements.txt /workdir/ RUN pip3 install -r requirements.txt COPY *.py /workdir/ import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from supervised.automl import AutoML import argparse import os parser = argparse.ArgumentParser(description=\u0026#34;Structured data regression\u0026#34;) parser.add_argument(\u0026#34;--input\u0026#34;, type=str, help=\u0026#34;csv file with all examples\u0026#34;) parser.add_argument(\u0026#34;--target-col\u0026#34;, type=str, help=\u0026#34;column with target values\u0026#34;) parser.add_argument(\u0026#34;--mode\u0026#34;, type=str, default=\u0026#39;Explain\u0026#39;, help=\u0026#34;mode\u0026#34;) parser.add_argument(\u0026#34;--random_state\u0026#34;, type=int, default=42, help=\u0026#34;random seed\u0026#34;) parser.add_argument(\u0026#34;--output\u0026#34;, metavar=\u0026#34;DIR\u0026#34;, default=\u0026#39;./output\u0026#39;, help=\u0026#34;output directory\u0026#34;) def load_data(input_csv, target_col): # Load the data data = pd.read_csv(input_csv, header=0) targets = data[target_col] features = data.drop(target_col, axis = 1) # Create data splits X_train, X_test, y_train, y_test = train_test_split( features, targets, test_size=0.25, random_state=123, ) return X_train, X_test, y_train, y_test def main(): args = parser.parse_args() if os.path.isfile(args.input): input_files = [args.input] else: # Directory for dirpath, dirs, files in os.walk(args.input): input_files = [ os.path.join(dirpath, filename) for filename in files if filename.endswith(\u0026#39;.csv\u0026#39;) ] print(\u0026#34;Datasets: {}\u0026#34;.format(input_files)) os.makedirs(args.output, exist_ok=True) for filename in input_files: experiment_name = os.path.basename(os.path.splitext(filename)[0]) # Data loading and Exploration X_train, X_test, y_train, y_test = load_data(filename, args.target_col) # Fit model automl = AutoML(total_time_limit=60*60, results_path=args.output) # 1 hour automl.fit(X_train, y_train) # compute the MSE on test data predictions = automl.predict_all(X_test) print(\u0026#34;Test MSE:\u0026#34;, mean_squared_error(y_test, predictions)) if __name__ == \u0026#34;__main__\u0026#34;: main() mljar-supervised ",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "automl", "mljar"],
    "id": "78ece1ba158c7c2774e6a1ec38d6c4fa"
  },
  {
    "title": "Multi-Pipeline DAG",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Tutorials",
    "description": "Learn how to build a DAG with multiple pipelines.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/tutorials/multi-ml/",
    "relURI": "/latest/build-dags/tutorials/multi-ml/",
    "body": "In this tutorial, we\u0026rsquo;ll build a multi-pipeline DAG to train a regression model on housing market data to predict the value of homes in Boston. This tutorial builds on the skills learned from the previous tutorials, (Standard ML Pipeline and AutoML Pipeline.\nBefore You Start # You must have Pachyderm installed and running on your cluster You should have already completed the Standard ML Pipeline tutorial You must be familiar with jsonnet This tutorial assumes your active context is localhost:80 Tutorial # Our Docker image\u0026rsquo;s user code for this tutorial is built on top of the civisanalytics/datascience-python base image, which includes the necessary dependencies. It uses pandas to import the structured dataset and the scikit-learn library to train the model.\nEach pipeline in this tutorial executes a Python script, versions the artifacts (datasets, models, etc.), and gives you a full lineage of the model. Once it is set up, you can change, add, or remove data and Pachyderm will automatically keep everything up to date, creating data splits, computing data analysis metrics, and training the model.\n1. Create an Input Repo # Create a project named multipipeline-tutorial.\npachctl create project multipipeline-tutorial Set the project as current.\npachctl config update context --project multipipeline-tutorial Create a new data repository called csv_data where we will put our dataset.\npachctl create repo csv_data 2. Create the Pipelines # We\u0026rsquo;ll deploy each stage in our ML process as a Pachyderm pipeline. Organizing our work into pipelines allows us to keep track of artifacts created in our ML development process. We can extend or add pipelines at any point to add new functionality or features, while keeping track of code and data changes simultaneously.\n1. Data Analysis Pipeline # The data analysis pipeline creates a pair plot and a correlation matrix showing the relationship between features. By seeing what features are positively or negatively correlated to the target value (or each other), it can helps us understand what features may be valuable to the model.\nCreate a file named data_analysis.json with the following contents:\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;data_analysis\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;Data analysis pipeline that creates pairplots and correlation matrices for csv files.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;csv_data\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python\u0026#34;, \u0026#34;data_analysis.py\u0026#34;, \u0026#34;--input\u0026#34;, \u0026#34;/pfs/csv_data/\u0026#34;, \u0026#34;--target-col\u0026#34;, \u0026#34;MEDV\u0026#34;, \u0026#34;--output\u0026#34;, \u0026#34;/pfs/out/\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;jimmywhitaker/housing-prices-int:dev0.2\u0026#34; } } Save the file.\nCreate the pipeline.\npachctl create pipeline -f /path/to/data_analysis.json 2. Split Pipeline # Split the input csv files into train and test sets. As we new data is added, we will always have access to previous versions of the splits to reproduce experiments and test results.\nBoth the split pipeline and the data_analysis pipeline take the csv_data as input but have no dependencies on each other. Pachyderm is able to recognize this. It can run each pipeline simultaneously, scaling each horizontally.\nCreate a file named split.json with the following contents:\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;split\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;A pipeline that splits tabular data into training and testing sets.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;csv_data\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python\u0026#34;, \u0026#34;split.py\u0026#34;, \u0026#34;--input\u0026#34;, \u0026#34;/pfs/csv_data/\u0026#34;, \u0026#34;--test-size\u0026#34;, \u0026#34;0.1\u0026#34;, \u0026#34;--output\u0026#34;, \u0026#34;/pfs/out/\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;jimmywhitaker/housing-prices-int:dev0.2\u0026#34; } } Save the file.\nCreate the pipeline.\npachctl create pipeline -f /path/to/split.json 3. Regression Pipeline # To train the regression model using scikit-learn. In our case, we will train a Random Forest Regressor ensemble. After splitting the data into features and targets (X and y), we can fit the model to our parameters. Once the model is trained, we will compute our score (r^2) on the test set.\nAfter the model is trained we output some visualizations to evaluate its effectiveness of it using the learning curve and other statistics.\nCreate a file named regression.json with the following contents:\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;regression\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;A pipeline that trains and tests a regression model for tabular.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;glob\u0026#34;: \u0026#34;/*/\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;split\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python\u0026#34;, \u0026#34;regression.py\u0026#34;, \u0026#34;--input\u0026#34;, \u0026#34;/pfs/split/\u0026#34;, \u0026#34;--target-col\u0026#34;, \u0026#34;MEDV\u0026#34;, \u0026#34;--output\u0026#34;, \u0026#34;/pfs/out/\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;jimmywhitaker/housing-prices-int:dev0.2\u0026#34; } } Save the file.\nCreate the pipeline.\npachctl create pipeline -f /path/to/regression.json 3. Upload the Dataset # Download our first example data set, housing-simplified-1.csv. Add the data to your repo. pachctl put file csv_data@master:housing-simplified.csv -f /path/to/housing-simplified-1.csv 4. Download the Results # Once the pipeline has finished, download the results.\npachctl get file regression@master:/ --recursive --output . 5. Update the Dataset # Download our second example data set, housing-simplified-2.csv. Add the data to your repo. pachctl put file csv_data@master:housing-simplified.csv -f /path/to/housing-simplified-2.csv 6. Inspect the Data # We can use the diff command and ancestry syntax to see what has changed between the two versions of the dataset.\npachctl diff file csv_data@master csv_data@master^ Bonus Step: Rolling Back # If you need to roll back to a previous dataset commit, you can do so with the create branch command and ancestry syntax.\npachctl create branch csv_data@master --head csv_data@master^ User Code Assets # The Docker image used in this tutorial was built with the following assets:\nAssets: Dockerfile Regression.py Split.py Utils.py Nb_utils.py FROM civisanalytics/datascience-python RUN pip install seaborn WORKDIR /workdir/ COPY *.py /workdir/ import argparse import os from os import path import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import joblib from utils import plot_learning_curve from sklearn.model_selection import ShuffleSplit from sklearn import datasets, ensemble, linear_model from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import cross_val_score from sklearn.metrics import r2_score parser = argparse.ArgumentParser(description=\u0026#34;Structured data regression\u0026#34;) parser.add_argument(\u0026#34;--input\u0026#34;, type=str, help=\u0026#34;directory with train.csv and test.csv\u0026#34;) parser.add_argument(\u0026#34;--target-col\u0026#34;, type=str, default=\u0026#34;MEDV\u0026#34;, help=\u0026#34;column with target values\u0026#34;) parser.add_argument(\u0026#34;--output\u0026#34;, metavar=\u0026#34;DIR\u0026#34;, default=\u0026#39;./output\u0026#39;, help=\u0026#34;output directory\u0026#34;) def load_data(input_csv, target_col): # Load the Boston housing dataset data = pd.read_csv(input_csv, header=0) targets = data[target_col] features = data.drop(target_col, axis = 1) return data, features, targets def train_model(features, targets): # Train a Random Forest Regression model reg = ensemble.RandomForestRegressor(random_state=1) scores = cross_val_score(reg, features, targets, cv=10) print(\u0026#34;Cross Val Score: {:2f} (+/- {:2f})\u0026#34;.format(scores.mean(), scores.std() * 2)) reg.fit(features,targets) return reg def test_model(model, features, targets): # Train a Random Forest Regression model score = r2_score(model.predict(features), targets) return \u0026#34;Test Score: {:2f}\u0026#34;.format(score) def create_learning_curve(estimator, features, targets): plt.clf() title = \u0026#34;Learning Curves (Random Forest Regressor)\u0026#34; cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0) plot_learning_curve(estimator, title, features, targets, ylim=(0.5, 1.01), cv=cv, n_jobs=4) def main(): args = parser.parse_args() input_dirs = [] file_list = os.listdir(args.input) if \u0026#39;train.csv\u0026#39; in file_list and \u0026#39;test.csv\u0026#39; in file_list: input_dirs = [args.input] else: # Directory of directories for root, dirs, files in os.walk(args.input): for dir in dirs: file_list = os.listdir(os.path.join(root, dir)) if \u0026#39;train.csv\u0026#39; in file_list and \u0026#39;test.csv\u0026#39; in file_list: input_dirs.append(os.path.join(root,dir)) print(\u0026#34;Datasets: {}\u0026#34;.format(input_dirs)) os.makedirs(args.output, exist_ok=True) for dir in input_dirs: experiment_name = os.path.basename(os.path.splitext(dir)[0]) train_filename = os.path.join(dir,\u0026#39;train.csv\u0026#39;) test_filename = os.path.join(dir,\u0026#39;test.csv\u0026#39;) # Data loading train_data, train_features, train_targets = load_data(train_filename, args.target_col) print(\u0026#34;Training set has {} data points with {} variables each.\u0026#34;.format(*train_data.shape)) test_data, test_features, test_targets = load_data(test_filename, args.target_col) print(\u0026#34;Testing set has {} data points with {} variables each.\u0026#34;.format(*test_data.shape)) reg = train_model(train_features, train_targets) test_results = test_model(reg, test_features, test_targets) create_learning_curve(reg, train_features, train_targets) plt.savefig(path.join(args.output, experiment_name + \u0026#39;_cv_reg_output.png\u0026#39;)) print(test_results) # Save model and test score joblib.dump(reg, path.join(args.output, experiment_name + \u0026#39;_model.sav\u0026#39;)) with open(path.join(args.output, experiment_name + \u0026#39;_test_results.txt\u0026#39;), \u0026#34;w\u0026#34;) as text_file: text_file.write(test_results) if __name__ == \u0026#34;__main__\u0026#34;: main() import argparse import os from os import path import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import joblib from utils import load_data from sklearn.model_selection import train_test_split parser = argparse.ArgumentParser(description=\u0026#34;Structured data regression\u0026#34;) parser.add_argument(\u0026#34;--input\u0026#34;, type=str, help=\u0026#34;csv file with all examples\u0026#34;) parser.add_argument(\u0026#34;--output\u0026#34;, metavar=\u0026#34;DIR\u0026#34;, default=\u0026#39;./output\u0026#39;, help=\u0026#34;output directory\u0026#34;) parser.add_argument(\u0026#34;--test-size\u0026#34;, type=float, default=0.2, help=\u0026#34;percentage of data to use for testing (\\\u0026#34;0.2\\\u0026#34; = 20% used for testing, 80% for training\u0026#34;) parser.add_argument(\u0026#34;--seed\u0026#34;, type=int, default=42, help=\u0026#34;random seed\u0026#34;) def main(): args = parser.parse_args() if os.path.isfile(args.input): input_files = [args.input] else: # Directory for dirpath, dirs, files in os.walk(args.input): input_files = [ os.path.join(dirpath, filename) for filename in files if filename.endswith(\u0026#39;.csv\u0026#39;) ] print(\u0026#34;Datasets: {}\u0026#34;.format(input_files)) for filename in input_files: file_basename = os.path.basename(os.path.splitext(filename)[0]) os.makedirs(os.path.join(args.output,file_basename), exist_ok=True) # Data loading data = load_data(filename) train, test = train_test_split(data, test_size=args.test_size, random_state=args.seed) train.to_csv(os.path.join(args.output, file_basename, \u0026#39;train.csv\u0026#39;), header=True, index=False) test.to_csv(os.path.join(args.output, file_basename, \u0026#39;test.csv\u0026#39;), header=True, index=False) if __name__ == \u0026#34;__main__\u0026#34;: main() import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.datasets import load_digits from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit def load_data(input_csv, target_col=None): # Load the Boston housing dataset data = pd.read_csv(input_csv, header=0) if target_col: targets = data[target_col] features = data.drop(target_col, axis = 1) print(\u0026#34;Dataset has {} data points with {} variables each.\u0026#34;.format(*data.shape)) return data, features, targets return data def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)): \u0026#34;\u0026#34;\u0026#34; Generate 3 plots: the test and training learning curve, the training samples vs fit times curve, the fit times vs score curve. Parameters ---------- estimator : object type that implements the \u0026#34;fit\u0026#34; and \u0026#34;predict\u0026#34; methods An object of that type which is cloned for each validation. title : string Title for the chart. X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification or regression; None for unsupervised learning. axes : array of 3 axes, optional (default=None) Axes to use for plotting the curves. ylim : tuple, shape (ymin, ymax), optional Defines minimum and maximum yvalues plotted. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`StratifiedKFold` used. If the estimator is not a classifier or if ``y`` is neither binary nor multiclass, :class:`KFold` is used. Refer :ref:`User Guide \u0026lt;cross_validation\u0026gt;` for the various cross-validators that can be used here. n_jobs : int or None, optional (default=None) Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary \u0026lt;n_jobs\u0026gt;` for more details. train_sizes : array-like, shape (n_ticks,), dtype float or int Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5)) \u0026#34;\u0026#34;\u0026#34; if axes is None: _, axes = plt.subplots(1, 3, figsize=(20, 5)) axes[0].set_title(title) if ylim is not None: axes[0].set_ylim(*ylim) axes[0].set_xlabel(\u0026#34;Training examples\u0026#34;) axes[0].set_ylabel(\u0026#34;Score\u0026#34;) train_sizes, train_scores, test_scores, fit_times, _ = \\ learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, return_times=True) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) fit_times_mean = np.mean(fit_times, axis=1) fit_times_std = np.std(fit_times, axis=1) # Plot learning curve axes[0].grid() axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\u0026#34;r\u0026#34;) axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\u0026#34;g\u0026#34;) axes[0].plot(train_sizes, train_scores_mean, \u0026#39;o-\u0026#39;, color=\u0026#34;r\u0026#34;, label=\u0026#34;Training score\u0026#34;) axes[0].plot(train_sizes, test_scores_mean, \u0026#39;o-\u0026#39;, color=\u0026#34;g\u0026#34;, label=\u0026#34;Cross-validation score\u0026#34;) axes[0].legend(loc=\u0026#34;best\u0026#34;) # Plot n_samples vs fit_times axes[1].grid() axes[1].plot(train_sizes, fit_times_mean, \u0026#39;o-\u0026#39;) axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std, fit_times_mean + fit_times_std, alpha=0.1) axes[1].set_xlabel(\u0026#34;Training examples\u0026#34;) axes[1].set_ylabel(\u0026#34;fit_times\u0026#34;) axes[1].set_title(\u0026#34;Scalability of the model\u0026#34;) # Plot fit_time vs score axes[2].grid() axes[2].plot(fit_times_mean, test_scores_mean, \u0026#39;o-\u0026#39;) axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1) axes[2].set_xlabel(\u0026#34;fit_times\u0026#34;) axes[2].set_ylabel(\u0026#34;Score\u0026#34;) axes[2].set_title(\u0026#34;Performance of the model\u0026#34;) return plt import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import joblib from os import path from sklearn.model_selection import ShuffleSplit from sklearn import datasets, ensemble, linear_model from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import cross_val_score from sklearn.metrics import r2_score def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)): \u0026#34;\u0026#34;\u0026#34; Generate 3 plots: the test and training learning curve, the training samples vs fit times curve, the fit times vs score curve. Parameters ---------- estimator : object type that implements the \u0026#34;fit\u0026#34; and \u0026#34;predict\u0026#34; methods An object of that type which is cloned for each validation. title : string Title for the chart. X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification or regression; None for unsupervised learning. axes : array of 3 axes, optional (default=None) Axes to use for plotting the curves. ylim : tuple, shape (ymin, ymax), optional Defines minimum and maximum yvalues plotted. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`StratifiedKFold` used. If the estimator is not a classifier or if ``y`` is neither binary nor multiclass, :class:`KFold` is used. Refer :ref:`User Guide \u0026lt;cross_validation\u0026gt;` for the various cross-validators that can be used here. n_jobs : int or None, optional (default=None) Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary \u0026lt;n_jobs\u0026gt;` for more details. train_sizes : array-like, shape (n_ticks,), dtype float or int Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5)) \u0026#34;\u0026#34;\u0026#34; if axes is None: _, axes = plt.subplots(1, 1, figsize=(5, 5)) axes.set_title(title) if ylim is not None: axes.set_ylim(*ylim) axes.set_xlabel(\u0026#34;Training examples\u0026#34;) axes.set_ylabel(\u0026#34;Score\u0026#34;) train_sizes, train_scores, test_scores, fit_times, _ = \\ learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, return_times=True) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) fit_times_mean = np.mean(fit_times, axis=1) fit_times_std = np.std(fit_times, axis=1) # Plot learning curve axes.grid() axes.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\u0026#34;r\u0026#34;) axes.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\u0026#34;g\u0026#34;) axes.plot(train_sizes, train_scores_mean, \u0026#39;o-\u0026#39;, color=\u0026#34;r\u0026#34;, label=\u0026#34;Training score\u0026#34;) axes.plot(train_sizes, test_scores_mean, \u0026#39;o-\u0026#39;, color=\u0026#34;g\u0026#34;, label=\u0026#34;Cross-validation score\u0026#34;) axes.legend(loc=\u0026#34;best\u0026#34;) return plt def create_pairplot(data): plt.clf() # Calculate and show pairplot sns.pairplot(data, height=2.5) plt.tight_layout() def create_corr_matrix(data): plt.clf() # Calculate and show correlation matrix sns.set() corr = data.corr() # Generate a mask for the upper triangle mask = np.triu(np.ones_like(corr, dtype=np.bool)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio sns_plot = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, annot=True, cbar_kws={\u0026#34;shrink\u0026#34;: .5}) def data_analysis(data): create_pairplot(data) plt.show() create_corr_matrix(data) plt.show() def load_data(input_data, target_col): # Load the Boston housing dataset data = input_data targets = data[target_col] features = data.drop(target_col, axis = 1) return data, features, targets def train_model(features, targets): # Train a Random Forest Regression model reg = ensemble.RandomForestRegressor(random_state=1) scores = cross_val_score(reg, features, targets, cv=10) print(\u0026#34;Cross Val Score: {:2f} (+/- {:2f})\u0026#34;.format(scores.mean(), scores.std() * 2)) reg.fit(features,targets) return reg def test_model(model, features, targets): # Train a Random Forest Regression model score = r2_score(model.predict(features), targets) return \u0026#34;Test Score: {:2f}\u0026#34;.format(score) def create_learning_curve(estimator, features, targets): plt.clf() title = \u0026#34;Learning Curves (Random Forest Regressor)\u0026#34; cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0) plot_learning_curve(estimator, title, features, targets, ylim=(0.5, 1.01), cv=cv, n_jobs=4) def set_dtypes(data): for key in data: data[key] = data[key].astype(float) return data ",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "automl", "mljar"],
    "id": "9bd758b358d510e94ccbe342dc709013"
  },
  {
    "title": "Data Parallelism Pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Tutorials",
    "description": "Learn how to build a scalable inference pipeline using data parallelism.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/tutorials/data-parallelism/",
    "relURI": "/latest/build-dags/tutorials/data-parallelism/",
    "body": "In this tutorial, we‚Äôll build a scalable inference data parallelism pipeline for breast cancer detection using data parallelism.\nBefore You Start # You must have a Pachyderm cluster up and running You should have some basic familiarity with Pachyderm pipeline specs \u0026ndash; see the Transform, Cross Input, Resource Limits, Resource Requests, and Parallelism sections in particular Tutorial # Our Docker image\u0026rsquo;s user code for this tutorial is built on top of the pytorch/pytorch base image, which includes necessary dependencies. The underlying code and pre-trained breast cancer detection model comes from this repo, developed by the Center of Data Science and Department of Radiology at NYU. Their original paper can be found here.\n1. Create a Project \u0026amp; Input Repos # Create a project named data-parallelism-tutorial.\npachctl create project data-parallelism-tutorial Set the project as current.\npachctl config update context --project data-parallelism-tutorial Create the following repos:\npachctl create repo models pachctl create repo sample_data 2. Create a Classification Pipeline # We\u0026rsquo;re going to need to first build a pipeline that will classify the breast cancer images. We\u0026rsquo;ll use a cross input to combine the sample data and models.\nCreate a file named bc_classification.json with the following contents:\nResource: GPU CPU { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bc_classification\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;Run breast cancer classification.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;cross\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;sample_data\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;models\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34; } } ] }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;/bin/bash\u0026#34;, \u0026#34;run.sh\u0026#34;, \u0026#34;gpu\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/breast_cancer_classifier:1.11.6\u0026#34; }, \u0026#34;resource_limits\u0026#34;: { \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nvidia.com/gpu\u0026#34;, \u0026#34;number\u0026#34;: 1 } }, \u0026#34;resource_requests\u0026#34;: { \u0026#34;memory\u0026#34;: \u0026#34;4G\u0026#34;, \u0026#34;cpu\u0026#34;: 1 }, \u0026#34;parallelism_spec\u0026#34;: { \u0026#34;constant\u0026#34;: 8 } } { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bc_classification_cpu\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;Run breast cancer classification.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;cross\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;sample_data\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;models\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34; } } ] }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;/bin/bash\u0026#34;, \u0026#34;run.sh\u0026#34;, \u0026#34;cpu\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/breast_cancer_classifier:1.11.6\u0026#34; }, \u0026#34;parallelism_spec\u0026#34;: { \u0026#34;constant\u0026#34;: 4 } } Save the file.\nCreate the pipeline.\npachctl create pipeline -f /path/to/bc_classification.json üí° Datum Shape # When you define a glob pattern in your pipeline, you are defining how Pachyderm should split the data so that the code can execute as parallel jobs without having to modify the underlying implementation.\nIn this case, we are treating each exam (4 images and a list file) as a single datum. Each datum is processed individually, allowing parallelized computation for each exam that is added. The file structure for our sample_data is organized as follows:\nsample_data/ ‚îú‚îÄ‚îÄ \u0026lt;unique_exam_id\u0026gt; ‚îÇ ‚îú‚îÄ‚îÄ L_CC.png ‚îÇ ‚îú‚îÄ‚îÄ L_MLO.png ‚îÇ ‚îú‚îÄ‚îÄ R_CC.png ‚îÇ ‚îú‚îÄ‚îÄ R_MLO.png ‚îÇ ‚îî‚îÄ‚îÄ gen_exam_list_before_cropping.pkl ‚îú‚îÄ‚îÄ \u0026lt;unique_exam_id\u0026gt; ‚îÇ ‚îú‚îÄ‚îÄ L_CC.png ‚îÇ ‚îú‚îÄ‚îÄ L_MLO.png ‚îÇ ‚îú‚îÄ‚îÄ R_CC.png ‚îÇ ‚îú‚îÄ‚îÄ R_MLO.png ‚îÇ ‚îî‚îÄ‚îÄ gen_exam_list_before_cropping.pkl ... The gen_exam_list_before_cropping.pkl is a pickled version of the image list, a requirement of the underlying library being used.\n3. Upload Dataset # Open or download this github repo.\ngh repo clone pachyderm/docs-content Navigate to this tutorial.\ncd content/latest/build-dags/tutorials/data-parallelism Upload the sample_data and models folders to your repos.\npachctl put file -r sample_data@master -f sample_data/ pachctl put file -r models@master -f models/ User Code Assets # The Docker image used in this tutorial was built with the following assets:\nAssets: Dockerfile Run.sh FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-devel # Update NVIDIA\u0026#39;s apt-key # Announcement: https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772 ENV DISTRO ubuntu1804 ENV CPU_ARCH x86_64 RUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/$DISTRO/$CPU_ARCH/3bf863cc.pub RUN apt-get update \u0026amp;\u0026amp; apt-get install -y git libgl1-mesa-glx libglib2.0-0 WORKDIR /workspace RUN git clone https://github.com/jimmywhitaker/breast_cancer_classifier.git /workspace RUN pip install --upgrade pip \u0026amp;\u0026amp; pip install -r requirements.txt RUN pip install matplotlib --ignore-installed RUN apt-get -y install tree COPY . /workspace #!/bin/bash NUM_PROCESSES=10 DEVICE_TYPE=$1 NUM_EPOCHS=10 HEATMAP_BATCH_SIZE=100 GPU_NUMBER=0 ID=$(ls /pfs/sample_data/ | head -n 1) DATA_FOLDER=\u0026#34;/pfs/sample_data/${ID}/\u0026#34; INITIAL_EXAM_LIST_PATH=\u0026#34;/pfs/sample_data/${ID}/gen_exam_list_before_cropping.pkl\u0026#34; PATCH_MODEL_PATH=\u0026#34;/pfs/models/sample_patch_model.p\u0026#34; IMAGE_MODEL_PATH=\u0026#34;/pfs/models/sample_image_model.p\u0026#34; IMAGEHEATMAPS_MODEL_PATH=\u0026#34;/pfs/models/sample_imageheatmaps_model.p\u0026#34; CROPPED_IMAGE_PATH=\u0026#34;/pfs/out/${ID}/cropped_images\u0026#34; CROPPED_EXAM_LIST_PATH=\u0026#34;/pfs/out/${ID}/cropped_images/cropped_exam_list.pkl\u0026#34; EXAM_LIST_PATH=\u0026#34;/pfs/out/${ID}/data.pkl\u0026#34; HEATMAPS_PATH=\u0026#34;/pfs/out/${ID}/heatmaps\u0026#34; IMAGE_PREDICTIONS_PATH=\u0026#34;/pfs/out/${ID}/image_predictions.csv\u0026#34; IMAGEHEATMAPS_PREDICTIONS_PATH=\u0026#34;/pfs/out/${ID}/imageheatmaps_predictions.csv\u0026#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo \u0026#39;Stage 1: Crop Mammograms\u0026#39; python3 src/cropping/crop_mammogram.py \\ --input-data-folder $DATA_FOLDER \\ --output-data-folder $CROPPED_IMAGE_PATH \\ --exam-list-path $INITIAL_EXAM_LIST_PATH \\ --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\ --num-processes $NUM_PROCESSES echo \u0026#39;Stage 2: Extract Centers\u0026#39; python3 src/optimal_centers/get_optimal_centers.py \\ --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\ --data-prefix $CROPPED_IMAGE_PATH \\ --output-exam-list-path $EXAM_LIST_PATH \\ --num-processes $NUM_PROCESSES echo \u0026#39;Stage 3: Generate Heatmaps\u0026#39; python3 src/heatmaps/run_producer.py \\ --model-path $PATCH_MODEL_PATH \\ --data-path $EXAM_LIST_PATH \\ --image-path $CROPPED_IMAGE_PATH \\ --batch-size $HEATMAP_BATCH_SIZE \\ --output-heatmap-path $HEATMAPS_PATH \\ --device-type $DEVICE_TYPE \\ --gpu-number $GPU_NUMBER echo \u0026#39;Stage 4a: Run Classifier (Image)\u0026#39; python3 src/modeling/run_model.py \\ --model-path $IMAGE_MODEL_PATH \\ --data-path $EXAM_LIST_PATH \\ --image-path $CROPPED_IMAGE_PATH \\ --output-path $IMAGE_PREDICTIONS_PATH \\ --use-augmentation \\ --num-epochs $NUM_EPOCHS \\ --device-type $DEVICE_TYPE \\ --gpu-number $GPU_NUMBER echo \u0026#39;Stage 4b: Run Classifier (Image+Heatmaps)\u0026#39; python3 src/modeling/run_model.py \\ --model-path $IMAGEHEATMAPS_MODEL_PATH \\ --data-path $EXAM_LIST_PATH \\ --image-path $CROPPED_IMAGE_PATH \\ --output-path $IMAGEHEATMAPS_PREDICTIONS_PATH \\ --use-heatmaps \\ --heatmaps-path $HEATMAPS_PATH \\ --use-augmentation \\ --num-epochs $NUM_EPOCHS \\ --device-type $DEVICE_TYPE \\ --gpu-number $GPU_NUMBER ",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "automl", "mljar", "inference"],
    "id": "0f8e765abe382a5d95b259668b1aab34"
  },
  {
    "title": "Task Parallelism Pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Tutorials",
    "description": "Learn how to build a scalable inference pipeline using task parallelism.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/tutorials/task-parallelism/",
    "relURI": "/latest/build-dags/tutorials/task-parallelism/",
    "body": "In this tutorial, we‚Äôll build a scalable inference pipeline for breast cancer detection using task parallelism.\nBefore You Start # You must have a Pachyderm cluster up and running You should have some basic familiarity with Pachyderm pipeline specs \u0026ndash; see the Transform, Join Input, Resource Limits, Resource Requests, and Parallelism sections in particular Tutorial # Our Docker image\u0026rsquo;s user code for this tutorial is built on top of the pytorch/pytorch base image, which includes necessary dependencies. The underlying code and pre-trained breast cancer detection model comes from this repo, developed by the Center of Data Science and Department of Radiology at NYU. Their original paper can be found here.\n1. Create an Input Repo # Make sure your Tutorials project we created in the Standard ML Pipeline tutorial is set to your active context. (This would only change if you have updated your active context since completing the first tutorial.)\npachctl config get context localhost:80 # { # \u0026#34;pachd_address\u0026#34;: \u0026#34;grpc://localhost:80\u0026#34;, # \u0026#34;cluster_deployment_id\u0026#34;: \u0026#34;KhpCZx7c8prdB268SnmXjELG27JDCaji\u0026#34;, # \u0026#34;project\u0026#34;: \u0026#34;Tutorials\u0026#34; # } Create the following repos:\npachctl create repo models pachctl create repo sample_data 2. Create CPU Pipelines # In task parallelism, we separate out the CPU-based preprocessing and GPU-related tasks, saving us cloud costs when scaling. By separating inference into multiple tasks, each task pipeline can be updated independently, allowing ease of model deployment and collaboration.\nWe can split the run.sh script used in the previous tutorial (Data Parallelism Pipeline) into 5 separate processing steps (4 already defined in the script + a visualization step) which will become Pachyderm pipelines, so each can be scaled separately.\nCrop Pipeline # Create a file named crop.json with the following contents: { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;crop\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;Remove background of image and save cropped files.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;sample_data\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;/bin/bash\u0026#34;, \u0026#34;multi-stage/crop.sh\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/breast_cancer_classifier:1.11.6\u0026#34; } } Save the file. Create the pipeline. pachctl create pipeline -f /path/to/crop.json Extract Centers Pipeline # Create a file named extract_centers.json with the following contents: { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;extract_centers\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;Compute and Extract Optimal Image Centers.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;crop\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;/bin/bash\u0026#34;, \u0026#34;multi-stage/extract_centers.sh\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/breast_cancer_classifier:1.11.6\u0026#34; } } Save the file. Create the pipeline. pachctl create pipeline -f /path/to/extract_centers.json 3. Create GPU Pipelines # Generate Heatmaps Pipeline # Create a file named generate_heatmaps.json with the following contents: { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;generate_heatmaps\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;Generates benign and malignant heatmaps for cropped images using patch classifier.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;cross\u0026#34;: [ { \u0026#34;join\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;crop\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/(*)\u0026#34;, \u0026#34;join_on\u0026#34;: \u0026#34;$1\u0026#34;, \u0026#34;lazy\u0026#34;: false } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;extract_centers\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/(*)\u0026#34;, \u0026#34;join_on\u0026#34;: \u0026#34;$1\u0026#34;, \u0026#34;lazy\u0026#34;: false } } ] }, { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;models\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;lazy\u0026#34;: false } } ] }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;/bin/bash\u0026#34;, \u0026#34;multi-stage/generate_heatmaps.sh\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/breast_cancer_classifier:1.11.6\u0026#34; }, \u0026#34;resource_limits\u0026#34;: { \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nvidia.com/gpu\u0026#34;, \u0026#34;number\u0026#34;: 1 } }, \u0026#34;resource_requests\u0026#34;: { \u0026#34;memory\u0026#34;: \u0026#34;4G\u0026#34;, \u0026#34;cpu\u0026#34;: 1 } } Save the file. Create the pipeline. pachctl create pipeline -f /path/to/generate_heatmaps.json Classify Pipeline # Create a file named classify.json with the following contents: { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;classify\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;Runs the image only model and image+heatmaps model for breast cancer prediction.\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;cross\u0026#34;: [ { \u0026#34;join\u0026#34;: [ { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;crop\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/(*)\u0026#34;, \u0026#34;join_on\u0026#34;: \u0026#34;$1\u0026#34; } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;extract_centers\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/(*)\u0026#34;, \u0026#34;join_on\u0026#34;: \u0026#34;$1\u0026#34; } }, { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;generate_heatmaps\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/(*)\u0026#34;, \u0026#34;join_on\u0026#34;: \u0026#34;$1\u0026#34; } } ] }, { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;models\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34; } } ] }, \u0026#34;transform\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;/bin/bash\u0026#34;, \u0026#34;multi-stage/classify.sh\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;pachyderm/breast_cancer_classifier:1.11.6\u0026#34; }, \u0026#34;resource_limits\u0026#34;: { \u0026#34;gpu\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nvidia.com/gpu\u0026#34;, \u0026#34;number\u0026#34;: 1 } }, \u0026#34;resource_requests\u0026#34;: { \u0026#34;memory\u0026#34;: \u0026#34;4G\u0026#34;, \u0026#34;cpu\u0026#34;: 1 } } Save the file. Create the pipeline pachctl create pipeline -f /path/to/classify.json 4. Upload Dataset # Open or download this github repo. gh repo clone pachyderm/docs-content Navigate to this tutorial. cd content/latest/build-dags/tutorials/task-parallelism Upload the sample_data and models folders to your repos. pachctl put file -r sample_data@master -f sample_data/ pachctl put file -r models@master -f models/ User Code Assets # The Docker image used in this tutorial was built with the following assets:\nAssets: Dockerfile Crop.sh Extract_Centers.sh Generate_Heatmaps.sh Classify.sh Visualize_Heatmaps.sh FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-devel # Update NVIDIA\u0026#39;s apt-key # Announcement: https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772 ENV DISTRO ubuntu1804 ENV CPU_ARCH x86_64 RUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/$DISTRO/$CPU_ARCH/3bf863cc.pub RUN apt-get update \u0026amp;\u0026amp; apt-get install -y git libgl1-mesa-glx libglib2.0-0 WORKDIR /workspace RUN git clone https://github.com/jimmywhitaker/breast_cancer_classifier.git /workspace RUN pip install --upgrade pip \u0026amp;\u0026amp; pip install -r requirements.txt RUN pip install matplotlib --ignore-installed RUN apt-get -y install tree COPY . /workspace #!/bin/bash NUM_PROCESSES=10 ID=$(ls /pfs/sample_data/ | head -n 1) DATA_FOLDER=\u0026#34;/pfs/sample_data/${ID}/\u0026#34; INITIAL_EXAM_LIST_PATH=\u0026#34;/pfs/sample_data/${ID}/gen_exam_list_before_cropping.pkl\u0026#34; CROPPED_IMAGE_PATH=\u0026#34;/pfs/out/${ID}/cropped_images\u0026#34; CROPPED_EXAM_LIST_PATH=\u0026#34;/pfs/out/${ID}/cropped_images/cropped_exam_list.pkl\u0026#34; EXAM_LIST_PATH=\u0026#34;/pfs/out/${ID}/data.pkl\u0026#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo \u0026#39;Stage 1: Crop Mammograms\u0026#39; python3 src/cropping/crop_mammogram.py \\ --input-data-folder $DATA_FOLDER \\ --output-data-folder $CROPPED_IMAGE_PATH \\ --exam-list-path $INITIAL_EXAM_LIST_PATH \\ --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\ --num-processes $NUM_PROCESSES #!/bin/bash DEVICE_TYPE=\u0026#39;gpu\u0026#39; NUM_EPOCHS=10 HEATMAP_BATCH_SIZE=100 GPU_NUMBER=0 ID=$(ls /pfs/crop/ | head -n 1) IMAGEHEATMAPS_MODEL_PATH=\u0026#34;/pfs/models/sample_imageheatmaps_model.p\u0026#34; CROPPED_IMAGE_PATH=\u0026#34;/pfs/crop/${ID}/cropped_images\u0026#34; EXAM_LIST_PATH=\u0026#34;/pfs/extract_centers/${ID}/data.pkl\u0026#34; HEATMAPS_PATH=\u0026#34;/pfs/generate_heatmaps/${ID}/heatmaps\u0026#34; IMAGEHEATMAPS_PREDICTIONS_PATH=\u0026#34;/pfs/out/${ID}/imageheatmaps_predictions.csv\u0026#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo \u0026#39;Stage 4b: Run Classifier (Image+Heatmaps)\u0026#39; python3 src/modeling/run_model.py \\ --model-path $IMAGEHEATMAPS_MODEL_PATH \\ --data-path $EXAM_LIST_PATH \\ --image-path $CROPPED_IMAGE_PATH \\ --output-path $IMAGEHEATMAPS_PREDICTIONS_PATH \\ --use-heatmaps \\ --heatmaps-path $HEATMAPS_PATH \\ --use-augmentation \\ --num-epochs $NUM_EPOCHS \\ --device-type $DEVICE_TYPE \\ --gpu-number $GPU_NUMBER #!/bin/bash NUM_PROCESSES=10 ID=$(ls /pfs/crop/ | head -n 1) CROPPED_IMAGE_PATH=\u0026#34;/pfs/crop/${ID}/cropped_images\u0026#34; CROPPED_EXAM_LIST_PATH=\u0026#34;/pfs/crop/${ID}/cropped_images/cropped_exam_list.pkl\u0026#34; EXAM_LIST_PATH=\u0026#34;/pfs/out/${ID}/data.pkl\u0026#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo \u0026#39;Stage 2: Extract Centers\u0026#39; python3 src/optimal_centers/get_optimal_centers.py \\ --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\ --data-prefix $CROPPED_IMAGE_PATH \\ --output-exam-list-path $EXAM_LIST_PATH \\ --num-processes $NUM_PROCESSES #!/bin/bash DEVICE_TYPE=\u0026#39;gpu\u0026#39; HEATMAP_BATCH_SIZE=100 GPU_NUMBER=0 ID=$(ls /pfs/crop/ | head -n 1) PATCH_MODEL_PATH=\u0026#34;/pfs/models/sample_patch_model.p\u0026#34; CROPPED_IMAGE_PATH=\u0026#34;/pfs/crop/${ID}/cropped_images\u0026#34; EXAM_LIST_PATH=\u0026#34;/pfs/extract_centers/${ID}/data.pkl\u0026#34; HEATMAPS_PATH=\u0026#34;/pfs/out/${ID}/heatmaps\u0026#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo \u0026#39;Stage 3: Generate Heatmaps\u0026#39; python3 src/heatmaps/run_producer.py \\ --model-path $PATCH_MODEL_PATH \\ --data-path $EXAM_LIST_PATH \\ --image-path $CROPPED_IMAGE_PATH \\ --batch-size $HEATMAP_BATCH_SIZE \\ --output-heatmap-path $HEATMAPS_PATH \\ --device-type $DEVICE_TYPE \\ --gpu-number $GPU_NUMBER #!/bin/bash ID=$(ls /pfs/crop/ | head -n 1) CROPPED_IMAGE_PATH=\u0026#34;/pfs/crop/${ID}/cropped_images/\u0026#34; HEATMAPS_PATH=\u0026#34;/pfs/generate_heatmaps/${ID}/heatmaps/\u0026#34; OUTPUT_PATH=\u0026#34;/pfs/out/${ID}/\u0026#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo \u0026#39;Stage 5: Visualize Heatmaps\u0026#39; python3 src/heatmaps/visualize_heatmaps.py \\ --image-path $CROPPED_IMAGE_PATH \\ --heatmap-path $HEATMAPS_PATH \\ --output-path $OUTPUT_PATH ",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "automl", "mljar", "inference"],
    "id": "1921ce7e0907bc6eb5a0b89c1fd23198"
  },
  {
    "title": "Docker Image + User Code",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Tutorials",
    "description": "Learn how to build a Docker image that contains your user code and all required dependencies.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/build-dags/tutorials/user-code/",
    "relURI": "/latest/build-dags/tutorials/user-code/",
    "body": "In this tutorial, you\u0026rsquo;ll learn how to build a Docker image that contains your user code and all required dependencies. You\u0026rsquo;ll then learn how to reference this Docker image in a pipeline spec.\nBefore You Start # You must have Docker installed on your machine. You should be familiar with the Pachyderm pipeline specification. You should be comfortable in a programming language of your choice (Python is most common). Tutorial # Pipelines in Pachyderm use Docker images to execute user code. When you specify an image in a pipeline spec, Pachyderm deploys the image to the cluster. During pipeline execution, Pachyderm pulls the image from the registry and creates containers from it to process data in the pipeline.\nNone of our tutorials require that you build your own Docker images. But eventually, you\u0026rsquo;ll want to build your own Docker images so that you can precisely control how your data is transformed. We recommend that you read through this tutorial for context on how pipelines use Docker images to transform your data, but you don\u0026rsquo;t need to follow along with the steps until you are ready to build your first custom pipeline/DAG.\nTo jump into a hands-on tutorial that uses a pre-built Docker image, check out our other tutorials in this section, starting with the Standard ML Pipeline tutorial.\n1. Create a Working Directory # Create a working directory for your all the files we\u0026rsquo;ll need to create a Docker image with your code.\nmkdir -p ~/Pachyderm/tutorials/my-first-image cd ~/Pachyderm/tutorials/my-first-image 2. Create a Dockerfile # To create a Docker image with your user code, you\u0026rsquo;ll need to first create a Dockerfile. The Dockerfile defines the environment and dependencies needed to run your code using the files located inside of your working directory. The following is an example of a simple working directory with a Dockerfile.\n. ‚îú‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ app.py ‚îî‚îÄ‚îÄ requirements.txt Create a file named Dockerfile in your working directory. Add a base image to your Dockerfile. The base image is the starting point for your Docker image. You can use any image that has the dependencies you need to run your code. Add a WORKDIR command to your Dockerfile. The WORKDIR command sets the working directory for your Docker image. This is the directory where your code will be run. Add a COPY command to your Dockerfile. The COPY command copies the contents of your working directory into the Docker image. This is where you\u0026rsquo;ll copy your code and any other files needed to run your code. Add a RUN command to your Dockerfile. The RUN command executes a command in your Docker image. This is where you\u0026rsquo;ll install any dependencies needed to run your code. Add a CMD command to your Dockerfile. The CMD command defines the command that will be run when the Docker image is run. This is where you\u0026rsquo;ll define the command to run your actual user code. Python Example:\nFROM python:3.9-slim-buster # Set the working directory to /app WORKDIR /app # Copy the current directory contents into the container at /app COPY . /app # Install any needed packages specified in requirements.txt RUN pip install --trusted-host pypi.python.org -r requirements.txt # Define environment variable ENV NAME World # Run app.py when the container launches CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] 3. Add Your Code # Create a file named app.py in your working directory. Write your desired transformation code in the app.py file. Create a file named requirements.txt in your working directory. List all of the dependencies needed to run your code in the requirements.txt file. 4. Build the Docker Image # Open the terminal and navigate to your working directory. Run the following command. docker build -t \u0026lt;image-name\u0026gt;:\u0026lt;tag\u0026gt; . 4. Register the Docker Image # In order to make your Docker image available to your Pachyderm cluster, you\u0026rsquo;ll need to push it to a Docker registry. In this tutorial, we\u0026rsquo;ll be using Docker Hub.\nCreate a Docker Hub account. Run the following command to log into Docker Hub. docker login Run the following commands to tag and push your Docker image to Docker Hub. docker tag \u0026lt;image-name\u0026gt;:\u0026lt;tag\u0026gt; \u0026lt;docker-hub-username\u0026gt;/\u0026lt;image-name\u0026gt;:\u0026lt;tag\u0026gt; docker push \u0026lt;docker-hub-username\u0026gt;/\u0026lt;image-name\u0026gt;:\u0026lt;tag\u0026gt; Run the following command to verify that your Docker image was pushed to Docker Hub. docker images Run the following command to verify that your Docker image is available on Docker Hub. docker pull \u0026lt;docker-hub-username\u0026gt;/\u0026lt;image-name\u0026gt;:\u0026lt;tag\u0026gt; 5. Use Your Docker Image # Now that you have a Docker image with your code, you can reference it in a pipeline spec. The following is an example of a pipeline spec that references a Docker image with user code.\nCreate a file named my-pipeline.json in your working directory.\nDefine the pipeline spec using the options found in the pipeline specification docs.\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-pipeline\u0026#34; }, \u0026#34;transform\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;\u0026lt;dockerhub-username\u0026gt;/\u0026lt;image-name\u0026gt;:\u0026lt;tag\u0026gt;\u0026#34;, \u0026#34;cmd\u0026#34;: [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;], \u0026#34;stdin\u0026#34;: [\u0026#34;input\u0026#34;], \u0026#34;stdout\u0026#34;: [\u0026#34;output\u0026#34;] }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;input-repo\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/*\u0026#34; } } } Create a pipeline using the my-pipeline.json file.\npachctl create pipeline -f my-pipeline.json Inspect your pipeline to verify that it was created successfully.\npachctl inspect pipeline my-pipeline ",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["tutorials"],
    "id": "9f77a9557fb0acae17450d7969557344"
  },
  {
    "title": "Export Data",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Export transformed data using data egress and other methods.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/",
    "relURI": "/latest/export-data/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e2ce9dc6d479ebd494c97688c9566f47"
  },
  {
    "title": "Egress To An SQL Database",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Export Data",
    "description": "Learn how to use data egress to end data to an SQL database.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/sql-egress/",
    "relURI": "/latest/export-data/sql-egress/",
    "body": " ‚ö†Ô∏è SQL Egress is an experimental feature.\nPachyderm already implements egress to object storage as an optional egress field in the pipeline specification. Similarly, our SQL egress lets you seamlessly export data from a Pachyderm-powered pipeline output repo to an SQL database.\nSpecifically, we help you connect to a remote database and push the content of CSV files to interface tables, matching their column names and casting their content into their respective SQL datatype.\nInterface tables are intermediate tables used for staging the data being egressed from Pachyderm to your data warehouse. They are the tables your SQL Egress pipeline inserts its data into and should be dedicated tables. The content of your interface tables matches the content of the latest output commit of your pipeline.\n‚ÑπÔ∏è Best Practice.\nA new output commit will trigger a delete of all data in the interface tables before inserting more recent values. As a best practice, we strongly recommend to create a separate database for Pachyderm Egress.\nAs of today, we support the following drivers:\npostgres and postgresql : connect to Postgresql (or compatible databases such as Redshift). mysql : connect to MySQL (or compatible databases such as MariaDB). snowflake : connect to Snowflake. Use SQL Egress # To egress data from the output commit of a pipeline to an SQL database, you will need to:\nOn your cluster\nCreate a secret containing your database password.\nIn the Specification file of your egress pipeline\nReference your secret by providing its name, provide the connection string to the database and choose the format of the files (CSV for now - we are planning on adding JSON soon) containing the data to insert.\nIn your user code\nWrite your data into CSV files placed in root directories named after the table you want to insert them into. You can have multiple directories.\n1. Create a Secret # Create a secret containing your database password in the field PACHYDERM_SQL_PASSWORD. This secret is identical to the database secret of Pachyderm SQL Ingest. Refer to the SQL Ingest page for instructions on how to create your secret.\n2. Update your Pipeline Spec # Append an egress section to your pipeline specification file, then fill in:\nthe url: the connection string to your database.\nthe file_format type: CSV for now.\nthe name: the Kubernetes secret name.\nthe columns: Optional array for egress of CSV files with headers only. The order of the columns in this array must match the order of the schema columns; however, the CSV columns can be any order. So if the array is [\u0026ldquo;foo\u0026rdquo;, \u0026ldquo;bar\u0026rdquo;] and the CSV file is:\nbar,foo 1,\u0026#34;string\u0026#34; 2,\u0026#34;text!\u0026#34; The following table will be written to the database:\nfoo | bar =============== string | 1 text! | 2 Example # { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;egress\u0026#34; }, \u0026#34;input\u0026#34;: { \u0026#34;pfs\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;input_repo\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;in\u0026#34; } }, \u0026#34;transform\u0026#34;: { ... }, \u0026#34;egress\u0026#34;: { \u0026#34;sql_database\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;snowflake://pachyderm@WHMUWUD-CJ80657/PACH_DB/PUBLIC?warehouse=COMPUTE_WH\u0026#34;, \u0026#34;file_format\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;CSV\u0026#34;, \u0026#34;columns\u0026#34;: [\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;] }, \u0026#34;secret\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;snowflakesecret\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;PACHYDERM_SQL_PASSWORD\u0026#34; } } } } 3. In your User Code, Write Your Data to Directories Named After Each Table # The user code of your pipeline determines what data should be egressed and to which tables. Data (in the form of CSV files) that the pipeline writes to the output repo is interpreted as tables corresponding to directories.\nEach top-level directory is named after the table you want to egress its content to. All of the files reachable in the walk of each root directory are parsed in the given format indicated in the egress section of the pipeline specification file (CSV for now), then inserted in their corresponding table. Find more information on how to format your CSV file depending on your targeted SQL Data Type in our SQL Ingest Formatting section.\n‚ö†Ô∏è All interface tables must pre-exist before an insertion. Files in the root produce an error as they do not correspond to a table. The directory structure below the top level does not matter. The first directory in the path is the table; everything else is walked until a file is found. All the data in those files is inserted into the table. The order of the values in each line of a CSV must match the order of the columns in the schema of your interface table unless you were using headers AND specified the \u0026quot;columns\u0026quot;: [\u0026quot;foo\u0026quot;, \u0026quot;bar\u0026quot;], field in your pipeline specification file. Example # \u0026#34;1\u0026#34;,\u0026#34;Tim\u0026#34;,\u0026#34;2017-03-12T21:51:45Z\u0026#34;,\u0026#34;true\u0026#34; \u0026#34;12\u0026#34;,\u0026#34;Tom\u0026#34;,\u0026#34;2017-07-25T21:51:45Z\u0026#34;,\u0026#34;true\u0026#34; \u0026#34;33\u0026#34;,\u0026#34;Tam\u0026#34;,\u0026#34;2017-01-01T21:51:45Z\u0026#34;,\u0026#34;false\u0026#34; \u0026#34;54\u0026#34;,\u0026#34;Pach\u0026#34;,\u0026#34;2017-05-15T21:51:45Z\u0026#34;,\u0026#34;true\u0026#34; ‚ÑπÔ∏è Pachyderm queries the schema of the interface tables before insertion then parses the data into their SQL data types. Each insertion creates a new row in your table. Troubleshooting # You have a pipeline running but do not see any update in your database?\nCheck your logs:\npachctl list pipeline pachctl logs -p \u0026lt;your-pipeline-name\u0026gt; --master ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["sql", "data-operations", "export"],
    "id": "788e1ce8de49e90cb8a76970f89c3d00"
  },
  {
    "title": "Export via Egress",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Export Data",
    "description": "Learn how to push pipeline results to an external datastore using the egress pipeline spec attribute.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/export-data-egress/",
    "relURI": "/latest/export-data/export-data-egress/",
    "body": "The egress field in the Pachyderm pipeline specification enables you to push the results of a pipeline to an external datastore such as Amazon S3, Google Cloud Storage, or Azure Blob Storage. After the user code has finished running, but before the job is marked as successful, Pachyderm pushes the data to the specified destination.\n‚ÑπÔ∏è Make sure that your cluster has been configured to work with your object store.\nPick the egress protocol that applies to your storage:\nCloud Platform Protocol Example Google Cloud Storage gs:// gs://gs-bucket/gs-dir Amazon S3 s3:// s3://s3-endpoint/s3-bucket/s3-dir Azure Blob Storage wasb:// wasb://default-container@storage-account/az-dir Example # \u0026#34;egress\u0026#34;: { \u0026#34;URL\u0026#34;: \u0026#34;s3://bucket/dir\u0026#34; }, ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["egress", "data-operations", "export"],
    "id": "96b16efc992ac7b96323e8b2af848083"
  },
  {
    "title": "Export via PachCTL",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Export Data",
    "description": "Learn how to export data using the pachctl get command.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/export-data-pachctl/",
    "relURI": "/latest/export-data/export-data-pachctl/",
    "body": "To export your data with PachCTL:\nList the files in the given directory:\npachctl list file \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;:\u0026lt;dir\u0026gt; Example:\npachctl list myrepo@master:labresults System Response:\nNAME TYPE SIZE /labresults/T1606331395-LIPID-PATID2-CLIA24D9871327.txt file 101B /labresults/T1606707557-LIPID-PATID1-CLIA24D9871327.txt file 101B /labresults/T1606707579-LIPID-PATID3-CLIA24D9871327.txt file 100B /labresults/T1606707597-LIPID-PATID4-CLIA24D9871327.txt file 101B Get the contents of a specific file:\npachctl get file \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;:\u0026lt;path/to/file\u0026gt; Example:\npachctl get file myrepo@master:/labresults/T1606331395-LIPID-PATID2-CLIA24D9871327.txt System Response:\nPID|PATID2 ORC|ORD777889 OBX|1|NM|2093-3^Cholesterol|212|mg/dL OBX|2|NM|2571-8^Triglyceride|110|mg/dL ‚ÑπÔ∏è You can view the parent, grandparent, and any previous commit by using the caret (^) symbol followed by a number that corresponds to an ancestor in sequence:\nView a parent commit\npachctl list commit \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;^:\u0026lt;path/to/file\u0026gt; pachctl get file \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;^:\u0026lt;path/to/file\u0026gt; View an \u0026lt;n\u0026gt; parent of a commit\npachctl list commit \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;^\u0026lt;n\u0026gt;:\u0026lt;path/to/file\u0026gt; pachctl get file \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;^\u0026lt;n\u0026gt;:\u0026lt;path/to/file\u0026gt; Example:\npachctl get file datas@master^4:user_data.csv If the file does not exist in that revision, Pachyderm displays an error message.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pachctl", "data-operations", "export"],
    "id": "20fa2ab056ca395c4e816ee4a366c378"
  },
  {
    "title": "Mount a Repo Locally",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Export Data",
    "description": "Learn how to mount a repository as a local filesystem using the pachctl mount command.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/mount-repo-to-local-computer/",
    "relURI": "/latest/export-data/mount-repo-to-local-computer/",
    "body": " ‚ö†Ô∏è Pachyderm uses FUSE to mount repositories as local filesystems. Because Apple has announced phasing out support for macOS kernel extensions, including FUSE, this functionality is no longer stable on macOS Catalina (10.15) or later.\nPachyderm enables you to mount a repository as a local filesystem on your computer by using the pachctl mount command. This command uses the Filesystem in Userspace (FUSE) user interface to export a Pachyderm File System (PFS) to a Unix computer system. This functionality is useful when you want to pull data locally to experiment, review the results of a pipeline, or modify the files in the input repository directly.\nYou can mount a Pachyderm repo in one of the following modes:\nRead-only ‚Äî you can read the mounted files to further experiment with them locally, but cannot modify them. Read-write ‚Äî you can read mounted files, modify their contents, and push them back into your centralized Pachyderm input repositories. Prerequisites # You must have the following configured for this functionality to work:\nUnix or Unix-like operating system, such as Ubuntu 16.04 or macOS Yosemite or later.\nFUSE for your operating system installed:\nOn macOS, run:\nbrew install osxfuse On Ubuntu, run:\nsudo apt-get install -y fuse For more information, see:\nFUSE for macOS\nMounting Repositories in Read-Only Mode # By default, Pachyderm mounts all repositories in read-only mode. You can access the files through your file browser or enable third-party applications access. Read-only access enables you to explore and experiment with the data, without modifying it. For example, you can mount your repo to a local computer and then open that directory in a Jupyter Notebook for exploration.\n‚ÑπÔ∏è The pachctl mount command allows you to mount not only the default branch, typically a master branch, but also other Pachyderm branches. By default, Pachyderm mounts the master branch. However, if you add a branch to the name of the repo, the HEAD of that branch will be mounted.\nExample:\npachctl mount images --repos images@staging You can also mount a specific commit, but because commits might be on multiple branches, modifying them might result in data deletion in the HEAD of the branches. Therefore, you can only mount commits in read-only mode. If you want to write to a specific commit that is not the HEAD of a branch, you can create a new branch with that commit as HEAD.\nMounting Repositories in Read-Write Mode # Running the pachctl mount command with the --write flag grants you write access to the mounted repositories, which means that you can open the files for editing and put them back to the Pachyderm repository.\n‚ö†Ô∏è Your changes are saved to the Pachyderm repository only after you interrupt the pachctl mount with CTRL+C or with pachctl unmount, unmount /\u0026lt;path-to-mount\u0026gt;, or fusermount -u /\u0026lt;path-to-mount\u0026gt;.\nFor example, you have the OpenCV example pipeline up and running. If you want to edit files in the images repository, experiment with brightness and contrast settings in liberty.png, and finally have your edges pipeline process those changes. If you do not mount the images repo, you would have to first download the files to your computer, edit them, and then put them back to the repository. The pachctl mount command automates all these steps for you. You can mount just the images repo or all Pachyderm repositories as directories on you machine, edit as needed, and, when done, exit the pachctl mount command. Upon exiting the pachctl mount command, Pachyderm uploads all the changes to the corresponding repository.\nIf someone else modifies the files while you are working on them locally, their changes will likely be overwritten when you exit pachctl mount. This happens because Therefore, make sure that you do not work on the same files while someone else is working on them.\n‚ÑπÔ∏è Use writable mount ONLY when you have sole ownership over the mounted data. Otherwise, merge conflicts or unexpected data overwrites can occur.\nBecause output repositories are created by the Pachyderm pipelines, they are immutable. Only a pipeline can change and update files in these repositories. If you try to change a file in an output repo, you will get an error message.\nHow to Mount/Unmount a Pachyderm Repo # To mount a Pachyderm repo on a local computer, complete the following steps:\nIn a terminal, go to a directory in which you want to mount a Pachyderm repo. It can be any new empty directory on your local computer. For example, mydirectory.\nRun pachctl mount for a repository and branch that you want to mount:\npachctl mount \u0026lt;path-on-your-computer\u0026gt; [flags] Example:\nIf you want to mount all the repositories in your Pachyderm cluster to a mydirectory directory on your computer and give WRITE access to them, run: pachctl mount mydirectory --write If you want to mount the master branch of the images repo and enable file editing in this repository, run: pachctl mount mydirectory --repos images@master+w To give read-only access, omit +w.\nSystem Response:\nro for images: \u0026amp;{Branch:master Write:true} ri: repo:\u0026lt;name:\u0026#34;montage\u0026#34; \u0026gt; created:\u0026lt;seconds:1591812554 nanos:348079652 \u0026gt; size_bytes:1345398 description:\u0026#34;Output repo for pipeline montage.\u0026#34; branches:\u0026lt;repo:\u0026lt;name:\u0026#34;montage\u0026#34; \u0026gt; name:\u0026#34;master\u0026#34; \u0026gt; continue ri: repo:\u0026lt;name:\u0026#34;edges\u0026#34; \u0026gt; created:\u0026lt;seconds:1591812554 nanos:201592492 \u0026gt; size_bytes:136795 description:\u0026#34;Output repo for pipeline edges.\u0026#34; branches:\u0026lt;repo:\u0026lt;name:\u0026#34;edges\u0026#34; \u0026gt; name:\u0026#34;master\u0026#34; \u0026gt; continue ri: repo:\u0026lt;name:\u0026#34;images\u0026#34; \u0026gt; created:\u0026lt;seconds:1591812554 nanos:28450609 \u0026gt; size_bytes:244068 branches:\u0026lt;repo:\u0026lt;name:\u0026#34;images\u0026#34; \u0026gt; name:\u0026#34;master\u0026#34; \u0026gt; MkdirAll /var/folders/jl/mm3wrxqd75l9r1_d0zktphdw0000gn/T/pfs201409498/images The command runs in your terminal until you terminate it by pressing CTRL+C.\nTip Mount multiple repos at once by appending each mount instruction to the same command. For example, the following will mount both repos to the /mydirectory directory. pachctl mount ./mydirectory -r first_repo@master -r second_repo@master You can check that the repo was mounted by running the mount command in your terminal:\nmount /dev/disk1s1 on / (apfs, local, read-only, journaled) devfs on /dev (devfs, local, nobrowse) /dev/disk1s2 on /System/Volumes/Data (apfs, local, journaled, nobrowse) /dev/disk1s5 on /private/var/vm (apfs, local, journaled, nobrowse) map auto_home on /System/Volumes/Data/home (autofs, automounted, nobrowse) pachctl@osxfuse0 on /Users/testuser/mydirectory (osxfuse, nodev, nosuid, synchronous, mounted by testuser) Access your mountpoint.\nFor example, in macOS, open Finder, press CMD + SHIFT + G, and type the mountpoint location. If you have mounted the repo to ~/mydirectory, type ~/mydirectory.\nEdit the files as needed.\nWhen ready, add your changes to the Pachyderm repo by stopping the pachctl mount command with CTRL+C or by running pachctl unmount \u0026lt;mountpoint\u0026gt; (or unmount /\u0026lt;path-to-mount\u0026gt;, or fusermount -u /\u0026lt;path-to-mount\u0026gt;).\nIf you have mounted a writable Pachyderm share, interrupting the pachctl mount command results in the upload of your changes to the corresponding repo and branch, which is equivalent to running the pachctl put file command. You can check that Pachyderm runs a new job for this work by listing current jobs with pachctl list job.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pachctl", "data-operations", "export"],
    "id": "649cc08d7340c388dd2716581b307f4e"
  },
  {
    "title": "S3 Gateway Operations",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Export Data",
    "description": "Learn which S3 Gateway operations are supported.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/s3-gateway-operations/",
    "relURI": "/latest/export-data/s3-gateway-operations/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f8569d2e8f99454388fe0a07d90efff6"
  },
  {
    "title": "Create S3 Bucket",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway Operations",
    "description": "Learn how to create an S3 bucket through the S3 Gateway.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/s3-gateway-operations/create-bucket/",
    "relURI": "/latest/export-data/s3-gateway-operations/create-bucket/",
    "body": "Call the create an S3 bucket command on your S3 client to create a branch in a Pachyderm repository. For example, let\u0026rsquo;s create the master branch of the repo foo in project bar.\nTool: AWS S3 CLI MinIO Create a bucket named foo in project bar. aws --endpoint-url http://localhost:30600/ s3 mb s3://master.foo.bar # make_bucket: master.foo.bar verify that the S3 bucket has been created: aws --endpoint-url http://localhost:30600/ s3 ls # 2022-12-7 22:46:08 master.foo.bar Create a bucket named foo in project bar. mc mb local/master.foo.bar # Bucket created successfully `local/master.foo.bar`. Verify that the S3 bucket has been created: mc ls local # [2021-04-26 22:46:08] 0B master.foo.bar/ ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "4d49f67a97434d4f485bab209798b704"
  },
  {
    "title": "Delete an S3 Object",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway Operations",
    "description": "Learn how to delete an S3 object through the S3 Gateway.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/s3-gateway-operations/remove-object/",
    "relURI": "/latest/export-data/s3-gateway-operations/remove-object/",
    "body": "You can call the delete an S3 Object command on your S3 client to delete a file from a Pachyderm repository. For example, let\u0026rsquo;s delete the file test.csv from the master branch of the foo repo within the bar project.\nTool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 rm s3://master.foo.bar/test.csv # delete: s3://master.foo.bar/test.csv mc rm local/master.foo.bar/test.csv # Removing `local/master.foo.bar/test.csv`. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "94f06350f9004ce53b7b87cef64b2c78"
  },
  {
    "title": "Delete Empty S3 Bucket",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway Operations",
    "description": "Learn how to delete an empty S3 bucket through the S3 Gateway.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/s3-gateway-operations/delete-bucket/",
    "relURI": "/latest/export-data/s3-gateway-operations/delete-bucket/",
    "body": "You can call the delete an empty S3 bucket command on your S3 client to delete a Pachyderm repository. For example, let\u0026rsquo;s delete the the repo foo in project bar.\nTool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 rb s3://master.foo.bar # remove_bucket: master.foo.bar mc rb local/master.foo.bar # Removed `local/master.foo.bar` successfully. ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "c1b2268ded2c7d866dbfd229fcb44a8a"
  },
  {
    "title": "Get an S3 Object",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway Operations",
    "description": "Learn how to get an S3 object through the S3 Gateway.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/s3-gateway-operations/get-object/",
    "relURI": "/latest/export-data/s3-gateway-operations/get-object/",
    "body": "You can call the get an S3 object command on your S3 client to download a file by specifying the branch.repo.project it lives in. For example, let\u0026rsquo;s get the test.csv file from master.foo.bar.\nTool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 cp s3://master.foo.bar/test.csv . # download: s3://master.foo.bar/test.csv to ./test.csv mc cp local/master.foo.bar/test.csv . # test.csv: 2.56 MiB / 2.56 MiB ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì 100.00% 1.26 MiB/s 2s Versioning # Most operations act on the HEAD of the given branch. However, if your object store library or tool supports versioning, you can get objects in non-HEAD commits by using the commit ID as the S3 object version ID or use the following syntax --bucket \u0026lt;commit\u0026gt;.\u0026lt;branch\u0026gt;.\u0026lt;repo\u0026gt;.\u0026lt;project\u0026gt;\nTo retrieve the file file.txt in the commit a5984442ce6b4b998879513ff3da17da on the master branch of the repo foo in project bar:\nGet Object By: path id aws s3api get-object --bucket a5984442ce6b4b998879513ff3da17da.master.foo.bar --profile gcp-pf --endpoint http://localhost:30600 --key file.txt export.txt aws s3api get-object --bucket master.foo.bar --profile gcp-pf --endpoint http://localhost:30600 --key file.txt --version-id a5984442ce6b4b998879513ff3da17da export.txt { \u0026#34;AcceptRanges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2021-06-03T01:31:36+00:00\u0026#34;, \u0026#34;ContentLength\u0026#34;: 5, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;b5fdc0b3557bd4de47045f9c69fa8e54102bcecc36f8743ab88df90f727ff899\\\u0026#34;\u0026#34;, \u0026#34;VersionId\u0026#34;: \u0026#34;a5984442ce6b4b998879513ff3da17da\u0026#34;, \u0026#34;ContentType\u0026#34;: \u0026#34;text/plain; charset=utf-8\u0026#34;, \u0026#34;Metadata\u0026#34;: {} } ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "06a88204fbb04a90e0f938e4b4a57383"
  },
  {
    "title": "List S3 Buckets",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway Operations",
    "description": "Learn how to list S3 buckets through the S3 Gateway.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/s3-gateway-operations/list-buckets/",
    "relURI": "/latest/export-data/s3-gateway-operations/list-buckets/",
    "body": "You can check the list of filesystem objects in your Pachyderm repository by running an S3 client ls command.\nTool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600 s3 ls # 2021-04-26 15:09:50 master.train.myproject # 2021-04-26 14:58:50 master.pre_process.myproject # 2021-04-26 14:58:09 master.split.myproject # 2021-04-26 14:58:09 stats.split.myproject mc ls local # [2021-04-26 15:09:50 PDT] 0B master.train.myproject/ # [2021-04-26 14:58:50 PDT] 0B master.pre_process.myproject/ # [2021-04-26 14:58:09 PDT] 0B master.split.myproject/ # [2021-04-26 14:58:09 PDT] 0B stats.split.myproject/ ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "339947da42b489544bce46953a599042"
  },
  {
    "title": "List S3 Objects",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway Operations",
    "description": "Learn how to list S3 objects through the S3 Gateway.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/s3-gateway-operations/list-objects/",
    "relURI": "/latest/export-data/s3-gateway-operations/list-objects/",
    "body": "You can list the contents of a given Pachyderm repository using the following commands.\nTool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 ls s3://master.raw_data.myproject # 2021-04-26 11:22:23 2685061 github_issues_medium.csv mc ls local/master.raw_data.myproject # [2021-04-26 12:11:37 PDT] 2.6MiB github_issues_medium.csv ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8fa4f7acb1c002459f62256f3e1b60f3"
  },
  {
    "title": "Write an S3 Object",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "S3 Gateway Operations",
    "description": "Learn how to write an S3 object through the S3 Gateway.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/export-data/s3-gateway-operations/write-object/",
    "relURI": "/latest/export-data/s3-gateway-operations/write-object/",
    "body": "You can write an S3 object to a Pachyderm repo within a project by performing the following commands:\nTool: AWS S3 CLI MinIO Create the object: aws --endpoint-url http://localhost:30600/ s3 cp test.csv s3://master.foo.bar # upload: ./test.csv to s3://master.foo.bar/test.csv Check that the object was added: aws --endpoint-url http://localhost:30600/ s3 ls s3://master.foo.bar/ # 2021-04-26 12:11:37 2685061 github_issues_medium.csv # 2021-04-26 12:11:37 62 test.csv Create the object: mc cp test.csv local/master.foo.bar/test.csv # test.csv: 62 B / 62 B ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì 100.00% 206 B/s 0s Check that the object was added: mc ls local/master.foo.bar # [2021-04-26 12:11:37 PDT] 2.6MiB github_issues_medium.csv # [2021-04-26 12:11:37 PDT] 62B test.csv ‚ÑπÔ∏è Not all the repositories that you see in the results of the ls command are repositories that can be written to. Some of them might be read-only. Note that you should have writing access to the input repo in order to be able to add files to it.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "4c6e9befd7d9a91ab79caf46e51951c2"
  },
  {
    "title": "Integrate",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Integrate with popular tools such as JupyterLab.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/integrate/",
    "relURI": "/latest/integrate/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "0b0b6c2050a45a33ca03289ac500a050"
  },
  {
    "title": "Google BigQuery",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Integrate",
    "description": "Learn how to use the Google BigQuery connector to ingest data.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/integrate/bigquery/",
    "relURI": "/latest/integrate/bigquery/",
    "body": "This connector ingests the result of a BigQuery query into Pachyderm. With this connector, you can easily create pipelines that read data from BigQuery and process it using Pachyderm\u0026rsquo;s powerful data parallelism and versioning capabilities. It uses python and the python-bigquery-pandas library built by Google.\nBefore You Start # You must have a Google Cloud Platform account with a project that has BigQuery enabled.\nYou must download the following files:\ndockerfile\ngbq_ingest.jsonnet\ngbq_ingest.py\nrequirements.txt\n1. Create a Dataset \u0026amp; Service Account # Before using this connector, you will need to create a BigQuery dataset and a service account with the necessary permissions.\nGo to the BigQuery web UI and create a new dataset. In the GCP Console, go to the IAM \u0026amp; admin section and create a new service account. Grant the service account the BigQuery Data Viewer and BigQuery Data Editor roles for the dataset you created in step 1. Download the private key file for the service account, give it a descriptive name (e.g., gbq-pachyderm-creds.json), and save it to a secure location. Once you have completed these steps, you can use the connector by simply specifying your service account key file and the name of your BigQuery query in the jsonnet pipeline spec for your pipeline.\n2. Create a Pachyderm Secret # Create the secret (be sure to add the namespace if your cluster is deployed in one).\nNamespaced: Yes No kubectl create secret generic gbqsecret --from-file=gbq-pachyderm-creds.json -n mynamespace kubectl create secret generic gbqsecret --from-file=gbq-pachyderm-creds.json 3. Create the Jsonnet Pipeline # Run the pipeline template with jsonnet. Note, this pipeline will not run immediately, but rather will wait until the next cron tick. E.g. if you have it set to 5m, you will wait 5 minutes until the first run of the pipeline.\n$ pachctl update pipeline --jsonnet gbq_ingest.jsonnet \\ --arg inputQuery=\u0026#34;SELECT country_name, alpha_2_code FROM bigquery-public-data.utility_us.country_code_iso WHERE alpha_2_code LIKE \u0026#39;A%\u0026#39;\u0026#34; \\ --arg outFile=\u0026#34;gbq_output.parquet\u0026#34; \\ --arg project=\u0026#34;\u0026lt;gcp_project_name\u0026gt;\u0026#34; \\ --arg cronSpec=\u0026#34;@every 5m\u0026#34; --arg credsFile=\u0026#34;gbq-pachyderm-creds.json\u0026#34; Additional Details # Cron Spec # In some cases, you may not want your cron pipeline to run every 5 minutes as shown in the example above. For example, when testing a large ingestion, you may want to run the pipeline manually. To do this, you can use a cron specification that never triggers, such as --arg cronSpec=\u0026quot;* * 31 2 *\u0026quot;. This value will never trigger the cron pipeline (it refers to a nonexistent date - 31st of Feb).\nTo manually trigger the cron pipeline, run the following command:\npachctl run cron gbq_ingest For more information on using cron with Pachyderm pipelines, see the cron documentation.\nConfiguring your own pipeline spec # You can configure your own pipeline spec with the secret by using these parameters in the pipeline spec.\n\u0026#34;secrets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;gbqsecret\u0026#34;, \u0026#34;mount_path\u0026#34;: \u0026#34;/kubesecret/\u0026#34;}] and\n\u0026#34;env\u0026#34;: { \u0026#34;GOOGLE_APPLICATION_CREDENTIALS\u0026#34;: \u0026#34;/kubesecret/gbq-pachyderm-creds.json\u0026#34; }, ",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "bigquery", "google"],
    "id": "0dae4317d4cca12be72c6e9ac6a9c337"
  },
  {
    "title": "JupyterLab",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Integrate",
    "description": "Learn how to install and use the JupyterLab Mount Extension.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/integrate/jupyterlab-extension/",
    "relURI": "/latest/integrate/jupyterlab-extension/",
    "body": "Use the JupyterLab extension to:\nConnect your Notebook to a Pachyderm cluster Browse, explore, and analyze data stored in Pachyderm directly from your Notebook Run and test out your pipeline code before creating a Docker image Install the Extension # There are two main ways to install the Jupyter Lab extension:\n‚≠ê Via Docker: Fastest implementation! üß™ Locally: Great for development and testing Examples # Make sure to check our data science notebook examples running on Pachyderm, from a market sentiment NLP implementation using a FinBERT model to pipelines training a regression model on the Boston Housing Dataset. You will also find integration examples with open-source products, such as labeling or model serving applications.\n",
    "beta": "true",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "jupyterlab", "notebooks"],
    "id": "0250417fccfdb82118804c8f67def556"
  },
  {
    "title": "Docker Installation Guide",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "JupyterLab",
    "description": "Learn how to install and use the JupyterLab Mount Extension using a Docker image.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/integrate/jupyterlab-extension/docker-install/",
    "relURI": "/latest/integrate/jupyterlab-extension/docker-install/",
    "body": " Install to Existing Docker Image # You can choose between Pachyderm\u0026rsquo;s pre-built image (a custom version of jupyter/scipy-notebook) or add the extension to your own image. Pachyderm\u0026rsquo;s image includes:\nThe extension jupyterlab-pachyderm FUSE A pre-created /pfs directory that mounts to and grants ownership to the JupyterLab User A mount-server binary Option 1: Pre-Built Image # Open your terminal. Run the following: docker run -it -p 8888:8888 -e GRANT_SUDO=yes --user root --device /dev/fuse --privileged --entrypoint /opt/conda/bin/jupyter pachyderm/notebooks-user:v2.6.1 lab --allow-root Open the UI using the link provided in the terminal following: [I 2023-01-26 19:07:00.245 ServerApp] Jupyter Server 1.16.0 is running at: [I 2023-01-26 19:07:00.245 ServerApp] http://fb66b212ca13:8888/lab?token=013dbb47fc32c0f1ec8277a399e8ccf0e4eb87055942a21d [I 2023-01-26 19:07:00.245 ServerApp] or http://127.0.0.1:8888/lab?token=013dbb47fc32c0f1ec8277a399e8ccf0e4eb87055942a21d Navigate to the connection tab. You will need to provide a link formatted like the following: http://localhost:80 Navigate to the Launcher view in Jupyter and select Terminal. Input the following command: pachctl version If you see a pachctl and pachd version, you are good to go. Option 2: Custom Dockerfile # Replace the following ${PACHCTL_VERSION} with the version of pachctl that matches your cluster\u0026rsquo;s, and update \u0026lt;version\u0026gt; with the release number of the extension.\nYou can find the latest available version of our Pachyderm Mount Extension in PyPi.\n# This runs the following section as root; if adding to an existing Dockerfile, set the user back to whatever you need. USER root # This is the directory files will be mounted to, mirroring how pipelines are run. RUN mkdir -p /pfs # If you are not using \u0026#34;jovyan\u0026#34; as your notebook user, replace the user here. RUN chown $NB_USER /pfs # Fuse is a requirement for the mount extension RUN apt-get clean \u0026amp;\u0026amp; RUN apt-get update \u0026amp;\u0026amp; apt-get -y install curl fuse # Install the mount-server binary RUN curl -f -o mount-server.deb -L https://github.com/pachyderm/pachyderm/releases/download/v${PACHCTL_VERSION}/mount-server_${PACHCTL_VERSION}_amd64.deb RUN dpkg -i mount-server.deb # Optionally Install Pachctl - Set the version of Pachctl that matches your cluster deployment. RUN curl -f -o pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v${PACHCTL_VERSION}/pachctl_${PACHCTL_VERSION}_amd64.deb RUN dpkg -i pachctl.deb # This sets the user back to the notebook user account (i.e., Jovyan) USER $NB_UID # Replace the version here with the version of the extension you would like to install from https://pypi.org/project/jupyterlab-pachyderm/ RUN pip install jupyterlab-pachyderm==\u0026lt;version\u0026gt; Then, build, tag, and push your image.\n",
    "beta": "true",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "jupyterlab", "notebooks", "docker"],
    "id": "f778ab504b03160bd7790073e1a67919"
  },
  {
    "title": "Local Installation Guide",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "JupyterLab",
    "description": "Learn how to locally install and use the JupyterLab Mount Extension.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/integrate/jupyterlab-extension/local-install/",
    "relURI": "/latest/integrate/jupyterlab-extension/local-install/",
    "body": " Before You Start # You must have a Pachyderm cluster running. Install Jupyter Lab (pip install jupyterlab) Install FUSE ‚ö†Ô∏è Local installation of FUSE requires a reboot to access your Startup Security Utility and enable kernel extensions (kexts) after you have downloaded all of the necessary pre-requisites.\nInstall jupyterlab pachyderm (pip install jupyterlab-pachyderm) Download mount-server binary Local Installation Steps # Open your terminal. Navigate to your downloads folder. Copy the mount-server binary you downloaded from the pre-requisites into a folder included within your $PATH so that your jupyterlab-pachyderm extension can find it: sudo cp mount-server /usr/local/bin Open your zshrc profile: vim ~/.zshrc Create a /pfs directory to mount your data to. This is the default directory used; alternatively, you can define an empty output folder that PFS should mount by adding export PFS_MOUNT_DIR=/\u0026lt;directory\u0026gt;/\u0026lt;path\u0026gt; to your bash/zshrc profile. Update the source by restarting your computer or executing the following command: source ~/.zshrc Run jupyter lab. If you have an existing pachyderm config file at ~/.pachyderm/config.json, the extension automatically connects to the active context. Otherwise, you must enter the cluster address manually in the extension UI.\n",
    "beta": "true",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "jupyterlab", "notebooks"],
    "id": "71386baab744c0613e6bf0c3c35fbbc7"
  },
  {
    "title": "User Guide",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "JupyterLab",
    "description": "Learn how to use the JupyterLab Mount Extension.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/integrate/jupyterlab-extension/user-guide/",
    "relURI": "/latest/integrate/jupyterlab-extension/user-guide/",
    "body": " Select a Project # You can filter mountable repositories by selecting a project.\nOpen the JupyterLab UI. Navigate to the Pachyderm Mount tab (). Navigate to the Project dropdown. Select an existing project or the default project. Create a Repo \u0026amp; Repo Branch # Open the JupyterLab UI.\nOpen a Terminal from the launcher.\nInput the following:\npachctl create repo demo pachctl create branch demo@master Open the Pachyderm Mount tab ().\nCheck the Unmounted Repositories section.\nüí° Your repo is created within the project set to your current context.\nCreate a Pipeline # Open the JupyterLab UI. Create a notebook from the launcher (it can be left blank). Navigate to the Pachyderm Mount tab (). Select Pipeline in the side panel. Input values for all of the following: Name: The name of your pipeline. Image: The Docker Hub image that has your user code. Requirements: A requirements.txt file that contains the dependencies for your code. Input Spec: The input spec for your pipeline in YAML format. See the Pipeline Specification for input options. Select Save. Select Create Pipeline. Track the status of your pipeline using the command pachctl list pipelines in a terminal or view the pipeline in Console. üí° You can view the full compiled pipeline spec from the Pipline Spec Preview section.\nMount a Repo Branch # Open the JupyterLab UI. Navigate to the Pachyderm Mount tab (). Navigate to the Unmounted Repositories section. Scroll to a repository\u0026rsquo;s row. Select Mount. Mount (and Test) a Datum # You can mount to a specific datum in your repository from the JupyterLab UI using an input spec. This is useful when:\nWorking on data that is deeply nested within a specific directory of your repository. Testing and exploring viable glob patterns to use for your datums. Open the JupyterLab UI.\nNavigate to the Pachyderm Mount tab ().\nMount to a repo from the Unmounted Repositories section. (e.g., mounting to demo would look like /pfs/demo/ in the file browser).\nNavigate to the Mounted Repositories section and select Datum.\nYou should see the following:\npfs: repo: demo branch: master glob: / Update the glob pattern to match the datums you wish to focus on.\nDirectory Example # pfs: repo: demo branch: master glob: /images/2022/* Extension Example # pfs: repo: demo branch: master glob: /images/**.png Select Mount Datums.\nThe file browser updates to display the matching datums.\nWhen you return to the mounted view by selecting Back, the file browser will return to displaying datums that match your default glob pattern.\nExplore Directories \u0026amp; Files # At the bottom of the Mounted Repositories tab, you\u0026rsquo;ll find the file browser.\nMounted repositories are nested within the root /pfs (Pachyderm\u0026rsquo;s File System) These repositories are read-only Mounted repositories have a / glob pattern applied to their directories and files Files only downloaded locally when you access them (saving you time) Using the previous example, while the Demo repository is mounted, you can select the demo folder to reveal the example myfile.txt.\n",
    "beta": "true",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "jupyterlab", "notebooks"],
    "id": "40554dec346f95c575751d745dbf5479"
  },
  {
    "title": "Troubleshooting",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "JupyterLab",
    "description": "Learn how to troubleshoot the JupyterLab Mount Extension.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/integrate/jupyterlab-extension/troubleshooting/",
    "relURI": "/latest/integrate/jupyterlab-extension/troubleshooting/",
    "body": "In general, restarting your server should resolve most JupyterLab Mount Extension issues. To restart your server, run the following command from the terminal window in Jupyterlab:\npkill -f \u0026#34;mount-server\u0026#34; The server restarts by itself.\nKnown Issues # M1 Users With Docker Desktop \u0026lt; 4.6 # A documented issue between qemu and Docker Desktop prevents you from running our pre-built Mount Extension Image in Docker Desktop.\nWe recommend the following:\nUse Podman (See installation instructions) brew install podman podman machine init --disk-size 50 podman machine start podman machine ssh sudo rpm-ostree install qemu-user-static \u0026amp;\u0026amp; sudo systemctl reboot THEN then replace the keyword docker with podman in all the commands above.\nOr make sure that your qemu version is \u0026gt; 6.2 ",
    "beta": "true",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "jupyterlab", "notebooks"],
    "id": "175946bb2fa95181e05bac24ea8aaba8"
  },
  {
    "title": "Label Studio",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Integrate",
    "description": "Learn how to use the Label Studio connector to ingest data.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/integrate/label-studio/",
    "relURI": "/latest/integrate/label-studio/",
    "body": "Label Studio supports many different types of data labeling tasks, while Pachyderm allows you to incorporate data versioning and data-driven pipelines, enabling the management of the data loop. This integration connects a Pachyderm versioned data backend with Label Studio to support versioning datasets and tracking the data lineage of pipelines built off the versioned datasets.\nBefore You Start # You must have a Pachyderm cluster deployed. 1. Deploy Label Studio # The following one-liner will map your local configuration into the container to connect to Pachyderm. If you are performing another form of authentication, then you may need to use the entrypoint /bin/bash to configure the container before running /usr/local/bin/label-studio.\n$ docker run -it --rm -p8080:8080 -v ~/.pachyderm/config.json:/root/.pachyderm/config.json --device=/dev/fuse --cap-add SYS_ADMIN --name label-studio --entrypoint=/usr/local/bin/label-studio jimmywhitaker/label-studio:pach2.2-ls1.4v3 Once running, we can access label studio by visiting: http://localhost:8080/.\nOnce we create a user, new project , and select our labeling task (in our case we\u0026rsquo;ll use the \u0026ldquo;Object Detection with Bounding Boxes\u0026rdquo; template), we can configure the Cloud Storage settings to point to Pachyderm, using the Pachyderm Storage Type.\n2. Configure Source and Target Storage # Selecting Cloud Storage from the Label Studio settings will allow us to add Source and Target Storage sync our data with.\nFirst, let\u0026rsquo;s create two data repositories in Pachyderm for source and target storage. The source will be where we pull our unlabeled data from, and our target storage is where we\u0026rsquo;ll write our labels.\npachctl create repo images pachctl put file images@master:liberty.png -f http://imgur.com/46Q8nDz.png pachctl create repo labels Next, we can add Pachyderm as our source and target storages by configuring them in the Label Studio Settings as shown below:\nSource Storage # First, we will select Pachyderm as our storage type. The \u0026ldquo;Storage Title\u0026rdquo; will be the mounted directory name in the Label Studio container. In general, you shouldn\u0026rsquo;t have to worry about this very much, it\u0026rsquo;s mainly used as a way to keep track of things, should you have many source repos. The Repository Name will be the name and branch of our repo where we should pull our data from. Here, for example, images@master.\nWe can check the connection to make sure it is correct by pressing the \u0026ldquo;Check Connection\u0026rdquo; button and then once it is saved, we can sync our data from the source storage. When sync all of the data from that branch of the repo is downloaded into the Label Studio container.\nNote: This is a one time operation, so we must press this button whenever we want to sync new examples.\nTarget Storage # Configuring our target storage is roughly the same as our source storage. We configure the title and repo name.\nHowever, with Target Storage, the \u0026ldquo;sync storage\u0026rdquo; button has two roles:\nWhen it is pressed the first time, the repository is mounted, but no data is transferred. This is necessary to have a place to accumulate our annotations. (Think of it as a staging area for an upcoming commit.) The second time it is pressed, it commits all files that have been labeled to the Pachyderm repository (in our case labels@master). Note: In the image we only show labels but under the hood the code defaults to master if no other branch if provided. This is also the case with Source Storage.\nThis functionality is very beneficial because it means that we can have a single commit that contains all of our annotations instead of a commit per annotation, improving the speed of our data labeling.\nOnce configured, our storage should look like this:\nAnd when we move to our labeling environment, we see our example image present.\nCommit our Annotations to Pachyderm # After we have annotated our image data, we can commit all of our annotations to Pachyderm.\nTo do this we navigate back to Cloud Storage in our settings, and press the Sync Storage button on our Target Storage (labels@master). Under the hood, this will unmount the repo (committing the data) and then remount it again with the newest version of the branch. After the data is committed, it should look like the following:\nIn Pachyderm, we can verify that our data was committed by running:\n$ pachctl list file labels@master NAME TYPE SIZE /1.json file 1.228KiB Building the LS Docker image from scratch # If you want to build the Label Studio docker image from scratch with the Pachyderm cloud storage backend, you can run the following:\n$ git clone https://github.com/pachyderm/label-studio.git $ cd label-studio $ git checkout -b pachyderm $ docker build -t jimmywhitaker/label-studio:pach2.2-ls1.4v3 . ",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "label", "studio", "label-studio"],
    "id": "93eb4e700f5551f99a6535f81463576b"
  },
  {
    "title": "Superb AI",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Integrate",
    "description": "Learn how to use the Superb AI connector to ingest data.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/integrate/superb-ai/",
    "relURI": "/latest/integrate/superb-ai/",
    "body": "Connect your Superb.ai project to Pachyderm to automatically version and save data you\u0026rsquo;ve labeled in Superb AI to use in downstream machine learning workflows.\nThis integration ingests the data into Pachyderm on a cron schedule. Once your data is ingested into Pachyderm, you can perform data tests, train a model, or any other type of data automation you may want to do, all while having full end-to-end reproducibility.\nBefore You Start # You must have a Superb.AI account You must have a Pachyderm cluster Download the example code and unzip it. (or download this repo. gh repo clone pachyderm/docs-content and navigate to docs-content/docs/latest/integrate/superb-ai) How to Use the Superb AI Connector # Generate an Access API Key in SuperbAI. Put the key and your user name in the secrets.json file. Create the Pachyderm secret pachctl create secret -f secrets.json Create the cron pipeline to synchronize your Sample project from SuperbAI to Pachyderm. This pipeline will run every minute to check for new data (you can configure it to run more or less often in the cron spec in sample_project.yml). pachctl create pipeline -f sample_project.yml Pachyderm will automatically kick off the pipeline and import the data from your sample project. ",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "superb-ai"],
    "id": "a4e5ef62b3018994d8f7466efff0c060"
  },
  {
    "title": "Weights and Biases",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Integrate",
    "description": "Learn how to use the Weights and Biases connector to track your data science experiments.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/integrate/weights-and-biases/",
    "relURI": "/latest/integrate/weights-and-biases/",
    "body": "Connect Pachyderm to Weights and Biases to track your data science experiments. Using Pachyderm as our execution platform, we can version our executions, code, data, and models while still tracking everything in W\u0026amp;B.\nHere we\u0026rsquo;ll use Pachyderm to manage our data and train our model.\nBefore You Start # You must have a W\u0026amp;B account You must have a Pachyderm cluster Download the example code and unzip it. (or download this repo. gh repo clone pachyderm/docs-content and navigate to docs-content/docs/latest/integrate/weights-and-biases) How to Use the Weights and Biases Connector # Create a Pachyderm cluster. Create a W\u0026amp;B Account Copy your W\u0026amp;B API Key into the secrets.json file. We\u0026rsquo;ll use this file to make a Pachyderm secret. This keeps our access keys from being built into our container or put in plaintext somewhere. Create the secret with pachctl create secret -f secrets.json Run make all to create a data repository and the pipeline. üí° Downloading the data locally and then pushing it to a remote cluster seems like an extra step, especially when dealing with a standard dataset like MNIST. However, if we think about a real-world use case where multiple teams may be manipulating the data (removing examples, adding classes, etc.) then having a history for each of these models can be very useful. In most production settings with supervised learning, the labeling environment can be directly connected to the data repository, automating this step.\nAbout the MNIST example # Creates a project in W\u0026amp;B with the name of the Pachyderm pipeline. Trains an MNIST classifier in a Pachyderm Job. Logs training info from training to W\u0026amp;B. If the Data or Pachyderm Pipeline changes, it kicks off a new training process. ",
    "beta": "false",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["integrations", "weights-and-biases"],
    "id": "bab594f314b83b6d13f3f0a93a4eb6dc"
  },
  {
    "title": "Run Commands",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Access the platform's API using PachCTL commands.",
    "date": "October 11, 2022",
    "uri": "https://docs.pachyderm.com/latest/run-commands/",
    "relURI": "/latest/run-commands/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e383479f197ed1e2d71b5cac64287956"
  },
  {
    "title": "Pachctl",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl/",
    "relURI": "/latest/run-commands/pachctl/",
    "body": " pachctl # Synopsis # Access the Pachyderm API.\nEnvironment variables: PACH_CONFIG=, the path where pachctl will attempt to load your config. JAEGER_ENDPOINT=:, the Jaeger server to connect to, if PACH_TRACE is set PACH_TRACE={true,false}, if true, and JAEGER_ENDPOINT is set, attach a Jaeger trace to any outgoing RPCs. PACH_TRACE_DURATION=, the amount of time for which PPS should trace a pipeline after \u0026lsquo;pachctl create-pipeline\u0026rsquo; (PACH_TRACE must also be set).\nOptions # -h, --help help for pachctl --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a35c55612cf1dd4bb5f6c5913ef8add3"
  },
  {
    "title": "Pachctl auth",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth/",
    "relURI": "/latest/run-commands/pachctl_auth/",
    "body": " pachctl auth # Auth commands manage access to data in a Pachyderm cluster\nSynopsis # Auth commands manage access to data in a Pachyderm cluster\nOptions # -h, --help help for auth Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "20656c923cc3d4400c4fa26107dc0789"
  },
  {
    "title": "Pachctl auth activate",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_activate/",
    "relURI": "/latest/run-commands/pachctl_auth_activate/",
    "body": " pachctl auth activate # Activate Pachyderm\u0026rsquo;s auth system\nSynopsis # Activate Pachyderm\u0026rsquo;s auth system, and restrict access to existing data to the root user\npachctl auth activate [flags] Options # --client-id string The client ID for this pachd (default \u0026#34;pachd\u0026#34;) --enterprise Activate auth on the active enterprise context -h, --help help for activate --issuer string The issuer for the OIDC service (default \u0026#34;http://pachd:1658/\u0026#34;) --only-activate Activate auth without configuring the OIDC service --redirect string The redirect URL for the OIDC service (default \u0026#34;http://localhost:30657/authorization-code/callback\u0026#34;) --scopes strings Comma-separated list of scopes to request (default [email,profile,groups,openid]) --supply-root-token Prompt the user to input a root token on stdin, rather than generating a random one. --trusted-peers strings Comma-separated list of OIDC client IDs to trust Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "d9d5e79d50b637ca5e9bdd902bcdb92d"
  },
  {
    "title": "Pachctl auth check",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_check/",
    "relURI": "/latest/run-commands/pachctl_auth_check/",
    "body": " pachctl auth check # Check whether a subject has a permission on a resource\nSynopsis # Check whether a subject has a permission on a resource\nOptions # -h, --help help for check Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "af5fdfa18b852e93b7555d437da1a0b5"
  },
  {
    "title": "Pachctl auth check project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_check_project/",
    "relURI": "/latest/run-commands/pachctl_auth_check_project/",
    "body": " pachctl auth check project # Check the permissions a user has on \u0026lsquo;project\u0026rsquo;\nSynopsis # Check the permissions a user has on \u0026lsquo;project\u0026rsquo;\npachctl auth check project \u0026lt;project\u0026gt; [user] [flags] Options # -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "321d2c698c9f3c083d88457a20940c1b"
  },
  {
    "title": "Pachctl auth check repo",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_check_repo/",
    "relURI": "/latest/run-commands/pachctl_auth_check_repo/",
    "body": " pachctl auth check repo # Check the permissions a user has on \u0026lsquo;repo\u0026rsquo;\nSynopsis # Check the permissions a user has on \u0026lsquo;repo\u0026rsquo;\npachctl auth check repo \u0026lt;repo\u0026gt; [\u0026lt;user\u0026gt;] [flags] Options # -h, --help help for repo --project string The project containing the repo. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "ae8dee25e6754673da816e29b8d7a965"
  },
  {
    "title": "Pachctl auth deactivate",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_deactivate/",
    "relURI": "/latest/run-commands/pachctl_auth_deactivate/",
    "body": " pachctl auth deactivate # Delete all ACLs, tokens, admins, IDP integrations and OIDC clients, and deactivate Pachyderm auth\nSynopsis # Deactivate Pachyderm\u0026rsquo;s auth and identity systems, which will delete ALL auth tokens, ACLs and admins, IDP integrations and OIDC clients, and expose all data in the cluster to any user with cluster access. Use with caution.\npachctl auth deactivate [flags] Options # --enterprise Deactivate auth on the active enterprise context -h, --help help for deactivate Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "88248a9ced779a1e979c2fc6eb510720"
  },
  {
    "title": "Pachctl auth get config",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_get-config/",
    "relURI": "/latest/run-commands/pachctl_auth_get-config/",
    "body": " pachctl auth get-config # Retrieve Pachyderm\u0026rsquo;s current auth configuration\nSynopsis # Retrieve Pachyderm\u0026rsquo;s current auth configuration\npachctl auth get-config [flags] Options # --enterprise Get auth config for the active enterprise context -h, --help help for get-config -o, --output-format string output format (\u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34;) (default \u0026#34;json\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "0354358c7651ee57b6ba5a3cc96162fa"
  },
  {
    "title": "Pachctl auth get groups",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_get-groups/",
    "relURI": "/latest/run-commands/pachctl_auth_get-groups/",
    "body": " pachctl auth get-groups # Get the list of groups a user belongs to\nSynopsis # Get the list of groups a user belongs to. If no user is specified, the current user\u0026rsquo;s groups are listed.\npachctl auth get-groups [username] [flags] Options # --enterprise Get group membership info from the enterprise server -h, --help help for get-groups Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "afce283847d1bc4a12c5f2b0104ae5e2"
  },
  {
    "title": "Pachctl auth get robot token",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_get-robot-token/",
    "relURI": "/latest/run-commands/pachctl_auth_get-robot-token/",
    "body": " pachctl auth get-robot-token # Get an auth token for a robot user with the specified name.\nSynopsis # Get an auth token for a robot user with the specified name.\npachctl auth get-robot-token [username] [flags] Options # --enterprise Get a robot token for the enterprise context -h, --help help for get-robot-token -q, --quiet if set, only print the resulting token (if successful). This is useful for scripting, as the output can be piped to use-auth-token --ttl string if set, the resulting auth token will have the given lifetime. If not set, the token does not expire. This flag should be a golang duration (e.g. \u0026#34;30s\u0026#34; or \u0026#34;1h2m3s\u0026#34;). Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "eb729b44280403b0fbb383c556ea279a"
  },
  {
    "title": "Pachctl auth get",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_get/",
    "relURI": "/latest/run-commands/pachctl_auth_get/",
    "body": " pachctl auth get # Get the role bindings for a resource\nSynopsis # Get the role bindings for a resource\nOptions # -h, --help help for get Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "080f23131579fd798afaa9aa568ec2b0"
  },
  {
    "title": "Pachctl auth get cluster",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_get_cluster/",
    "relURI": "/latest/run-commands/pachctl_auth_get_cluster/",
    "body": " pachctl auth get cluster # Get the role bindings for \u0026lsquo;cluster\u0026rsquo;\nSynopsis # Get the role bindings for \u0026lsquo;cluster\u0026rsquo;\npachctl auth get cluster [flags] Options # -h, --help help for cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "7105091a31d331b5784ba51d24a28b8a"
  },
  {
    "title": "Pachctl auth get enterprise",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_get_enterprise/",
    "relURI": "/latest/run-commands/pachctl_auth_get_enterprise/",
    "body": " pachctl auth get enterprise # Get the role bindings for the enterprise server\nSynopsis # Get the role bindings for the enterprise server\npachctl auth get enterprise [flags] Options # -h, --help help for enterprise Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "d7329caa65bbbefbbd41cf8e0f30987b"
  },
  {
    "title": "Pachctl auth get project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_get_project/",
    "relURI": "/latest/run-commands/pachctl_auth_get_project/",
    "body": " pachctl auth get project # Get the role bindings for \u0026lsquo;project\u0026rsquo;\nSynopsis # Get the role bindings for \u0026lsquo;project\u0026rsquo;\npachctl auth get project \u0026lt;project\u0026gt; [flags] Options # -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "76c1f847742954501af63b59d33a3543"
  },
  {
    "title": "Pachctl auth get repo",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_get_repo/",
    "relURI": "/latest/run-commands/pachctl_auth_get_repo/",
    "body": " pachctl auth get repo # Get the role bindings for \u0026lsquo;repo\u0026rsquo;\nSynopsis # Get the role bindings for \u0026lsquo;repo\u0026rsquo;\npachctl auth get repo \u0026lt;repo\u0026gt; [flags] Options # -h, --help help for repo --project string The project containing the repo. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "3a857dc5ed72dc6e55d1a7636c5178c1"
  },
  {
    "title": "Pachctl auth login",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_login/",
    "relURI": "/latest/run-commands/pachctl_auth_login/",
    "body": " pachctl auth login # Log in to Pachyderm\nSynopsis # Login to Pachyderm. Any resources that have been restricted to the account you have with your ID provider (e.g. GitHub, Okta) account will subsequently be accessible.\npachctl auth login [flags] Options # --enterprise Login for the active enterprise context -h, --help help for login -t, --id-token If set, read an ID token on stdin to authenticate the user -b, --no-browser If set, don\u0026#39;t try to open a web browser Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "1fdd0f8e1876c30521fe6b4ec8e0df1e"
  },
  {
    "title": "Pachctl auth logout",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_logout/",
    "relURI": "/latest/run-commands/pachctl_auth_logout/",
    "body": " pachctl auth logout # Log out of Pachyderm by deleting your local credential\nSynopsis # Log out of Pachyderm by deleting your local credential. Note that it\u0026rsquo;s not necessary to log out before logging in with another account (simply run \u0026lsquo;pachctl auth login\u0026rsquo; twice) but \u0026rsquo;logout\u0026rsquo; can be useful on shared workstations.\npachctl auth logout [flags] Options # --enterprise Log out of the active enterprise context -h, --help help for logout Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b951852bc13ab63d28c6027757c30595"
  },
  {
    "title": "Pachctl auth revoke",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_revoke/",
    "relURI": "/latest/run-commands/pachctl_auth_revoke/",
    "body": " pachctl auth revoke # Revoke a Pachyderm auth token\nSynopsis # Revoke a Pachyderm auth token.\npachctl auth revoke [flags] Options # --enterprise Revoke an auth token (or all auth tokens minted for one user) on the enterprise server -h, --help help for revoke --token string Pachyderm auth token that should be revoked (one of --token or --user must be set) --user string User whose Pachyderm auth tokens should be revoked (one of --token or --user must be set) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "3723ed6c6a226b43feaf42491dcdc14b"
  },
  {
    "title": "Pachctl auth roles for permission",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_roles-for-permission/",
    "relURI": "/latest/run-commands/pachctl_auth_roles-for-permission/",
    "body": " pachctl auth roles-for-permission # List roles that grant the given permission\nSynopsis # List roles that grant the given permission\npachctl auth roles-for-permission \u0026lt;permission\u0026gt; [flags] Options # -h, --help help for roles-for-permission Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e305513151ff87df40bbedb600386244"
  },
  {
    "title": "Pachctl auth rotate root token",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_rotate-root-token/",
    "relURI": "/latest/run-commands/pachctl_auth_rotate-root-token/",
    "body": " pachctl auth rotate-root-token # Rotate the root user\u0026rsquo;s auth token\nSynopsis # Rotate the root user\u0026rsquo;s auth token\npachctl auth rotate-root-token [flags] Options # -h, --help help for rotate-root-token --supply-token string An auth token to rotate to. If left blank, one will be auto-generated. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "75244e5c64911eecf632c298a8c9b01f"
  },
  {
    "title": "Pachctl auth set config",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_set-config/",
    "relURI": "/latest/run-commands/pachctl_auth_set-config/",
    "body": " pachctl auth set-config # Set Pachyderm\u0026rsquo;s current auth configuration\nSynopsis # Set Pachyderm\u0026rsquo;s current auth configuration\npachctl auth set-config [flags] Options # --enterprise Set auth config for the active enterprise context -f, --file string input file (to use as the new config (default \u0026#34;-\u0026#34;) -h, --help help for set-config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "28eaf825cbc7e79be5d495433f3408d1"
  },
  {
    "title": "Pachctl auth set",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_set/",
    "relURI": "/latest/run-commands/pachctl_auth_set/",
    "body": " pachctl auth set # Set the role bindings for a resource\nSynopsis # Set the role bindings for a resource\nOptions # -h, --help help for set Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "785a75019462396ed955ba0ae18f2c5c"
  },
  {
    "title": "Pachctl auth set cluster",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_set_cluster/",
    "relURI": "/latest/run-commands/pachctl_auth_set_cluster/",
    "body": " pachctl auth set cluster # Set the roles that \u0026lsquo;subject\u0026rsquo; has on the \u0026lsquo;cluster\u0026rsquo;\nSynopsis # Set the roles that \u0026lsquo;subject\u0026rsquo; has on the \u0026lsquo;cluster\u0026rsquo;\npachctl auth set cluster [role1,role2 | none ] subject [flags] Options # -h, --help help for cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "bc4a79327dd26c31ebc3bc6669a5bbff"
  },
  {
    "title": "Pachctl auth set enterprise",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_set_enterprise/",
    "relURI": "/latest/run-commands/pachctl_auth_set_enterprise/",
    "body": " pachctl auth set enterprise # Set the roles that \u0026lsquo;subject\u0026rsquo; has on the enterprise server\nSynopsis # Set the roles that \u0026lsquo;subject\u0026rsquo; has on the enterprise server\npachctl auth set enterprise [role1,role2 | none ] subject [flags] Options # -h, --help help for enterprise Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "72101dbc9c3c1f3509e6b291792c0c32"
  },
  {
    "title": "Pachctl auth set project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_set_project/",
    "relURI": "/latest/run-commands/pachctl_auth_set_project/",
    "body": " pachctl auth set project # Set the roles that \u0026lsquo;subject\u0026rsquo; has on \u0026lsquo;project\u0026rsquo;\nSynopsis # Set the roles that \u0026lsquo;subject\u0026rsquo; has on \u0026lsquo;project\u0026rsquo;\npachctl auth set project \u0026lt;project\u0026gt; [role1,role2 | none ] \u0026lt;subject\u0026gt; [flags] Options # -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a27e46e3649724d8f8dfaec73780e17a"
  },
  {
    "title": "Pachctl auth set repo",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_set_repo/",
    "relURI": "/latest/run-commands/pachctl_auth_set_repo/",
    "body": " pachctl auth set repo # Set the roles that \u0026lsquo;subject\u0026rsquo; has on \u0026lsquo;repo\u0026rsquo;\nSynopsis # Set the roles that \u0026lsquo;subject\u0026rsquo; has on \u0026lsquo;repo\u0026rsquo;\npachctl auth set repo \u0026lt;repo\u0026gt; [role1,role2 | none ] \u0026lt;subject\u0026gt; [flags] Options # -h, --help help for repo --project string The project containing the repo. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b25891223e062a3f625a0af4538d6a34"
  },
  {
    "title": "Pachctl auth use auth token",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_use-auth-token/",
    "relURI": "/latest/run-commands/pachctl_auth_use-auth-token/",
    "body": " pachctl auth use-auth-token # Read a Pachyderm auth token from stdin, and write it to the current user\u0026rsquo;s Pachyderm config file\nSynopsis # Read a Pachyderm auth token from stdin, and write it to the current user\u0026rsquo;s Pachyderm config file\npachctl auth use-auth-token [flags] Options # --enterprise Use the token for the enterprise context -h, --help help for use-auth-token Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "585deb05fe353a9e81e4e3478c9bb04e"
  },
  {
    "title": "Pachctl auth whoami",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_auth_whoami/",
    "relURI": "/latest/run-commands/pachctl_auth_whoami/",
    "body": " pachctl auth whoami # Print your Pachyderm identity\nSynopsis # Print your Pachyderm identity.\npachctl auth whoami [flags] Options # --enterprise -h, --help help for whoami Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "d17473cd5d7c398fcc8ed6d3ac40204d"
  },
  {
    "title": "Pachctl buildinfo",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_buildinfo/",
    "relURI": "/latest/run-commands/pachctl_buildinfo/",
    "body": " pachctl buildinfo # Print go buildinfo.\nSynopsis # Print information about the build environment.\npachctl buildinfo [flags] Options # -h, --help help for buildinfo Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9ae77939735ff54f3cb905c45fb19cd5"
  },
  {
    "title": "Pachctl completion",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_completion/",
    "relURI": "/latest/run-commands/pachctl_completion/",
    "body": " pachctl completion # Print or install terminal completion code.\nSynopsis # Print or install terminal completion code.\nOptions # -h, --help help for completion Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "291f8e2f7eb99727123ecda1970ac69f"
  },
  {
    "title": "Pachctl completion bash",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_completion_bash/",
    "relURI": "/latest/run-commands/pachctl_completion_bash/",
    "body": " pachctl completion bash # Print or install the bash completion code.\nSynopsis # Print or install the bash completion code.\npachctl completion bash [flags] Options # -h, --help help for bash --install Install the completion. --path string Path to install the completions to. (default \u0026#34;/etc/bash_completion.d/pachctl\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "3794679213534a8ca7b93d3da1d1ad16"
  },
  {
    "title": "Pachctl completion zsh",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_completion_zsh/",
    "relURI": "/latest/run-commands/pachctl_completion_zsh/",
    "body": " pachctl completion zsh # Print or install the zsh completion code.\nSynopsis # Print or install the zsh completion code.\npachctl completion zsh [flags] Options # -h, --help help for zsh --install Install the completion. --path string Path to install the completions to. (default \u0026#34;_pachctl\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "4bf82f314746fd249e8955af9ebc563c"
  },
  {
    "title": "Pachctl config",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config/",
    "relURI": "/latest/run-commands/pachctl_config/",
    "body": " pachctl config # Manages the pachyderm config.\nSynopsis # Gets/sets pachyderm config values.\nOptions # -h, --help help for config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "98524a4c2ff8fcc61b3ea8441e7e670c"
  },
  {
    "title": "Pachctl config delete",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_delete/",
    "relURI": "/latest/run-commands/pachctl_config_delete/",
    "body": " pachctl config delete # Commands for deleting pachyderm config values\nSynopsis # Commands for deleting pachyderm config values\nOptions # -h, --help help for delete Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "11a8d2ee7efa2587c8253bf2981bacd6"
  },
  {
    "title": "Pachctl config delete context",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_delete_context/",
    "relURI": "/latest/run-commands/pachctl_config_delete_context/",
    "body": " pachctl config delete context # Deletes a context.\nSynopsis # Deletes a context.\npachctl config delete context \u0026lt;context\u0026gt; [flags] Options # -h, --help help for context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "94c43de335286ea01a86348cb3584b99"
  },
  {
    "title": "Pachctl config get",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_get/",
    "relURI": "/latest/run-commands/pachctl_config_get/",
    "body": " pachctl config get # Commands for getting pachyderm config values\nSynopsis # Commands for getting pachyderm config values\nOptions # -h, --help help for get Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "04e2dca941f2eaaba4a53f923cd425d1"
  },
  {
    "title": "Pachctl config get active context",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_get_active-context/",
    "relURI": "/latest/run-commands/pachctl_config_get_active-context/",
    "body": " pachctl config get active-context # Gets the currently active context.\nSynopsis # Gets the currently active context.\npachctl config get active-context [flags] Options # -h, --help help for active-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a175e3dcfca32db14c8a7a87dfcbbf30"
  },
  {
    "title": "Pachctl config get active enterprise context",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_get_active-enterprise-context/",
    "relURI": "/latest/run-commands/pachctl_config_get_active-enterprise-context/",
    "body": " pachctl config get active-enterprise-context # Gets the currently active enterprise context.\nSynopsis # Gets the currently active enterprise context.\npachctl config get active-enterprise-context [flags] Options # -h, --help help for active-enterprise-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "0e028447871ea7144e35c3a457a26ffb"
  },
  {
    "title": "Pachctl config get context",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_get_context/",
    "relURI": "/latest/run-commands/pachctl_config_get_context/",
    "body": " pachctl config get context # Gets a context.\nSynopsis # Gets the config of a context by its name.\npachctl config get context \u0026lt;context\u0026gt; [flags] Options # -h, --help help for context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8abccd4c735bba22cc99071e2ea1e4de"
  },
  {
    "title": "Pachctl config get metrics",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_get_metrics/",
    "relURI": "/latest/run-commands/pachctl_config_get_metrics/",
    "body": " pachctl config get metrics # Gets whether metrics are enabled.\nSynopsis # Gets whether metrics are enabled.\npachctl config get metrics [flags] Options # -h, --help help for metrics Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "77ebe5eef256105d0a064e491849d608"
  },
  {
    "title": "Pachctl config import kube",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_import-kube/",
    "relURI": "/latest/run-commands/pachctl_config_import-kube/",
    "body": " pachctl config import-kube # Import a kubernetes context as a Pachyderm context, and set the active Pachyderm context.\nSynopsis # Import a kubernetes context as a Pachyderm context. By default the current kubernetes context is used.\npachctl config import-kube \u0026lt;context\u0026gt; [flags] Options # -e, --enterprise Configure an enterprise server context. -h, --help help for import-kube -k, --kubernetes string Specify the kubernetes context\u0026#39;s values to import. -n, --namespace string Specify a namespace where Pachyderm is deployed. -o, --overwrite Overwrite a context if it already exists. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "bd2db0939507ce584efb3be063eb3734"
  },
  {
    "title": "Pachctl config list",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_list/",
    "relURI": "/latest/run-commands/pachctl_config_list/",
    "body": " pachctl config list # Commands for listing pachyderm config values\nSynopsis # Commands for listing pachyderm config values\nOptions # -h, --help help for list Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "bc4ea8d4ed026e7e26d1cd8ca3d14f42"
  },
  {
    "title": "Pachctl config list context",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_list_context/",
    "relURI": "/latest/run-commands/pachctl_config_list_context/",
    "body": " pachctl config list context # Lists contexts.\nSynopsis # Lists contexts.\npachctl config list context [flags] Options # -h, --help help for context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "bfa12734a2479bc50a46956d57d7644f"
  },
  {
    "title": "Pachctl config set",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_set/",
    "relURI": "/latest/run-commands/pachctl_config_set/",
    "body": " pachctl config set # Commands for setting pachyderm config values\nSynopsis # Commands for setting pachyderm config values\nOptions # -h, --help help for set Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "02f43ca01c02920d1e5c0b6207c5511f"
  },
  {
    "title": "Pachctl config set active context",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_set_active-context/",
    "relURI": "/latest/run-commands/pachctl_config_set_active-context/",
    "body": " pachctl config set active-context # Sets the currently active context.\nSynopsis # Sets the currently active context.\npachctl config set active-context \u0026lt;context\u0026gt; [flags] Options # -h, --help help for active-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "fc9482234f0c179f67ce3dfb143d6c34"
  },
  {
    "title": "Pachctl config set active enterprise context",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_set_active-enterprise-context/",
    "relURI": "/latest/run-commands/pachctl_config_set_active-enterprise-context/",
    "body": " pachctl config set active-enterprise-context # Sets the currently active enterprise context.\nSynopsis # Sets the currently active enterprise context.\npachctl config set active-enterprise-context \u0026lt;context\u0026gt; [flags] Options # -h, --help help for active-enterprise-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "00b4b34230c499cec51130287780d5ed"
  },
  {
    "title": "Pachctl config set context",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_set_context/",
    "relURI": "/latest/run-commands/pachctl_config_set_context/",
    "body": " pachctl config set context # Set a context.\nSynopsis # Set a context config from a given name and a JSON configuration file on stdin\npachctl config set context \u0026lt;context\u0026gt; [flags] Options # -h, --help help for context --overwrite Overwrite a context if it already exists. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "0b2a726e9114956006467b6b39e771e6"
  },
  {
    "title": "Pachctl config set metrics",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_set_metrics/",
    "relURI": "/latest/run-commands/pachctl_config_set_metrics/",
    "body": " pachctl config set metrics # Sets whether metrics are enabled.\nSynopsis # Sets whether metrics are enabled.\npachctl config set metrics (true | false) [flags] Options # -h, --help help for metrics Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "4f9d7b5dab75f02a238a862abd869d17"
  },
  {
    "title": "Pachctl config update",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_update/",
    "relURI": "/latest/run-commands/pachctl_config_update/",
    "body": " pachctl config update # Commands for updating pachyderm config values\nSynopsis # Commands for updating pachyderm config values\nOptions # -h, --help help for update Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "963bd889fd2d7507c643fee39cf069c5"
  },
  {
    "title": "Pachctl config update context",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_config_update_context/",
    "relURI": "/latest/run-commands/pachctl_config_update_context/",
    "body": " pachctl config update context # Updates a context.\nSynopsis # Updates an existing context config from a given name (or the currently-active context, if no name is given).\npachctl config update context [\u0026lt;context\u0026gt;] [flags] Options # --auth-info string Set a new k8s auth info. --cluster-name string Set a new cluster name. -h, --help help for context --namespace string Set a new namespace. --pachd-address string Set a new name pachd address. --project string Set a new project. --remove-cluster-deployment-id Remove the cluster deployment ID field, which will be repopulated on the next \u0026#39;pachctl\u0026#39; call using this context. --server-cas string Set new trusted CA certs. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8a483f4c6d37a67f3c8adc549cf09f16"
  },
  {
    "title": "Pachctl connect",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_connect/",
    "relURI": "/latest/run-commands/pachctl_connect/",
    "body": " pachctl connect # Connect to a Pachyderm Cluster\nSynopsis # Creates a Pachyderm context at the given address and sets it as active\npachctl connect \u0026lt;address\u0026gt; [flags] Options # -h, --help help for connect Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "108ca440575388b0a0982431e2b508c9"
  },
  {
    "title": "Pachctl copy",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_copy/",
    "relURI": "/latest/run-commands/pachctl_copy/",
    "body": " pachctl copy # Copy a Pachyderm resource.\nSynopsis # Copy a Pachyderm resource.\nOptions # -h, --help help for copy Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "3d009a60d24645a0246d17a72b397a13"
  },
  {
    "title": "Pachctl copy file",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_copy_file/",
    "relURI": "/latest/run-commands/pachctl_copy_file/",
    "body": " pachctl copy file # Copy files between pfs paths.\nSynopsis # Copy files between pfs paths.\npachctl copy file \u0026lt;src-repo\u0026gt;@\u0026lt;src-branch-or-commit\u0026gt;:\u0026lt;src-path\u0026gt; \u0026lt;dst-repo\u0026gt;@\u0026lt;dst-branch-or-commit\u0026gt;:\u0026lt;dst-path\u0026gt; [flags] Examples # # copy between repos within the current project defined by the pachyderm context # defaults to the \u0026#34;default\u0026#34; project $ pachctl copy file srcRepo@master:/file destRepo@master:/file # copy within a specified project $ pachctl copy file srcRepo@master:/file destRepo@master:/file --project myProject # copy from the current project to a different project # here, srcRepo is in the current project, while destRepo is in myProject $ pachctl copy file srcRepo@master:/file destRepo@master:/file --dest-project myProject # copy from a different project to the current project # here, srcRepo is in myProject, while destRepo is in the current project $ pachctl copy file srcRepo@master:/file destRepo@master:/file --src-project myProject # copy between repos across two different projects # here, srcRepo is in project1, while destRepo is in project2 $ pachctl copy file srcRepo@master:/file destRepo@master:/file --src-project project1 --dest-project project2 Options # -a, --append Append to the existing content of the file, either from previous commits or previous calls to \u0026#39;put file\u0026#39; within this commit. --dest-project string Project in which the destination repo is located. This overrides --project. -h, --help help for file --project string Project in which both source and destination repos are located. (default \u0026#34;openCV\u0026#34;) --src-project string Project in which the source repo is located. This overrides --project. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "db3bfe9187b147b4ca4b35c9eb4ff31b"
  },
  {
    "title": "Pachctl create",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_create/",
    "relURI": "/latest/run-commands/pachctl_create/",
    "body": " pachctl create # Create a new instance of a Pachyderm resource.\nSynopsis # Create a new instance of a Pachyderm resource.\nOptions # -h, --help help for create Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "71ad0aa6d0d35e4c1ce00805ccafaffa"
  },
  {
    "title": "Pachctl create branch",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_create_branch/",
    "relURI": "/latest/run-commands/pachctl_create_branch/",
    "body": " pachctl create branch # Create a new branch, or update an existing branch, on a repo.\nSynopsis # Create a new branch, or update an existing branch, on a repo, starting a commit on the branch will also create it, so there\u0026rsquo;s often no need to call this.\npachctl create branch \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; [flags] Options # --head string The head of the newly created branch. Either pass the commit with format: \u0026lt;branch-or-commit\u0026gt;, or fully-qualified as \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;=\u0026lt;id\u0026gt; -h, --help help for branch --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) -p, --provenance []string The provenance for the branch. format: \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; (default []) -t, --trigger string The branch to trigger this branch on. --trigger-all Only trigger when all conditions are met, rather than when any are met. --trigger-commits int The number of commits to use in triggering. --trigger-cron string The cron spec to use in triggering. --trigger-size string The data size to use in triggering. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "399bcfc58a7080667b87d35af6ae54de"
  },
  {
    "title": "Pachctl create pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_create_pipeline/",
    "relURI": "/latest/run-commands/pachctl_create_pipeline/",
    "body": " pachctl create pipeline # Create a new pipeline.\nSynopsis # Create a new pipeline from a pipeline specification. For details on the format, see https://docs.pachyderm.com/latest/reference/pipeline_spec/.\npachctl create pipeline [flags] Options # --arg stringArray Top-level argument passed to the Jsonnet template in --jsonnet (which must be set if any --arg arguments are passed). Value must be of the form \u0026#39;param=value\u0026#39;. For multiple args, --arg may be set more than once. -f, --file string A JSON file (url or filepath) containing one or more pipelines. \u0026#34;-\u0026#34; reads from stdin (the default behavior). Exactly one of --file and --jsonnet must be set. -h, --help help for pipeline --jsonnet string BETA: A Jsonnet template file (url or filepath) for one or more pipelines. \u0026#34;-\u0026#34; reads from stdin. Exactly one of --file and --jsonnet must be set. Jsonnet templates must contain a top-level function; strings can be passed to this function with --arg (below) --project string The project in which to create the pipeline. (default \u0026#34;openCV\u0026#34;) -p, --push-images If true, push local docker images into the docker registry. -r, --registry string The registry to push images to. (default \u0026#34;index.docker.io\u0026#34;) -u, --username string The username to push images as. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "ee97a2d6de258c63ed08c1d1c6744869"
  },
  {
    "title": "Pachctl create project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_create_project/",
    "relURI": "/latest/run-commands/pachctl_create_project/",
    "body": " pachctl create project # Create a new project.\nSynopsis # Create a new project.\npachctl create project \u0026lt;project\u0026gt; [flags] Options # -d, --description string The description of the newly-created project. -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b254c3d7baa0c3113d3c05877f3a6301"
  },
  {
    "title": "Pachctl create repo",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_create_repo/",
    "relURI": "/latest/run-commands/pachctl_create_repo/",
    "body": " pachctl create repo # Create a new repo.\nSynopsis # Create a new repo.\npachctl create repo \u0026lt;repo\u0026gt; [flags] Options # -d, --description string A description of the repo. -h, --help help for repo --project string The project to create the repo in. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a04416c194cb2ca2e4d7789c608dcd41"
  },
  {
    "title": "Pachctl create secret",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_create_secret/",
    "relURI": "/latest/run-commands/pachctl_create_secret/",
    "body": " pachctl create secret # Create a secret on the cluster.\nSynopsis # Create a secret on the cluster.\npachctl create secret [flags] Options # -f, --file string File containing Kubernetes secret. -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "1267e452680bb02adf60b35416582920"
  },
  {
    "title": "Pachctl debug",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_debug/",
    "relURI": "/latest/run-commands/pachctl_debug/",
    "body": " pachctl debug # Debug commands for analyzing a running cluster.\nSynopsis # Debug commands for analyzing a running cluster.\nOptions # -h, --help help for debug Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "16d50ca5fbcb59c8f4e1de264c05e37d"
  },
  {
    "title": "Pachctl debug analyze",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_debug_analyze/",
    "relURI": "/latest/run-commands/pachctl_debug_analyze/",
    "body": " pachctl debug analyze # Start a local pachd server to analyze a debug dump.\nSynopsis # Start a local pachd server to analyze a debug dump.\npachctl debug analyze \u0026lt;file\u0026gt; [flags] Options # -h, --help help for analyze -p, --port int launch a debug server on the given port. If unset, choose a free port automatically Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "21492ea67cce2fb36c4b050b0124c9ac"
  },
  {
    "title": "Pachctl debug binary",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_debug_binary/",
    "relURI": "/latest/run-commands/pachctl_debug_binary/",
    "body": " pachctl debug binary # Collect a set of binaries.\nSynopsis # Collect a set of binaries.\npachctl debug binary \u0026lt;file\u0026gt; [flags] Options # -h, --help help for binary --pachd Only collect the binary from pachd. -p, --pipeline string Only collect the binary from the worker pods for the given pipeline. -w, --worker string Only collect the binary from the given worker pod. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "002b74a7278ad4ae17d0898f4abc1471"
  },
  {
    "title": "Pachctl debug dump",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_debug_dump/",
    "relURI": "/latest/run-commands/pachctl_debug_dump/",
    "body": " pachctl debug dump # Collect a standard set of debugging information.\nSynopsis # Collect a standard set of debugging information.\npachctl debug dump \u0026lt;file\u0026gt; [flags] Options # --database Only collect the dump from pachd\u0026#39;s database. -h, --help help for dump -l, --limit int Limit sets the limit for the number of commits / jobs that are returned for each repo / pipeline in the dump. --pachd Only collect the dump from pachd. -p, --pipeline string Only collect the dump from the worker pods for the given pipeline. --timeout duration Set an absolute timeout on the debug dump operation. (default 30m0s) -w, --worker string Only collect the dump from the given worker pod. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "819a878f2f1cabbf30aa28c309a94a6a"
  },
  {
    "title": "Pachctl debug log level",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_debug_log-level/",
    "relURI": "/latest/run-commands/pachctl_debug_log-level/",
    "body": " pachctl debug log-level # Change the log level across Pachyderm.\nSynopsis # Change the log level across Pachyderm.\npachctl debug log-level \u0026lt;level\u0026gt; [flags] Options # -d, --duration duration how long to log at the non-default level (default 5m0s) -g, --grpc adjust the grpc log level instead of the pachyderm log level -h, --help help for log-level -r, --recursive set the log level on all pachyderm pods; if false, only the pachd that handles this RPC (default true) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "922818cd5c4382f91152753f3528fc3b"
  },
  {
    "title": "Pachctl debug profile",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_debug_profile/",
    "relURI": "/latest/run-commands/pachctl_debug_profile/",
    "body": " pachctl debug profile # Collect a set of pprof profiles.\nSynopsis # Collect a set of pprof profiles.\npachctl debug profile \u0026lt;profile\u0026gt; \u0026lt;file\u0026gt; [flags] Options # -d, --duration duration Duration to run a CPU profile for. (default 1m0s) -h, --help help for profile --pachd Only collect the profile from pachd. -p, --pipeline string Only collect the profile from the worker pods for the given pipeline. -w, --worker string Only collect the profile from the given worker pod. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "4d89f69b071b0aafb518cc5b15f12b56"
  },
  {
    "title": "Pachctl delete",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete/",
    "relURI": "/latest/run-commands/pachctl_delete/",
    "body": " pachctl delete # Delete an existing Pachyderm resource.\nSynopsis # Delete an existing Pachyderm resource.\nOptions # -h, --help help for delete Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8a6b862de4aa6c36ef9ec03c31e6a4fd"
  },
  {
    "title": "Pachctl delete all",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete_all/",
    "relURI": "/latest/run-commands/pachctl_delete_all/",
    "body": " pachctl delete all # Delete everything.\nSynopsis # Delete all repos, commits, files, pipelines and jobs. This resets the cluster to its initial state.\npachctl delete all [flags] Options # -h, --help help for all Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "5b1bbcb6e051b842a49b736d5012d6fa"
  },
  {
    "title": "Pachctl delete branch",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete_branch/",
    "relURI": "/latest/run-commands/pachctl_delete_branch/",
    "body": " pachctl delete branch # Delete a branch\nSynopsis # Delete a branch, while leaving the commits intact\npachctl delete branch \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; [flags] Options # -f, --force remove the branch regardless of errors; use with care -h, --help help for branch --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "1d7d728356b1b7efe72f421199fe677a"
  },
  {
    "title": "Pachctl delete commit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete_commit/",
    "relURI": "/latest/run-commands/pachctl_delete_commit/",
    "body": " pachctl delete commit # Delete the sub-commits of a commit.\nSynopsis # Delete the sub-commits of a commit. The data in the sub-commits will be lost. This operation is only supported if none of the sub-commits have children.\npachctl delete commit \u0026lt;commit-id\u0026gt; [flags] Options # -h, --help help for commit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8432498e6ffdf3c58fe776f8a1b4502f"
  },
  {
    "title": "Pachctl delete file",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete_file/",
    "relURI": "/latest/run-commands/pachctl_delete_file/",
    "body": " pachctl delete file # Delete a file.\nSynopsis # Delete a file.\npachctl delete file \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;:\u0026lt;path/in/pfs\u0026gt; [flags] Options # -h, --help help for file --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) -r, --recursive Recursively delete the files in a directory. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9e2a35ceb145e45d2a98a3f3e930e761"
  },
  {
    "title": "Pachctl delete job",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete_job/",
    "relURI": "/latest/run-commands/pachctl_delete_job/",
    "body": " pachctl delete job # Delete a job.\nSynopsis # Delete a job.\npachctl delete job \u0026lt;pipeline\u0026gt;@\u0026lt;job\u0026gt; [flags] Options # -h, --help help for job --project string Project within which to delete job (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "3103fc600abfe64add8f0641c990f133"
  },
  {
    "title": "Pachctl delete pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete_pipeline/",
    "relURI": "/latest/run-commands/pachctl_delete_pipeline/",
    "body": " pachctl delete pipeline # Delete a pipeline.\nSynopsis # Delete a pipeline.\npachctl delete pipeline (\u0026lt;pipeline\u0026gt;|--all) [flags] Options # --all delete all pipelines -A, --all-projects delete pipelines from all projects; only valid with --all -f, --force delete the pipeline regardless of errors; use with care -h, --help help for pipeline --keep-repo delete the pipeline, but keep the output repo data around (the pipeline cannot be recreated later with the same name unless the repo is deleted) --project string project containing project (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9f6aa66591b593171757b8a68e3b6341"
  },
  {
    "title": "Pachctl delete project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete_project/",
    "relURI": "/latest/run-commands/pachctl_delete_project/",
    "body": " pachctl delete project # Delete a project.\nSynopsis # Delete a project.\npachctl delete project \u0026lt;project\u0026gt; [flags] Options # -f, --force remove the project regardless of errors; use with care -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f9fed7f1622a3e3ddb2a0b8ea4ac329d"
  },
  {
    "title": "Pachctl delete repo",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete_repo/",
    "relURI": "/latest/run-commands/pachctl_delete_repo/",
    "body": " pachctl delete repo # Delete a repo.\nSynopsis # Delete a repo.\npachctl delete repo \u0026lt;repo\u0026gt; [flags] Options # --all remove all repos -A, --all-projects delete repos from all projects; only valid with --all -f, --force remove the repo regardless of errors; use with care -h, --help help for repo --project string project in which repo is located (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9d15878793f9cd908cdf3046f600b825"
  },
  {
    "title": "Pachctl delete secret",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete_secret/",
    "relURI": "/latest/run-commands/pachctl_delete_secret/",
    "body": " pachctl delete secret # Delete a secret from the cluster.\nSynopsis # Delete a secret from the cluster.\npachctl delete secret [flags] Options # -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8ba609f2bad33b3ab7c46f3d8be3503d"
  },
  {
    "title": "Pachctl delete transaction",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_delete_transaction/",
    "relURI": "/latest/run-commands/pachctl_delete_transaction/",
    "body": " pachctl delete transaction # Cancel and delete an existing transaction.\nSynopsis # Cancel and delete an existing transaction.\npachctl delete transaction [\u0026lt;transaction\u0026gt;] [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "c86abac74ef82f706e58f6e21bb60718"
  },
  {
    "title": "Pachctl diff",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_diff/",
    "relURI": "/latest/run-commands/pachctl_diff/",
    "body": " pachctl diff # Show the differences between two Pachyderm resources.\nSynopsis # Show the differences between two Pachyderm resources.\nOptions # -h, --help help for diff Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b7ad910d8f7929317e5202ea4ee6ae29"
  },
  {
    "title": "Pachctl diff file",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_diff_file/",
    "relURI": "/latest/run-commands/pachctl_diff_file/",
    "body": " pachctl diff file # Return a diff of two file trees stored in Pachyderm\nSynopsis # Return a diff of two file trees stored in Pachyderm\npachctl diff file \u0026lt;new-repo\u0026gt;@\u0026lt;new-branch-or-commit\u0026gt;:\u0026lt;new-path\u0026gt; [\u0026lt;old-repo\u0026gt;@\u0026lt;old-branch-or-commit\u0026gt;:\u0026lt;old-path\u0026gt;] [flags] Examples # # Return the diff of the file \u0026#34;path\u0026#34; of the repo \u0026#34;foo\u0026#34; between the head of the # \u0026#34;master\u0026#34; branch and its parent. $ pachctl diff file foo@master:path # Return the diff between the master branches of repos foo and bar at paths # path1 and path2, respectively. $ pachctl diff file foo@master:path1 bar@master:path2 Options # --diff-command string Use a program other than git to diff files. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for file --name-only Show only the names of changed files. --no-pager Don\u0026#39;t pipe output into a pager (i.e. less). --old-project string Project in which second, older repo is located. --project string Project in which first repo is located. (default \u0026#34;openCV\u0026#34;) -s, --shallow Don\u0026#39;t descend into sub directories. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "4e40dc9894ab43f37fe17ea8d70d281d"
  },
  {
    "title": "Pachctl draw",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_draw/",
    "relURI": "/latest/run-commands/pachctl_draw/",
    "body": " pachctl draw # Draw an ASCII representation of an existing Pachyderm resource.\nSynopsis # Draw an ASCII representation of an existing Pachyderm resource.\nOptions # -h, --help help for draw Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "5833afbe077b48ba0391539bc6eb2d17"
  },
  {
    "title": "Pachctl draw pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_draw_pipeline/",
    "relURI": "/latest/run-commands/pachctl_draw_pipeline/",
    "body": " pachctl draw pipeline # Draw a DAG\nSynopsis # Draw a DAG\npachctl draw pipeline [flags] Options # --box-width int Character width of each box in the DAG (default 11) -c, --commit string Commit at which you would to draw the DAG --edge-height int Number of vertical lines spanned by each edge (default 5) -h, --help help for pipeline --project string Project containing pipelines. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f3864cfcfb07ca81ede1e880001062a3"
  },
  {
    "title": "Pachctl edit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_edit/",
    "relURI": "/latest/run-commands/pachctl_edit/",
    "body": " pachctl edit # Edit the value of an existing Pachyderm resource.\nSynopsis # Edit the value of an existing Pachyderm resource.\nOptions # -h, --help help for edit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b8f14bdc1c1a3f0d861d9c7fc9a095cb"
  },
  {
    "title": "Pachctl edit pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_edit_pipeline/",
    "relURI": "/latest/run-commands/pachctl_edit_pipeline/",
    "body": " pachctl edit pipeline # Edit the manifest for a pipeline in your text editor.\nSynopsis # Edit the manifest for a pipeline in your text editor.\npachctl edit pipeline \u0026lt;pipeline\u0026gt; [flags] Options # --editor string Editor to use for modifying the manifest. -h, --help help for pipeline -o, --output string Output format: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project of pipeline to edit. (default \u0026#34;openCV\u0026#34;) --reprocess If true, reprocess datums that were already processed by previous version of the pipeline. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a1c8abca5f3923a8a5f95135cdc010b7"
  },
  {
    "title": "Pachctl enterprise",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_enterprise/",
    "relURI": "/latest/run-commands/pachctl_enterprise/",
    "body": " pachctl enterprise # Enterprise commands enable Pachyderm Enterprise features\nSynopsis # Enterprise commands enable Pachyderm Enterprise features\nOptions # -h, --help help for enterprise Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "6c966bbd50c72f0920b658e61ac1e9b4"
  },
  {
    "title": "Pachctl enterprise deactivate",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_enterprise_deactivate/",
    "relURI": "/latest/run-commands/pachctl_enterprise_deactivate/",
    "body": " pachctl enterprise deactivate # Deactivate the enterprise service\nSynopsis # Deactivate the enterprise service\npachctl enterprise deactivate [flags] Options # -h, --help help for deactivate Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f166994a2faf91caf2db519650dee2c4"
  },
  {
    "title": "Pachctl enterprise get state",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_enterprise_get-state/",
    "relURI": "/latest/run-commands/pachctl_enterprise_get-state/",
    "body": " pachctl enterprise get-state # Check whether the Pachyderm cluster has enterprise features activated\nSynopsis # Check whether the Pachyderm cluster has enterprise features activated\npachctl enterprise get-state [flags] Options # --enterprise Activate auth on the active enterprise context -h, --help help for get-state Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "0b3fa75b46b8a71903527a83886c77b0"
  },
  {
    "title": "Pachctl enterprise heartbeat",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_enterprise_heartbeat/",
    "relURI": "/latest/run-commands/pachctl_enterprise_heartbeat/",
    "body": " pachctl enterprise heartbeat # Sync the enterprise state with the license server immediately.\nSynopsis # Sync the enterprise state with the license server immediately.\npachctl enterprise heartbeat [flags] Options # --enterprise Make the enterprise server refresh its state -h, --help help for heartbeat Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f32ac02976c3f54549b2a37927d7f3fe"
  },
  {
    "title": "Pachctl enterprise pause status",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_enterprise_pause-status/",
    "relURI": "/latest/run-commands/pachctl_enterprise_pause-status/",
    "body": " pachctl enterprise pause-status # Get the pause status of the cluster.\nSynopsis # Get the pause the cluster: normal, partially-paused or paused.\npachctl enterprise pause-status [flags] Options # -h, --help help for pause-status Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "573e601ea40737d6503179ca234adb93"
  },
  {
    "title": "Pachctl enterprise pause",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_enterprise_pause/",
    "relURI": "/latest/run-commands/pachctl_enterprise_pause/",
    "body": " pachctl enterprise pause # Pause the cluster.\nSynopsis # Pause the cluster.\npachctl enterprise pause [flags] Options # -h, --help help for pause Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "955756eded8ab88eee1fdc5f87f61ae4"
  },
  {
    "title": "Pachctl enterprise register",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_enterprise_register/",
    "relURI": "/latest/run-commands/pachctl_enterprise_register/",
    "body": " pachctl enterprise register # Register the cluster with an enterprise license server\nSynopsis # Register the cluster with an enterprise license server\npachctl enterprise register [flags] Options # --cluster-deployment-id string the deployment id of the cluster being registered --enterprise-server-address string the address for the pachd to reach the enterprise server -h, --help help for register --id string the id for this cluster --pachd-address string the address for the enterprise server to reach this pachd --pachd-user-address string the address for a user to reach this pachd Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "334fbd84e579a5fcfa75ed09737d48a9"
  },
  {
    "title": "Pachctl enterprise sync contexts",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_enterprise_sync-contexts/",
    "relURI": "/latest/run-commands/pachctl_enterprise_sync-contexts/",
    "body": " pachctl enterprise sync-contexts # Pull all available Pachyderm Cluster contexts into your pachctl config\nSynopsis # Pull all available Pachyderm Cluster contexts into your pachctl config\npachctl enterprise sync-contexts [flags] Options # -h, --help help for sync-contexts Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8555adf4df3828e87767affb7c455fda"
  },
  {
    "title": "Pachctl enterprise unpause",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_enterprise_unpause/",
    "relURI": "/latest/run-commands/pachctl_enterprise_unpause/",
    "body": " pachctl enterprise unpause # Unpause the cluster.\nSynopsis # Unpause the cluster.\npachctl enterprise unpause [flags] Options # -h, --help help for unpause Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "399cf70ff9d2212c70cf4d4afd0eab50"
  },
  {
    "title": "Pachctl exit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_exit/",
    "relURI": "/latest/run-commands/pachctl_exit/",
    "body": " pachctl exit # Exit the pachctl shell.\nSynopsis # Exit the pachctl shell.\npachctl exit [flags] Options # -h, --help help for exit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "37d7a7895b4dc222fe6af0abeb771cb7"
  },
  {
    "title": "Pachctl find",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_find/",
    "relURI": "/latest/run-commands/pachctl_find/",
    "body": " pachctl find # Find a file addition, modification, or deletion in a commit.\nSynopsis # fInd a file addition, modification, or deletion in a commit.\nOptions # -h, --help help for find Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "225a821419929698954a38ff49f756ed"
  },
  {
    "title": "Pachctl find commit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_find_commit/",
    "relURI": "/latest/run-commands/pachctl_find_commit/",
    "body": " pachctl find commit # find commits with reference to within a branch starting from repo@commitID\nSynopsis # find commits with reference to within a branch starting from repo@commitID\npachctl find commit \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;:\u0026lt;path/in/pfs\u0026gt; [flags] Options # -h, --help help for commit --json print the response in json --limit uint32 Number of matching commits to return --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) --timeout duration Search duration timeout Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9b8140513c62a6e380896a74dfa40413"
  },
  {
    "title": "Pachctl finish",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_finish/",
    "relURI": "/latest/run-commands/pachctl_finish/",
    "body": " pachctl finish # Finish a Pachyderm resource.\nSynopsis # Finish a Pachyderm resource.\nOptions # -h, --help help for finish Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "536afd3d59e7667fce182b851be34ad6"
  },
  {
    "title": "Pachctl finish commit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_finish_commit/",
    "relURI": "/latest/run-commands/pachctl_finish_commit/",
    "body": " pachctl finish commit # Finish a started commit.\nSynopsis # Finish a started commit. Commit-id must be a writeable commit.\npachctl finish commit \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt; [flags] Options # --description string A description of this commit\u0026#39;s contents (synonym for --message) -f, --force finish the commit even if it has provenance, which could break jobs; prefer \u0026#39;stop job\u0026#39; -h, --help help for commit -m, --message string A description of this commit\u0026#39;s contents (overwrites any existing commit description) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "6b42eb169dd78bfd8d89233c3195c211"
  },
  {
    "title": "Pachctl finish transaction",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_finish_transaction/",
    "relURI": "/latest/run-commands/pachctl_finish_transaction/",
    "body": " pachctl finish transaction # Execute and clear the currently active transaction.\nSynopsis # Execute and clear the currently active transaction.\npachctl finish transaction [\u0026lt;transaction\u0026gt;] [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "73f8f18d13e28a3624ad8a80475caddd"
  },
  {
    "title": "Pachctl fsck",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_fsck/",
    "relURI": "/latest/run-commands/pachctl_fsck/",
    "body": " pachctl fsck # Run a file system consistency check on pfs.\nSynopsis # Run a file system consistency check on the pachyderm file system, ensuring the correct provenance relationships are satisfied.\npachctl fsck [flags] Options # -f, --fix Attempt to fix as many issues as possible. -h, --help help for fsck --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) --zombie string A single commit to check for zombie files --zombie-all Check all pipelines for zombie files: files corresponding to old inputs that were not properly deleted Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "150fe558cb02a337e717952a8631054b"
  },
  {
    "title": "Pachctl get",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_get/",
    "relURI": "/latest/run-commands/pachctl_get/",
    "body": " pachctl get # Get the raw data represented by a Pachyderm resource.\nSynopsis # Get the raw data represented by a Pachyderm resource.\nOptions # -h, --help help for get Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "abd77bda77bfe116f757a0625777ecec"
  },
  {
    "title": "Pachctl get file",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_get_file/",
    "relURI": "/latest/run-commands/pachctl_get_file/",
    "body": " pachctl get file # Return the contents of a file.\nSynopsis # Return the contents of a file.\npachctl get file \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;:\u0026lt;path/in/pfs\u0026gt; [flags] Examples # # get a single file \u0026#34;XXX\u0026#34; on branch \u0026#34;master\u0026#34; in repo \u0026#34;foo\u0026#34; $ pachctl get file foo@master:XXX # get file \u0026#34;XXX\u0026#34; in the parent of the current head of branch \u0026#34;master\u0026#34; # in repo \u0026#34;foo\u0026#34; $ pachctl get file foo@master^:XXX # get file \u0026#34;XXX\u0026#34; in the grandparent of the current head of branch \u0026#34;master\u0026#34; # in repo \u0026#34;foo\u0026#34; $ pachctl get file foo@master^2:XXX # get file \u0026#34;test[].txt\u0026#34; on branch \u0026#34;master\u0026#34; in repo \u0026#34;foo\u0026#34; # the path is interpreted as a glob pattern: quote and protect regex characters $ pachctl get file \u0026#39;foo@master:/test\\[\\].txt\u0026#39; # get all files under the directory \u0026#34;XXX\u0026#34; on branch \u0026#34;master\u0026#34; in repo \u0026#34;foo\u0026#34; $ pachctl get file foo@master:XXX -r Options # -h, --help help for file --offset int The number of bytes in the file to skip ahead when reading. -o, --output string The path where data will be downloaded. --progress {true|false} Whether or not to print the progress bars. (default true) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) -r, --recursive Download multiple files, or recursively download a directory. --retry {true|false} Whether to append the missing bytes to an existing file. No-op if the file doesn\u0026#39;t exist. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "af98be44cdbdfadae2f8f1ba297a87d7"
  },
  {
    "title": "Pachctl glob",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_glob/",
    "relURI": "/latest/run-commands/pachctl_glob/",
    "body": " pachctl glob # Print a list of Pachyderm resources matching a glob pattern.\nSynopsis # Print a list of Pachyderm resources matching a glob pattern.\nOptions # -h, --help help for glob Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "dccc9f8d37956e7500a21b9bfdb2bb51"
  },
  {
    "title": "Pachctl glob file",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_glob_file/",
    "relURI": "/latest/run-commands/pachctl_glob_file/",
    "body": " pachctl glob file # Return files that match a glob pattern in a commit.\nSynopsis # Return files that match a glob pattern in a commit (that is, match a glob pattern in a repo at the state represented by a commit). Glob patterns are documented here.\npachctl glob file \u0026#34;\u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;:\u0026lt;pattern\u0026gt;\u0026#34; [flags] Examples # # Return files in repo \u0026#34;foo\u0026#34; on branch \u0026#34;master\u0026#34; that start # with the character \u0026#34;A\u0026#34;. Note how the double quotation marks around the # parameter are necessary because otherwise your shell might interpret the \u0026#34;*\u0026#34;. $ pachctl glob file \u0026#34;foo@master:A*\u0026#34; # Return files in repo \u0026#34;foo\u0026#34; on branch \u0026#34;master\u0026#34; under directory \u0026#34;data\u0026#34;. $ pachctl glob file \u0026#34;foo@master:data/*\u0026#34; # If you only want to view all files on a given repo branch, use \u0026#34;list file -f \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;\u0026#34; instead. Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for file -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8dbcd36201472cbd5f3dae2acfe0c8b1"
  },
  {
    "title": "Pachctl idp",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp/",
    "relURI": "/latest/run-commands/pachctl_idp/",
    "body": " pachctl idp # Commands to manage identity provider integrations\nSynopsis # Commands to manage identity provider integrations\nOptions # -h, --help help for idp Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "27764c5ec7435a10b577d936593e0e89"
  },
  {
    "title": "Pachctl idp create client",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_create-client/",
    "relURI": "/latest/run-commands/pachctl_idp_create-client/",
    "body": " pachctl idp create-client # Create a new OIDC client.\nSynopsis # Create a new OIDC client.\npachctl idp create-client [flags] Options # --config string The file to read the YAML-encoded client configuration from, or \u0026#39;-\u0026#39; for stdin. (default \u0026#34;-\u0026#34;) -h, --help help for create-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "d5f3f1c96f9a5ffd29c9b4a484805654"
  },
  {
    "title": "Pachctl idp create connector",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_create-connector/",
    "relURI": "/latest/run-commands/pachctl_idp_create-connector/",
    "body": " pachctl idp create-connector # Create a new identity provider connector.\nSynopsis # Create a new identity provider connector.\npachctl idp create-connector [flags] Options # --config string The file to read the YAML-encoded connector configuration from, or \u0026#39;-\u0026#39; for stdin. (default \u0026#34;-\u0026#34;) -h, --help help for create-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "2db2efd36787c3a154a229bf7b7eb5d9"
  },
  {
    "title": "Pachctl idp delete client",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_delete-client/",
    "relURI": "/latest/run-commands/pachctl_idp_delete-client/",
    "body": " pachctl idp delete-client # Delete an OIDC client.\nSynopsis # Delete an OIDC client.\npachctl idp delete-client \u0026lt;client ID\u0026gt; [flags] Options # -h, --help help for delete-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9520fce746d34d4aa84f1646bc0f63ab"
  },
  {
    "title": "Pachctl idp delete connector",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_delete-connector/",
    "relURI": "/latest/run-commands/pachctl_idp_delete-connector/",
    "body": " pachctl idp delete-connector # Delete an identity provider connector\nSynopsis # Delete an identity provider connector\npachctl idp delete-connector [flags] Options # -h, --help help for delete-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a0cefc0425aa3243c58ac47cf9f351e7"
  },
  {
    "title": "Pachctl idp get client",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_get-client/",
    "relURI": "/latest/run-commands/pachctl_idp_get-client/",
    "body": " pachctl idp get-client # Get an OIDC client.\nSynopsis # Get an OIDC client.\npachctl idp get-client \u0026lt;client ID\u0026gt; [flags] Options # -h, --help help for get-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "c879f22a3fc3ca5ede422e28c825becb"
  },
  {
    "title": "Pachctl idp get config",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_get-config/",
    "relURI": "/latest/run-commands/pachctl_idp_get-config/",
    "body": " pachctl idp get-config # Get the identity server config\nSynopsis # Get the identity server config\npachctl idp get-config [flags] Options # -h, --help help for get-config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "1b2521dbf33d545df8985e57b4cd28be"
  },
  {
    "title": "Pachctl idp get connector",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_get-connector/",
    "relURI": "/latest/run-commands/pachctl_idp_get-connector/",
    "body": " pachctl idp get-connector # Get the config for an identity provider connector.\nSynopsis # Get the config for an identity provider connector.\npachctl idp get-connector \u0026lt;connector id\u0026gt; [flags] Options # -h, --help help for get-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "dd1fb7e61b3ea5670c26d62a96689950"
  },
  {
    "title": "Pachctl idp list client",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_list-client/",
    "relURI": "/latest/run-commands/pachctl_idp_list-client/",
    "body": " pachctl idp list-client # List OIDC clients.\nSynopsis # List OIDC clients.\npachctl idp list-client [flags] Options # -h, --help help for list-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "cef806d0db12f8186d73a0f13c88c74f"
  },
  {
    "title": "Pachctl idp list connector",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_list-connector/",
    "relURI": "/latest/run-commands/pachctl_idp_list-connector/",
    "body": " pachctl idp list-connector # List identity provider connectors\nSynopsis # List identity provider connectors\npachctl idp list-connector [flags] Options # -h, --help help for list-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e07b2754ec8c71d589b0a08051508158"
  },
  {
    "title": "Pachctl idp set config",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_set-config/",
    "relURI": "/latest/run-commands/pachctl_idp_set-config/",
    "body": " pachctl idp set-config # Set the identity server config\nSynopsis # Set the identity server config\npachctl idp set-config [flags] Options # --config string The file to read the YAML-encoded configuration from, or \u0026#39;-\u0026#39; for stdin. (default \u0026#34;-\u0026#34;) -h, --help help for set-config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "37967dbd4d91c7897332d7680b2b0dac"
  },
  {
    "title": "Pachctl idp update client",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_update-client/",
    "relURI": "/latest/run-commands/pachctl_idp_update-client/",
    "body": " pachctl idp update-client # Update an OIDC client.\nSynopsis # Update an OIDC client.\npachctl idp update-client [flags] Options # --config string The file to read the YAML-encoded client configuration from, or \u0026#39;-\u0026#39; for stdin. (default \u0026#34;-\u0026#34;) -h, --help help for update-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "d3d5c6142cb5af76fd7ae5ffc53b936e"
  },
  {
    "title": "Pachctl idp update connector",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_idp_update-connector/",
    "relURI": "/latest/run-commands/pachctl_idp_update-connector/",
    "body": " pachctl idp update-connector # Update an existing identity provider connector.\nSynopsis # Update an existing identity provider connector. Only fields which are specified are updated.\npachctl idp update-connector [flags] Options # --config string The file to read the YAML-encoded connector configuration from, or \u0026#39;-\u0026#39; for stdin. (default \u0026#34;-\u0026#34;) -h, --help help for update-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "0e0e4ce55e7a2a8b9654a7ec4618d8e4"
  },
  {
    "title": "Pachctl inspect",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect/",
    "relURI": "/latest/run-commands/pachctl_inspect/",
    "body": " pachctl inspect # Show detailed information about a Pachyderm resource.\nSynopsis # Show detailed information about a Pachyderm resource.\nOptions # -h, --help help for inspect Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a1bb2a3a4daa5f12b9332f114230bb16"
  },
  {
    "title": "Pachctl inspect branch",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_branch/",
    "relURI": "/latest/run-commands/pachctl_inspect_branch/",
    "body": " pachctl inspect branch # Return info about a branch.\nSynopsis # Return info about a branch.\npachctl inspect branch \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for branch -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "87e4a74b57d8b274d26e4c3f447919c3"
  },
  {
    "title": "Pachctl inspect cluster",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_cluster/",
    "relURI": "/latest/run-commands/pachctl_inspect_cluster/",
    "body": " pachctl inspect cluster # Returns info about the pachyderm cluster\nSynopsis # Returns info about the pachyderm cluster\npachctl inspect cluster [flags] Options # -h, --help help for cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9bcd4786b4a5ee4a9e8f2a272a9a7d7b"
  },
  {
    "title": "Pachctl inspect commit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_commit/",
    "relURI": "/latest/run-commands/pachctl_inspect_commit/",
    "body": " pachctl inspect commit # Return info about a commit.\nSynopsis # Return info about a commit.\npachctl inspect commit \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "c54e405950c417ed79942aa73ac90914"
  },
  {
    "title": "Pachctl inspect datum",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_datum/",
    "relURI": "/latest/run-commands/pachctl_inspect_datum/",
    "body": " pachctl inspect datum # Display detailed info about a single datum.\nSynopsis # Display detailed info about a single datum. Requires the pipeline to have stats enabled.\npachctl inspect datum \u0026lt;pipeline\u0026gt;@\u0026lt;job\u0026gt; \u0026lt;datum\u0026gt; [flags] Options # -h, --help help for datum -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project containing the job (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9771b17623f42fd7f3330407eda5fee6"
  },
  {
    "title": "Pachctl inspect file",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_file/",
    "relURI": "/latest/run-commands/pachctl_inspect_file/",
    "body": " pachctl inspect file # Return info about a file.\nSynopsis # Return info about a file.\npachctl inspect file \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;:\u0026lt;path/in/pfs\u0026gt; [flags] Options # -h, --help help for file -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b198ecc792eb3379f8e647ac5fb0eeb1"
  },
  {
    "title": "Pachctl inspect job",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_job/",
    "relURI": "/latest/run-commands/pachctl_inspect_job/",
    "body": " pachctl inspect job # Return info about a job.\nSynopsis # Return info about a job.\npachctl inspect job \u0026lt;pipeline\u0026gt;@\u0026lt;job\u0026gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for job -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string project containing job (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "d8315d79dc6d726b15fad614896856e8"
  },
  {
    "title": "Pachctl inspect pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_pipeline/",
    "relURI": "/latest/run-commands/pachctl_inspect_pipeline/",
    "body": " pachctl inspect pipeline # Return info about a pipeline.\nSynopsis # Return info about a pipeline.\npachctl inspect pipeline \u0026lt;pipeline\u0026gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for pipeline -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project of pipeline to inspect. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "d8406e44822751c5c02bfdc2590d04f2"
  },
  {
    "title": "Pachctl inspect project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_project/",
    "relURI": "/latest/run-commands/pachctl_inspect_project/",
    "body": " pachctl inspect project # Inspect a project.\nSynopsis # Inspect a project.\npachctl inspect project \u0026lt;project\u0026gt; [flags] Options # -h, --help help for project -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "2f64bf4c3000f805f8216e2f4b46dbf8"
  },
  {
    "title": "Pachctl inspect repo",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_repo/",
    "relURI": "/latest/run-commands/pachctl_inspect_repo/",
    "body": " pachctl inspect repo # Return info about a repo.\nSynopsis # Return info about a repo.\npachctl inspect repo \u0026lt;repo\u0026gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for repo -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "582504cd2dc4f9be3d5df1f3eddff64b"
  },
  {
    "title": "Pachctl inspect secret",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_secret/",
    "relURI": "/latest/run-commands/pachctl_inspect_secret/",
    "body": " pachctl inspect secret # Inspect a secret from the cluster.\nSynopsis # Inspect a secret from the cluster.\npachctl inspect secret [flags] Options # -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "51698971bd5f6e5d4c32cbc083d9d38c"
  },
  {
    "title": "Pachctl inspect transaction",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_inspect_transaction/",
    "relURI": "/latest/run-commands/pachctl_inspect_transaction/",
    "body": " pachctl inspect transaction # Print information about an open transaction.\nSynopsis # Print information about an open transaction.\npachctl inspect transaction [\u0026lt;transaction\u0026gt;] [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for transaction -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "31ea6fc49d3fb25de30a380a1979d86d"
  },
  {
    "title": "Pachctl kube events",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_kube-events/",
    "relURI": "/latest/run-commands/pachctl_kube-events/",
    "body": " pachctl kube-events # Return the kubernetes events.\nSynopsis # Return the kubernetes events.\npachctl kube-events [flags] Options # -h, --help help for kube-events --raw Return log messages verbatim from server. --since string Return log messages more recent than \u0026#34;since\u0026#34;. (default \u0026#34;0\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "28deffbcc38859ef46f2eba06692f741"
  },
  {
    "title": "Pachctl license",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_license/",
    "relURI": "/latest/run-commands/pachctl_license/",
    "body": " pachctl license # License commmands manage the Enterprise License service\nSynopsis # License commands manage the Enterprise License service\nOptions # -h, --help help for license Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "373d2a6c1a8be1eca78b1fe3861c4b8f"
  },
  {
    "title": "Pachctl license activate",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_license_activate/",
    "relURI": "/latest/run-commands/pachctl_license_activate/",
    "body": " pachctl license activate # Activate the license server with an activation code\nSynopsis # Activate the license server with an activation code\npachctl license activate [flags] Options # -h, --help help for activate --no-register Activate auth on the active enterprise context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "ede40421e54aa3213647767c9852e6bc"
  },
  {
    "title": "Pachctl license add cluster",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_license_add-cluster/",
    "relURI": "/latest/run-commands/pachctl_license_add-cluster/",
    "body": " pachctl license add-cluster # Register a new cluster with the license server.\nSynopsis # Register a new cluster with the license server.\npachctl license add-cluster [flags] Options # --address string The host and port where the cluster can be reached -h, --help help for add-cluster --id string The id for the cluster to register --secret string The shared secret to use to authenticate this cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "393455aa9dcea6590a6345ef494fd75e"
  },
  {
    "title": "Pachctl license delete all",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_license_delete-all/",
    "relURI": "/latest/run-commands/pachctl_license_delete-all/",
    "body": " pachctl license delete-all # Delete all data from the license server\nSynopsis # Delete all data from the license server\npachctl license delete-all [flags] Options # -h, --help help for delete-all Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "38f654bbc07cbda8f830077e906a29f5"
  },
  {
    "title": "Pachctl license delete cluster",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_license_delete-cluster/",
    "relURI": "/latest/run-commands/pachctl_license_delete-cluster/",
    "body": " pachctl license delete-cluster # Delete a cluster registered with the license server.\nSynopsis # Delete a cluster registered with the license server.\npachctl license delete-cluster [flags] Options # -h, --help help for delete-cluster --id string The id for the cluster to delete Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e28510f7eddf35155e16ecdbfee4df1f"
  },
  {
    "title": "Pachctl license get state",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_license_get-state/",
    "relURI": "/latest/run-commands/pachctl_license_get-state/",
    "body": " pachctl license get-state # Get the configuration of the license service.\nSynopsis # Get the configuration of the license service.\npachctl license get-state [flags] Options # -h, --help help for get-state Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "3ae73c2bf1061397a51c926aa609106a"
  },
  {
    "title": "Pachctl license list clusters",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_license_list-clusters/",
    "relURI": "/latest/run-commands/pachctl_license_list-clusters/",
    "body": " pachctl license list-clusters # List clusters registered with the license server.\nSynopsis # List clusters registered with the license server.\npachctl license list-clusters [flags] Options # -h, --help help for list-clusters Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "1387a6f0b29a5e41c2a5c0606f6529ec"
  },
  {
    "title": "Pachctl license update cluster",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_license_update-cluster/",
    "relURI": "/latest/run-commands/pachctl_license_update-cluster/",
    "body": " pachctl license update-cluster # Update an existing cluster registered with the license server.\nSynopsis # Update an existing cluster registered with the license server.\npachctl license update-cluster [flags] Options # --address string The host and port where the cluster can be reached by the enterprise server --cluster-deployment-id string The deployment id of the updated cluster -h, --help help for update-cluster --id string The id for the cluster to update --user-address string The host and port where the cluster can be reached by a user Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "57d7d2050d937c27ce23ad1d4d5374cf"
  },
  {
    "title": "Pachctl list",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list/",
    "relURI": "/latest/run-commands/pachctl_list/",
    "body": " pachctl list # Print a list of Pachyderm resources of a specific type.\nSynopsis # Print a list of Pachyderm resources of a specific type.\nOptions # -h, --help help for list Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "7c99edbddb63e03ebe0e118f2e7892b1"
  },
  {
    "title": "Pachctl list branch",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list_branch/",
    "relURI": "/latest/run-commands/pachctl_list_branch/",
    "body": " pachctl list branch # Return all branches on a repo.\nSynopsis # Return all branches on a repo.\npachctl list branch \u0026lt;repo\u0026gt; [flags] Options # -h, --help help for branch -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "763dd9c8c7630eed5d1ac922fd41c905"
  },
  {
    "title": "Pachctl list commit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list_commit/",
    "relURI": "/latest/run-commands/pachctl_list_commit/",
    "body": " pachctl list commit # Return a list of commits.\nSynopsis # Return a list of commits, either across the entire pachyderm cluster or restricted to a single repo.\npachctl list commit [\u0026lt;commit-id\u0026gt;|\u0026lt;repo\u0026gt;[@\u0026lt;branch-or-commit\u0026gt;]] [flags] Examples # # return all commits $ pachctl list commit # return commits in repo \u0026#34;foo\u0026#34; $ pachctl list commit foo # return all sub-commits in a commit $ pachctl list commit \u0026lt;commit-id\u0026gt; # return commits in repo \u0026#34;foo\u0026#34; on branch \u0026#34;master\u0026#34; $ pachctl list commit foo@master # return the last 20 commits in repo \u0026#34;foo\u0026#34; on branch \u0026#34;master\u0026#34; $ pachctl list commit foo@master -n 20 # return commits in repo \u0026#34;foo\u0026#34; on branch \u0026#34;master\u0026#34; since commit XXX $ pachctl list commit foo@master --from XXX Options # --all return all types of commits, including aliases -x, --expand show one line for each sub-commmit and include more columns -f, --from string list all commits since this commit --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit -n, --number int list only this many commits; if set to zero, list all commits --origin string only return commits of a specific type -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project in which commit is located. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "6ebc5456be4444bf0ea94a459fcb3ab5"
  },
  {
    "title": "Pachctl list datum",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list_datum/",
    "relURI": "/latest/run-commands/pachctl_list_datum/",
    "body": " pachctl list datum # Return the datums in a job.\nSynopsis # Return the datums in a job.\npachctl list datum \u0026lt;pipeline\u0026gt;@\u0026lt;job\u0026gt; [flags] Options # -f, --file string The JSON file containing the pipeline to list datums from, the pipeline need not exist -h, --help help for datum -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project containing the job (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "12f5fa3ae8a853a59e61ae076e448bfb"
  },
  {
    "title": "Pachctl list file",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list_file/",
    "relURI": "/latest/run-commands/pachctl_list_file/",
    "body": " pachctl list file # Return the files in a directory.\nSynopsis # Return the files in a directory.\npachctl list file \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;[:\u0026lt;path/in/pfs\u0026gt;] [flags] Examples # # list top-level files on branch \u0026#34;master\u0026#34; in repo \u0026#34;foo\u0026#34; $ pachctl list file foo@master # list files under directory \u0026#34;dir\u0026#34; on branch \u0026#34;master\u0026#34; in repo \u0026#34;foo\u0026#34; $ pachctl list file foo@master:dir # list top-level files in the parent commit of the current head of \u0026#34;master\u0026#34; # in repo \u0026#34;foo\u0026#34; $ pachctl list file foo@master^ # list top-level files in the grandparent of the current head of \u0026#34;master\u0026#34; # in repo \u0026#34;foo\u0026#34; $ pachctl list file foo@master^2 # list file under directory \u0026#34;dir[1]\u0026#34; on branch \u0026#34;master\u0026#34; in repo \u0026#34;foo\u0026#34; # : quote and protect regex characters $ pachctl list file \u0026#39;foo@master:dir\\[1\\]\u0026#39; Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for file -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "b2bd6e8097ca1214ffbbeb88f99de837"
  },
  {
    "title": "Pachctl list job",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list_job/",
    "relURI": "/latest/run-commands/pachctl_list_job/",
    "body": " pachctl list job # Return info about jobs.\nSynopsis # Return info about jobs.\npachctl list job [\u0026lt;job-id\u0026gt;] [flags] Examples # # Return a summary list of all jobs $ pachctl list job # Return all sub-jobs in a job $ pachctl list job \u0026lt;job-id\u0026gt; # Return all sub-jobs split across all pipelines $ pachctl list job --expand # Return only the sub-jobs from the most recent version of pipeline \u0026#34;foo\u0026#34; $ pachctl list job -p foo # Return all sub-jobs from all versions of pipeline \u0026#34;foo\u0026#34; $ pachctl list job -p foo --history all # Return all sub-jobs whose input commits include foo@XXX and bar@YYY $ pachctl list job -i foo@XXX -i bar@YYY # Return all sub-jobs in pipeline foo and whose input commits include bar@YYY $ pachctl list job -p foo -i bar@YYY Options # -A, --all-projects Show jobs from all projects. -x, --expand Show one line for each sub-job and include more columns --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for job --history string Return jobs from historical versions of pipelines. (default \u0026#34;none\u0026#34;) -i, --input strings List jobs with a specific set of input commits. format: \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt; --no-pager Don\u0026#39;t pipe output into a pager (i.e. less). -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) -p, --pipeline string Limit to jobs made by pipeline. --project string Limit to jobs in the project specified. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml --state stringArray Return only sub-jobs with the specified state. Can be repeated to include multiple states Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "eda93f6442a181e94b065f0efb6bce6d"
  },
  {
    "title": "Pachctl list pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list_pipeline/",
    "relURI": "/latest/run-commands/pachctl_list_pipeline/",
    "body": " pachctl list pipeline # Return info about all pipelines.\nSynopsis # Return info about all pipelines.\npachctl list pipeline [\u0026lt;pipeline\u0026gt;] [flags] Options # -A, --all-projects Show pipelines form all projects. -c, --commit string List the pipelines as they existed at this commit. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for pipeline --history string Return revision history for pipelines. (default \u0026#34;none\u0026#34;) -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project containing pipelines. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml -s, --spec Output \u0026#39;create pipeline\u0026#39; compatibility specs. --state stringArray Return only pipelines with the specified state. Can be repeated to include multiple states Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a2503cc361b061b5e5e881f06bbdcf50"
  },
  {
    "title": "Pachctl list project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list_project/",
    "relURI": "/latest/run-commands/pachctl_list_project/",
    "body": " pachctl list project # Return all projects.\nSynopsis # Return all projects.\npachctl list project \u0026lt;repo\u0026gt; [flags] Options # -h, --help help for project -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "9ce4f171d956f3cff6c2a9fac3924532"
  },
  {
    "title": "Pachctl list repo",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list_repo/",
    "relURI": "/latest/run-commands/pachctl_list_repo/",
    "body": " pachctl list repo # Return a list of repos.\nSynopsis # Return a list of repos. By default, hide system repos like pipeline metadata\npachctl list repo [flags] Options # --all include system repos of all types -A, --all-projects show repos from all projects --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for repo -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string project in which repo is located (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml --type string only include repos of the given type (default \u0026#34;user\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a37593e9484a71dca80d282b8f909c8f"
  },
  {
    "title": "Pachctl list secret",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list_secret/",
    "relURI": "/latest/run-commands/pachctl_list_secret/",
    "body": " pachctl list secret # List all secrets from a namespace in the cluster.\nSynopsis # List all secrets from a namespace in the cluster.\npachctl list secret [flags] Options # -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e729b2ac490ddf627d83b106eeb45c36"
  },
  {
    "title": "Pachctl list transaction",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_list_transaction/",
    "relURI": "/latest/run-commands/pachctl_list_transaction/",
    "body": " pachctl list transaction # List transactions.\nSynopsis # List transactions.\npachctl list transaction [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for transaction -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "1c4b47995703940d4a8091cf0d409cf4"
  },
  {
    "title": "Pachctl logs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_logs/",
    "relURI": "/latest/run-commands/pachctl_logs/",
    "body": " pachctl logs # Return logs from a job.\nSynopsis # Return logs from a job.\npachctl logs [--pipeline=\u0026lt;pipeline\u0026gt;|--job=\u0026lt;pipeline\u0026gt;@\u0026lt;job\u0026gt;] [--datum=\u0026lt;datum\u0026gt;] [flags] Examples # # Return logs emitted by recent jobs in the \u0026#34;filter\u0026#34; pipeline $ pachctl logs --pipeline=filter # Return logs emitted by the job aedfa12aedf $ pachctl logs --job=aedfa12aedf # Return logs emitted by the pipeline \\\u0026#34;filter\\\u0026#34; while processing /apple.txt and a file with the hash 123aef $ pachctl logs --pipeline=filter --inputs=/apple.txt,123aef Options # --datum string Filter for log lines for this datum (accepts datum ID) -f, --follow Follow logs as more are created. -h, --help help for logs --inputs string Filter for log lines generated while processing these files (accepts PFS paths or file hashes) -j, --job string Filter for log lines from this job (accepts job ID) --master Return log messages from the master process (pipeline must be set). -p, --pipeline string Filter the log for lines from this pipeline (accepts pipeline name) --project string Project containing the job. (default \u0026#34;openCV\u0026#34;) --raw Return log messages verbatim from server. --since string Return log messages more recent than \u0026#34;since\u0026#34;. (default \u0026#34;24h\u0026#34;) -t, --tail int Lines of recent logs to display. --worker Return log messages from the worker process. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "ab8a797657c016054cab0c3e17fb72d0"
  },
  {
    "title": "Pachctl loki",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_loki/",
    "relURI": "/latest/run-commands/pachctl_loki/",
    "body": " pachctl loki # Query the loki logs.\nSynopsis # Query the loki logs.\npachctl loki \u0026lt;query\u0026gt; [flags] Options # -h, --help help for loki --since string Return log messages more recent than \u0026#34;since\u0026#34;. (default \u0026#34;0\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "05f8653a9773d00155e70e5da6f0dd8d"
  },
  {
    "title": "Pachctl mount",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_mount/",
    "relURI": "/latest/run-commands/pachctl_mount/",
    "body": " pachctl mount # Mount pfs locally. This command blocks.\nSynopsis # Mount pfs locally. This command blocks.\npachctl mount \u0026lt;path/to/mount/point\u0026gt; [flags] Options # -d, --debug Turn on debug messages. -h, --help help for mount --project string Project in which repo is located. (default \u0026#34;default\u0026#34;) -r, --repos []string Repos and branches / commits to mount, arguments should be of the form \u0026#34;repo[@branch=commit][+w]\u0026#34;, where the trailing flag \u0026#34;+w\u0026#34; indicates write. You can omit the branch when specifying a commit unless the same commit ID is on multiple branches in the repo. (default []) -w, --write Allow writing to pfs through the mount. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f609ba8066f81059f7c84aa2c86be08b"
  },
  {
    "title": "Pachctl port forward",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_port-forward/",
    "relURI": "/latest/run-commands/pachctl_port-forward/",
    "body": " pachctl port-forward # Forward a port on the local machine to pachd. This command blocks.\nSynopsis # Forward a port on the local machine to pachd. This command blocks.\npachctl port-forward [flags] Options # --console-port uint16 The local port to bind the console service to. (default 4000) --dex-port uint16 The local port to bind the identity service to. (default 30658) -h, --help help for port-forward --namespace string Kubernetes namespace Pachyderm is deployed in. --oidc-port uint16 The local port to bind pachd\u0026#39;s OIDC callback to. (default 30657) -p, --port uint16 The local port to bind pachd to. (default 30650) --remote-console-port uint16 The remote port to bind the console service to. (default 4000) --remote-dex-port uint16 The local port to bind the identity service to. (default 1658) --remote-oidc-port uint16 The remote port that OIDC callback is bound to in the cluster. (default 1657) --remote-port uint16 The remote port that pachd is bound to in the cluster. (default 1650) --remote-s3gateway-port uint16 The remote port that the s3 gateway is bound to. (default 1600) -s, --s3gateway-port uint16 The local port to bind the s3gateway to. (default 30600) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e13a510a6f8e38577d071c48448e52d6"
  },
  {
    "title": "Pachctl put",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_put/",
    "relURI": "/latest/run-commands/pachctl_put/",
    "body": " pachctl put # Insert data into Pachyderm.\nSynopsis # Insert data into Pachyderm.\nOptions # -h, --help help for put Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "116b2968f2dc5b110f1d06795331c799"
  },
  {
    "title": "Pachctl put file",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_put_file/",
    "relURI": "/latest/run-commands/pachctl_put_file/",
    "body": " pachctl put file # Put a file into the filesystem.\nSynopsis # Put a file into the filesystem. This command supports a number of ways to insert data into PFS.\npachctl put file \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt;[:\u0026lt;path/to/file\u0026gt;] [flags] Examples # # Put data from stdin at repo@branch:/path $ echo \u0026#34;data\u0026#34; | pachctl put file repo@branch:/path # Put a file from the local filesystem at repo@branch:/file $ pachctl put file repo@branch -f file # Put a file from the local filesystem at repo@branch:/path $ pachctl put file repo@branch:/path -f file # Put the contents of a directory at repo@branch:/dir/file $ pachctl put file -r repo@branch -f dir # Put the contents of a directory at repo@branch:/path/file (without /dir) $ pachctl put file -r repo@branch:/path -f dir # Put the data from a URL at repo@branch:/example.png $ pachctl put file repo@branch -f http://host/example.png # Put the data from a URL at repo@branch:/dir/example.png $ pachctl put file repo@branch:/dir -f http://host/example.png # Put the data from an S3 bucket at repo@branch:/s3_object $ pachctl put file repo@branch -r -f s3://my_bucket # Put several files or URLs that are listed in file. # Files and URLs should be newline delimited. $ pachctl put file repo@branch -i file # Put several files or URLs that are listed at URL. # NOTE this URL can reference local files, so it could cause you to put sensitive # files into your Pachyderm cluster. $ pachctl put file repo@branch -i http://host/path Options # -a, --append Append to the existing content of the file, either from previous commits or previous calls to \u0026#39;put file\u0026#39; within this commit. --compress Compress data during upload. This parameter might help you upload your uncompressed data, such as CSV files, to Pachyderm faster. Use \u0026#39;compress\u0026#39; with caution, because if your data is already compressed, this parameter might slow down the upload speed instead of increasing. -f, --file strings The file to be put, it can be a local file or a URL. (default [-]) --full-path If true, use the entire path provided to -f as the target filename in PFS. By default only the base of the path is used. -h, --help help for file -i, --input-file string Read filepaths or URLs from a file. If - is used, paths are read from the standard input. -p, --parallelism int The maximum number of files that can be uploaded in parallel. (default 10) --progress Print progress bars. (default true) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) -r, --recursive Recursively put the files in a directory. --untar If true, file(s) with the extension .tar are untarred and put as a separate file for each file within the tar stream(s). gzipped (.tar.gz or .tgz) tar file(s) are handled as well Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "fda6cd57a043304efb7ba42197b1b163"
  },
  {
    "title": "Pachctl restart",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_restart/",
    "relURI": "/latest/run-commands/pachctl_restart/",
    "body": " pachctl restart # Cancel and restart an ongoing task.\nSynopsis # Cancel and restart an ongoing task.\nOptions # -h, --help help for restart Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8a43eef08cf416bc58e0e48f0dc24f8e"
  },
  {
    "title": "Pachctl restart datum",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_restart_datum/",
    "relURI": "/latest/run-commands/pachctl_restart_datum/",
    "body": " pachctl restart datum # Restart a stuck datum during a currently running job.\nSynopsis # Restart a stuck datum during a currently running job; does not solve failed datums. Optionally, you can configure a job to skip failed datums via the transform.err_cmd setting of your pipeline spec.\npachctl restart datum \u0026lt;pipeline\u0026gt;@\u0026lt;job\u0026gt; \u0026lt;datum-path1\u0026gt;,\u0026lt;datum-path2\u0026gt;,... [flags] Options # -h, --help help for datum --project string Project containing the datum job (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "27dbe4f9651976a3440c0cd7e92dac8c"
  },
  {
    "title": "Pachctl resume",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_resume/",
    "relURI": "/latest/run-commands/pachctl_resume/",
    "body": " pachctl resume # Resume a stopped task.\nSynopsis # Resume a stopped task.\nOptions # -h, --help help for resume Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "bc7318ce08ff7e82cfb7de03ad17dd53"
  },
  {
    "title": "Pachctl resume transaction",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_resume_transaction/",
    "relURI": "/latest/run-commands/pachctl_resume_transaction/",
    "body": " pachctl resume transaction # Set an existing transaction as active.\nSynopsis # Set an existing transaction as active.\npachctl resume transaction \u0026lt;transaction\u0026gt; [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "6f05b41851288e696768879f922e26d6"
  },
  {
    "title": "Pachctl run",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_run/",
    "relURI": "/latest/run-commands/pachctl_run/",
    "body": " pachctl run # Manually run a Pachyderm resource.\nSynopsis # Manually run a Pachyderm resource.\nOptions # -h, --help help for run Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "c51c3e995f1c52d8b13f7109970a8bd3"
  },
  {
    "title": "Pachctl run cron",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_run_cron/",
    "relURI": "/latest/run-commands/pachctl_run_cron/",
    "body": " pachctl run cron # Run an existing Pachyderm cron pipeline now\nSynopsis # Run an existing Pachyderm cron pipeline now\npachctl run cron \u0026lt;pipeline\u0026gt; [flags] Examples # # Run a cron pipeline \u0026#34;clock\u0026#34; now $ pachctl run cron clock Options # -h, --help help for cron --project string Project containing pipeline. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "00735f11bb49abb7c657eec48b241663"
  },
  {
    "title": "Pachctl run pfs load test",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_run_pfs-load-test/",
    "relURI": "/latest/run-commands/pachctl_run_pfs-load-test/",
    "body": " pachctl run pfs-load-test # Run a PFS load test.\nSynopsis # Run a PFS load test.\npachctl run pfs-load-test \u0026lt;spec-file\u0026gt; [flags] Examples # Specification: -- CommitSpec -- count: int modifications: [ ModificationSpec ] fileSources: [ FileSourceSpec ] validator: ValidatorSpec -- ModificationSpec -- count: int putFile: PutFileSpec -- PutFileSpec -- count: int source: string -- FileSourceSpec -- name: string random: RandomFileSourceSpec -- RandomFileSourceSpec -- directory: RandomDirectorySpec sizes: [ SizeSpec ] incrementPath: bool -- RandomDirectorySpec -- depth: SizeSpec run: int -- SizeSpec -- min: int max: int prob: int [0, 100] -- ValidatorSpec -- frequency: FrequencySpec -- FrequencySpec -- count: int prob: int [0, 100] Example: count: 5 modifications: - count: 5 putFile: count: 5 source: \u0026#34;random\u0026#34; fileSources: - name: \u0026#34;random\u0026#34; random: directory: depth: 3 run: 3 size: - min: 1000 max: 10000 prob: 30 - min: 10000 max: 100000 prob: 30 - min: 1000000 max: 10000000 prob: 30 - min: 10000000 max: 100000000 prob: 10 validator: {} Options # -b, --branch string The branch to use for generating the load. -h, --help help for pfs-load-test --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) -s, --seed int The seed to use for generating the load. --state-id string The ID of the base state to use for the load. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "cddc454f21e4f3609475e02eae2dab11"
  },
  {
    "title": "Pachctl run pps load test",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_run_pps-load-test/",
    "relURI": "/latest/run-commands/pachctl_run_pps-load-test/",
    "body": " pachctl run pps-load-test # Run a PPS load test.\nSynopsis # Run a PPS load test.\npachctl run pps-load-test \u0026lt;spec-file\u0026gt; [flags] Options # -d, --dag string The DAG specification file to use for the load test -h, --help help for pps-load-test -p, --parallelism int The parallelism to use for the pipelines. --pod-patch string The pod patch file to use for the pipelines. -s, --seed int The seed to use for generating the load. --state-id string The ID of the base state to use for the load. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f05cc3fa58a14b3a7fe26c318f74bf38"
  },
  {
    "title": "Pachctl shell",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_shell/",
    "relURI": "/latest/run-commands/pachctl_shell/",
    "body": " pachctl shell # Run the pachyderm shell.\nSynopsis # Run the pachyderm shell.\npachctl shell [flags] Options # -h, --help help for shell --max-completions int The maximum number of completions to show in the shell, defaults to 64. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "84ff2968f05482492d302e389227b1a4"
  },
  {
    "title": "Pachctl squash",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_squash/",
    "relURI": "/latest/run-commands/pachctl_squash/",
    "body": " pachctl squash # Squash an existing Pachyderm resource.\nSynopsis # Squash an existing Pachyderm resource.\nOptions # -h, --help help for squash Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "aa3c22774621b4eb0817a44685bc0043"
  },
  {
    "title": "Pachctl squash commit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_squash_commit/",
    "relURI": "/latest/run-commands/pachctl_squash_commit/",
    "body": " pachctl squash commit # Squash the sub-commits of a commit.\nSynopsis # Squash the sub-commits of a commit. The data in the sub-commits will remain in their child commits. The squash will fail if it includes a commit with no children\npachctl squash commit \u0026lt;commit-id\u0026gt; [flags] Options # -h, --help help for commit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "38fbc209767dab230481742be8c90b35"
  },
  {
    "title": "Pachctl start",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_start/",
    "relURI": "/latest/run-commands/pachctl_start/",
    "body": " pachctl start # Start a Pachyderm resource.\nSynopsis # Start a Pachyderm resource.\nOptions # -h, --help help for start Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "987d4576e6d7660f0fb20f1653526370"
  },
  {
    "title": "Pachctl start commit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_start_commit/",
    "relURI": "/latest/run-commands/pachctl_start_commit/",
    "body": " pachctl start commit # Start a new commit.\nSynopsis # Start a new commit with parent-commit as the parent on the given branch; if the branch does not exist, it will be created.\npachctl start commit \u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt; [flags] Examples # # Start a commit in repo \u0026#34;test\u0026#34; on branch \u0026#34;master\u0026#34; $ pachctl start commit test@master # Start a commit with \u0026#34;master\u0026#34; as the parent in repo \u0026#34;test\u0026#34;, on a new branch \u0026#34;patch\u0026#34;; essentially a fork. $ pachctl start commit test@patch -p master # Start a commit with XXX as the parent in repo \u0026#34;test\u0026#34; on the branch \u0026#34;fork\u0026#34; $ pachctl start commit test@fork -p XXX Options # --description string A description of this commit\u0026#39;s contents (synonym for --message) -h, --help help for commit -m, --message string A description of this commit\u0026#39;s contents -p, --parent string The parent of the new commit, unneeded if branch is specified and you want to use the previous head of the branch as the parent. --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "2d9bb47cc9b134c1c797bd5f6a84208f"
  },
  {
    "title": "Pachctl start pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_start_pipeline/",
    "relURI": "/latest/run-commands/pachctl_start_pipeline/",
    "body": " pachctl start pipeline # Restart a stopped pipeline.\nSynopsis # Restart a stopped pipeline.\npachctl start pipeline \u0026lt;pipeline\u0026gt; [flags] Options # -h, --help help for pipeline --project string Project containing pipeline. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f3c0e3f0d5f5122a7033a4986dc9a9a2"
  },
  {
    "title": "Pachctl start transaction",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_start_transaction/",
    "relURI": "/latest/run-commands/pachctl_start_transaction/",
    "body": " pachctl start transaction # Start a new transaction.\nSynopsis # Start a new transaction.\npachctl start transaction [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "0bbd5748bde35ce1570faf6f0603f23e"
  },
  {
    "title": "Pachctl stop",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_stop/",
    "relURI": "/latest/run-commands/pachctl_stop/",
    "body": " pachctl stop # Cancel an ongoing task.\nSynopsis # Cancel an ongoing task.\nOptions # -h, --help help for stop Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "57602926c5caf6a8d54f6d67470a4946"
  },
  {
    "title": "Pachctl stop job",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_stop_job/",
    "relURI": "/latest/run-commands/pachctl_stop_job/",
    "body": " pachctl stop job # Stop a job.\nSynopsis # Stop a job. The job will be stopped immediately.\npachctl stop job \u0026lt;pipeline\u0026gt;@\u0026lt;job\u0026gt; [flags] Options # -h, --help help for job --project string Project containing the job (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "26112fcd2168a572e8df856ba57332de"
  },
  {
    "title": "Pachctl stop pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_stop_pipeline/",
    "relURI": "/latest/run-commands/pachctl_stop_pipeline/",
    "body": " pachctl stop pipeline # Stop a running pipeline.\nSynopsis # Stop a running pipeline.\npachctl stop pipeline \u0026lt;pipeline\u0026gt; [flags] Options # -h, --help help for pipeline --project string Project containing pipeline. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "062f29ad69d19c795fe67d2a5d267ef8"
  },
  {
    "title": "Pachctl stop transaction",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_stop_transaction/",
    "relURI": "/latest/run-commands/pachctl_stop_transaction/",
    "body": " pachctl stop transaction # Stop modifying the current transaction.\nSynopsis # Stop modifying the current transaction.\npachctl stop transaction [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "077c43dc0290088d954d465fb63904a3"
  },
  {
    "title": "Pachctl subscribe",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_subscribe/",
    "relURI": "/latest/run-commands/pachctl_subscribe/",
    "body": " pachctl subscribe # Wait for notifications of changes to a Pachyderm resource.\nSynopsis # Wait for notifications of changes to a Pachyderm resource.\nOptions # -h, --help help for subscribe Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "710fed30b1322b77a8d6db37d0b9681c"
  },
  {
    "title": "Pachctl subscribe commit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_subscribe_commit/",
    "relURI": "/latest/run-commands/pachctl_subscribe_commit/",
    "body": " pachctl subscribe commit # Print commits as they are created (finished).\nSynopsis # Print commits as they are created in the specified repo and branch. By default, all existing commits on the specified branch are returned first. A commit is only considered \u0026lsquo;created\u0026rsquo; when it\u0026rsquo;s been finished.\npachctl subscribe commit \u0026lt;repo\u0026gt;[@\u0026lt;branch\u0026gt;] [flags] Examples # # subscribe to commits in repo \u0026#34;test\u0026#34; on branch \u0026#34;master\u0026#34; $ pachctl subscribe commit test@master # subscribe to commits in repo \u0026#34;test\u0026#34; on branch \u0026#34;master\u0026#34;, but only since commit XXX. $ pachctl subscribe commit test@master --from XXX # subscribe to commits in repo \u0026#34;test\u0026#34; on branch \u0026#34;master\u0026#34;, but only for new commits created from now on. $ pachctl subscribe commit test@master --new Options # --all return all types of commits, including aliases --from string subscribe to all commits since this commit --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit --new subscribe to only new commits created from now on --origin string only return commits of a specific type -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "ea05c6d559f530df2feaa6a913f412b4"
  },
  {
    "title": "Pachctl unmount",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_unmount/",
    "relURI": "/latest/run-commands/pachctl_unmount/",
    "body": " pachctl unmount # Unmount pfs.\nSynopsis # Unmount pfs.\npachctl unmount \u0026lt;path/to/mount/point\u0026gt; [flags] Options # -a, --all unmount all pfs mounts -h, --help help for unmount Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e8d3ba779aa41ce2ea7a97f53421ec1a"
  },
  {
    "title": "Pachctl update",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_update/",
    "relURI": "/latest/run-commands/pachctl_update/",
    "body": " pachctl update # Change the properties of an existing Pachyderm resource.\nSynopsis # Change the properties of an existing Pachyderm resource.\nOptions # -h, --help help for update Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "67169f66f6a616143d3578dec786e875"
  },
  {
    "title": "Pachctl update pipeline",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_update_pipeline/",
    "relURI": "/latest/run-commands/pachctl_update_pipeline/",
    "body": " pachctl update pipeline # Update an existing Pachyderm pipeline.\nSynopsis # Update a Pachyderm pipeline with a new pipeline specification. For details on the format, see https://docs.pachyderm.com/latest/reference/pipeline-spec/.\npachctl update pipeline [flags] Options # --arg stringArray Top-level argument passed to the Jsonnet template in --jsonnet (which must be set if any --arg arguments are passed). Value must be of the form \u0026#39;param=value\u0026#39;. For multiple args, --arg may be set more than once. -f, --file string A JSON file (url or filepath) containing one or more pipelines. \u0026#34;-\u0026#34; reads from stdin (the default behavior). Exactly one of --file and --jsonnet must be set. -h, --help help for pipeline --jsonnet string BETA: A Jsonnet template file (url or filepath) for one or more pipelines. \u0026#34;-\u0026#34; reads from stdin. Exactly one of --file and --jsonnet must be set. Jsonnet templates must contain a top-level function; strings can be passed to this function with --arg (below) --project string The project in which to update the pipeline. (default \u0026#34;openCV\u0026#34;) -p, --push-images If true, push local docker images into the docker registry. -r, --registry string The registry to push images to. (default \u0026#34;index.docker.io\u0026#34;) --reprocess If true, reprocess datums that were already processed by previous version of the pipeline. -u, --username string The username to push images as. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "f272a69cae55afeb68177ca6e9e7d4b5"
  },
  {
    "title": "Pachctl update project",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_update_project/",
    "relURI": "/latest/run-commands/pachctl_update_project/",
    "body": " pachctl update project # Update a project.\nSynopsis # Update a project.\npachctl update project \u0026lt;project\u0026gt; [flags] Options # -d, --description string The description of the updated project. -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "ffc2b4c765be413a2199831f3ba8e5de"
  },
  {
    "title": "Pachctl update repo",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_update_repo/",
    "relURI": "/latest/run-commands/pachctl_update_repo/",
    "body": " pachctl update repo # Update a repo.\nSynopsis # Update a repo.\npachctl update repo \u0026lt;repo\u0026gt; [flags] Options # -d, --description string A description of the repo. -h, --help help for repo --project string Project in which repo is located. (default \u0026#34;openCV\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "c87a5bcf84d679f46dc986266bbda682"
  },
  {
    "title": "Pachctl version",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_version/",
    "relURI": "/latest/run-commands/pachctl_version/",
    "body": " pachctl version # Print Pachyderm version information.\nSynopsis # Print Pachyderm version information.\npachctl version [flags] Options # --client-only If set, only print pachctl\u0026#39;s version, but don\u0026#39;t make any RPCs to pachd. Useful if pachd is unavailable --enterprise If set, \u0026#39;pachctl version\u0026#39; will run on the active enterprise context. -h, --help help for version -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml --timeout string If set, \u0026#39;pachctl version\u0026#39; will timeout after the given duration (formatted as a golang time duration--a number followed by ns, us, ms, s, m, or h). If --client-only is set, this flag is ignored. If unset, pachctl will use a default timeout; if set to 0s, the call will never time out. (default \u0026#34;default\u0026#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "de22870b1732a2616f8e8558b3b81889"
  },
  {
    "title": "Pachctl wait",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_wait/",
    "relURI": "/latest/run-commands/pachctl_wait/",
    "body": " pachctl wait # Wait for the side-effects of a Pachyderm resource to propagate.\nSynopsis # Wait for the side-effects of a Pachyderm resource to propagate.\nOptions # -h, --help help for wait Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "3757254573ebb9b5faa73203697a9632"
  },
  {
    "title": "Pachctl wait commit",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_wait_commit/",
    "relURI": "/latest/run-commands/pachctl_wait_commit/",
    "body": " pachctl wait commit # Wait for the specified commit to finish and return it.\nSynopsis # Wait for the specified commit to finish and return it.\npachctl wait commit \u0026lt;repo\u0026gt;@\u0026lt;branch-or-commit\u0026gt; [flags] Examples # # wait for the commit foo@XXX to finish and return it $ pachctl wait commit foo@XXX -b bar@baz Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string Project containing commit. (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8336dba1d8b8621c81ca2336a7b161ef"
  },
  {
    "title": "Pachctl wait job",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Run Commands",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/run-commands/pachctl_wait_job/",
    "relURI": "/latest/run-commands/pachctl_wait_job/",
    "body": " pachctl wait job # Wait for a job to finish then return info about the job.\nSynopsis # Wait for a job to finish then return info about the job.\npachctl wait job \u0026lt;job\u0026gt;|\u0026lt;pipeline\u0026gt;@\u0026lt;job\u0026gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for job -o, --output string Output format when --raw is set: \u0026#34;json\u0026#34; or \u0026#34;yaml\u0026#34; (default \u0026#34;json\u0026#34;) --project string project containing job (default \u0026#34;openCV\u0026#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e14aaf055de930e3f6d2296faffc14da"
  },
  {
    "title": "Debug",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Troubleshoot pipelines using PachCTL to explore logged user events.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/debug/",
    "relURI": "/latest/debug/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "eda3519b692857e59df55b2901c44c61"
  },
  {
    "title": "Common Issues",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Debug",
    "description": "Learn how to troubleshoot common issues.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/debug/common-issues/",
    "relURI": "/latest/debug/common-issues/",
    "body": " Cannot connect via pachctl - context deadline exceeded # Symptom # You may be using the pachd address config value or environment variable to specify how pachctl talks to your Pachyderm cluster, or you may be forwarding the Pachydermport. In any event, you might see something similar to:\npachctl version COMPONENT VERSION pachctl 2.6.1 context deadline exceeded Also, you might get this message if pachd is not running.\nRecourse # It\u0026rsquo;s possible that the connection is just taking a while. Occasionally this can happen if your cluster is far away (deployed in a region across the country). Check your internet connection.\nIt\u0026rsquo;s also possible that you haven\u0026rsquo;t poked a hole in the firewall to access the node on this port. Usually to do that you adjust a security rule (in AWS parlance a security group). For example, on AWS, if you find your node in the web console and click on it, you should see a link to the associated security group. Inspect that group. There should be a way to \u0026ldquo;add a rule\u0026rdquo; to the group. You\u0026rsquo;ll want to enable TCP access (ingress) on port 30650. You\u0026rsquo;ll usually be asked which incoming IPs should be whitelisted. You can choose to use your own, or enable it for everyone (0.0.0.0/0).\nCertificate Error When Using Kubectl # Symptom # This can happen on any request using kubectl (e.g. kubectl get all). In particular you\u0026rsquo;ll see:\nkubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;6\u0026#34;, GitVersion:\u0026#34;v1.6.4\u0026#34;, GitCommit:\u0026#34;d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2017-05-19T20:41:24Z\u0026#34;, GoVersion:\u0026#34;go1.8.1\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/amd64\u0026#34;} Unable to connect to the server: x509: certificate signed by unknown authority Recourse # Check if you\u0026rsquo;re on any sort of VPN or other egress proxy that would break SSL. Also, there is a possibility that your credentials have expired. In the case where you\u0026rsquo;re using GKE and gcloud, renew your credentials via:\nkubectl get all Unable to connect to the server: x509: certificate signed by unknown authority gcloud container clusters get-credentials my-cluster-name-dev Fetching cluster endpoint and auth data. kubeconfig entry generated for my-cluster-name-dev. kubectl config current-context gke_my-org_us-east1-b_my-cluster-name-dev Uploads and Downloads are Slow # Symptom # Any pachctl put file or pachctl get file commands are slow.\nRecourse # If you do not explicitly set the pachd address config value, pachctl will default to using port forwarding, which throttles traffic to ~1MB/s. If you need to do large downloads/uploads you should consider using pachd address config value. You\u0026rsquo;ll also want to make sure you\u0026rsquo;ve allowed ingress access through any firewalls to your k8s cluster.\nNaming a Repo with an Unsupported Symbol # Symptom # A Pachyderm repo was accidentally named starting with a dash (-) and the repository is treated as a command flag instead of a repository.\nRecourse # Pachyderm supports standard bash utilities that you can use to resolve this and similar problems. For example, in this case, you can specify double dashes (--) to delete the repository. Double dashes signify the end of options and tell the shell to process the rest arguments as filenames and objects.\nFor more information, see man bash.\nFailed Uploads # Symptom # A file upload, particularly a recursive one of many files, fails. You may see log messages containing the following in either pipeline logs, pachd logs, or from the pachctl command locally:\npachctl errror: an error occurred forwarding XXXXX -\u0026gt; 650: error forwarding port 650 pachctl error: EOF pachd or worker: all SubConns are in TransientFailure, latest connection error: connection error: desc = \\\u0026quot;transport: Error while dialing dial tcp 127.0.0.1:653: connect: connection refused\\\u0026quot;; retrying in XXXX.XXXXXs\u0026quot;} Recourse # Either pachd or your pipeline\u0026rsquo;s worker sidecar may be getting OOM killed as it grows while getting data from object storage.\nYou can give the storage container more resources by increasing the cache_size parameter in your pipeline spec. Increase it to what you can afford; its default is 64M.(If you‚Äôre using a release prior to 1.10.0 and you have cluster-wide or namepace policies on resource limits, you may need to manually edit the pipeline RC.)\nIf it still gets OOM killed by k8s, there are a couple of environment variables you can set in your pachd deployment to limit the amount of memory the sidecar and pachd use.\nSTORAGE_UPLOAD_CONCURRENCY_LIMIT limits the parallelism to put files into the storage backend. Default is 100. STORAGE_PUT_FILE_CONCURRENCY_LIMIT limits the number of parallel downloads pachd will initiate. Default is also 100. You may use a binary search technique to hone in on a value appropriate for a production pipeline:\nfor cache_size, max it out. If it works, halve it. If its OOM killed, increase the value by 50%. and so on for the CONCURRENCY_LIMITS, halve and increase by 50% until you get a value that works.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "14f44a3437763b0ab1431ab7f617c737"
  },
  {
    "title": "Debug Pipelines",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Debug",
    "description": "Learn how to troubleshoot pipelines.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/debug/pipelines/",
    "relURI": "/latest/debug/pipelines/",
    "body": " Introduction # Job failures can occur for a variety of reasons, but they generally categorize into 4 failure types:\nYou hit one of the Pachyderm Community Edition Scaling Limits. User-code-related: An error in the user code running inside the container or the json pipeline config. Data-related: A problem with the input data such as incorrect file type or file name. System- or infrastructure-related: An error in Pachyderm or Kubernetes such as missing credentials, transient network errors, or resource constraints (for example, out-of-memory\u0026ndash;OOM\u0026ndash;killed). In this document, we\u0026rsquo;ll show you the tools for determining what kind of failure it is. For each of the failure modes, we‚Äôll describe Pachyderm‚Äôs and Kubernetes‚Äôs specific retry and error-reporting behaviors as well as typical user triaging methodologies.\nFailed jobs in a pipeline will propagate information to downstream pipelines with empty commits to preserve provenance and make tracing the failed job easier. A failed job is no longer running.\nIn this document, we\u0026rsquo;ll describe what you\u0026rsquo;ll see, how Pachyderm will respond, and techniques for triaging each of those three categories of failure.\nAt the bottom of the document, we\u0026rsquo;ll provide specific troubleshooting steps for specific scenarios.\nPipeline exists but never runs All your pods or jobs get evicted Determining the kind of failure # First off, you can see the status of Pachyderm\u0026rsquo;s jobs with pachctl list job --expand, which will show you the status of all jobs. For a failed job, use pachctl inspect job \u0026lt;job-id\u0026gt; to find out more about the failure. The different categories of failures are addressed below.\nCommunity Edition Scaling Limits # If you are running on the Community Edition, you might have hit the limit set on the number of pipelines and/or parallel workers.\nThat scenario is quite easy to troubleshoot:\nCheck your number of pipelines and parallelism settings (\u0026quot;parallelism_spec\u0026quot; attribute in your pipeline specification files) against our limits.\nAdditionally, your stderr and pipeline logs (pachctl log -p \u0026lt;pipeline name\u0026gt; --master or pachctl log -p \u0026lt;pipeline name\u0026gt; --worker) should contain one or both of those messages:\nnumber of pipelines limit exceeded: Pachyderm Community Edition requires an activation key to create more than 16 total pipelines (you have X). Use the command pachctl license activate to enter your key.\nPachyderm offers readily available activation keys for proofs-of-concept, startups, academic, nonprofit, or open-source projects. Tell us about your project to get one.\nmax number of workers exceeded: This pipeline will only create a total of 8 workers (you specified X). Pachyderm Community Edition requires an activation key to create pipelines with constant parallelism greater than 8. Use the command pachctl license activate to enter your key.\nPachyderm offers readily available activation keys for proofs-of-concept, startups, academic, nonprofit, or open-source projects. Tell us about your project to get one.\nTo lift those limitations, Request an Enterprise Edition trial token. Check out our Enterprise features for more details on our Enterprise Offer.\nUser Code Failures # When there‚Äôs an error in user code, the typical error message you‚Äôll see is\nfailed to process datum \u0026lt;UUID\u0026gt; with error: \u0026lt;user code error\u0026gt; This means Pachyderm successfully got to the point where it was running user code, but that code exited with a non-zero error code. If any datum in a pipeline fails, the entire job will be marked as failed, but datums that did not fail will not need to be reprocessed on future jobs. You can use pachctl inspect datum \u0026lt;job-id\u0026gt; \u0026lt;datum-id\u0026gt; or pachctl logs with the --pipeline, --job or --datum flags to get more details.\nThere are some cases where users may want mark a datum as successful even for a non-zero error code by setting the transform.accept_return_code field in the pipeline config .\nRetries # Pachyderm will automatically retry user code three (3) times before marking the datum as failed. This mitigates datums failing for transient connection reasons.\nTriage # pachctl logs --job=\u0026lt;job_ID\u0026gt; or pachctl logs --pipeline=\u0026lt;pipeline_name\u0026gt; will print out any logs from your user code to help you triage the issue. Kubernetes will rotate logs occasionally so if nothing is being returned, you‚Äôll need to make sure that you have a persistent log collection tool running in your cluster.\nIn cases where user code is failing, changes first need to be made to the code and followed by updating the Pachyderm pipeline. This involves building a new docker container with the corrected code, modifying the Pachyderm pipeline config to use the new image, and then calling pachctl update pipeline -f updated_pipeline_config.json. Depending on the issue/error, user may or may not want to also include the --reprocess flag with update pipeline.\nData Failures # When there‚Äôs an error in the data, this will typically manifest in a user code error such as\nfailed to process datum \u0026lt;UUID\u0026gt; with error: \u0026lt;user code error\u0026gt; This means Pachyderm successfully got to the point where it was running user code, but that code exited with a non-zero error code, usually due to being unable to find a file or a path, a misformatted file, or incorrect fields/data within a file. If any datum in a pipeline fails, the entire job will be marked as failed. Datums that did not fail will not need to be reprocessed on future jobs.\nRetries # Just like with user code failures, Pachyderm will automatically retry running a datum 3 times before marking the datum as failed. This mitigates datums failing for transient connection reasons.\nTriage # Data failures can be triaged in a few different way depending on the nature of the failure and design of the pipeline.\nIn some cases, where malformed datums are expected to happen occasionally, they can be ‚Äúswallowed‚Äù (e.g. marked as successful using transform.accept_return_codes or written out to a ‚Äúfailed_datums‚Äù directory and handled within user code). This would simply require the necessary updates to the user code and pipeline config as described above. For cases where your code detects bad input data, a \u0026ldquo;dead letter queue\u0026rdquo; design pattern may be needed. Many Pachyderm developers use a special directory in each output repo for \u0026ldquo;bad data\u0026rdquo; and pipelines with globs for detecting bad data direct that data for automated and manual intervention.\nIf a few files as part of the input commit are causing the failure, they can simply be removed from the HEAD commit with start commit, delete file, finish commit. The files can also be corrected in this manner as well. This method is similar to a revert in Git \u0026ndash; the ‚Äúbad‚Äù data will still live in the older commits in Pachyderm, but will not be part of the HEAD commit and therefore not processed by the pipeline.\nSystem-level Failures # System-level failures are the most varied and often hardest to debug. We‚Äôll outline a few common patterns and triage steps. Generally, you‚Äôll need to look at deeper logs to find these errors using pachctl logs --pipeline=\u0026lt;pipeline_name\u0026gt; --raw and/or --master and kubectl logs pod \u0026lt;pod_name\u0026gt;.\nHere are some of the most common system-level failures:\nMalformed or missing credentials such that a pipeline cannot connect to object storage, registry, or other external service. In the best case, you‚Äôll see permission denied errors, but in some cases you‚Äôll only see ‚Äúdoes not exist‚Äù errors (this is common reading from object stores) Out-of-memory (OOM) killed or other resource constraint issues such as not being able to schedule pods on available cluster resources. Network issues trying to connect Pachd, etcd, or other internal or external resources Failure to find or pull a docker image from the registry Retries # For system-level failures, Pachyderm or Kubernetes will generally continually retry the operation with exponential backoff. If a job is stuck in a given state (e.g. starting, merging) or a pod is in CrashLoopBackoff, those are common signs of a system-level failure mode.\nTriage # Triaging system failures varies as widely as the issues do themselves. Here are options for the common issues mentioned previously.\nCredentials: check your secrets in k8s, make sure they‚Äôre added correctly to the pipeline config, and double check your roles/perms within the cluster OOM: Increase the memory limit/request or node size for your pipeline. If you are very resource constrained, making your datums smaller to require less resources may be necessary. Network: Check to make sure etcd and pachd are up and running, that k8s DNS is correctly configured for pods to resolve each other and outside resources, firewalls and other networking configurations allow k8s components to reach each other, and ingress controllers are configured correctly Check your container image name in the pipeline config and image_pull_secret. Specific scenarios # All pods or jobs get evicted # Symptom # After creating a pipeline, a job starts but never progresses through any datums.\nRecourse # Run kubectl get pods and see if the command returns pods that are marked Evicted. If you run kubectl describe \u0026lt;pod-name\u0026gt; with one of those evicted pods, you might get an error saying that it was evicted due to disk pressure. This means that your nodes are not configured with a big enough root volume size. You need to make sure that each node\u0026rsquo;s root volume is big enough to store the biggest datum you expect to process anywhere on your DAG plus the size of the output files that will be written for that datum.\nLet\u0026rsquo;s say you have a repo with 100 folders. You have a single pipeline with this repo as an input, and the glob pattern is /*. That means each folder will be processed as a single datum. If the biggest folder is 50GB and your pipeline\u0026rsquo;s output is about three times as big, then your root volume size needs to be bigger than:\n50 GB (to accommodate the input) + 50 GB x 3 (to accommodate the output) = 200GB In this case we would recommend 250GB to be safe. If your root volume size is less than 50GB (many defaults are 20GB), this pipeline will fail when downloading the input. The pod may get evicted and rescheduled to a different node, where the same thing will happen.\nPipeline exists but never runs # Symptom # You can see the pipeline via pachctl list pipeline, but if you look at the job via pachctl list job --expand, it\u0026rsquo;s marked as running with 0/0 datums having been processed.\nIf you inspect the job via pachctl inspect job \u0026lt;pipeline_name\u0026gt;@\u0026lt;jobID\u0026gt;, you don\u0026rsquo;t see any worker set.\nE.g:\nWorker Status: WORKER JOB DATUM STARTED ... If you do kubectl get pod you see the worker pod for your pipeline, e.g:\npo/pipeline-foo-5-v1-273zc But it\u0026rsquo;s state is Pending or CrashLoopBackoff.\nRecourse # First make sure that there is no parent job still running. Do pachctl list job --expand| grep yourPipelineName to see if there are pending jobs on this pipeline that were kicked off prior to your job. A parent job is the job that corresponds to the parent output commit of this pipeline. A job will block until all parent jobs complete.\nIf there are no parent jobs that are still running, then continue debugging:\nDescribe the pod via:\nkubectl describe po/pipeline-foo-5-v1-273zc If the state is CrashLoopBackoff, you\u0026rsquo;re looking for a descriptive error message. One such cause for this behavior might be if you specified an image for your pipeline that does not exist.\nIf the state is Pending it\u0026rsquo;s likely the cluster doesn\u0026rsquo;t have enough resources. In this case, you\u0026rsquo;ll see a could not schedule type of error message which should describe which resource you\u0026rsquo;re low on. This is more likely to happen if you\u0026rsquo;ve set resource requests (cpu/mem/gpu) for your pipelines. In this case, you\u0026rsquo;ll just need to scale up your resources. You can use your cloud provider\u0026rsquo;s auto scaling groups to increase the size of your instance group. It can take up to 10 minutes for the changes to go into effect.\nCannot Delete Pipelines with an etcd Error # Failed to delete a pipeline with an etcdserver error.\nSymptom # Deleting pipelines fails with the following error:\npachctl delete pipeline pipeline-name etcdserver: too many operations in txn request (XXXXXX comparisons, YYYYYYY writes: hint: set --max-txn-ops on the ETCD cluster to at least the largest of those values) Recourse # When a Pachyderm cluster reaches a certain scale, you need to adjust the default parameters provided for certain etcd flags. Depending on how you deployed Pachyderm, you need to either edit the etcd Deployment or StatefulSet.\nkubectl edit deploy etcd or\nkubectl edit statefulset etcd In the spec/template/containers/command path, set the value for max-txn-ops to a value appropriate for your cluster, in line with the advice in the error above: larger than the greater of XXXXXX or YYYYYYY.\nPipeline is stuck in starting # Symptom # After starting a pipeline, running the pachctl list pipeline command returns the starting status for a very long time. The kubectl get pods command returns the pipeline pods in a pending state indefinitely.\nRecourse # Run the kubectl describe pod \u0026lt;pipeline-pod\u0026gt; and analyze the information in the output of that command. Often, this type of error is associated with insufficient amount of CPU, memory, and GPU resources in your cluster.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["pipelines"],
    "id": "d56b1c64f8a394c690399b97ab01e7f3"
  },
  {
    "title": "Troubleshooting Deployments",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Debug",
    "description": "Learn how to troubleshoot deployments.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/debug/deployment/",
    "relURI": "/latest/debug/deployment/",
    "body": "A common issue related to a deployment: getting a CrashLoopBackoff error.\nPod stuck in CrashLoopBackoff # Symptoms # The pachd pod keeps crashing/restarting:\nkubectl get all NAME READY STATUS RESTARTS AGE po/etcd-281005231-qlkzw 1/1 Running 0 7m po/pachd-1333950811-0sm1p 0/1 CrashLoopBackOff 6 7m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/etcd 100.70.40.162 \u0026lt;nodes\u0026gt; 2379:30938/TCP 7m svc/kubernetes 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 9m svc/pachd 100.70.227.151 \u0026lt;nodes\u0026gt; 650:30650/TCP,651:30651/TCP 7m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/etcd 1 1 1 1 7m deploy/pachd 1 1 1 0 7m NAME DESIRED CURRENT READY AGE rs/etcd-281005231 1 1 1 7m rs/pachd-1333950811 1 1 0 7m Recourse # First describe the pod:\nkubectl describe po/pachd-1333950811-0sm1p If you see an error including Error attaching EBS volume or similar, see the recourse for that error here under the corresponding section below. If you don\u0026rsquo;t see that error, but do see something like:\n1m 3s 9 {kubelet ip-172-20-48-123.us-west-2.compute.internal} Warning FailedSync Error syncing pod, skipping: failed to \u0026#34;StartContainer\u0026#34; for \u0026#34;pachd\u0026#34; with CrashLoopBackOff: \u0026#34;Back-off 2m40s restarting failed container=pachd pod=pachd-1333950811-0sm1p_default(a92b6665-506a-11e7-8e07-02e3d74c49ac)\u0026#34; it means Kubernetes tried running pachd, but pachd generated an internal error. To see the specifics of this internal error, check the logs for the pachd pod:\nkubectl logs po/pachd-1333950811-0sm1p ‚ÑπÔ∏è If you\u0026rsquo;re using a log aggregator service (e.g. the default in GKE), you won\u0026rsquo;t see any logs when using kubectl logs ... in this way. You will need to look at your logs UI (e.g. in GKE\u0026rsquo;s case the stackdriver console).\nThese logs will most likely reveal the issue directly, or at the very least, a good indicator as to what\u0026rsquo;s causing the problem. For example, you might see, BucketRegionError: incorrect region, the bucket is not in 'us-west-2' region. In that case, your object store bucket in a different region than your Pachydermcluster and the fix would be to recreate the bucket in the same region as your pachydermm cluster.\nIf the error / recourse isn\u0026rsquo;t obvious from the error message, post the error as well as the pachd logs in our Slack channel, or open a GitHub Issue and provide the necessary details prompted by the issue template. Please do be sure provide these logs either way as it is extremely helpful in resolving the issue.\nPod stuck in CrashLoopBackoff - with error attaching volume # Symptoms # A pod (could be the pachd pod or a worker pod) fails to startup, and is stuck in CrashLoopBackoff. If you execute kubectl describe po/pachd-xxxx, you\u0026rsquo;ll see an error message like the following at the bottom of the output:\n30s 30s 1 {attachdetach } Warning FailedMount Failed to attach volume \u0026#34;etcd-volume\u0026#34; on node \u0026#34;ip-172-20-44-17.us-west-2.compute.internal\u0026#34; with: Error attaching EBS volume \u0026#34;vol-0c1d403ac05096dfe\u0026#34; to instance \u0026#34;i-0a12e00c0f3fb047d\u0026#34;: VolumeInUse: vol-0c1d403ac05096dfe is already attached to an instance This would indicate that the persistent volume claim is failing to get attached to the node in your kubernetes cluster.\nRecourse # Your best bet is to manually detach the volume and restart the pod.\nFor example, to resolve this issue when Pachyderm is deployed to AWS, pull up your AWS web console and look up the node mentioned in the error message (ip-172-20-44-17.us-west-2.compute.internal in our case). Then on the bottom pane for the attached volume. Follow the link to the attached volume, and detach the volume. You may need to \u0026ldquo;Force Detach\u0026rdquo; it.\nOnce it\u0026rsquo;s detached (and marked as available). Restart the pod by killing it, e.g:\nkubectl delete po/pachd-xxx It will take a moment for a new pod to get scheduled.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["deployment"],
    "id": "38a3ba21eafc244203af33b910fdb653"
  },
  {
    "title": "View Audit Logs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Debug",
    "description": "View a list of user-entered commands using pachctl.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/debug/view-user-logs/",
    "relURI": "/latest/debug/view-user-logs/",
    "body": " How to View Audit Logs # Open a terminal. Input the following command, replacing user command with pachctl terms: pachctl logs | grep \u0026#39;user command\u0026#39; Review logs. pachctl logs | grep \u0026#39;create project\u0026#39; {\u0026#34;log\u0026#34;:\u0026#34;{\\\u0026#34;severity\\\u0026#34;:\\\u0026#34;info\\\u0026#34;,\\\u0026#34;time\\\u0026#34;:\\\u0026#34;2023-01-24T15:46:35.447335088Z\\\u0026#34;,\\\u0026#34;logger\\\u0026#34;:\\\u0026#34;grpc.admin_v2.API/InspectCluster\\\u0026#34;,\\\u0026#34;caller\\\u0026#34;:\\\u0026#34;logging/interceptor.go:529\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;request for admin_v2.API/InspectCluster\\\u0026#34;,\\\u0026#34;service\\\u0026#34;:\\\u0026#34;admin_v2.API\\\u0026#34;,\\\u0026#34;method\\\u0026#34;:\\\u0026#34;InspectCluster\\\u0026#34;,\\\u0026#34;x-request-id\\\u0026#34;:[\\\u0026#34;3384fceb-68a7-4003-92ad-3bc8102d9e72\\\u0026#34;],\\\u0026#34;command\\\u0026#34;:[\\\u0026#34;pachctl create project dogs\\\u0026#34;],\\\u0026#34;peer\\\u0026#34;:\\\u0026#34;10.1.5.2:50784\\\u0026#34;,\\\u0026#34;request\\\u0026#34;:\\\u0026#34;client_version:\\u003cmajor:2 minor:5 additional:\\\\\\\u0026#34;-alpha.4\\\\\\\u0026#34; git_commit:\\\\\\\u0026#34;1a252e4f760513c820353d47227009472213713a\\\\\\\u0026#34; git_tree_modified:\\\\\\\u0026#34;true\\\\\\\u0026#34; build_date:\\\\\\\u0026#34;2023-01-19T22:43:41Z\\\\\\\u0026#34; go_version:\\\\\\\u0026#34;go1.19.5\\\\\\\u0026#34; platform:\\\\\\\u0026#34;arm64\\\\\\\u0026#34; \\u003e \\\u0026#34;}\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stderr\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-01-24T15:46:35.447552838Z\u0026#34;} {\u0026#34;log\u0026#34;:\u0026#34;{\\\u0026#34;severity\\\u0026#34;:\\\u0026#34;info\\\u0026#34;,\\\u0026#34;time\\\u0026#34;:\\\u0026#34;2023-01-24T15:46:35.447439005Z\\\u0026#34;,\\\u0026#34;logger\\\u0026#34;:\\\u0026#34;grpc.admin_v2.API/InspectCluster\\\u0026#34;,\\\u0026#34;caller\\\u0026#34;:\\\u0026#34;logging/interceptor.go:529\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;response for admin_v2.API/InspectCluster\\\u0026#34;,\\\u0026#34;service\\\u0026#34;:\\\u0026#34;admin_v2.API\\\u0026#34;,\\\u0026#34;method\\\u0026#34;:\\\u0026#34;InspectCluster\\\u0026#34;,\\\u0026#34;x-request-id\\\u0026#34;:[\\\u0026#34;3384fceb-68a7-4003-92ad-3bc8102d9e72\\\u0026#34;],\\\u0026#34;command\\\u0026#34;:[\\\u0026#34;pachctl create project dogs\\\u0026#34;],\\\u0026#34;peer\\\u0026#34;:\\\u0026#34;10.1.5.2:50784\\\u0026#34;,\\\u0026#34;response\\\u0026#34;:\\\u0026#34;id:\\\\\\\u0026#34;9d417960076d4f63969254424a25a651\\\\\\\u0026#34; deployment_id:\\\\\\\u0026#34;LImXqySxvSBudquUR0vlohA1NeHnsTrr\\\\\\\u0026#34; version_warnings_ok:true \\\u0026#34;,\\\u0026#34;messagesSent\\\u0026#34;:1,\\\u0026#34;messagesReceived\\\u0026#34;:1,\\\u0026#34;grpc.code\\\u0026#34;:0,\\\u0026#34;duration\\\u0026#34;:0.000194958}\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stderr\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-01-24T15:46:35.448185588Z\u0026#34;} {\u0026#34;log\u0026#34;:\u0026#34;{\\\u0026#34;severity\\\u0026#34;:\\\u0026#34;info\\\u0026#34;,\\\u0026#34;time\\\u0026#34;:\\\u0026#34;2023-01-24T15:46:35.452357963Z\\\u0026#34;,\\\u0026#34;logger\\\u0026#34;:\\\u0026#34;grpc.pfs_v2.API/CreateProject\\\u0026#34;,\\\u0026#34;caller\\\u0026#34;:\\\u0026#34;logging/interceptor.go:529\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;request for pfs_v2.API/CreateProject\\\u0026#34;,\\\u0026#34;service\\\u0026#34;:\\\u0026#34;pfs_v2.API\\\u0026#34;,\\\u0026#34;method\\\u0026#34;:\\\u0026#34;CreateProject\\\u0026#34;,\\\u0026#34;x-request-id\\\u0026#34;:[\\\u0026#34;b1fd3d34-7fd1-4ca1-bcb3-6130b3ece8e8\\\u0026#34;],\\\u0026#34;command\\\u0026#34;:[\\\u0026#34;pachctl create project dogs\\\u0026#34;],\\\u0026#34;peer\\\u0026#34;:\\\u0026#34;10.1.5.2:50784\\\u0026#34;,\\\u0026#34;request\\\u0026#34;:\\\u0026#34;project:\\u003cname:\\\\\\\u0026#34;dogs\\\\\\\u0026#34; \\u003e \\\u0026#34;}\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stderr\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-01-24T15:46:35.452537213Z\u0026#34;} {\u0026#34;log\u0026#34;:\u0026#34;{\\\u0026#34;severity\\\u0026#34;:\\\u0026#34;info\\\u0026#34;,\\\u0026#34;time\\\u0026#34;:\\\u0026#34;2023-01-24T15:46:35.465953046Z\\\u0026#34;,\\\u0026#34;logger\\\u0026#34;:\\\u0026#34;grpc.pfs_v2.API/CreateProject\\\u0026#34;,\\\u0026#34;caller\\\u0026#34;:\\\u0026#34;logging/interceptor.go:529\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;response for pfs_v2.API/CreateProject\\\u0026#34;,\\\u0026#34;service\\\u0026#34;:\\\u0026#34;pfs_v2.API\\\u0026#34;,\\\u0026#34;method\\\u0026#34;:\\\u0026#34;CreateProject\\\u0026#34;,\\\u0026#34;x-request-id\\\u0026#34;:[\\\u0026#34;b1fd3d34-7fd1-4ca1-bcb3-6130b3ece8e8\\\u0026#34;],\\\u0026#34;command\\\u0026#34;:[\\\u0026#34;pachctl create project dogs\\\u0026#34;],\\\u0026#34;peer\\\u0026#34;:\\\u0026#34;10.1.5.2:50784\\\u0026#34;,\\\u0026#34;messagesSent\\\u0026#34;:1,\\\u0026#34;messagesReceived\\\u0026#34;:1,\\\u0026#34;grpc.code\\\u0026#34;:0,\\\u0026#34;duration\\\u0026#34;:0.013663667}\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stderr\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-01-24T15:46:35.46607713Z\u0026#34;} {\u0026#34;log\u0026#34;:\u0026#34;{\\\u0026#34;severity\\\u0026#34;:\\\u0026#34;info\\\u0026#34;,\\\u0026#34;time\\\u0026#34;:\\\u0026#34;2023-01-24T15:46:39.028400256Z\\\u0026#34;,\\\u0026#34;logger\\\u0026#34;:\\\u0026#34;grpc.admin_v2.API/InspectCluster\\\u0026#34;,\\\u0026#34;caller\\\u0026#34;:\\\u0026#34;logging/interceptor.go:529\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;request for admin_v2.API/InspectCluster\\\u0026#34;,\\\u0026#34;service\\\u0026#34;:\\\u0026#34;admin_v2.API\\\u0026#34;,\\\u0026#34;method\\\u0026#34;:\\\u0026#34;InspectCluster\\\u0026#34;,\\\u0026#34;x-request-id\\\u0026#34;:[\\\u0026#34;714c04e7-d180-4182-9230-3ec27e554d97\\\u0026#34;],\\\u0026#34;command\\\u0026#34;:[\\\u0026#34;pachctl create project cats\\\u0026#34;],\\\u0026#34;peer\\\u0026#34;:\\\u0026#34;10.1.5.2:60084\\\u0026#34;,\\\u0026#34;request\\\u0026#34;:\\\u0026#34;client_version:\\u003cmajor:2 minor:5 additional:\\\\\\\u0026#34;-alpha.4\\\\\\\u0026#34; git_commit:\\\\\\\u0026#34;1a252e4f760513c820353d47227009472213713a\\\\\\\u0026#34; git_tree_modified:\\\\\\\u0026#34;true\\\\\\\u0026#34; build_date:\\\\\\\u0026#34;2023-01-19T22:43:41Z\\\\\\\u0026#34; go_version:\\\\\\\u0026#34;go1.19.5\\\\\\\u0026#34; platform:\\\\\\\u0026#34;arm64\\\\\\\u0026#34; \\u003e \\\u0026#34;}\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stderr\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-01-24T15:46:39.028511715Z\u0026#34;} {\u0026#34;log\u0026#34;:\u0026#34;{\\\u0026#34;severity\\\u0026#34;:\\\u0026#34;info\\\u0026#34;,\\\u0026#34;time\\\u0026#34;:\\\u0026#34;2023-01-24T15:46:39.028438673Z\\\u0026#34;,\\\u0026#34;logger\\\u0026#34;:\\\u0026#34;grpc.admin_v2.API/InspectCluster\\\u0026#34;,\\\u0026#34;caller\\\u0026#34;:\\\u0026#34;logging/interceptor.go:529\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;response for admin_v2.API/InspectCluster\\\u0026#34;,\\\u0026#34;service\\\u0026#34;:\\\u0026#34;admin_v2.API\\\u0026#34;,\\\u0026#34;method\\\u0026#34;:\\\u0026#34;InspectCluster\\\u0026#34;,\\\u0026#34;x-request-id\\\u0026#34;:[\\\u0026#34;714c04e7-d180-4182-9230-3ec27e554d97\\\u0026#34;],\\\u0026#34;command\\\u0026#34;:[\\\u0026#34;pachctl create project cats\\\u0026#34;],\\\u0026#34;peer\\\u0026#34;:\\\u0026#34;10.1.5.2:60084\\\u0026#34;,\\\u0026#34;response\\\u0026#34;:\\\u0026#34;id:\\\\\\\u0026#34;9d417960076d4f63969254424a25a651\\\\\\\u0026#34; deployment_id:\\\\\\\u0026#34;LImXqySxvSBudquUR0vlohA1NeHnsTrr\\\\\\\u0026#34; version_warnings_ok:true \\\u0026#34;,\\\u0026#34;messagesSent\\\u0026#34;:1,\\\u0026#34;messagesReceived\\\u0026#34;:1,\\\u0026#34;grpc.code\\\u0026#34;:0,\\\u0026#34;duration\\\u0026#34;:0.000077375}\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stderr\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-01-24T15:46:39.028556673Z\u0026#34;} {\u0026#34;log\u0026#34;:\u0026#34;{\\\u0026#34;severity\\\u0026#34;:\\\u0026#34;info\\\u0026#34;,\\\u0026#34;time\\\u0026#34;:\\\u0026#34;2023-01-24T15:46:39.030396298Z\\\u0026#34;,\\\u0026#34;logger\\\u0026#34;:\\\u0026#34;grpc.pfs_v2.API/CreateProject\\\u0026#34;,\\\u0026#34;caller\\\u0026#34;:\\\u0026#34;logging/interceptor.go:529\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;request for pfs_v2.API/CreateProject\\\u0026#34;,\\\u0026#34;service\\\u0026#34;:\\\u0026#34;pfs_v2.API\\\u0026#34;,\\\u0026#34;method\\\u0026#34;:\\\u0026#34;CreateProject\\\u0026#34;,\\\u0026#34;x-request-id\\\u0026#34;:[\\\u0026#34;a0beac90-f6ad-4c49-8a39-188076e3d03d\\\u0026#34;],\\\u0026#34;command\\\u0026#34;:[\\\u0026#34;pachctl create project cats\\\u0026#34;],\\\u0026#34;peer\\\u0026#34;:\\\u0026#34;10.1.5.2:60084\\\u0026#34;,\\\u0026#34;request\\\u0026#34;:\\\u0026#34;project:\\u003cname:\\\\\\\u0026#34;cats\\\\\\\u0026#34; \\u003e \\\u0026#34;}\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stderr\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-01-24T15:46:39.030474673Z\u0026#34;} {\u0026#34;log\u0026#34;:\u0026#34;{\\\u0026#34;severity\\\u0026#34;:\\\u0026#34;info\\\u0026#34;,\\\u0026#34;time\\\u0026#34;:\\\u0026#34;2023-01-24T15:46:39.031706548Z\\\u0026#34;,\\\u0026#34;logger\\\u0026#34;:\\\u0026#34;grpc.pfs_v2.API/CreateProject\\\u0026#34;,\\\u0026#34;caller\\\u0026#34;:\\\u0026#34;logging/interceptor.go:529\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;response for pfs_v2.API/CreateProject\\\u0026#34;,\\\u0026#34;service\\\u0026#34;:\\\u0026#34;pfs_v2.API\\\u0026#34;,\\\u0026#34;method\\\u0026#34;:\\\u0026#34;CreateProject\\\u0026#34;,\\\u0026#34;x-request-id\\\u0026#34;:[\\\u0026#34;a0beac90-f6ad-4c49-8a39-188076e3d03d\\\u0026#34;],\\\u0026#34;command\\\u0026#34;:[\\\u0026#34;pachctl create project cats\\\u0026#34;],\\\u0026#34;peer\\\u0026#34;:\\\u0026#34;10.1.5.2:60084\\\u0026#34;,\\\u0026#34;messagesSent\\\u0026#34;:1,\\\u0026#34;messagesReceived\\\u0026#34;:1,\\\u0026#34;grpc.code\\\u0026#34;:0,\\\u0026#34;duration\\\u0026#34;:0.001327458}\\n\u0026#34;,\u0026#34;stream\u0026#34;:\u0026#34;stderr\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-01-24T15:46:39.031799756Z\u0026#34;} Useful Search Terms # Command delete branch delete commit delete file delete job delete pipeline delete repo delete secret delete transaction edit pipeline ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "a12db5668a64ea44030999d400a134dd"
  },
  {
    "title": "View Kubernetes Logs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Debug",
    "description": "View the Kubernetes Log using PachCTL.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/debug/view-k8s-events/",
    "relURI": "/latest/debug/view-k8s-events/",
    "body": "The kube-event-tail pod in your Pachyderm cluster stores Kubernetes logs which are discarded after a certain amount of time. You can view these logs to obtain insights on key events. There are three event types: informational, warning, and error.\nHow to View Kubernetes Logs # Open a terminal. Input the following command, replacing user command with pachctl terms: pachctl kube-events Review logs. LAST SEEN TYPE REASON OBJECT MESSAGE 20 minutes ago ScalingReplicaSet Deployment/pachd Scaled up replica set pachd-84f599bccd to 1 19 minutes ago ScalingReplicaSet Deployment/pachd Scaled down replica set pachd-65fc687687 to 0 from 1 21 minutes ago SandboxChanged Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Container image \u0026#34;pachyderm/kube-event-tail:v0.0.7\u0026#34; already present on machine 21 minutes ago Created Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Created container kube-event-tail 21 minutes ago Started Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Started container kube-event-tail 21 minutes ago SandboxChanged Pod/pachyderm-loki-0 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-loki-0 Container image \u0026#34;grafana/loki:2.6.1\u0026#34; already present on machine 21 minutes ago Created Pod/pachyderm-loki-0 Created container loki 21 minutes ago Started Pod/pachyderm-loki-0 Started container loki 20 minutes ago Warning Unhealthy Pod/pachyderm-loki-0 Readiness probe failed: HTTP probe failed with statuscode: 503 20 minutes ago Warning Unhealthy Pod/pachyderm-loki-0 Liveness probe failed: HTTP probe failed with statuscode: 503 21 minutes ago SandboxChanged Pod/pachyderm-promtail-b8plv Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-promtail-b8plv Container image \u0026#34;docker.io/grafana/promtail:2.6.1\u0026#34; already present on machine 21 minutes ago Created Pod/pachyderm-promtail-b8plv Created container promtail 21 minutes ago Started Pod/pachyderm-promtail-b8plv Started container promtail 21 minutes ago SandboxChanged Pod/pachyderm-proxy-7d757c85bb-zp5ht Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-proxy-7d757c85bb-zp5ht Container image \u0026#34;envoyproxy/envoy-distroless:v1.24.1\u0026#34; already present on machine 21 minutes ago Created Pod/pachyderm-proxy-7d757c85bb-zp5ht Created container envoy 21 minutes ago Started Pod/pachyderm-proxy-7d757c85bb-zp5ht Started container envoy 21 minutes ago Warning Unhealthy Pod/pachyderm-proxy-7d757c85bb-zp5ht Readiness probe failed: HTTP probe failed with statuscode: 503 21 minutes ago SandboxChanged Pod/pg-bouncer-746bb45867-hgd57 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pg-bouncer-746bb45867-hgd57 Container image \u0026#34;pachyderm/pgbouncer:1.16.2\u0026#34; already present on machine 21 minutes ago Created Pod/pg-bouncer-746bb45867-hgd57 Created container pg-bouncer 21 minutes ago Started Pod/pg-bouncer-746bb45867-hgd57 Started container pg-bouncer 20 minutes ago Warning Unhealthy Pod/pg-bouncer-746bb45867-hgd57 Liveness probe failed: psql: error: FATAL: pgbouncer cannot connect to server 21 minutes ago SandboxChanged Pod/pipeline-joins-inner-join-v1-pfrnh Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pipeline-joins-inner-join-v1-pfrnh Container image \u0026#34;pachyderm/worker:2.5.0-alpha.4\u0026#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-inner-join-v1-pfrnh Created container init 21 minutes ago Started Pod/pipeline-joins-inner-join-v1-pfrnh Started container init 21 minutes ago Pulled Pod/pipeline-joins-inner-join-v1-pfrnh Container image \u0026#34;pachyderm/example-joins-inner-outer:2.1.0\u0026#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-inner-join-v1-pfrnh Created container user 21 minutes ago Started Pod/pipeline-joins-inner-join-v1-pfrnh Started container user 20 minutes ago Pulled Pod/pipeline-joins-inner-join-v1-pfrnh Container image \u0026#34;pachyderm/pachd:2.5.0-alpha.4\u0026#34; already present on machine 20 minutes ago Created Pod/pipeline-joins-inner-join-v1-pfrnh Created container storage 20 minutes ago Started Pod/pipeline-joins-inner-join-v1-pfrnh Started container storage 20 minutes ago Warning BackOff Pod/pipeline-joins-inner-join-v1-pfrnh Back-off restarting failed container 19 minutes ago Killing Pod/pipeline-joins-inner-join-v1-pfrnh Stopping container user 19 minutes ago Killing Pod/pipeline-joins-inner-join-v1-pfrnh Stopping container storage 21 minutes ago SandboxChanged Pod/pipeline-joins-reduce-inner-v1-jjx6w Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pipeline-joins-reduce-inner-v1-jjx6w Container image \u0026#34;pachyderm/worker:2.5.0-alpha.4\u0026#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-reduce-inner-v1-jjx6w Created container init 21 minutes ago Started Pod/pipeline-joins-reduce-inner-v1-jjx6w Started container init 21 minutes ago Pulled Pod/pipeline-joins-reduce-inner-v1-jjx6w Container image \u0026#34;ubuntu:20.04\u0026#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-reduce-inner-v1-jjx6w Created container user 21 minutes ago Started Pod/pipeline-joins-reduce-inner-v1-jjx6w Started container user 20 minutes ago Pulled Pod/pipeline-joins-reduce-inner-v1-jjx6w Container image \u0026#34;pachyderm/pachd:2.5.0-alpha.4\u0026#34; already present on machine 20 minutes ago Created Pod/pipeline-joins-reduce-inner-v1-jjx6w Created container storage LAST SEEN TYPE REASON OBJECT MESSAGE 20 minutes ago Started Pod/pipeline-joins-reduce-inner-v1-jjx6w Started container storage 20 minutes ago Warning BackOff Pod/pipeline-joins-reduce-inner-v1-jjx6w Back-off restarting failed container 19 minutes ago Killing Pod/pipeline-joins-reduce-inner-v1-jjx6w Stopping container user 19 minutes ago Killing Pod/pipeline-joins-reduce-inner-v1-jjx6w Stopping container storage 21 minutes ago SandboxChanged Pod/postgres-0 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/postgres-0 Container image \u0026#34;docker.io/pachyderm/postgresql:13.3.0\u0026#34; already present on machine 21 minutes ago Created Pod/postgres-0 Created container postgres 21 minutes ago Started Pod/postgres-0 Started container postgres Key Events # Event Description CrashLoopBackOff A container in a pod keeps crashing and restarting. This typically indicates that there is an issue with the container that needs to be resolved. FailedScheduling The Kubernetes scheduler is unable to schedule a pod on any node. This typically indicates that there are insufficient resources available in the cluster or that there are constraints set that prevent the pod from being scheduled. OutOfMemory A container in a pod ran out of memory. This typically indicates that the container needs to be reconfigured with more memory or that there is an issue with the application running in the container that is causing it to consume too much memory. FailedCreatePodSandBox The Kubernetes API server failed to create a sandbox for a pod. This typically indicates that there is an issue with the node or the network that is preventing the creation of the sandbox. Evicted A pod is evicted from a node due to resource constraints or other reasons. This typically indicates that the node is running low on resources or that there is an issue with the pod that needs to be resolved. NodeNotReady A node is not ready to accept pods. This can occur due to various reasons such as network connectivity issues or insufficient resources. ImagePullBackOff An image pull operation fails. The container runtime is unable to pull the image from the specified registry or repository. FailedMount A mount operation failed, such as when a volume or configMap failed to mount. This can occur due to incorrect configurations, insufficient permissions, or a missing dependency. ‚ÑπÔ∏è These events are just a few of the many events that can occur in Kubernetes. It\u0026rsquo;s important to monitor your cluster for these and other events to ensure the health and stability of your applications.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "e2b1f31194883cd1f3a0a51ce379c4c5"
  },
  {
    "title": "SDKs",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Try out our SDKs for faster development.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/sdk/",
    "relURI": "/latest/sdk/",
    "body": "",
    "beta": "<no value>",
    "hidden": "true",
    "categories": [],
    "tags": [],
    "id": "755494214335a1a6b02e6fc63e635dab"
  },
  {
    "title": "Python SDK",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "SDKs",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/sdk/python/",
    "relURI": "/latest/sdk/python/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "8317fae1c45f018ea94b6ff0e92f8536"
  },
  {
    "title": "Contribute",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "Contribute to our open source code and documentation.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/contribute/",
    "relURI": "/latest/contribute/",
    "body": "",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "5e89982b3aa7d4fd9ab59c8e92ddba6e"
  },
  {
    "title": "Coding Conventions",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Contribute",
    "description": "Learn the coding conventions contributors to the code base follow.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/contribute/coding-conventions/",
    "relURI": "/latest/contribute/coding-conventions/",
    "body": "Interested in contributing to Pachyderm\u0026rsquo;s code? Learn the conventions here! For setup instructions, see Setup for Contributors.\nLanguages # The Pachyderm repository is written using Go, Shell, and Make. Exceptions to this are:\n/examples: For showcasing how to use the product in various languages. /doc: For building documentation using a python-based static site generator (MkDocs). Shell # See the Shell Style Guide for standard conventions. Add set -eou pipefail to your scripts. Go # See the Effective Go Style Guide for standard conventions.\nNaming # Consider the package name when naming an interface to avoid redundancy. For example, storage.Interface is better than storage.StorageInterface. Do not use uppercase characters, underscores, or dashes in package names. The package foo line should match the name of the directory in which the .go file exists. Importers can use a different name if they need to disambiguate. When multiple locks are present, give each lock a distinct name following Go conventions (e.g., stateLock, mapLock). Go Modules/Third-Party Code # See the Go Modules Usage and Troubleshooting Guide for managing Go modules. Go dependencies are managed with go modules. Use go get foo to add or update a package; for more specific versions, use go get foo@v1.2.3, go get foo@master, or go get foo@e3702bed2. YAML # See the Helm Best Practices guide series. Review # See the Go Code Review Comments guide for a list of common comments. See the Go Test Comments guide for a list of common test code comments. Make sure CI is passing for your branch. Checks # Run checks using make lint. Testing # All packages and significant functionality must come with test coverage. Local unit tests should pass before pushing to GitHub (make localtest or make integration-tests for integrations). Use short flag for local tests only. Avoid waiting for asynchronous things to happen; If possible, use a method of waiting directly (e.g. \u0026lsquo;flush commit\u0026rsquo; is much better than repeatedly trying to read from a commit). Run single tests or tests from a single package; the Go tool only supports tests that match a regular expression (for example, go test -v ./src/path/to/package -run ^TestMyTest). Documentation # When writing documentation, follow the Style Guide conventions. PRs that have only documentation changes, such as typos, is a great place to start and we welcome your help! ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["developers"],
    "id": "35b72ec736104fa2fa64a947d179925d"
  },
  {
    "title": "Contributor Setup",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Contribute",
    "description": "Learn how to set up your machine to contribute code.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/contribute/setup/",
    "relURI": "/latest/contribute/setup/",
    "body": " General requirements # First, go through the general Local Installation Instructions. Additionally, make sure you have the following installed:\ngolang 1.12+ docker jq pv shellcheck Bash helpers # To stay up to date, we recommend doing the following.\nFirst clone the code: (Note, as of 07/11/19 pachyderm is using go modules and recommends cloning the code outside of the $GOPATH, we use the location ~/workspace as an example, but the code can live anywhere)\ncd ~/workspace git clone git@github.com:pachyderm/pachyderm Then update your ~/.bash_profile by adding the line:\nsource ~/workspace/pachyderm/etc/contributing/bash_helpers And you\u0026rsquo;ll stay up to date!\nSpecial macOS configuration # File descriptor limit # If you\u0026rsquo;re running tests locally, you\u0026rsquo;ll need to up your file descriptor limit. To do this, first setup a LaunchDaemon to up the limit with sudo privileges:\nsudo cp ~/workspace/pachyderm/etc/contributing/com.apple.launchd.limit.plist /Library/LaunchDaemons/ Once you restart, this will take effect. To see the limits, run:\nlaunchctl limit maxfiles Before the change is in place you\u0026rsquo;ll see something like 256 unlimited. After the change you\u0026rsquo;ll see a much bigger number in the first field. This ups the system wide limit, but you\u0026rsquo;ll also need to set a per-process limit.\nSecond, up the per process limit by adding something like this to your ~/.bash_profile :\nulimit -n 12288 Unfortunately, even after setting that limit it never seems to report the updated version. So if you try\nulimit And just see unlimited, don\u0026rsquo;t worry, it took effect.\nTo make sure all of these settings are working, you can test that you have the proper setup by running:\nmake test-pfs-server If this fails with a timeout, you\u0026rsquo;ll probably also see \u0026rsquo;too many files\u0026rsquo; type of errors. If that test passes, you\u0026rsquo;re all good!\nTimeout helper # You\u0026rsquo;ll need the timeout utility to run the make launch task. To install on mac, do:\nbrew install coreutils And then make sure to prepend the following to your path:\nPATH=\u0026#34;/usr/local/opt/coreutils/libexec/gnubin:$PATH\u0026#34; Dev cluster # Now launch the dev cluster: make launch-dev-vm.\nAnd check it\u0026rsquo;s status: kubectl get all.\npachctl # This will install the dev version of pachctl:\ncd ~/workspace/pachyderm make install pachctl version And make sure that $GOPATH/bin is on your $PATH somewhere\nGetting some images in place for local test runs # The following commands will put some images that some of the tests rely on in place in your minikube cluster:\nFor pachyderm_entrypoint container:\nmake docker-build-test-entrypoint ./etc/kube/push-to-minikube.sh pachyderm_entrypoint For pachyderm/python-build container:\n(cd etc/pipeline-build; make push-to-minikube) Running tests # Now that we have a dev cluster, it\u0026rsquo;s nice to be able to run some tests locally as we are developing.\nTo run some specific tests, just use go test directly, e.g:\ngo test -v ./src/server/cmd/pachctl/cmd We don\u0026rsquo;t recommend trying to run all the tests locally, they take a while. Use CI for that.\nFully resetting # Instead of running the makefile targets to re-compile pachctl and redeploy a dev cluster, we have a script that you can use to fully reset your pachyderm environment:\nAll existing cluster data is deleted If possible, the virtual machine that the cluster is running on is wiped out pachctl is recompiled The dev cluster is re-deployed This reset is a bit more time consuming than running one-off Makefile targets, but comprehensively ensures that the cluster is in its expected state, and is especially helpful when you\u0026rsquo;re first getting started with contributions and don\u0026rsquo;t yet have a complete intuition on the various ways a cluster may get in an unexpected state. It\u0026rsquo;s been tested on docker for mac and minikube, but likely works in other kubernetes environments as well.\nTo run it, simply call ./etc/reset.py from the pachyderm repo root.\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["developers"],
    "id": "8ece4f0bc09928027781c3582d8c721b"
  },
  {
    "title": "Developing on Windows with VSCode",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Contribute",
    "description": "Learn how to set up your Windows machine to contribute via VS code.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/contribute/windows/",
    "relURI": "/latest/contribute/windows/",
    "body": " Before You Start # Installation Requirements # You must have all of the following installed before you can start development:\nDocker Desktop Go v1.15.x+ GoReleaser Git HyperV jq kubectl Make minikube ShellCheck VSCode Terminal Settings # Open VS Code. Open your terminal (ctrl+`). Add the following to your settings.json: \u0026#34;terminal.integrated.shell.windows\u0026#34;: \u0026#34;C:\\\\Program Files\\\\Git\\\\bin\\\\bash.exe\u0026#34;, This path may vary depending on where your git bash actually exists.\nGetting started # Open a terminal and navigate to a directory you\u0026rsquo;d like to store Pachyderm. Clone the pachyderm repo using git clone https://github.com/pachyderm/pachyderm. Launch Docker Desktop (with Kubernetes enabled) or start minikube. Provision ~10 GB of memory and ~4CPUs. Via minikube: minikube start --memory=10000mb --cpus=4 --disk-size=40000mb --driver=hyperv Via Docker Desktop: Open Docker Desktop and navigate to Preferences \u0026gt; Resources \u0026gt; Advanced. Build your pachyderm pachd and worker images via the task docker-build. Option 1: Navigate to Terminal \u0026gt; Run Task\u0026hellip; Option 2: Press ctrl+p and input task docker-build Build and install pachctl. Launch a Pachyderm cluster by running the task launch-dev. If the service does not come up promptly (the script never says all the pods are ready), see the Debugging section.\nDebugging # Common Commands # The following commands are used frequently when working with Pachyderm:\nkubectl get all: lists resources in the \u0026lsquo;default\u0026rsquo; namespace, where we deploy locally. kubectl logs -p \u0026lt;pod\u0026gt;: gets the logs from the previous attempt at running a pod; a good place to find errors. minikube logs: gets the logs from minikube itself, useful when a pod runs into a CreateContainerError. docker container ls: lists recently used or in-use docker containers; used to get logs more directly. docker logs \u0026lt;container\u0026gt;: gets the logs from a specific docker container. Gotchas # Docker can get confused by command-line windows-style paths; it reads : as a mode and fails to parse. You may want to export MSYS_NO_PATHCONV=1 to prevent the automated conversion of unix-to-windows paths. Kubernetes resource specs (specifically hostPath) do not work if you use a windows-style path. Instead, you must use a unix-style path where the drive letter is the first directory, e.g. /C/path/to/file. Etcd may fail to mmap files when in a directory shared with the host system. Full Restart # Minikube # If you\u0026rsquo;d like to completely restart, use the following terminal commands:\nminikube delete kubectl delete pvc -l suite=pachyderm minikube start --memory=10000mb --cpus=4 --disk-size=40000mb ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["developers", "windows"],
    "id": "bf36ef8333665fe141c410ade65ccbc5"
  },
  {
    "title": "Documentation Style Guide",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Contribute",
    "description": "Learn how to contribute content to the documentation.",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/contribute/docs-style-guide/",
    "relURI": "/latest/contribute/docs-style-guide/",
    "body": "Thank you for taking an interest in contributing to Pachyderm\u0026rsquo;s docs! üêò üìñ\nThis style guide provides editorial guidelines for writing clear and consistent Pachyderm product documentation. See our contribution guide for instructions on how to draft and submit changes.\nAudience # Pachyderm has two main audiences:\nMLOPs Engineers: They install and configure Pachyderm to transform data using pipelines. Data Scientists \u0026amp; Data/ML Engineers: They plan the development of pipelines and consume the outputs of Pachyderm\u0026rsquo;s data processing to feed AI/ML models. Be sure to provide links to pre-requisite or contextual materials whenever possible, as everyone\u0026rsquo;s experience level and career journey is different.\nVoice \u0026amp; Tone # Pachyderm\u0026rsquo;s voice in documentation should consistently convey a personality that is friendly, knowledgeable, and empathetic.\nThe tone of voice may vary depending on the type of content being written. For example, a danger notice may use an urgent and serious tone while a tutorial may use an energetic and instructive tone. Make sure the tone of your documentation aligns with the content. If you aren\u0026rsquo;t sure what tone to convey, ask yourself: \u0026ldquo;What is the reader likely feeling when presented this content? Why are they here?\u0026rdquo; and adjust your language to the most appropriate tone.\nLanguage \u0026amp; Grammar # The following guidelines are to be followed loosely; use your best judgment when approaching content \u0026ndash; there are always exceptions.\nUse Active Voice # Write in active voice to enforce clarity and simplicity.\nüëéüö´ Don\u0026rsquo;t üëç‚úÖ Do The update was failing due to a configuration issue. The update failed due to a configuration issue. A POST request is sent and a response is returned. Send a POST request; the endpoint sends a response. You can break this rule to emphasize an object (the image is installed) or de-emphasize a subject (5 errors were found in this article).\nUse Global English # Write documentation using Global English. Global English makes comprehension easier for all audiences by avoiding regional idioms/expressions and standardizing spelling words using the US English variant.\nPut Conditional Clauses First # Order conditional clauses first when drafting sentences; this empowers the reader to skip to the next step when the condition does not apply.\nüëéüö´ Don\u0026rsquo;t üëç‚úÖ Do See this page for more information on how to use this feature. For more information, see How to Use Console. Enable the CustomField setting if you want to map custom fields. To map custom fields, enable the CustomField setting. Write Accessibly # Be mindful of how you describe software behavior and users; in particular, avoid ableist language. Use generic \u0026ldquo;they/them\u0026rdquo; and \u0026ldquo;you\u0026rdquo; when describing users or actors; avoid the use of \u0026ldquo;obviously\u0026rdquo;, \u0026ldquo;simply\u0026rdquo;, \u0026ldquo;easily\u0026rdquo; \u0026ndash; every reader has a different level of expertise and familiarity with key concepts/tools.\nüëéüö´ Don\u0026rsquo;t üëç‚úÖ Do To start, simply enter the following command: To start, enter the following command: Configuring this setting just requires a simple API call. Make an API call to configure this setting. The results, without this setting enabled, might look crazy. Your results may be inconsistent or unreliable without this setting enabled. Formatting \u0026amp; Punctuation # Markdown # All documentation is written using Markdown syntax in .md files. See this official Markdown Cheat Sheet for a quick introduction to the syntax.\nCode Blocks # Use ``` to wrap long code samples into a code block.\nThis is a code block. Headers # Use title casing for all header titles.\nCapitalize the first and last word. Capitalize adjectives, adverbs, nouns, pronouns, and subordinate conjuctions. Lowercase articles (a, an, the) and coordinating conjunctions (and, but,for, nor, or, so, yet). üëéüö´ Don\u0026rsquo;t üëç‚úÖ Do How to develop sentient ai How to Develop Sentient AI How to use the pachctl cli How to Use the Pachctl CLI Links # Use meaningful link descriptions, such as the original article\u0026rsquo;s title or a one-line summarization of its contents.\nExamples:\n- See the [Pachyderm Technical Documentation Style Guide](../docs-style-guide) - Use the [official Pachyderm style guide](../docs-style-guide). Lists # Use numbered lists for sequential items, such as instructions. Use unbulleted lists for all other list types (like this list). Commas # Use the serial or Oxford comma in a list of three or more items to minimize any chance of confusion.\nüëéüö´ Don\u0026rsquo;t üëç‚úÖ Do I like swimming, biking and singing. I like swimming, biking, and singing. I only trust my parents, Madonna and Shakira. I only trust my parents, Madonna, and Shakira. UI Elements # Bold UI elements when mentioned in a set of instructions (a numbered list).\nNavigate to Settings \u0026gt; Desktop. Scroll to Push Notifications. Organization # Use Nested Headers # Remember to use all header sizes (h1-h6) to organize information. Each header should be a subset of topics or examples more narrow in scope than its parents. This enables readers to both gain more context and mentally parse the information at a glance.\nPublish Modular Topics # Avoid mixing objectives or use cases in one article; instead, organize and separate your content so that it is task-based. If there are many substasks (such as in a long-form tutorial), organize your content so that each major step is itself an article.\nExamples:\nThe below outlines are 4 articles, with the parent article linking to each modular sub-topic.\nHow to Locally Deploy Pachyderm MacOS Local Deployment Guide Linux Local Deployment Guide Windows Local Deployment Guide Images \u0026amp; Diagrams # Visualizations are helpful in learning complex workflows and UIs; however, they can expire quickly and take a lot of effort to maintain. Ask yourself the following questions when deciding whether or not to add visualizations:\nIs the user interface complex enough to warrant a screenshot? Can a diagram convey this concept more efficiently than words? How frequently does this visual need to be updated? Also, remember to add alt-text to your visualizations for screen readers.\nMermaid # You can build diagrams using mermaid.js in our documentation by:\nAdding mermaid: true to the frontmatter of an article. Drafting a diagram, like so: ```mermaid graph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D; ``` graph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D; Want to make an update to this style guide? Select Edit on Github and leave a suggestion as a pull request!\n",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": ["developers"],
    "id": "8bd55067daa32c4ab34857641369091d"
  },
  {
    "title": "Shared",
    "version": "latest",
    "isLatest": "",
    "pageKind": "section",
    "parent": "Latest",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/shared/",
    "relURI": "/latest/shared/",
    "body": "",
    "beta": "<no value>",
    "hidden": "true",
    "categories": [],
    "tags": [],
    "id": "47bd70d8dddba41bf41003de92bb469d"
  },
  {
    "title": "Log in",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Shared",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/shared/auth/log-in/",
    "relURI": "/latest/shared/auth/log-in/",
    "body": " How to Log in to a Cluster via IdP # Open a terminal. Input the following command: pachctl auth login Select the connector you wish to use. Provide your credentials ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "c56f4b221f1a070f887407ebde497d52"
  },
  {
    "title": "Docker",
    "version": "latest",
    "isLatest": "",
    "pageKind": "page",
    "parent": "Shared",
    "description": "",
    "date": "January 1, 1",
    "uri": "https://docs.pachyderm.com/latest/shared/setup/docker/",
    "relURI": "/latest/shared/setup/docker/",
    "body": " Before You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:\nOpen a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each of the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; All installation steps after 1. Install Docker Desktop must be run through the WSL terminal (Ubuntu) and not in Powershell.\nYou are now ready to continue to Step 1.\nYou must have Homebrew installed. /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; 1. Install Docker Desktop # Install Docker Desktop for your machine. Navigate to Settings for Mac, Windows, or Linux. Adjust your resources (~4 CPUs and ~12GB Memory) Enable Kubernetes Select Apply \u0026amp; Restart. 2. Install Pachctl CLI # Operating System: MacOs, Windows, \u0026amp; Darwin Debian brew tap pachyderm/tap \u0026amp;\u0026amp; brew install pachyderm/tap/pachctl@2.6 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.6.1/pachctl_2.6.1_amd64.deb \u0026amp;\u0026amp; sudo dpkg -i /tmp/pachctl.deb 3. Install \u0026amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install Pachyderm: Version: Community Edition Enterprise helm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:\nhelm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --set pachd.enterpriseLicenseKey=$(cat license.txt) --set ingress.host=localhost A mock user is created by default to get you started, with the username: admin and password: password.\nThis may take several minutes to complete.\n4. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 5. Connect to Cluster # pachctl connect http://localhost:80 ‚ÑπÔ∏è If the connection commands did not work together, run each separately.\nOptionally open your browser and navigate to the Console UI.\nüí° You can check your Pachyderm version and connection to pachd at any time with the following command:\npachctl version COMPONENT VERSION pachctl 2.6.1 pachd 2.6.1 ",
    "beta": "<no value>",
    "hidden": "<no value>",
    "categories": [],
    "tags": [],
    "id": "187a9cb23d07d4bc6db6423274bd7f79"
  }
]
