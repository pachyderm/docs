syntax = "proto3";
syntax = "proto3";

package transaction_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/transaction";

import "google/protobuf/empty.proto";
import "google/protobuf/timestamp.proto";

import "gogoproto/gogo.proto";

import "pfs/pfs.proto";
import "pps/pps.proto";

message DeleteAllRequest {
}

message TransactionRequest {
  // Exactly one of these fields should be set
  pfs_v2.CreateRepoRequest create_repo = 1;
  pfs_v2.DeleteRepoRequest delete_repo = 2;
  pfs_v2.StartCommitRequest start_commit = 3;
  pfs_v2.FinishCommitRequest finish_commit = 4;
  pfs_v2.SquashCommitSetRequest squash_commit_set = 5;
  pfs_v2.CreateBranchRequest create_branch = 6;
  pfs_v2.DeleteBranchRequest delete_branch = 7;
  pps_v2.UpdateJobStateRequest update_job_state = 8;
  pps_v2.CreatePipelineRequest create_pipeline = 9;
  pps_v2.StopJobRequest stop_job = 10;
}

message TransactionResponse {
  // At most, one of these fields should be set (most responses are empty)
  pfs_v2.Commit commit = 1; // Only used for StartCommit - any way we can deterministically provide this before finishing the transaction?
}

message Transaction {
  string id = 1 [(gogoproto.customname) = "ID"];
}

message TransactionInfo {
  Transaction transaction = 1;
  repeated TransactionRequest requests = 2;
  repeated TransactionResponse responses = 3;
  google.protobuf.Timestamp started = 4;
  uint64 version = 5;
}

message TransactionInfos {
  repeated TransactionInfo transaction_info = 1;
}

message BatchTransactionRequest {
  repeated TransactionRequest requests = 1;
}

message StartTransactionRequest {
}

message InspectTransactionRequest {
  Transaction transaction = 1;
}

message DeleteTransactionRequest {
  Transaction transaction = 1;
}

message ListTransactionRequest {
}

message FinishTransactionRequest {
  Transaction transaction = 1;
}

service API {
  // Transaction rpcs
  rpc BatchTransaction(BatchTransactionRequest) returns (TransactionInfo) {}
  rpc StartTransaction(StartTransactionRequest) returns (Transaction) {}
  rpc InspectTransaction(InspectTransactionRequest) returns (TransactionInfo) {}
  rpc DeleteTransaction(DeleteTransactionRequest) returns (google.protobuf.Empty) {}
  rpc ListTransaction(ListTransactionRequest) returns (TransactionInfos) {}
  rpc FinishTransaction(FinishTransactionRequest) returns (TransactionInfo) {}
  rpc DeleteAll(DeleteAllRequest) returns (google.protobuf.Empty) {}
}

syntax = "proto3";

package proxy;
option go_package = "github.com/pachyderm/pachyderm/v2/src/proxy";

message ListenRequest {
  string channel = 1;
}

message ListenResponse {
  string extra = 1;
}

service API {
  // Listen streams database events. 
  // It signals that it is internally set up by sending an initial empty ListenResponse.
  rpc Listen(ListenRequest) returns (stream ListenResponse) {}
}

syntax = "proto3";

package license_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/license";

import "google/protobuf/timestamp.proto";
import "gogoproto/gogo.proto";
import "enterprise/enterprise.proto";
import "protoextensions/log.proto";

message ActivateRequest {
  // activation_code is a Pachyderm enterprise activation code. New users can
  // obtain trial activation codes
  string activation_code = 1 [(log.half) = true];

  // expires is a timestamp indicating when this activation code will expire.
  // This should not generally be set (it's primarily used for testing), and is
  // only applied if it's earlier than the signed expiration time in
  // 'activation_code'.
  google.protobuf.Timestamp expires = 2;
}

message ActivateResponse {
  enterprise_v2.TokenInfo info = 1;
}

message GetActivationCodeRequest {}

message GetActivationCodeResponse {
  enterprise_v2.State state = 1;
  enterprise_v2.TokenInfo info = 2;
  string activation_code = 3 [(log.half) = true];
}

message DeactivateRequest{}
message DeactivateResponse{}

message AddClusterRequest {
  // id is the unique, immutable identifier for this cluster
  string id = 1;
  // address is the public GPRC address where the cluster can be reached
  string address = 2;
  // If set, secret specifies the shared secret this cluster will use
  // to authenticate to the license server. Otherwise a secret will be
  // generated and returned in the response.
  string secret = 3 [(log.half) = true];
  // The pachd address for users to connect to.
  string user_address = 4;
  // The deployment ID value generated by each Cluster
  string cluster_deployment_id = 5;
  // This field indicates whether the address points to an enterprise server
  bool enterprise_server = 6;
}

message AddClusterResponse {
  string secret = 1 [(log.half) = true];
}

message DeleteClusterRequest {
  string id = 1;
}
message DeleteClusterResponse {}

message ClusterStatus {
  string id = 1;
  string address = 2;
  string version = 3;
  bool auth_enabled = 4 [(gogoproto.moretags) = "db:\"auth_enabled\""];
  string client_id = 7 [(gogoproto.moretags) = "db:\"client_id\""];
  google.protobuf.Timestamp last_heartbeat = 5 [(gogoproto.moretags) = "db:\"last_heartbeat\"", (gogoproto.stdtime) = true];
  google.protobuf.Timestamp created_at = 6 [(gogoproto.moretags) = "db:\"created_at\"", (gogoproto.stdtime) = true];
}

// Note: Updates of the enterprise-server field are not allowed. In the worst case, a user can recreate their cluster if they need the field updated.
message UpdateClusterRequest {
  string id = 1;
  string address = 2;
  string user_address = 3;
  string cluster_deployment_id = 4;
  string secret = 5;
}
message UpdateClusterResponse{}

message ListClustersRequest {}
message ListClustersResponse {
  repeated ClusterStatus clusters = 1;
}

message DeleteAllRequest{}
message DeleteAllResponse {}

message HeartbeatRequest {
  string id = 1;
  string secret = 2 [(log.half) = true];
  string version = 3;
  bool auth_enabled = 4;
  string client_id = 5;
}

message HeartbeatResponse {
  enterprise_v2.LicenseRecord license = 1;
}

message UserClusterInfo {
  string id = 1 [(gogoproto.moretags) = "db:\"id\""];
  string cluster_deployment_id = 2 [(gogoproto.moretags) = "db:\"cluster_deployment_id\""];
  string address = 3 [(gogoproto.moretags) = "db:\"user_address\""];
  bool enterprise_server = 4 [(gogoproto.moretags) = "db:\"is_enterprise_server\""];
}

message ListUserClustersRequest {}

message ListUserClustersResponse {
  repeated UserClusterInfo clusters = 1;
}

service API {
  // Activate enables the license service by setting the enterprise activation
  // code to serve.
  rpc Activate(ActivateRequest) returns (ActivateResponse) {}
  rpc GetActivationCode(GetActivationCodeRequest) returns (GetActivationCodeResponse) {}

  // DeleteAll deactivates the server and removes all data.
  rpc DeleteAll(DeleteAllRequest) returns (DeleteAllResponse) {}

  // CRUD operations for the pachds registered with this server.
  rpc AddCluster(AddClusterRequest) returns (AddClusterResponse) {}
  rpc DeleteCluster(DeleteClusterRequest) returns (DeleteClusterResponse) {}
  rpc ListClusters(ListClustersRequest) returns (ListClustersResponse) {}
  rpc UpdateCluster(UpdateClusterRequest) returns (UpdateClusterResponse) {}

  // Heartbeat is the RPC registered pachds make to the license server
  // to communicate their status and fetch updates.
  rpc Heartbeat(HeartbeatRequest) returns (HeartbeatResponse) {}

  // Lists all clusters available to user
  rpc ListUserClusters(ListUserClustersRequest) returns (ListUserClustersResponse) {}
}

syntax = "proto3";

package identity_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/identity";

import "google/protobuf/timestamp.proto";
import "google/protobuf/struct.proto";
import "gogoproto/gogo.proto";

import "protoextensions/log.proto";

// User represents an IDP user that has authenticated via OIDC
message User {
  string email = 1;
  google.protobuf.Timestamp last_authenticated = 2 [(gogoproto.moretags) = "db:\"last_authenticated\"", (gogoproto.stdtime) = true];
}

// IdentityServerConfig is the configuration for the identity web server.
// When the configuration is changed the web server is reloaded automatically.
message IdentityServerConfig {
  string issuer = 1;
  string id_token_expiry = 2 [(gogoproto.moretags) = "db:\"id_token_expiry\""];
  string rotation_token_expiry = 3 [(gogoproto.moretags) = "db:\"rotation_token_expiry\""];
}

message SetIdentityServerConfigRequest {
  IdentityServerConfig config = 1;
}

message SetIdentityServerConfigResponse {}


message GetIdentityServerConfigRequest {}

message GetIdentityServerConfigResponse {
  IdentityServerConfig config = 1;
}

// IDPConnector represents a connection to an identity provider
message IDPConnector {
  // ID is the unique identifier for this connector.
  string id = 1;

  // Name is the human-readable identifier for this connector,
  // which will be shown to end users when they're authenticating.
  string name = 2;

  // Type is the type of the IDP ex. `saml`, `oidc`, `github`.
  string type = 3;

  // ConfigVersion must be incremented every time a connector is
  // updated, to avoid concurrent updates conflicting.
  int64 configVersion = 4;

  // This is left for backwards compatibility, but we want users to use the config defined below.
  string jsonConfig = 5 [(log.mask) = true];

  // Config is the configuration for the upstream IDP, which varies based on the type.
  // We make the assumption that this is either yaml or JSON.
  google.protobuf.Struct config = 6 [(log.mask) = true];
}

message CreateIDPConnectorRequest {
  IDPConnector connector = 1;
}

message CreateIDPConnectorResponse {}

message UpdateIDPConnectorRequest {
  IDPConnector connector = 1;
}

message UpdateIDPConnectorResponse {}

message ListIDPConnectorsRequest {}

message ListIDPConnectorsResponse {
  repeated IDPConnector connectors = 1;
}

message GetIDPConnectorRequest {
  string id = 1;
}

message GetIDPConnectorResponse {
  IDPConnector connector = 1;
}

message DeleteIDPConnectorRequest {
  string id = 1;
}

message DeleteIDPConnectorResponse {}

message OIDCClient {
  string id = 1;
  repeated string redirect_uris = 2;
  repeated string trusted_peers = 3;
  string name = 4;
  string secret = 5 [(log.mask) = true];
}

message CreateOIDCClientRequest {
  OIDCClient client = 1;
}

message CreateOIDCClientResponse {
  OIDCClient client = 1;
}

message GetOIDCClientRequest {
  string id = 1;
}

message GetOIDCClientResponse {
  OIDCClient client = 1;
}

message ListOIDCClientsRequest {}

message ListOIDCClientsResponse {
  repeated OIDCClient clients = 1;
}

message UpdateOIDCClientRequest {
  OIDCClient client = 1;
}

message UpdateOIDCClientResponse {}

message DeleteOIDCClientRequest {
  string id = 1;
}

message DeleteOIDCClientResponse {}

message DeleteAllRequest {}
message DeleteAllResponse {}

service API {
  rpc SetIdentityServerConfig(SetIdentityServerConfigRequest) returns (SetIdentityServerConfigResponse) {}
  rpc GetIdentityServerConfig(GetIdentityServerConfigRequest) returns (GetIdentityServerConfigResponse) {}
  rpc CreateIDPConnector(CreateIDPConnectorRequest) returns (CreateIDPConnectorResponse) {}
  rpc UpdateIDPConnector(UpdateIDPConnectorRequest) returns (UpdateIDPConnectorResponse) {}
  rpc ListIDPConnectors(ListIDPConnectorsRequest) returns (ListIDPConnectorsResponse) {}
  rpc GetIDPConnector(GetIDPConnectorRequest) returns (GetIDPConnectorResponse) {}
  rpc DeleteIDPConnector(DeleteIDPConnectorRequest) returns (DeleteIDPConnectorResponse) {}
  rpc CreateOIDCClient(CreateOIDCClientRequest) returns (CreateOIDCClientResponse) {}
  rpc UpdateOIDCClient(UpdateOIDCClientRequest) returns (UpdateOIDCClientResponse) {}
  rpc GetOIDCClient(GetOIDCClientRequest) returns (GetOIDCClientResponse) {}
  rpc ListOIDCClients(ListOIDCClientsRequest) returns (ListOIDCClientsResponse) {}
  rpc DeleteOIDCClient(DeleteOIDCClientRequest) returns (DeleteOIDCClientResponse) {}
  rpc DeleteAll(DeleteAllRequest) returns (DeleteAllResponse) {}
}

syntax = "proto3";

package enterprise_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/enterprise";

import "google/protobuf/timestamp.proto";

import "protoextensions/log.proto";

// Enterprise data structures

// LicenseRecord is the record we store in etcd for a Pachyderm enterprise
// token that has been provided to a Pachyderm license server
message LicenseRecord {
  string activation_code = 1 [(log.half) = true];

  google.protobuf.Timestamp expires = 2;
}

// EnterpriseConfig is the configuration we store for heartbeating
// to the license server.
message EnterpriseConfig {
  // license_server is the address of the grpc license service
  string license_server = 1;
  // id is the unique identifier for this pachd, which is registered
  // with the license service
  string id = 2;
  // secret is a shared secret between this pachd and the license service
  string secret = 3;
}

// EnterpriseRecord is a protobuf we cache in etcd to store the
// enterprise status.
message EnterpriseRecord {
  // license is the cached LicenseRecord retrieved from the most recent
  // heartbeat to the license server.
  LicenseRecord license = 1;

  // last_heartbeat is the timestamp of the last successful heartbeat
  // to the license server
  google.protobuf.Timestamp last_heartbeat = 2;

  // heartbeat_failed is set if the license is still valid, but
  // the pachd is no longer registered with an enterprise server.
  // This is the same as the expired state, where auth is locked
  // but not disabled.
  bool heartbeat_failed = 3;
}

enum State {
  NONE = 0;
  ACTIVE = 1;
  EXPIRED = 2;
  HEARTBEAT_FAILED = 3;
}

// TokenInfo contains information about the currently active enterprise token
message TokenInfo {
  // expires indicates when the current token expires (unset if there is no
  // current token)
  google.protobuf.Timestamp expires = 1;
}

//// Enterprise Activation API

message ActivateRequest {
  string license_server = 1;
  string id = 2;
  string secret = 3 [(log.half) = true];
}
message ActivateResponse {}

message GetStateRequest {}

message GetStateResponse {
  State state = 1;
  TokenInfo info = 2;

  // activation_code will always be an empty string,
  // call GetEnterpriseCode to get the activation code
  string activation_code = 3 [(log.half) = true];
}

message GetActivationCodeRequest {}

message GetActivationCodeResponse {
  State state = 1;
  TokenInfo info = 2;
  string activation_code = 3 [(log.half) = true];
}

// Heartbeat in the enterprise service just triggers a heartbeat for
// testing purposes. The RPC used to communicate with the license
// service is defined in the license service.
message HeartbeatRequest{}
message HeartbeatResponse{}

message DeactivateRequest{}
message DeactivateResponse{}

message PauseRequest{}
message PauseResponse{}

message UnpauseRequest{}
message UnpauseResponse{}

message PauseStatusRequest{}
message PauseStatusResponse{
  enum PauseStatus {
    UNPAUSED = 0;
    PARTIALLY_PAUSED = 1;
    PAUSED = 2;
  }
  PauseStatus status = 1;
}

service API {
  // Provide a Pachyderm enterprise token, enabling Pachyderm enterprise
  // features, such as the Pachyderm Dashboard and Auth system
  rpc Activate(ActivateRequest) returns (ActivateResponse) {}
  rpc GetState(GetStateRequest) returns (GetStateResponse) {}
  rpc GetActivationCode(GetActivationCodeRequest) returns (GetActivationCodeResponse) {}

  // Heartbeat is used in testing to trigger a heartbeat on demand. Normally this happens
  // on a timer.
  rpc Heartbeat(HeartbeatRequest) returns (HeartbeatResponse) {}

  // Deactivate removes a cluster's enterprise activation
  // token and sets its enterprise state to NONE.
  rpc Deactivate(DeactivateRequest) returns (DeactivateResponse) {}

  // Pause pauses the cluster.
  rpc Pause(PauseRequest) returns (PauseResponse) {}
  // Unpause unpauses the cluser.
  rpc Unpause(UnpauseRequest) returns (UnpauseResponse) {}
  rpc PauseStatus(PauseStatusRequest) returns (PauseStatusResponse) {}
}

syntax = "proto3";

package auth_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/auth";

import "gogoproto/gogo.proto";
import "google/protobuf/timestamp.proto";
import "protoextensions/log.proto";

/* A note on users
 *
 * In Pachyderm, usernames are structured strings. This makes both
 * our API and our data model more flexible (at the loss of some type safety).
 * Basically, anywhere that Pachyderm refers to a subject (i.e. TokenInfo) or
 * principal (ACL, the 'admins' collection), that username will have some
 * structured prefix.
 *
 * Note that externally-facing principals ({Get,Set}{Scope,ACL}, ModifyAdmins,
 * ListAdmins) will have their own conventions
 *
 * The current user formats are:
 * 1) Users synced from an identity provider:
 *      "user:username"
 * 2) Pachyderm robot users:
 *      "robot:robot_user_1"
 * 3) Pachyderm pipelines:
 *      "pipeline:terasort"
 */

//// Activation API

// ActivateRequest enables authentication on the cluster. It issues an auth token
// with no expiration for the irrevocable admin user `pach:root`.
message ActivateRequest {
  // If set, this token is used as the root user login token. Otherwise the root token
  // is randomly generated and returned in the response.
  string root_token = 1 [(log.mask) = true];
}

message ActivateResponse {
  // pach_token authenticates the caller with Pachyderm (if you want to perform
  // Pachyderm operations after auth has been activated as themselves, you must
  // present this token along with your regular request)
  string pach_token = 1 [(log.mask) = true];
}

message DeactivateRequest {}
message DeactivateResponse {}

message RotateRootTokenRequest {
  // root_token is used as the new root token value. If it's unset, then a token will be auto-generated.
  string root_token = 1 [(log.mask) = true];
}

message RotateRootTokenResponse {
  string root_token = 1 [(log.mask) = true];
}

// Configure Pachyderm's auth system with an OIDC provider
message OIDCConfig{
  string issuer = 1;
  string client_id = 2 [(gogoproto.customname) = "ClientID"];
  string client_secret = 3 [(log.mask) = true];
  string redirect_uri = 4 [(gogoproto.customname) = "RedirectURI"];
  repeated string scopes = 5;
  bool require_email_verified = 6;

 // localhost_issuer ignores the contents of the issuer claim and makes all
 // OIDC requests to the embedded OIDC provider. This is necessary to support
 // some network configurations like Minikube.
 bool localhost_issuer = 7;

  // user_accessible_issuer_host can be set to override the host used
  // in the OAuth2 authorization URL in case the OIDC issuer isn't
  // accessible outside the cluster. This requires a fully formed URL with scheme of either http or https.
  // This is necessary to support some configurations like Minikube.
  string user_accessible_issuer_host = 8;
}

message GetConfigurationRequest {}
message GetConfigurationResponse {
  OIDCConfig configuration = 1;
}
message SetConfigurationRequest {
  OIDCConfig configuration = 1;
}
message SetConfigurationResponse {}

//// Authentication data structures

// TokenInfo is the 'value' of an auth token 'key' in the 'tokens' collection
message TokenInfo {
  // Subject (i.e. Pachyderm account) that a given token authorizes.
  // See the note at the top of the doc for an explanation of subject structure.
  string subject = 1;
  google.protobuf.Timestamp expiration = 2 [(gogoproto.moretags) = "db:\"expiration\"", (gogoproto.stdtime) = true];
  string hashed_token = 3 [(gogoproto.moretags) = "db:\"token_hash\""];
}

//// Authentication API

message AuthenticateRequest {
  // Exactly one of 'id_token' or 'one_time_password' must be set:

  // This is the session state that Pachyderm creates in order to keep track of
  // information related to the current OIDC session.
  string oidc_state = 1 [(gogoproto.customname) = "OIDCState", (log.half) = true];

  // This is an ID Token issued by the OIDC provider.
  string id_token = 2 [(log.half) = true];
}

message AuthenticateResponse {
  // pach_token authenticates the caller with Pachyderm (if you want to perform
  // Pachyderm operations after auth has been activated as themselves, you must
  // present this token along with your regular request)
  string pach_token = 1 [(log.mask) = true];
}

message WhoAmIRequest {}

message WhoAmIResponse {
  string username = 1;
  google.protobuf.Timestamp expiration = 2 [(gogoproto.moretags) = "db:\"expiration\"", (gogoproto.stdtime) = true];
}

message GetRolesForPermissionRequest {
  Permission permission = 1;
}

message GetRolesForPermissionResponse {
  repeated Role roles = 1;
}

//// Authorization data structures

// Roles represents the set of roles a principal has
message Roles {
  map<string, bool> roles = 1;
}

// RoleBinding represents the set of roles principals have on a given Resource
message RoleBinding {
  // principal -> roles. All principal names include the structured prefix indicating their type.
  map<string, Roles> entries = 1;
}

// Permission represents the ability to perform a given operation on a Resource
enum Permission {
  PERMISSION_UNKNOWN = 0;

  CLUSTER_MODIFY_BINDINGS                          = 100;
  CLUSTER_GET_BINDINGS                             = 101;
  CLUSTER_GET_PACHD_LOGS                           = 148;
  CLUSTER_GET_LOKI_LOGS                            = 150;

  CLUSTER_AUTH_ACTIVATE                            = 102;
  CLUSTER_AUTH_DEACTIVATE                          = 103;
  CLUSTER_AUTH_GET_CONFIG                          = 104;
  CLUSTER_AUTH_SET_CONFIG                          = 105;
  CLUSTER_AUTH_GET_ROBOT_TOKEN                     = 139;
  CLUSTER_AUTH_MODIFY_GROUP_MEMBERS                = 109;
  CLUSTER_AUTH_GET_GROUPS                          = 110;
  CLUSTER_AUTH_GET_GROUP_USERS                     = 111;
  CLUSTER_AUTH_EXTRACT_TOKENS                      = 112;
  CLUSTER_AUTH_RESTORE_TOKEN                       = 113;
  CLUSTER_AUTH_GET_PERMISSIONS_FOR_PRINCIPAL       = 141;
  CLUSTER_AUTH_DELETE_EXPIRED_TOKENS               = 140;
  CLUSTER_AUTH_REVOKE_USER_TOKENS                  = 142;
  CLUSTER_AUTH_ROTATE_ROOT_TOKEN                   = 147;

  CLUSTER_ENTERPRISE_ACTIVATE            = 114;
  CLUSTER_ENTERPRISE_HEARTBEAT           = 115;
  CLUSTER_ENTERPRISE_GET_CODE            = 116;
  CLUSTER_ENTERPRISE_DEACTIVATE          = 117;
  CLUSTER_ENTERPRISE_PAUSE               = 149;

  CLUSTER_IDENTITY_SET_CONFIG            = 118;
  CLUSTER_IDENTITY_GET_CONFIG            = 119;
  CLUSTER_IDENTITY_CREATE_IDP            = 120;
  CLUSTER_IDENTITY_UPDATE_IDP            = 121;
  CLUSTER_IDENTITY_LIST_IDPS             = 122;
  CLUSTER_IDENTITY_GET_IDP               = 123;
  CLUSTER_IDENTITY_DELETE_IDP            = 124;
  CLUSTER_IDENTITY_CREATE_OIDC_CLIENT    = 125;
  CLUSTER_IDENTITY_UPDATE_OIDC_CLIENT    = 126;
  CLUSTER_IDENTITY_LIST_OIDC_CLIENTS     = 127;
  CLUSTER_IDENTITY_GET_OIDC_CLIENT       = 128;
  CLUSTER_IDENTITY_DELETE_OIDC_CLIENT    = 129;

  CLUSTER_DEBUG_DUMP                     = 131;

  CLUSTER_LICENSE_ACTIVATE               = 132;
  CLUSTER_LICENSE_GET_CODE               = 133;
  CLUSTER_LICENSE_ADD_CLUSTER            = 134;
  CLUSTER_LICENSE_UPDATE_CLUSTER         = 135;
  CLUSTER_LICENSE_DELETE_CLUSTER         = 136;
  CLUSTER_LICENSE_LIST_CLUSTERS          = 137;

  // TODO(actgardner): Make k8s secrets into nouns and add an Update RPC
  CLUSTER_CREATE_SECRET  = 143;
  CLUSTER_LIST_SECRETS   = 144;
  SECRET_DELETE          = 145;
  SECRET_INSPECT         = 146;

  CLUSTER_DELETE_ALL             = 138;

  REPO_READ                   = 200;
  REPO_WRITE                  = 201;
  REPO_MODIFY_BINDINGS        = 202;
  REPO_DELETE                 = 203;
  REPO_INSPECT_COMMIT         = 204;
  REPO_LIST_COMMIT            = 205;
  REPO_DELETE_COMMIT          = 206;
  REPO_CREATE_BRANCH          = 207;
  REPO_LIST_BRANCH            = 208;
  REPO_DELETE_BRANCH          = 209;
  REPO_INSPECT_FILE           = 210;
  REPO_LIST_FILE              = 211;
  REPO_ADD_PIPELINE_READER    = 212;
  REPO_REMOVE_PIPELINE_READER = 213;
  REPO_ADD_PIPELINE_WRITER    = 214;

  PIPELINE_LIST_JOB     = 301;

  PROJECT_CREATE = 400;
  PROJECT_DELETE = 401;
  PROJECT_LIST_REPO = 402;
  PROJECT_CREATE_REPO = 403;
  PROJECT_MODIFY_BINDINGS = 404;
}

// ResourceType represents the type of a Resource
enum ResourceType {
  RESOURCE_TYPE_UNKNOWN = 0;
  CLUSTER   = 1;
  REPO      = 2;
  SPEC_REPO = 3;
  PROJECT   = 4;
}

// Resource represents any resource that has role-bindings in the system
message Resource {
  ResourceType type = 1;
  string name = 2;
}

message Users {
  map<string, bool> usernames = 1;
}

message Groups {
  map<string, bool> groups = 1;
}

message Role {
  string name = 1;
  repeated Permission permissions = 2;
  repeated ResourceType resource_types = 3;
}

//// Authorization API

message AuthorizeRequest {
  Resource resource = 1;

  // permissions are the operations the caller is attempting to perform
  repeated Permission permissions = 2;
}

message AuthorizeResponse {
  // authorized is true if the caller has the require permissions
  bool authorized = 1;

  // satisfied is the set of permission that the principal has
  repeated Permission satisfied = 2;

  // missing is the set of permissions that the principal lacks
  repeated Permission missing = 3;

  // principal is the principal the request was evaluated for
  string principal = 4;
}

// GetPermissions evaluates the current user's permissions on a resource
message GetPermissionsRequest {
  Resource resource = 1;
}

// GetPermissionsForPrincipal evaluates an arbitrary principal's permissions
// on a resource
message GetPermissionsForPrincipalRequest {
  Resource resource = 1;

  string principal = 2;
}

message GetPermissionsResponse {
  // permissions is the set of permissions the principal has
  repeated Permission permissions = 1;

  // roles is the set of roles the principal has
  repeated string roles = 2;
}


message ModifyRoleBindingRequest {
  // resource is the resource to modify the role bindings on
  Resource resource = 1;

  // principal is the principal to modify the roles binding for
  string principal = 2;

  // roles is the set of roles for principal - an empty list
  // removes all role bindings
  repeated string roles = 3;
}

message ModifyRoleBindingResponse {}

message GetRoleBindingRequest {
  Resource resource = 1;
}

message GetRoleBindingResponse {
  RoleBinding binding = 1;
}

//////////////////////////////
//// OIDC Data Structures ////
//////////////////////////////

// SessionInfo stores information associated with one OIDC authentication
// session (i.e. a single instance of a single user logging in). Sessions are
// short-lived and stored in the 'oidc-authns' collection, keyed by the OIDC
// 'state' token (30-character CSPRNG-generated string). 'GetOIDCLogin'
// generates and inserts entries, then /authorization-code/callback retrieves
// an access token from the ID provider and uses it to retrive the caller's
// email and store it in 'email', and finally Authorize() returns a Pachyderm
// token identified with that email address as a subject in Pachyderm.
message SessionInfo {
  // nonce is used by /authorization-code/callback to validate session
  // continuity with the IdP after a user has arrived there from GetOIDCLogin().
  // This is a 30-character CSPRNG-generated string.
  string nonce = 1 [(log.half) = true];
  // email contains the email adddress associated with a user in their OIDC ID
  // provider. Currently users are identified with their email address rather
  // than their OIDC subject identifier to make switching between OIDC ID
  // providers easier for users, and to make user identities more easily
  // comprehensible in Pachyderm. The OIDC spec doesn't require that users'
  // emails be present or unique, but we think this will be preferable in
  // practice.
  string email = 2;
  // conversion_err indicates whether an error was encountered while exchanging
  // an auth code for an access token, or while obtaining a user's email (in
  // /authorization-code/callback). Storing the error state here allows any
  // sibling calls to Authenticate() (i.e. using the same OIDC state token) to
  // notify their caller that an error has occurred. We avoid passing the caller
  // any details of the error (which are logged by Pachyderm) to avoid giving
  // information to a user who has network access to Pachyderm but not an
  // account in the OIDC provider.
  bool conversion_err = 3;
}

//// OIDC API

message GetOIDCLoginRequest {
}

message GetOIDCLoginResponse {
  // The login URL generated for the OIDC object
  string login_url = 1 [(gogoproto.customname) = "LoginURL", (log.half) = true];
  string state = 2 [(log.half) = true];
}

// Robot token API (TODO: add access controls)

message GetRobotTokenRequest {
  // The returned token will allow the caller to access resources as this
  // robot user
  string robot = 1;

  // ttl indicates the requested (approximate) remaining lifetime of this token,
  // in seconds
  int64 ttl = 2 [(gogoproto.customname) = "TTL"];
}

message GetRobotTokenResponse {
  // A new auth token for the requested robot
  string token = 1 [(log.mask) = true];
}

message RevokeAuthTokenRequest {
  string token = 1 [(log.half) = true];
}

message RevokeAuthTokenResponse {
  int64 number = 1;
}

message SetGroupsForUserRequest {
  string username = 1;
  repeated string groups = 2;
}

message SetGroupsForUserResponse {}

message ModifyMembersRequest {
  string group = 1;
  repeated string add = 2;
  repeated string remove = 3;
}

message ModifyMembersResponse {}

message GetGroupsRequest {}

message GetGroupsForPrincipalRequest {
  string principal = 1;
}

message GetGroupsResponse {
  repeated string groups = 1;
}

message GetUsersRequest {
  string group = 1;
}

message GetUsersResponse {
  repeated string usernames = 1;
}

// ExtractAuthTokens returns all the hashed robot tokens that have been issued.
// User tokens are not extracted as they can be recreated by logging in.
message ExtractAuthTokensRequest {}

message ExtractAuthTokensResponse {
  repeated TokenInfo tokens = 1;
}

// RestoreAuthToken inserts a hashed token that has previously been extracted.
message RestoreAuthTokenRequest {
  TokenInfo token = 1;
}

message RestoreAuthTokenResponse {}

message RevokeAuthTokensForUserRequest {
  string username = 1;
}

message RevokeAuthTokensForUserResponse {
  int64 number = 1;
}

message DeleteExpiredAuthTokensRequest {}

message DeleteExpiredAuthTokensResponse {}

service API {
  // Activate/Deactivate the auth API. 'Activate' sets an initial set of admins
  // for the Pachyderm cluster, and 'Deactivate' removes all ACLs, tokens, and
  // admins from the Pachyderm cluster, making all data publicly accessable
  rpc Activate(ActivateRequest) returns (ActivateResponse) {}
  rpc Deactivate(DeactivateRequest) returns (DeactivateResponse) {}

  rpc GetConfiguration(GetConfigurationRequest) returns (GetConfigurationResponse) {}
  rpc SetConfiguration(SetConfigurationRequest) returns (SetConfigurationResponse) {}

  rpc Authenticate(AuthenticateRequest) returns (AuthenticateResponse) {}
  rpc Authorize(AuthorizeRequest) returns (AuthorizeResponse) {}
  rpc GetPermissions(GetPermissionsRequest) returns (GetPermissionsResponse) {}
  rpc GetPermissionsForPrincipal(GetPermissionsForPrincipalRequest) returns (GetPermissionsResponse) {}
  rpc WhoAmI(WhoAmIRequest) returns (WhoAmIResponse) {}
  rpc GetRolesForPermission(GetRolesForPermissionRequest) returns (GetRolesForPermissionResponse) {}

  rpc ModifyRoleBinding(ModifyRoleBindingRequest) returns (ModifyRoleBindingResponse) {}
  rpc GetRoleBinding(GetRoleBindingRequest) returns (GetRoleBindingResponse) {}

  rpc GetOIDCLogin(GetOIDCLoginRequest) returns (GetOIDCLoginResponse) {}

  rpc GetRobotToken(GetRobotTokenRequest) returns (GetRobotTokenResponse) {}
  rpc RevokeAuthToken(RevokeAuthTokenRequest) returns (RevokeAuthTokenResponse) {}
  rpc RevokeAuthTokensForUser(RevokeAuthTokensForUserRequest) returns (RevokeAuthTokensForUserResponse) {}

  rpc SetGroupsForUser(SetGroupsForUserRequest) returns (SetGroupsForUserResponse) {}
  rpc ModifyMembers(ModifyMembersRequest) returns (ModifyMembersResponse) {}
  rpc GetGroups(GetGroupsRequest) returns (GetGroupsResponse) {}
  rpc GetGroupsForPrincipal(GetGroupsForPrincipalRequest) returns (GetGroupsResponse) {}
  rpc GetUsers(GetUsersRequest) returns (GetUsersResponse) {}

  rpc ExtractAuthTokens(ExtractAuthTokensRequest) returns (ExtractAuthTokensResponse) {}
  rpc RestoreAuthToken(RestoreAuthTokenRequest) returns (RestoreAuthTokenResponse) {}

  rpc DeleteExpiredAuthTokens(DeleteExpiredAuthTokensRequest) returns (DeleteExpiredAuthTokensResponse) {}
  rpc RotateRootToken(RotateRootTokenRequest) returns (RotateRootTokenResponse) {}
}

syntax = "proto3";

package admin_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/admin";

import "gogoproto/gogo.proto";
import "version/versionpb/version.proto";

message ClusterInfo {
  string id = 1 [(gogoproto.customname) = "ID"];
  string deployment_id = 2 [(gogoproto.customname) = "DeploymentID"];

  bool version_warnings_ok = 3; // Let the client detect a server that can't generate warnings.
  repeated string version_warnings = 4; // Warnings about version skew.

  string proxy_host = 5;
  bool proxy_tls = 6;
}

message InspectClusterRequest {
  versionpb_v2.Version client_version = 1;
}

service API {
  rpc InspectCluster(InspectClusterRequest) returns (ClusterInfo) {}
}

syntax = "proto3";

package metrics;
option go_package = "github.com/pachyderm/pachyderm/v2/src/internal/metrics";

import "gogoproto/gogo.proto";

message Metrics {
    string cluster_id              =  1 [(gogoproto.customname) = "ClusterID"];
    string pod_id                  =  2 [(gogoproto.customname) = "PodID"];
    int64 nodes                    =  3;
    string version                 =  4;
    int64 repos                    =  5; // Number of repos
    int64 commits                  =  6; // Number of commits -- not used
    int64 files                    =  7; // Number of files -- not used
    uint64 bytes                   =  8; // Number of bytes in all repos
    int64 jobs                     =  9; // Number of jobs
    int64 pipelines                = 10; // Number of pipelines in the cluster -- not the same as DAG
    int64 archived_commits         = 11; // Number of archived commit -- not used
    int64 cancelled_commits        = 12; // Number of cancelled commits -- not used
    string activation_code         = 13; // Activation code
    uint64 max_branches            = 14; // Max branches in across all the repos
    int64 pps_spout                = 15; // Number of spout pipelines
    int64 pps_spout_service        = 16; // Number of spout services
    reserved 17; // int64 pps_build
    int64 cfg_egress               = 18; // Number of pipelines with Egress configured
    int64 cfg_standby              = 19; // Number of pipelines with Standby congigured
    int64 cfg_s3gateway            = 20; // Number of pipelines with S3 Gateway configured
    int64 cfg_services             = 21; // Number of pipelines with services configured
    int64 cfg_errcmd               = 22; // Number of pipelines with error cmd set
    int64 cfg_tfjob                = 24; // Number of pipelines with TFJobs configured
    int64 input_group              = 25; // Number of pipelines with group inputs
    int64 input_join               = 26; // Number of pipelines with join inputs
    int64 input_cross              = 27; // Number of pipelines with cross inputs
    int64 input_union              = 28; // Number of pipelines with union inputs
    int64 input_cron               = 29; // Number of pipelines with cron inputs
    int64 input_git                = 30; // Number of pipelines with git inputs
    int64 input_pfs                = 31; // Number of pfs inputs
    int64 input_commit             = 32; // Number of pfs inputs with commits
    int64 input_join_on            = 33; // Number of pfs inputs with join_on
    int64 input_outer_join         = 34; // Number of pipelines with outer joins
    int64 input_lazy               = 35; // Number of pipelines with lazy set
    int64 input_empty_files        = 36; // Number of pipelines with empty files set
    int64 input_s3                 = 37; // Number of pipelines with S3 input
    int64 input_trigger            = 38; // Number of pipelines with triggers set
    float resource_cpu_req         = 39; // Total CPU request across all pipelines
    float resource_cpu_req_max     = 40; // Max CPU resource requests set
    string resource_mem_req        = 41; // Sting of memory requests set across all pipelines
    int64 resource_gpu_req         = 42; // Total GPU requests across all pipelines
    int64 resource_gpu_req_max     = 43; // Max GPU request across all pipelines
    string resource_disk_req       = 44; // String of disk requests set across all pipelines
    float resource_cpu_limit       = 45; // Total CPU limits set across all pipelines
    float resource_cpu_limit_max   = 46; // Max CPU limit set
    string resource_mem_limit      = 47; // String of memory limits set
    int64 resource_gpu_limit       = 48; // Number of pipelines with
    int64 resource_gpu_limit_max   = 49; // Max GPU limit set
    string resource_disk_limit     = 50; // String of disk limits set across all pipelines
    uint64 max_parallelism         = 51; // Max parallelism set
    uint64 min_parallelism         = 52; // Min parallelism set
    uint64 num_parallelism         = 53; // Number of pipelines with parallelism set
    int64 enterprise_failures      = 54; // Number of times a command has failed due to an enterprise check
}

syntax = "proto3";

package extended;
option go_package = "github.com/pachyderm/pachyderm/v2/src/internal/tracing/extended";

// TraceProto contains information identifying a Jaeger trace. It's used to
// propagate traces that follow the lifetime of a long operation (e.g. creating
// a pipeline or running a job), and which live longer than any single RPC.
message TraceProto {
  // serialized_trace contains the info identifying a trace in Jaeger (a
  // (trace ID, span ID, sampled) tuple, basically)
  map<string, string> serialized_trace = 1;

  string project = 3;
  // pipeline specifies the target pipeline of this trace; this would be set for
  // a trace created by 'pachctl create-pipeline' or 'pachctl update-pipeline'
  // and would include the kubernetes RPCs involved in creating a pipeline
  string pipeline = 2;
}

syntax = "proto3";

package common;
option go_package = "github.com/pachyderm/pachyderm/v2/src/internal/collection";

import "gogoproto/gogo.proto";

message TestItem {
  string id = 1 [(gogoproto.customname) = "ID"];
  string value = 2;
  string data = 3;
}

syntax = "proto3";

package ppsload;
option go_package = "github.com/pachyderm/pachyderm/v2/src/internal/ppsload";

import "pfs/pfs.proto";

message State {
  pfs_v2.Branch branch = 1;
  string pfs_state_id = 2;
}

syntax = "proto3";

package config_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/internal/config";

import "gogoproto/gogo.proto";

// Config specifies the pachyderm config that is read and interpreted by the
// pachctl command-line tool. Right now, this is stored at
// $HOME/.pachyderm/config.
//
// Different versions of the pachyderm config are specified as subfields of this
// message (this allows us to make significant changes to the config structure
// without breaking existing users by defining a new config version).
//
// These structures are stored in a JSON format, so it should be safe to modify
// fields as long as compatibility is ensured with previous versions.
message Config {
    string user_id = 1 [(gogoproto.customname) = "UserID"];

    // Configuration options. Exactly one of these fields should be set
    // (depending on which version of the config is being used)
    ConfigV1 v1 = 2;
    ConfigV2 v2 = 3;
}

// ConfigV1 specifies v1 of the pachyderm config (June 30 2017 - June 2019)
message ConfigV1 {
    // A host:port pointing pachd at a pachyderm cluster.
    string pachd_address = 1;

    // Trusted root certificates (overrides installed certificates), formatted
    // as base64-encoded PEM
    string server_cas = 2 [(gogoproto.customname) = "ServerCAs"];

    // A secret token identifying the current pachctl user within their
    // pachyderm cluster. This is included in all RPCs sent by pachctl, and used
    // to determine if pachctl actions are authorized.
    string session_token = 3;

    // The currently active transaction for batching together pachctl commands.
    // This can be set or cleared via many of the `pachctl * transaction` commands.
    // This is the ID of the transaction object stored in the pachyderm etcd.
    string active_transaction = 4;
}

// ConfigV2 specifies v2 of the pachyderm config (June 2019 - present)
message ConfigV2 {
    string active_context = 1;
    string active_enterprise_context = 2;
    map<string, Context> contexts = 3;
    bool metrics = 4;
    int64 max_shell_completions = 5;
}

message Context {
    // Where this context came from
    ContextSource source = 1;

    // The hostname or IP address pointing pachd at a pachyderm cluster.
    string pachd_address = 2;

    // Trusted root certificates (overrides installed certificates), formatted
    // as base64-encoded PEM.
    string server_cas = 3 [(gogoproto.customname) = "ServerCAs"];

    // A secret token identifying the current pachctl user within their
    // pachyderm cluster. This is included in all RPCs sent by pachctl, and used
    // to determine if pachctl actions are authorized.
    string session_token = 4;

    // The currently active transaction for batching together pachctl commands.
    // This can be set or cleared via many of the `pachctl * transaction` commands.
    // This is the ID of the transaction object stored in the pachyderm etcd.
    string active_transaction = 5;

    // The k8s cluster name - used to construct a k8s context.
    string cluster_name = 6;

    // The k8s auth info - used to construct a k8s context.
    string auth_info = 7;

    // The k8s namespace - used to construct a k8s context.
    string namespace = 8;

    // A mapping of service -> port number, when port forwarding is
    // running for this context.
    map<string, uint32> port_forwarders = 9;

    // A unique ID for the cluster deployment. At client initialization time,
    // we ensure this is the same as what the cluster reports back, to prevent
    // us from connecting to the wrong cluster.
    string cluster_deployment_id = 10 [(gogoproto.customname) = "ClusterDeploymentID"];

    // A boolean that records whether the context points at an enterprise server.
    // If false, the context points at a stand-alone pachd.
    bool enterprise_server = 11;

    // The current project.
    string project = 12;
}

enum ContextSource {
    NONE = 0;
    CONFIG_V1 = 1;
    HUB = 2;
    IMPORTED = 3;
}

syntax = "proto3";

package fileset;
option go_package = "github.com/pachyderm/pachyderm/v2/src/internal/storage/fileset";

import "internal/storage/fileset/index/index.proto";

message Metadata {
  oneof value {
    Primitive primitive = 1;
    Composite composite = 2;
  }
}

message Composite {
  repeated string layers = 1;
}

message Primitive {
  index.Index deletive = 1;
  index.Index additive = 2;
  int64 size_bytes = 3;
}

message TestCacheValue {
  string file_set_id = 1;
}

syntax = "proto3";

package index;
option go_package = "github.com/pachyderm/pachyderm/v2/src/internal/storage/fileset/index";

import "internal/storage/chunk/chunk.proto";

// Index stores an index to and metadata about a range of files or a file.
message Index {
  string path = 1;
  // NOTE: range and file are mutually exclusive.
  Range range = 2;
  File file = 3;
  // NOTE: num_files and size_bytes did not exist in older versions of 2.x, so
  // they will not be set.
  int64 num_files = 4;
  int64 size_bytes = 5;
}

message Range {
  int64 offset = 1;
  string last_path = 2;
  chunk.DataRef chunk_ref = 3;
}

message File {
  string datum = 1;
  repeated chunk.DataRef data_refs = 2;
}

syntax = "proto3";

package chunk;
option go_package = "github.com/pachyderm/pachyderm/v2/src/internal/storage/chunk";

// DataRef is a reference to data within a chunk.
message DataRef {
  // The chunk the referenced data is located in.
  Ref ref = 1;
  // The hash of the data being referenced.
  bytes hash = 2;
  // The offset and size used for accessing the data within the chunk.
  int64 offset_bytes = 3;
  int64 size_bytes = 4;
}

enum CompressionAlgo {
  NONE = 0;
  GZIP_BEST_SPEED = 1;  
}

enum EncryptionAlgo {
  ENCRYPTION_ALGO_UNKNOWN = 0;
  CHACHA20 = 1;
}

message Ref {
  bytes id = 1;
  int64 size_bytes = 2;
  bool edge = 3;

  bytes dek = 4;
  EncryptionAlgo encryption_algo = 5;
  CompressionAlgo compression_algo = 6;
}

syntax = "proto3";

package task;
option go_package = "github.com/pachyderm/pachyderm/v2/src/internal/task";

import "gogoproto/gogo.proto";
import "google/protobuf/any.proto";

enum State {
  STATE_UNKNOWN = 0;
  RUNNING = 1;
  SUCCESS = 2;
  FAILURE = 3;
}

message Group {}

// TODO: Consider splitting this up into separate structures for each state in a oneof.
message Task {
  string id = 1 [(gogoproto.customname) = "ID"];
  State state = 2;
  google.protobuf.Any input = 3;
  google.protobuf.Any output = 4;
  string reason = 5;
  int64 index = 6;
}

message Claim {}

message TestTask {
  string id = 1 [(gogoproto.customname) = "ID"];
}

syntax = "proto3";

package pfsload;
option go_package = "github.com/pachyderm/pachyderm/v2/src/internal/pfsload";

import "pfs/pfs.proto";

message CommitSpec {
  int64 count = 1;
  repeated ModificationSpec modifications = 2;
  repeated FileSourceSpec file_sources = 3;
  ValidatorSpec validator = 4;
}

message ModificationSpec {
  int64 count = 1;
  PutFileSpec put_file = 2;
}

message PutFileSpec {
  int64 count = 1;
  string source = 2;
}

message PutFileTask {
  int64 count = 1;
  FileSourceSpec file_source = 2;
  int64 seed = 3;
  string auth_token = 4;
}

message PutFileTaskResult {
  string file_set_id = 1;
  bytes hash = 2;
}

message FileSourceSpec {
  string name = 1;
  RandomFileSourceSpec random = 2;
}

message RandomFileSourceSpec {
  RandomDirectorySpec directory = 1;
  repeated SizeSpec sizes = 2;
  bool increment_path = 3;
}

message RandomDirectorySpec {
  SizeSpec depth = 1;
  int64 run = 2;
}

message SizeSpec {
  int64 min_size = 1 [json_name="min"];
  int64 max_size = 2 [json_name="max"];
  int64 prob = 3;
}

message ValidatorSpec {
  FrequencySpec frequency = 1;
}

message FrequencySpec {
  int64 count = 1;
  int64 prob = 2; 
}

message State {
  message Commit {
    pfs_v2.Commit commit = 1;
    bytes hash = 2;
  }
  repeated Commit commits = 1;
}

syntax = "proto3";

// Note: etc/proto/protoc-gen-zap/protoextensions needs a version of this file compiled with the
// non-gogo protobuf compiler: `protoc --proto_path=. --go_out=. src/protoextensions/log.proto`
// and then copy the generated file out of the github.com/... directory `mv
// github.com/pachyderm/pachyderm/v2/src/protoextensions/log.pb.go
// etc/proto/protoc-gen-zap/protoextensions/`.

package log;
option go_package = "github.com/pachyderm/pachyderm/v2/src/protoextensions";

import "google/protobuf/descriptor.proto";

extend google.protobuf.FieldOptions {
  bool mask = 50001;
  bool half = 50002;
}

syntax = "proto3";

package pfsserver;
option go_package = "github.com/pachyderm/pachyderm/v2/src/server/pfs/server";

import "internal/storage/fileset/index/index.proto";
import "pfs/pfs.proto";

message ShardTask {
  repeated string inputs = 1;
  PathRange path_range = 2;
}

message ShardTaskResult {
  repeated CompactTask compact_tasks = 1;
}

message PathRange {
  string lower = 1;
  string upper = 2;
}

message CompactTask {
  repeated string inputs = 1;
  PathRange path_range = 2;
}

message CompactTaskResult {
  string id = 1;
}

message ConcatTask {
  repeated string inputs = 1;
}

message ConcatTaskResult {
  string id = 1;
}

message ValidateTask {
  string id = 1;
  PathRange path_range = 2;
}

message ValidateTaskResult {
  index.Index first = 1;
  index.Index last = 2;
  string error = 3;
  int64 size_bytes = 4;
}

message PutFileURLTask {
  string dst = 1;
  string datum = 2;
  string URL = 3;
  repeated string paths = 4;
  int64 start_offset = 5;
  int64 end_offset = 7;
}

message PutFileURLTaskResult {
  string id = 1;
}

message GetFileURLTask {
  string URL = 1;
  pfs_v2.File file = 2;
  pfs_v2.PathRange path_range = 3;
}

message GetFileURLTaskResult {}

syntax = "proto3";

package pachyderm.worker.pipeline.transform;
option go_package = "github.com/pachyderm/pachyderm/v2/src/server/worker/pipeline/transform";

import "gogoproto/gogo.proto";

import "pfs/pfs.proto";
import "pps/pps.proto";
import "server/worker/datum/datum.proto";

message CreateParallelDatumsTask {
  pps_v2.Job job = 1;
  string salt = 2;
  string file_set_id = 3;
  string base_file_set_id = 4;
  pfs_v2.PathRange path_range = 5;
} 

message CreateParallelDatumsTaskResult {
  string file_set_id = 1;
  datum.Stats stats = 2;
}

message CreateSerialDatumsTask {
  pps_v2.Job job = 1;
  string salt = 2;
  string file_set_id = 3;
  pfs_v2.Commit base_meta_commit = 4;
  bool no_skip = 5;
  pfs_v2.PathRange path_range = 6;
}

message CreateSerialDatumsTaskResult {
  string file_set_id = 1;
  string output_delete_file_set_id = 2;
  string meta_delete_file_set_id = 3;
  datum.Stats stats = 4;
} 

message CreateDatumSetsTask {
  string file_set_id = 1;
  pfs_v2.PathRange path_range = 2;
  datum.SetSpec set_spec = 3;
} 

message CreateDatumSetsTaskResult {
  repeated pfs_v2.PathRange datum_sets = 1;
} 

message DatumSetTask {
  pps_v2.Job job = 1;
  string file_set_id = 2;
  pfs_v2.PathRange path_range = 3;
  pfs_v2.Commit output_commit = 4;
}

message DatumSetTaskResult {
  string output_file_set_id = 1;
  string meta_file_set_id = 2;
  datum.Stats stats = 3;
}


syntax = "proto3";

package datum;
option go_package = "github.com/pachyderm/pachyderm/v2/src/server/worker/datum";

import "gogoproto/gogo.proto";

import "pfs/pfs.proto";
import "pps/pps.proto";
import "server/worker/common/common.proto";

enum State {
  PROCESSED = 0;
  FAILED = 1;
  RECOVERED = 2;
}

message Meta {
  pps_v2.Job job = 1;
  repeated common.Input inputs = 2;
  string hash = 3;
  State state = 4;
  string reason = 5;
  pps_v2.ProcessStats stats = 6;
  int64 index = 7;
  string image_id = 8;
}

message Stats {
  pps_v2.ProcessStats process_stats = 1;
  int64 processed = 2;
  int64 skipped = 3;
  int64 total = 4;
  int64 failed = 5;
  int64 recovered = 6;
  string failed_id = 7 [(gogoproto.customname) = "FailedID"];
}

message PFSTask {
  pps_v2.PFSInput input = 1; 
  pfs_v2.PathRange path_range = 2;
  int64 base_index = 3;
  string auth_token = 4;
}

message PFSTaskResult {
  string file_set_id = 1;
}

message CrossTask {
  repeated string file_set_ids = 1;
  int64 base_file_set_index = 2;
  pfs_v2.PathRange base_file_set_path_range = 3;
  int64 base_index = 4;
  string auth_token = 5;
}

message CrossTaskResult {
  string file_set_id = 1;
}

message KeyTask {
  string file_set_id = 1;
  pfs_v2.PathRange path_range = 2;
  enum Type {
    JOIN = 0;
    GROUP = 1;
  }
  Type type = 3;
  string auth_token = 4;
}

message KeyTaskResult {
  string file_set_id = 1;
}

message MergeTask {
  repeated string file_set_ids = 1;
  pfs_v2.PathRange path_range = 2;
  enum Type {
    JOIN = 0;
    GROUP = 1;
  }
  Type type = 3;
  string auth_token = 4;
}

message MergeTaskResult {
  string file_set_id = 1;
}

message ComposeTask {
  repeated string file_set_ids = 1;
  string auth_token = 2;
}

message ComposeTaskResult {
  string file_set_id = 1;
}

message SetSpec {
  int64 number = 1;
  int64 size_bytes = 2;
}

syntax = "proto3";

package common;
option go_package = "github.com/pachyderm/pachyderm/v2/src/server/worker/common";

import "pfs/pfs.proto";
import "gogoproto/gogo.proto";

message Input {
  reserved 2;

  pfs_v2.FileInfo file_info = 1;
  string name = 3;
  string join_on = 4;
  bool outer_join = 5;
  string group_by = 6;
  bool lazy = 7;
  string branch = 8;
  string git_url = 9 [(gogoproto.customname) = "GitURL"];
  bool empty_files = 10;
  bool s3 = 11; // If set, workers won't create an input directory for this input
}

syntax = "proto3";

package pfs_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/pfs";

import "google/protobuf/empty.proto";
import "google/protobuf/timestamp.proto";
import "google/protobuf/wrappers.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/any.proto";

import "gogoproto/gogo.proto";

import "auth/auth.proto";

import "task/task.proto";

////  PFS Data structures (stored in etcd)

message Repo {
  string name = 1;
  string type = 2;
  Project project = 3;
}

message Branch {
  Repo repo = 1;
  string name = 2;
}

message File {
  Commit commit = 1;
  string path = 2;
  string datum = 3;
}

// RepoInfo is the main data structure representing a Repo in etcd
message RepoInfo {
  Repo repo = 1;
  google.protobuf.Timestamp created = 2;
  int64 size_bytes_upper_bound = 3;
  string description = 4;
  repeated Branch branches = 5;

  // Set by ListRepo and InspectRepo if Pachyderm's auth system is active, but
  // not stored in etcd. To set a user's auth scope for a repo, use the
  // Pachyderm Auth API (in src/client/auth/auth.proto)
  AuthInfo auth_info = 6;

  // Details are only provided when explicitly requested
  message Details {
    int64 size_bytes = 1;
  }
  Details details = 7;
}

// AuthInfo includes the caller's access scope for a resource, and is returned
// by services like ListRepo, InspectRepo, and ListProject, but is not persisted in the database.
// It's used by the Pachyderm dashboard to render repo access appropriately.
// To set a user's auth scope for a resource, use the Pachyderm Auth API (in src/auth/auth.proto)
message AuthInfo {
  // The callers access level to the relevant resource. These are very granular
  // permissions - for the end user it makes sense to show them the roles
  // they have instead.
  repeated auth_v2.Permission permissions = 1;

  // The caller's roles on the relevant resource. This includes inherited
  // roles from the cluster, project, group membership, etc.
  repeated string roles = 2;
}

message BranchInfo {
  Branch branch = 1;
  Commit head = 2;
  repeated Branch provenance = 3;
  repeated Branch subvenance = 4;
  repeated Branch direct_provenance = 5;
  Trigger trigger = 6;
}

// Trigger defines the conditions under which a head is moved, and to which
// branch it is moved.
message Trigger {
  // Which branch this trigger refers to
  string branch = 1;
  // All indicates that all conditions must be satisfied before the trigger
  // happens, otherwise any conditions being satisfied will trigger it.
  bool all = 2;
  // Triggers if the cron spec has been satisfied since the last trigger and
  // there's been a new commit.
  string cron_spec = 3;
  // Triggers if there's been `size` new data added since the last trigger.
  string size = 4;
  // Triggers if there's been `commits` new commits added since the last trigger.
  int64 commits = 5;
}

// These are the different places where a commit may be originated from
enum OriginKind {
  ORIGIN_KIND_UNKNOWN = 0;
  USER = 1;
  AUTO = 2;
  FSCK = 3;
}

message CommitOrigin {
  OriginKind kind = 1;
}
// Commit is a reference to a commit (e.g. the collection of branches and the
// collection of currently-open commits in etcd are collections of Commit
// protos)
message Commit {
  Repo repo = 3;
  string id = 2 [(gogoproto.customname) = "ID"];
  // only used by the client
  Branch branch = 1;
}

// CommitInfo is the main data structure representing a commit in etcd
message CommitInfo {
  reserved 9;
  Commit commit = 1;
  CommitOrigin origin = 2;
  // description is a user-provided script describing this commit
  string description = 3;
  Commit parent_commit = 4;
  repeated Commit child_commits = 5;
  google.protobuf.Timestamp started = 6;
  google.protobuf.Timestamp finishing = 7;
  google.protobuf.Timestamp finished = 8;
  repeated Commit direct_provenance = 13;
  string error = 10;
  int64 size_bytes_upper_bound = 11;

  // Details are only provided when explicitly requested
  message Details {
    int64 size_bytes = 1;
    google.protobuf.Duration compacting_time = 2;
    google.protobuf.Duration validating_time = 3;
  }
  Details details = 12;
}

message CommitSet {
  string id = 1 [(gogoproto.customname) = "ID"];
}

message CommitSetInfo {
  CommitSet commit_set = 1;
  repeated CommitInfo commits = 2;
}

enum FileType {
  RESERVED = 0;
  FILE = 1;
  DIR = 2;
}

message FileInfo {
  File file = 1;
  FileType file_type = 2;
  google.protobuf.Timestamp committed = 3;
  int64 size_bytes = 4;
  bytes hash = 5;
}

message Project {
  string name = 1;
}

message ProjectInfo {
  Project project = 1;
  string description = 2;
  AuthInfo auth_info = 3;
  google.protobuf.Timestamp created_at = 4;
}

// PFS API

message CreateRepoRequest {
  Repo repo = 1;
  string description = 2;
  bool update = 3;
}

message InspectRepoRequest {
  Repo repo = 1;
}

message ListRepoRequest {
  // type is the type of (system) repos that should be returned
  // an empty string requests all repos
  string type = 1;
  // projects filters out repos that do not belong in the list, while no projects means list all repos.
  repeated Project projects = 2;
}

message DeleteRepoRequest {
  Repo repo = 1;
  bool force = 2;
}

// DeleteReposRequest is used to delete more than one repo at once.
message DeleteReposRequest {
  // All repos in each project will be deleted if the caller has
  // permission.
  repeated Project projects = 1;
  bool force = 2;
  // If all is set, then all repos in all projects will be deleted if the caller
  // has permission.
  bool all = 3;
}

message DeleteReposResponse {
  repeated Repo repos = 1;
}

// CommitState describes the states a commit can be in.
// The states are increasingly specific, i.e. a commit that is FINISHED also counts as STARTED.
enum CommitState {
  COMMIT_STATE_UNKNOWN = 0;
  STARTED = 1; // The commit has been started, all commits satisfy this state.
  READY = 2; // The commit has been started, and all of its provenant commits have been finished.
  FINISHING = 3; // The commit is in the process of being finished.
  FINISHED = 4; // The commit has been finished.
}

message StartCommitRequest {
  // parent may be empty in which case the commit that Branch points to will be used as the parent.
  // If the branch does not exist, the commit will have no parent.
  Commit parent = 1;
  // description is a user-provided string describing this commit
  string description = 2;
  Branch branch = 3;
}

message FinishCommitRequest {
  Commit commit = 1;
  // description is a user-provided string describing this commit. Setting this
  // will overwrite the description set in StartCommit
  string description = 2;
  string error = 3;
  bool force = 4;
}

message InspectCommitRequest {
  Commit commit = 1;
  // Wait causes inspect commit to wait until the commit is in the desired state.
  CommitState wait = 2;
}

message ListCommitRequest {
  Repo repo = 1;
  Commit from = 2;
  Commit to = 3;
  int64 number = 4;
  bool reverse = 5;  // Return commits oldest to newest
  bool all = 6; // Return commits of all kinds (without this, aliases are excluded)
  OriginKind origin_kind = 7; // Return only commits of this kind (mutually exclusive with all)
  google.protobuf.Timestamp started_time = 8; // Return commits started before this time
}

message InspectCommitSetRequest {
  CommitSet commit_set = 1;
  bool wait = 2; // When true, wait until all commits in the set are finished
}

message ListCommitSetRequest {
  Project project = 1;
}

message SquashCommitSetRequest {
  CommitSet commit_set = 1;
}

message DropCommitSetRequest {
  CommitSet commit_set = 1;
}

message SubscribeCommitRequest {
  Repo repo = 1;
  string branch = 2;
  // only commits created since this commit are returned
  Commit from = 3;
  // Don't return commits until they're in (at least) the desired state.
  CommitState state = 4;
  bool all = 5; // Return commits of all kinds (without this, aliases are excluded)
  OriginKind origin_kind = 6; // Return only commits of this kind (mutually exclusive with all)
}

message ClearCommitRequest {
  Commit commit = 1;
}

message CreateBranchRequest {
  Commit head = 1;
  Branch branch = 2;
  repeated Branch provenance = 3;
  Trigger trigger = 4;
  bool new_commit_set = 5; // overrides the default behavior of using the same CommitSet as 'head'
}

message FindCommitsRequest {
  Commit start = 1;
  string file_path = 2;
  uint32 limit = 3; // a limit of 0 means there is no upper bound on the limit.
}

message FindCommitsResponse {
  oneof result {
    Commit found_commit = 1;
    Commit last_searched_commit = 2;
  }
  uint32 commits_searched = 3;
}

message InspectBranchRequest {
  Branch branch = 1;
}

message ListBranchRequest {
  Repo repo = 1;
  bool reverse = 2; // Returns branches oldest to newest
}

message DeleteBranchRequest {
  Branch branch = 1;
  bool force = 2;
}

message CreateProjectRequest {
  Project project = 1;
  string description = 2;
  bool update = 3;
}

message InspectProjectRequest {
  Project project = 1;
}

message ListProjectRequest {}

message DeleteProjectRequest {
  Project project = 1;
  bool force = 2;
}

enum Delimiter {
  NONE = 0;
  JSON = 1;
  LINE = 2;
  SQL = 3;
  CSV = 4;
}

message AddFile {
  string path = 1;
  string datum = 2;

  message URLSource {
    string URL = 1;
    bool recursive = 2;
    uint32 concurrency = 3;
  }
  oneof source {
    google.protobuf.BytesValue raw = 3;
    URLSource url = 4;
  }
}

message DeleteFile {
  string path = 1;
  string datum = 2;
}

message CopyFile {
  string dst = 1;
  string datum = 2;
  File src = 3;
  bool append = 4;
}

message ModifyFileRequest {
  oneof body {
    Commit set_commit = 1;
    AddFile add_file = 2;
    DeleteFile delete_file = 3;
    CopyFile copy_file = 4;
  }
}

message GetFileRequest {
  File file = 1;
  string URL = 2;
  int64 offset = 3;
  PathRange path_range = 4;
// TODO:
//  int64 size_bytes = 3;
}

message InspectFileRequest {
  File file = 1;
}

message ListFileRequest {
  reserved 2;

  // File is the parent directory of the files we want to list. This sets the
  // repo, the commit/branch, and path prefix of files we're interested in
  // If the "path" field is omitted, a list of files at the top level of the repo
  // is returned
  File file = 1;
  // Marker for pagination. If set, the files that come after the marker in
  // lexicographical order will be returned. If reverse is also set, the files
  // that come before the marker in lexicographical order will be returned.
  File paginationMarker = 3;
  // Number of files to return
  int64 number = 4;
  // If true, return files in reverse order
  bool reverse = 5;

// TODO:
//  // History indicates how many historical versions you want returned. Its
//  // semantics are:
//  // 0: Return the files as they are at the commit in `file`. FileInfo.File
//  //    will equal File in this request.
//  // 1: Return the files as they are in the last commit they were modified in.
//  //    (This will have the same hash as if you'd passed 0, but
//  //    FileInfo.File.Commit will be different.
//  // 2: Return the above and the files as they are in the next-last commit they
//  //    were modified in.
//  // 3: etc.
//  //-1: Return all historical versions.
//  int64 history = 3;
}

message WalkFileRequest {
    File file = 1;
    // Marker for pagination. If set, the files that come after the marker in
    // lexicographical order will be returned. If reverse is also set, the files
    // that come before the marker in lexicographical order will be returned.
    File paginationMarker = 2;
    // Number of files to return
    int64 number = 3;
    // If true, return files in reverse order
    bool reverse = 4;
}

message GlobFileRequest {
  Commit commit = 1;
  string pattern = 2;
  PathRange path_range = 3;
}

message DiffFileRequest {
  File new_file = 1;
  // OldFile may be left nil in which case the same path in the parent of
  // NewFile's commit will be used.
  File old_file = 2;
  bool shallow = 3;
}

message DiffFileResponse {
  FileInfo new_file = 1;
  FileInfo old_file = 2;
}

message FsckRequest {
  bool fix = 1;
  oneof zombie_check {
    Commit zombie_target = 2;
    // run zombie data detection against all pipelines
    bool zombie_all = 3;
  }
}

message FsckResponse {
  string fix = 1;
  string error = 2;
}

message CreateFileSetResponse {
  string file_set_id = 1;
}

message GetFileSetRequest {
  Commit commit = 1;
}

message AddFileSetRequest {
  Commit commit = 1;
  string file_set_id = 2;
}

message RenewFileSetRequest {
  string file_set_id = 1;
  int64 ttl_seconds = 2;
}

message ComposeFileSetRequest {
  repeated string file_set_ids = 1;
  int64 ttl_seconds = 2;
  bool compact = 3;
}

message ShardFileSetRequest {
  string file_set_id = 1;
}

message PathRange {
  string lower = 1;
  string upper = 2;
}

message ShardFileSetResponse {
  repeated PathRange shards = 1;
}

message CheckStorageRequest {
  bool read_chunk_data = 1;
  bytes chunk_begin = 2;
  bytes chunk_end = 3;
}

message CheckStorageResponse {
  int64 chunk_object_count = 1;
}

message PutCacheRequest {
  string key = 1;
  google.protobuf.Any value = 2;
  repeated string file_set_ids = 3;
  string tag = 4;
}

message GetCacheRequest {
  string key = 1;
}

message GetCacheResponse {
  google.protobuf.Any value = 1;
}

message ClearCacheRequest {
  string tag_prefix = 1;
}

message ActivateAuthRequest {}
message ActivateAuthResponse {}

message RunLoadTestRequest {
  string spec = 1;
  Branch branch = 2;
  int64 seed = 3;
  string state_id = 4;
}

message RunLoadTestResponse {
  string spec = 1;
  Branch branch = 2;
  int64 seed = 3;
  string error = 4;
  google.protobuf.Duration duration = 5;
  string state_id = 6;
}

message ObjectStorageEgress {
  string url = 1;
}
message SQLDatabaseEgress {
  message FileFormat {
    enum Type {
        UNKNOWN = 0;
        CSV = 1;
        JSON = 2;
        PARQUET = 3;
    }
    Type type = 1;
    repeated string columns = 2;
  }
  message Secret {
    string name = 1;
    string key = 2;
  }

  string url = 1;
  FileFormat file_format = 2;
  Secret secret = 3;
}
message EgressRequest {
  pfs_v2.Commit commit = 1;
  oneof target {
    ObjectStorageEgress object_storage = 2;
    SQLDatabaseEgress sql_database = 3;
  }
}
message EgressResponse {
  message ObjectStorageResult {
    int64 bytes_written = 1;
  }
  message SQLDatabaseResult {
    map<string, int64> rows_written = 1;
  }

  oneof result {
    ObjectStorageResult object_storage = 1;
    SQLDatabaseResult sql_database = 2;
  }
}

service API {
  // CreateRepo creates a new repo.
  rpc CreateRepo(CreateRepoRequest) returns (google.protobuf.Empty) {}
  // InspectRepo returns info about a repo.
  rpc InspectRepo(InspectRepoRequest) returns (RepoInfo) {}
  // ListRepo returns info about all repos.
  rpc ListRepo(ListRepoRequest) returns (stream RepoInfo) {}
  // DeleteRepo deletes a repo.
  rpc DeleteRepo(DeleteRepoRequest) returns (google.protobuf.Empty) {}
  // DeleteRepos deletes more than one repo at once.  It attempts to
  // delete every repo matching the DeleteReposRequest.  When deleting
  // all repos matching a project, any repos not deletable by the
  // caller will remain, and the project will not be empty; this is
  // not an error.  The returned DeleteReposResponse will contain a
  // list of all actually-deleted repos.
  rpc DeleteRepos(DeleteReposRequest) returns (DeleteReposResponse) {}

  // StartCommit creates a new write commit from a parent commit.
  rpc StartCommit(StartCommitRequest) returns (Commit) {}
  // FinishCommit turns a write commit into a read commit.
  rpc FinishCommit(FinishCommitRequest) returns (google.protobuf.Empty) {}
  // ClearCommit removes all data from the commit.
  rpc ClearCommit(ClearCommitRequest) returns (google.protobuf.Empty) {}
  // InspectCommit returns the info about a commit.
  rpc InspectCommit(InspectCommitRequest) returns (CommitInfo) {}
  // ListCommit returns info about all commits.
  rpc ListCommit(ListCommitRequest) returns (stream CommitInfo) {}
  // SubscribeCommit subscribes for new commits on a given branch.
  rpc SubscribeCommit(SubscribeCommitRequest) returns (stream CommitInfo) {}

  // InspectCommitSet returns the info about a CommitSet.
  rpc InspectCommitSet(InspectCommitSetRequest) returns (stream CommitInfo) {}
  // ListCommitSet returns info about all CommitSets.
  rpc ListCommitSet(ListCommitSetRequest) returns (stream CommitSetInfo) {}
  // SquashCommitSet squashes the commits of a CommitSet into their children.
  rpc SquashCommitSet(SquashCommitSetRequest) returns (google.protobuf.Empty) {}
  // DropCommitSet drops the commits of a CommitSet and all data included in the commits.
  rpc DropCommitSet(DropCommitSetRequest) returns (google.protobuf.Empty) {}
  // FindCommits searches for commits that reference a supplied file being modified in a branch.
  rpc FindCommits(FindCommitsRequest) returns (stream FindCommitsResponse) {}

  // CreateBranch creates a new branch.
  rpc CreateBranch(CreateBranchRequest) returns (google.protobuf.Empty) {}
  // InspectBranch returns info about a branch.
  rpc InspectBranch(InspectBranchRequest) returns (BranchInfo) {}
  // ListBranch returns info about the heads of branches.
  rpc ListBranch(ListBranchRequest) returns (stream BranchInfo) {}
  // DeleteBranch deletes a branch; note that the commits still exist.
  rpc DeleteBranch(DeleteBranchRequest) returns (google.protobuf.Empty) {}

  // ModifyFile performs modifications on a set of files.
  rpc ModifyFile(stream ModifyFileRequest) returns (google.protobuf.Empty) {}
  // GetFile returns the contents of a single file
  rpc GetFile(GetFileRequest) returns (stream google.protobuf.BytesValue) {}
  // GetFileTAR returns a TAR stream of the contents matched by the request
  rpc GetFileTAR(GetFileRequest) returns (stream google.protobuf.BytesValue) {}
  // InspectFile returns info about a file.
  rpc InspectFile(InspectFileRequest) returns (FileInfo) {}
  // ListFile returns info about all files.
  rpc ListFile(ListFileRequest) returns (stream FileInfo) {}
  // WalkFile walks over all the files under a directory, including children of children.
  rpc WalkFile(WalkFileRequest) returns (stream FileInfo) {}
  // GlobFile returns info about all files.
  rpc GlobFile(GlobFileRequest) returns (stream FileInfo) {}
  // DiffFile returns the differences between 2 paths at 2 commits.
  rpc DiffFile(DiffFileRequest) returns (stream DiffFileResponse) {}

  // ActivateAuth creates a role binding for all existing repos
  rpc ActivateAuth(ActivateAuthRequest) returns (ActivateAuthResponse) {}

  // DeleteAll deletes everything.
  rpc DeleteAll(google.protobuf.Empty) returns (google.protobuf.Empty) {}
  // Fsck does a file system consistency check for pfs.
  rpc Fsck(FsckRequest) returns (stream FsckResponse) {}

  // FileSet API
  // CreateFileSet creates a new file set.
  rpc CreateFileSet(stream ModifyFileRequest) returns (CreateFileSetResponse) {}
  // GetFileSet returns a file set with the data from a commit
  rpc GetFileSet(GetFileSetRequest) returns (CreateFileSetResponse) {}
  // AddFileSet associates a file set with a commit
  rpc AddFileSet(AddFileSetRequest) returns (google.protobuf.Empty) {}
  // RenewFileSet prevents a file set from being deleted for a set amount of time.
  rpc RenewFileSet(RenewFileSetRequest) returns (google.protobuf.Empty) {}
  // ComposeFileSet composes a file set from a list of file sets.
  rpc ComposeFileSet(ComposeFileSetRequest) returns (CreateFileSetResponse) {}
  rpc ShardFileSet(ShardFileSetRequest) returns (ShardFileSetResponse) {}
  // CheckStorage runs integrity checks for the storage layer.
  rpc CheckStorage(CheckStorageRequest) returns (CheckStorageResponse) {}
  rpc PutCache(PutCacheRequest) returns (google.protobuf.Empty) {}
  rpc GetCache(GetCacheRequest) returns (GetCacheResponse) {}
  rpc ClearCache(ClearCacheRequest) returns (google.protobuf.Empty) {}

  // RunLoadTest runs a load test.
  rpc RunLoadTest(RunLoadTestRequest) returns (RunLoadTestResponse) {}
  // RunLoadTestDefault runs the default load tests.
  rpc RunLoadTestDefault(google.protobuf.Empty) returns (RunLoadTestResponse) {}

  // ListTask lists PFS tasks
  rpc ListTask(taskapi.ListTaskRequest) returns (stream taskapi.TaskInfo) {}

  // Egress writes data from a commit to an external system
  rpc Egress(EgressRequest) returns (EgressResponse) {}

  // Project API
  // CreateProject creates a new project.
  rpc CreateProject(CreateProjectRequest) returns (google.protobuf.Empty) {}
  // InspectProject returns info about a project.
  rpc InspectProject(InspectProjectRequest) returns (ProjectInfo) {}
  // ListProject returns info about all projects.
  rpc ListProject(ListProjectRequest) returns (stream ProjectInfo) {}
  // DeleteProject deletes a project.
  rpc DeleteProject(DeleteProjectRequest) returns (google.protobuf.Empty) {}
}

syntax = "proto3";

import "google/protobuf/empty.proto";

package versionpb_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/version/versionpb";

message Version {
  uint32 major = 1;
  uint32 minor = 2;
  uint32 micro = 3;
  string additional = 4;
  string git_commit = 5;
  string git_tree_modified = 6;
  string build_date = 7;
  string go_version = 8;
  string platform = 9;
}

service API {
  rpc GetVersion(google.protobuf.Empty) returns (Version) {}
}

syntax = "proto3";

package taskapi;
option go_package = "github.com/pachyderm/pachyderm/v2/src/task";

import "gogoproto/gogo.proto";

enum State {
  UNKNOWN = 0;
  RUNNING = 1;
  SUCCESS = 2;
  FAILURE = 3;
  CLAIMED = 4; // not a real state used by task logic
}

message Group {
  string namespace = 1;
  string group = 2;
}

message TaskInfo {
  string id = 1 [(gogoproto.customname) = "ID"];
  Group group = 2;
  State state = 3;
  string reason = 4;
  string input_type = 5;
  string input_data = 6;
}

message ListTaskRequest {
  Group group = 1;
}


syntax = "proto3";

package pps_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/pps";

import "google/protobuf/empty.proto";
import "google/protobuf/timestamp.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/wrappers.proto";

import "gogoproto/gogo.proto";

import "pfs/pfs.proto";

import "task/task.proto";

message SecretMount {
  // Name must be the name of the secret in kubernetes.
  string name = 1;
  // Key of the secret to load into env_var, this field only has meaning if EnvVar != "".
  string key = 2;
  string mount_path = 3;
  string env_var = 4;
}

message Transform {
  string image = 1;
  repeated string cmd = 2;
  repeated string err_cmd = 3;
  map<string, string> env = 4;
  repeated SecretMount secrets = 5;
  repeated string image_pull_secrets = 6;
  repeated string stdin = 7;
  repeated string err_stdin = 8;
  repeated int64 accept_return_code = 9;
  bool debug = 10;
  string user = 11;
  string working_dir = 12;
  string dockerfile = 13;
  bool memory_volume = 14;
  bool datum_batching = 15;
}

message TFJob {
  // tf_job  is a serialized Kubeflow TFJob spec. Pachyderm sends this directly
  // to a kubernetes cluster on which kubeflow has been installed, instead of
  // creating a pipeline ReplicationController as it normally would.
  string tf_job = 1 [(gogoproto.customname) = "TFJob"];
}

message Egress {
  string URL = 1;
  oneof target {
    pfs_v2.ObjectStorageEgress object_storage = 2;
    pfs_v2.SQLDatabaseEgress sql_database = 3;
  }
}

message Job {
  Pipeline pipeline = 1;
  string id = 2 [(gogoproto.customname) = "ID"];
}

enum JobState {
  JOB_STATE_UNKNOWN = 0;
  JOB_CREATED = 1;
  JOB_STARTING = 2;
  JOB_RUNNING = 3;
  JOB_FAILURE = 4;
  JOB_SUCCESS = 5;
  JOB_KILLED = 6;
  JOB_EGRESSING = 7;
  JOB_FINISHING = 8;
  JOB_UNRUNNABLE = 9;
}

message Metadata {
  map<string, string> annotations = 1;
  map<string, string> labels = 2;
}

message Service {
  int32 internal_port = 1;
  int32 external_port = 2;
  string ip = 3 [(gogoproto.customname) = "IP"];
  string type = 4;
}

message Spout {
  Service service = 1;
}

message PFSInput {
  string project = 14;
  string name = 1;
  string repo = 2;
  string repo_type = 13;
  string branch = 3;
  string commit = 4;
  string glob = 5;
  string join_on = 6;
  bool outer_join = 7;
  string group_by = 8;
  bool lazy = 9;
  // EmptyFiles, if true, will cause files from this PFS input to be
  // presented as empty files. This is useful in shuffle pipelines where you
  // want to read the names of files and reorganize them using symlinks.
  bool empty_files = 10;
  // S3, if true, will cause the worker to NOT download or link files from this
  // input into the /pfs_v2 directory. Instead, an instance of our S3 gateway
  // service will run on each of the sidecars, and data can be retrieved from
  // this input by querying
  // http://<pipeline>-s3.<namespace>/<job id>.<input>/my/file
  bool s3 = 11;
  // Trigger defines when this input is processed by the pipeline, if it's nil
  // the input is processed anytime something is committed to the input branch.
  pfs_v2.Trigger trigger = 12;
}

message CronInput {
  string name = 1;
  string project = 7;
  string repo = 2;
  string commit = 3;
  string spec = 4;
  // Overwrite, if true, will expose a single datum that gets overwritten each
  // tick. If false, it will create a new datum for each tick.
  bool overwrite = 5;
  google.protobuf.Timestamp start = 6;
}


message Input {
  PFSInput pfs = 1;
  repeated Input join = 2;
  repeated Input group = 3;
  repeated Input cross = 4;
  repeated Input union = 5;
  CronInput cron = 6;
}

message JobInput {
  string name = 1;
  pfs_v2.Commit commit = 2;
  string glob = 3;
  bool lazy = 4;
}

message ParallelismSpec {
  // Starts the pipeline/job with a 'constant' workers, unless 'constant' is
  // zero. If 'constant' is zero (which is the zero value of ParallelismSpec),
  // then Pachyderm will choose the number of workers that is started,
  // (currently it chooses the number of workers in the cluster)
  uint64 constant = 1;
}

message InputFile {
  // This file's absolute path within its pfs repo.
  string path = 1;

  // This file's hash
  bytes hash = 2;
}

message Datum {
  // ID is the hash computed from all the files
  Job job = 1;
  string id = 2 [(gogoproto.customname) = "ID"];
}

enum DatumState {
  UNKNOWN = 0; // or not part of a job
  FAILED = 1;
  SUCCESS = 2;
  SKIPPED = 3;
  STARTING = 4;
  RECOVERED = 5;
}

message DatumInfo {
  Datum datum = 1;
  DatumState state = 2;
  ProcessStats stats = 3;
  pfs_v2.File pfs_state = 4;
  repeated pfs_v2.FileInfo data = 5;
  string image_id = 6;
}

message Aggregate {
  int64 count = 1;
  double mean = 2;
  double stddev = 3;
  double fifth_percentile = 4;
  double ninety_fifth_percentile = 5;
}

message ProcessStats {
  google.protobuf.Duration download_time = 1;
  google.protobuf.Duration process_time = 2;
  google.protobuf.Duration upload_time = 3;
  int64 download_bytes = 4;
  int64 upload_bytes = 5;
}

message AggregateProcessStats {
  Aggregate download_time = 1;
  Aggregate process_time = 2;
  Aggregate upload_time = 3;
  Aggregate download_bytes = 4;
  Aggregate upload_bytes = 5;
}

message WorkerStatus {
  string worker_id = 1 [(gogoproto.customname) = "WorkerID"];
  string job_id = 2 [(gogoproto.customname) = "JobID"];
  DatumStatus datum_status = 3;
}

message DatumStatus {
  // Started is the time processing on the current datum began.
  google.protobuf.Timestamp started = 1;
  repeated InputFile data = 2;
}

// ResourceSpec describes the amount of resources that pipeline pods should
// request from kubernetes, for scheduling.
message ResourceSpec {
  // The number of CPUs each worker needs (partial values are allowed, and
  // encouraged)
  float cpu = 1;

  // The amount of memory each worker needs (in bytes, with allowed
  // SI suffixes (M, K, G, Mi, Ki, Gi, etc).
  string memory = 2;

  // The spec for GPU resources.
  GPUSpec gpu = 3;

  // The amount of ephemeral storage each worker needs (in bytes, with allowed
  // SI suffixes (M, K, G, Mi, Ki, Gi, etc).
  string disk = 4;
}

message GPUSpec {
  // The type of GPU (nvidia.com/gpu or amd.com/gpu for example).
  string type = 1;
  // The number of GPUs to request.
  int64 number = 2;
}

message JobSetInfo {
  JobSet job_set = 1;
  repeated JobInfo jobs = 2;
}

// JobInfo is the data stored in the database regarding a given job.  The
// 'details' field contains more information about the job which is expensive to
// fetch, requiring querying workers or loading the pipeline spec from object
// storage.
message JobInfo {
  Job job = 1;
  uint64 pipeline_version = 2;
  pfs_v2.Commit output_commit = 3;
  // Job restart count (e.g. due to datum failure)
  uint64 restart = 4;

  // Counts of how many times we processed or skipped a datum
  int64 data_processed = 5;
  int64 data_skipped = 6;
  int64 data_total = 7;
  int64 data_failed = 8;
  int64 data_recovered = 9;

  // Download/process/upload time and download/upload bytes
  ProcessStats stats = 10;

  JobState state = 11;
  string reason = 12; // reason explains why the job is in the current state
  google.protobuf.Timestamp created = 13;
  google.protobuf.Timestamp started = 14;
  google.protobuf.Timestamp finished = 15;

  message Details {
    Transform transform = 1;
    ParallelismSpec parallelism_spec = 2;
    Egress egress = 3;
    Service service = 4;
    Spout spout = 5;
    repeated WorkerStatus worker_status = 6;
    ResourceSpec resource_requests = 7;
    ResourceSpec resource_limits = 8;
    ResourceSpec sidecar_resource_limits = 9;
    Input input = 10;
    string salt = 11;
    DatumSetSpec datum_set_spec = 12;
    google.protobuf.Duration datum_timeout = 13;
    google.protobuf.Duration job_timeout = 14;
    int64 datum_tries = 15;
    SchedulingSpec scheduling_spec = 16;
    string pod_spec = 17;
    string pod_patch = 18;
    ResourceSpec sidecar_resource_requests = 19;
  }
  Details details = 16;
}

enum WorkerState {
  WORKER_STATE_UNKNOWN = 0;
  POD_RUNNING = 1;
  POD_SUCCESS = 2;
  POD_FAILED = 3;
}

message Worker {
  string name = 1;
  WorkerState state = 2;
}

message Pipeline {
  pfs_v2.Project project = 2;
  string name = 1;
}

enum PipelineState {
  PIPELINE_STATE_UNKNOWN = 0;
  // There is a PipelineInfo + spec commit, but no RC
  // This happens when a pipeline has been created but not yet picked up by a
  // PPS server.
  PIPELINE_STARTING = 1;
  // A pipeline has a spec commit and a service + RC
  // This is the normal state of a pipeline.
  PIPELINE_RUNNING = 2;
  // Equivalent to STARTING (there is a PipelineInfo + commit, but no RC)
  // After some error caused runPipeline to exit, but before the pipeline is
  // re-run. This is when the exponential backoff is in effect.
  PIPELINE_RESTARTING = 3;
  // The pipeline has encountered unrecoverable errors and is no longer being
  // retried. It won't leave this state until the pipeline is updated.
  PIPELINE_FAILURE = 4;
  // The pipeline has been explicitly paused by the user (the pipeline spec's
  // Stopped field should be true if the pipeline is in this state)
  PIPELINE_PAUSED = 5;
  // The pipeline is fully functional, but there are no commits to process.
  PIPELINE_STANDBY = 6;
  // The pipeline's workers are crashing, or failing to come up, this may
  // resolve itself, the pipeline may make progress while in this state if the
  // problem is only being experienced by some workers.
  PIPELINE_CRASHING = 7;
}

// Toleration is a Kubernetes toleration.
message Toleration {
  // key is the taint key that the toleration applies to.  Empty means match all taint keys.
  string key = 1;
  // operator represents a key's relationship to the value.
  TolerationOperator operator = 2;
  // value is the taint value the toleration matches to.
  string value = 3;
  // effect indicates the taint effect to match.  Empty means match all taint effects.
  TaintEffect effect = 4;
  // toleration_seconds represents the period of time the toleration (which must be of effect
  // NoExecute, otherwise this field is ignored) tolerates the taint.  If not set, tolerate the
  // taint forever.
  google.protobuf.Int64Value toleration_seconds = 5;
}

// TolerationOperator relates a Toleration's key to its value.
enum TolerationOperator {
  EMPTY = 0; // K8s doesn't have this, but it's possible to represent something similar.
  EXISTS = 1; // "Exists"
  EQUAL = 2; // "Equal"
}

// TaintEffect is an effect that can be matched by a toleration.
enum TaintEffect {
  ALL_EFFECTS = 0; // Empty matches all effects.
  NO_SCHEDULE = 1; // "NoSchedule"
  PREFER_NO_SCHEDULE = 2; // "PreferNoSchedule"
  NO_EXECUTE = 3; // "NoExecute"
}

// PipelineInfo is proto for each pipeline that Pachd stores in the
// database. It tracks the state of the pipeline, and points to its metadata in
// PFS (and, by pointing to a PFS commit, de facto tracks the pipeline's
// version).  Any information about the pipeline _not_ stored in the database is
// in the Details object, which requires fetching the spec from PFS or other
// potentially expensive operations.
message PipelineInfo {
  Pipeline pipeline = 1;
  uint64 version = 2;
  pfs_v2.Commit spec_commit = 3; // The first spec commit for this version of the pipeline
  bool stopped = 4;

  // state indicates the current state of the pipeline
  PipelineState state = 5;
  // reason includes any error messages associated with a failed pipeline
  string reason = 6;

  reserved 7; // map<int32, int32> job_counts;
  // last_job_state indicates the state of the most recently created job
  JobState last_job_state = 8;

  // parallelism tracks the literal number of workers that this pipeline should
  // run.
  uint64 parallelism = 9;

  // The pipeline type is stored here so that we can internally know the type of
  // the pipeline without loading the spec from PFS.
  enum PipelineType {
    PIPELINT_TYPE_UNKNOWN = 0;
    PIPELINE_TYPE_TRANSFORM = 1;
    PIPELINE_TYPE_SPOUT = 2;
    PIPELINE_TYPE_SERVICE = 3;
  }
  PipelineType type = 10;

  string auth_token = 11;

  message Details {
    Transform transform = 1;
    // tf_job encodes a Kubeflow TFJob spec. Pachyderm uses this to create TFJobs
    // when running in a kubernetes cluster on which kubeflow has been installed.
    // Exactly one of 'tf_job' and 'transform' should be set
    TFJob tf_job = 2 [(gogoproto.customname) = "TFJob"];
    ParallelismSpec parallelism_spec = 3;
    Egress egress = 4;
    google.protobuf.Timestamp created_at = 5;

    string recent_error = 6;

    int64 workers_requested = 7;
    int64 workers_available = 8;

    string output_branch = 9;
    ResourceSpec resource_requests = 10;
    ResourceSpec resource_limits = 11;
    ResourceSpec sidecar_resource_limits = 12;
    Input input = 13;
    string description = 14;
    string salt = 16;

    string reason = 17;
    Service service = 19;
    Spout spout = 20;
    DatumSetSpec datum_set_spec = 21;
    google.protobuf.Duration datum_timeout = 22;
    google.protobuf.Duration job_timeout = 23;
    int64 datum_tries = 24;
    SchedulingSpec scheduling_spec = 25;
    string pod_spec = 26;
    string pod_patch = 27;
    bool s3_out = 28;
    Metadata metadata = 29;
    string reprocess_spec = 30;
    int64 unclaimed_tasks = 31;
    string worker_rc = 32;
    bool autoscaling = 33;
    repeated Toleration tolerations = 34;
    ResourceSpec sidecar_resource_requests = 35;
  }
  Details details = 12;
}

message PipelineInfos {
  repeated PipelineInfo pipeline_info = 1;
}

message JobSet {
  string id = 1 [(gogoproto.customname) = "ID"];
}

message InspectJobSetRequest {
  JobSet job_set = 1;
  bool wait = 2; // When true, wait until all jobs in the set are finished
  bool details = 3;
}

message ListJobSetRequest {
  bool details = 1;
  // A list of projects to filter jobs on, nil means don't filter.
  repeated pfs_v2.Project projects = 2;
  // we return job sets created before or after this time based on the reverse flag
  google.protobuf.Timestamp paginationMarker = 3;
  // number of results to return
  int64 number = 4;
  // if true, return results in reverse order
  bool reverse = 5;
}

message InspectJobRequest {
  // Callers should set either Job or OutputCommit, not both.
  Job job = 1;
  bool wait = 2; // wait until state is either FAILURE or SUCCESS
  bool details = 3;
}

message ListJobRequest {
  // A list of projects to filter jobs on, nil means don't filter.
  repeated pfs_v2.Project projects = 7;
  Pipeline pipeline = 1;                // nil means all pipelines
  repeated pfs_v2.Commit input_commit = 2; // nil means all inputs

  // History indicates return jobs from historical versions of pipelines
  // semantics are:
  // 0: Return jobs from the current version of the pipeline or pipelines.
  // 1: Return the above and jobs from the next most recent version
  // 2: etc.
  //-1: Return jobs from all historical versions.
  int64 history = 4;

  // Details indicates whether the result should include all pipeline details in
  // each JobInfo, or limited information including name and status, but
  // excluding information in the pipeline spec. Leaving this "false" can make
  // the call significantly faster in clusters with a large number of pipelines
  // and jobs.
  // Note that if 'input_commit' is set, this field is coerced to "true"
  bool details = 5;

  // A jq program string for additional result filtering
  string jqFilter = 6;

  // timestamp that is pagination marker
  google.protobuf.Timestamp paginationMarker = 8;

  // number of results to return
  int64 number = 9;

  // flag to indicated if results should be returned in reverse order
  bool reverse = 10;
}

// Streams open jobs until canceled
message SubscribeJobRequest {
  Pipeline pipeline = 1;
  bool details = 2; // Same as ListJobRequest.Details
}

message DeleteJobRequest {
  Job job = 1;
}

message StopJobRequest {
  Job job = 1;
  string reason = 3;
}

message UpdateJobStateRequest {
  Job job = 1;
  JobState state = 2;
  string reason = 3;
  uint64 restart = 5;
  int64 data_processed = 6;
  int64 data_skipped = 7;
  int64 data_failed = 8;
  int64 data_recovered = 9;
  int64 data_total = 10;
  ProcessStats stats = 11;
}

message GetLogsRequest {
  // The pipeline from which we want to get logs (required if the job in 'job'
  // was created as part of a pipeline. To get logs from a non-orphan job
  // without the pipeline that created it, you need to use ElasticSearch).
  Pipeline pipeline = 1;

  // The job from which we want to get logs.
  Job job = 2;

  // Names of input files from which we want processing logs. This may contain
  // multiple files, to query pipelines that contain multiple inputs. Each
  // filter may be an absolute path of a file within a pps repo, or it may be
  // a hash for that file (to search for files at specific versions)
  repeated string data_filters = 3;

  Datum datum = 4;

  // If true get logs from the master process
  bool master = 5;

  // Continue to follow new logs as they become available.
  bool follow = 6;

  // If nonzero, the number of lines from the end of the logs to return.  Note:
  // tail applies per container, so you will get tail * <number of pods> total
  // lines back.
  int64 tail = 7;

  // UseLokiBackend causes the logs request to go through the loki backend
  // rather than through kubernetes. This behavior can also be achieved by
  // setting the LOKI_LOGGING feature flag.
  bool use_loki_backend = 8;

  // Since specifies how far in the past to return logs from. It defaults to 24 hours.
  google.protobuf.Duration since = 9;
}

// LogMessage is a log line from a PPS worker, annotated with metadata
// indicating when and why the line was logged.
message LogMessage {
  // The job and pipeline for which a PFS file is being processed (if the job
  // is an orphan job, pipeline name and ID will be unset)
  string project_name = 10;
  string pipeline_name = 1;
  string job_id = 2 [(gogoproto.customname) = "JobID"];
  string worker_id = 3 [(gogoproto.customname) = "WorkerID"];
  string datum_id = 4 [(gogoproto.customname) = "DatumID"];
  bool master = 5;

  // The PFS files being processed (one per pipeline/job input)
  repeated InputFile data = 6;

  // User is true if log message comes from the users code.
  bool user = 7;

  // The message logged, and the time at which it was logged
  google.protobuf.Timestamp ts = 8;
  string message = 9;
}

message RestartDatumRequest {
  Job job = 1;
  repeated string data_filters = 2;
}

message InspectDatumRequest {
  Datum datum = 1;
}

message ListDatumRequest {
  // Filter restricts returned DatumInfo messages to those which match
  // all of the filtered attributes.
  message Filter {
    repeated DatumState state = 1;  // Must match one of the given states.
  }
  // Job and Input are two different ways to specify the datums you want.
  // Only one can be set.
  // Job is the job to list datums from.
  Job job = 1;
  // Input is the input to list datums from.
  // The datums listed are the ones that would be run if a pipeline was created
  // with the provided input.
  Input input = 2;
  Filter filter = 3;
  //datum id to start from. we do not include this datum in the response
  string paginationMarker = 4;
  // Number of datums to return
  int64 number = 5;
  // If true, return datums in reverse order
  bool reverse = 6;
}

// DatumSetSpec specifies how a pipeline should split its datums into datum sets.
message DatumSetSpec {
  // number, if nonzero, specifies that each datum set should contain `number`
  // datums. Datum sets may contain fewer if the total number of datums don't
  // divide evenly.
  int64 number = 1;
  // size_bytes, if nonzero, specifies a target size for each datum set.
  // Datum sets may be larger or smaller than size_bytes, but will usually be
  // pretty close to size_bytes in size.
  int64 size_bytes = 2;

  // per_worker, if nonzero, specifies how many datum sets should be created
  // for each worker. It can't be set with number or size_bytes.
  int64 per_worker = 3;
}

message SchedulingSpec {
  map<string, string> node_selector = 1;
  string priority_class_name = 2;
}

message CreatePipelineRequest {
  Pipeline pipeline = 1;
  // tf_job encodes a Kubeflow TFJob spec. Pachyderm uses this to create TFJobs
  // when running in a kubernetes cluster on which kubeflow has been installed.
  // Exactly one of 'tf_job' and 'transform' should be set
  TFJob tf_job = 2 [(gogoproto.customname) = "TFJob"];
  Transform transform = 3;
  ParallelismSpec parallelism_spec = 4;
  Egress egress = 5;
  bool update = 6;
  string output_branch = 7;
  // s3_out, if set, requires a pipeline's user to write to its output repo
  // via Pachyderm's s3 gateway (if set, workers will serve Pachyderm's s3
  // gateway API at http://<pipeline>-s3.<namespace>/<job id>.out/my/file).
  // In this mode /pfs_v2/out won't be walked or uploaded, and the s3 gateway
  // service in the workers will allow writes to the job's output commit
  bool s3_out = 8;
  ResourceSpec resource_requests = 9;
  ResourceSpec resource_limits = 10;
  ResourceSpec sidecar_resource_limits = 11;
  Input input = 12;
  string description = 13;
  // Reprocess forces the pipeline to reprocess all datums.
  // It only has meaning if Update is true
  bool reprocess = 15;
  Service service = 17;
  Spout spout = 18;
  DatumSetSpec datum_set_spec = 19;
  google.protobuf.Duration datum_timeout = 20;
  google.protobuf.Duration job_timeout = 21;
  string salt = 22;
  int64 datum_tries = 23;
  SchedulingSpec scheduling_spec = 24;
  string pod_spec = 25; // deprecated, use pod_patch below
  string pod_patch = 26; // a json patch will be applied to the pipeline's pod_spec before it's created;
  pfs_v2.Commit spec_commit = 27;
  Metadata metadata = 28;
  string reprocess_spec = 29;
  bool autoscaling = 30;
  repeated Toleration tolerations = 34;
  ResourceSpec sidecar_resource_requests = 35;
}

message InspectPipelineRequest {
  Pipeline pipeline = 1;
  // When true, return PipelineInfos with the details field, which requires
  // loading the pipeline spec from PFS.
  bool details = 2;
}

message ListPipelineRequest {
  // If non-nil, only return info about a single pipeline, this is redundant
  // with InspectPipeline unless history is non-zero.
  Pipeline pipeline = 1;
  // History indicates how many historical versions you want returned. Its
  // semantics are:
  // 0: Return the current version of the pipeline or pipelines.
  // 1: Return the above and the next most recent version
  // 2: etc.
  //-1: Return all historical versions.
  int64 history = 2;

  // When true, return PipelineInfos with the details field, which requires
  // loading the pipeline spec from PFS.
  bool details = 3;

  // A jq program string for additional result filtering
  string jqFilter = 4;

  // If non-nil, will return all the pipeline infos at this commit set
  pfs_v2.CommitSet commit_set = 5;

  // Projects to filter on. Empty list means no filter, so return all pipelines.
  repeated pfs_v2.Project projects = 6;
}

// Delete a pipeline.  If the deprecated all member is true, then delete all
// pipelines in the default project.
message DeletePipelineRequest {
  Pipeline pipeline = 1;
  bool all = 2 [deprecated = true];
  bool force = 3;
  bool keep_repo = 4;
}

// Delete more than one pipeline.
message DeletePipelinesRequest {
  // All pipelines in each project will be deleted if the caller has
  // permission.
  repeated pfs_v2.Project projects = 1;
  bool force = 2;
  bool keep_repo = 3;
  // If set, all pipelines in all projects will be deleted if the caller has
  // permission.
  bool all = 4;
}

message DeletePipelinesResponse {
  repeated Pipeline pipelines = 1;
}

message StartPipelineRequest {
  Pipeline pipeline = 1;
}

message StopPipelineRequest {
  Pipeline pipeline = 1;
}

message RunPipelineRequest {
  Pipeline pipeline = 1;
  repeated pfs_v2.Commit provenance = 2;
  string job_id = 3 [(gogoproto.customname) = "JobID"];
}

message RunCronRequest {
  Pipeline pipeline = 1;
}

message CreateSecretRequest {
  bytes file = 1;
}

message DeleteSecretRequest {
  Secret secret = 1;
}

message InspectSecretRequest {
  Secret secret = 1;
}

message Secret {
  string name = 1;
}

message SecretInfo {
  Secret secret = 1;
  string type = 2;
  google.protobuf.Timestamp creation_timestamp = 3;
}

message SecretInfos {
  repeated SecretInfo secret_info = 1;
}

message ActivateAuthRequest {}
message ActivateAuthResponse {}

message RunLoadTestRequest {
  string dag_spec = 1;
  string load_spec = 2;
  int64 seed = 3;
  int64 parallelism = 4;
  string pod_patch = 5;
  string state_id = 6;
}

message RunLoadTestResponse {
  string error = 1;
  string state_id = 2;
}

message RenderTemplateRequest {
  string template = 1;
  map<string, string> args = 2;
}

message RenderTemplateResponse {
  string json = 1;
  repeated CreatePipelineRequest specs = 2;
}

message LokiRequest {
  google.protobuf.Duration since = 1;
  string query = 2;
}

message LokiLogMessage {
  string message = 1;
}

service API {
  rpc InspectJob(InspectJobRequest) returns (JobInfo) {}
  rpc InspectJobSet(InspectJobSetRequest) returns (stream JobInfo) {}
  // ListJob returns information about current and past Pachyderm jobs.
  rpc ListJob(ListJobRequest) returns (stream JobInfo) {}
  rpc ListJobSet(ListJobSetRequest) returns (stream JobSetInfo) {}
  rpc SubscribeJob(SubscribeJobRequest) returns (stream JobInfo) {}
  rpc DeleteJob(DeleteJobRequest) returns (google.protobuf.Empty) {}
  rpc StopJob(StopJobRequest) returns (google.protobuf.Empty) {}
  rpc InspectDatum(InspectDatumRequest) returns (DatumInfo) {}
  // ListDatum returns information about each datum fed to a Pachyderm job
  rpc ListDatum(ListDatumRequest) returns (stream DatumInfo) {}
  rpc RestartDatum(RestartDatumRequest) returns (google.protobuf.Empty) {}

  rpc CreatePipeline(CreatePipelineRequest) returns (google.protobuf.Empty) {}
  rpc InspectPipeline(InspectPipelineRequest) returns (PipelineInfo) {}
  rpc ListPipeline(ListPipelineRequest) returns (stream PipelineInfo) {}
  rpc DeletePipeline(DeletePipelineRequest) returns (google.protobuf.Empty) {}
  rpc DeletePipelines(DeletePipelinesRequest) returns (DeletePipelinesResponse) {}
  rpc StartPipeline(StartPipelineRequest) returns (google.protobuf.Empty) {}
  rpc StopPipeline(StopPipelineRequest) returns (google.protobuf.Empty) {}
  rpc RunPipeline(RunPipelineRequest) returns (google.protobuf.Empty) {}
  rpc RunCron(RunCronRequest) returns (google.protobuf.Empty) {}

  rpc CreateSecret(CreateSecretRequest) returns (google.protobuf.Empty) {}
  rpc DeleteSecret(DeleteSecretRequest) returns (google.protobuf.Empty) {}
  rpc ListSecret(google.protobuf.Empty) returns (SecretInfos) {}
  rpc InspectSecret(InspectSecretRequest) returns (SecretInfo) {}

  // DeleteAll deletes everything
  rpc DeleteAll(google.protobuf.Empty) returns (google.protobuf.Empty) {}
  rpc GetLogs(GetLogsRequest) returns (stream LogMessage) {}

  // An internal call that causes PPS to put itself into an auth-enabled state
  // (all pipeline have tokens, correct permissions, etcd)
  rpc ActivateAuth(ActivateAuthRequest) returns (ActivateAuthResponse) {}

  // An internal call used to move a job from one state to another
  rpc UpdateJobState(UpdateJobStateRequest) returns(google.protobuf.Empty) {}

  // RunLoadTest runs a load test.
  rpc RunLoadTest(RunLoadTestRequest) returns (RunLoadTestResponse) {}
  // RunLoadTestDefault runs the default load test.
  rpc RunLoadTestDefault(google.protobuf.Empty) returns (RunLoadTestResponse) {}

  // RenderTemplate renders the provided template and arguments into a list of Pipeline specicifications
  rpc RenderTemplate(RenderTemplateRequest) returns (RenderTemplateResponse) {}

  // ListTask lists PPS tasks
  rpc ListTask(taskapi.ListTaskRequest) returns (stream taskapi.TaskInfo) {}

  // GetKubeEvents returns a stream of kubernetes events
  rpc GetKubeEvents(LokiRequest) returns (stream LokiLogMessage) {}

  // QueryLoki returns a stream of loki log messages given a query string
  rpc QueryLoki(LokiRequest) returns (stream LokiLogMessage) {}
}

syntax = "proto3";

package pachyderm.worker;
option go_package = "github.com/pachyderm/pachyderm/v2/src/worker";

import "pps/pps.proto";
import "gogoproto/gogo.proto";
import "google/protobuf/empty.proto";

message CancelRequest {
  string job_id = 1 [(gogoproto.customname) = "JobID"];
  repeated string data_filters = 2;
}

message CancelResponse {
  bool success = 1;
}

// Error indicates that the processing of the current datum errored.
// Datum error semantics with datum batching enabled are similar to datum error
// semantics without datum batching enabled in that the datum may be retried,
// recovered, or result with a job failure.
message NextDatumRequest {
  string error = 1;
}

// Env is a list of environment variables that should be set for the processing
// of the next datum.
message NextDatumResponse {
  repeated string env = 1;
}

service Worker {
  rpc Status(google.protobuf.Empty) returns (pps_v2.WorkerStatus) {}
  rpc Cancel(CancelRequest) returns (CancelResponse) {}
  // NextDatum should only be called by user code running in a pipeline with
  // datum batching enabled.
  // NextDatum will signal to the worker code that the user code is ready to
  // proceed to the next datum. This generally means setting up the next
  // datum's filesystem state and updating internal metadata similarly to datum
  // processing in a normal pipeline.
  // NextDatum is a synchronous operation, so user code should expect to block
  // on this until the next datum is set up for processing.
  // User code should generally be migratable to datum batching by wrapping it
  // in a loop that calls next datum.
  rpc NextDatum(NextDatumRequest) returns (NextDatumResponse) {}
}

syntax = "proto3";

package debug_v2;
option go_package = "github.com/pachyderm/pachyderm/v2/src/debug";

import "google/protobuf/wrappers.proto";
import "google/protobuf/duration.proto";

import "pps/pps.proto";

message ProfileRequest {
  Profile profile = 1;
  Filter filter = 2;
}

message Profile {
    string name = 1;
    google.protobuf.Duration duration = 2; // only meaningful if name == "cpu"
}

message Filter {
  oneof filter {
    bool pachd = 1;
    pps_v2.Pipeline pipeline = 2;
    Worker worker = 3;
    bool database = 4;
  }
}

message Worker {
   string pod = 1;
   bool redirected = 2;
}

message BinaryRequest {
  Filter filter = 1;
}

message DumpRequest {
  Filter filter = 1;
  // Limit sets the limit for the number of commits / jobs that are returned for each repo / pipeline in the dump.
  int64 limit = 2;
}

message SetLogLevelRequest {
    enum LogLevel {
      UNKNOWN = 0;
      DEBUG = 1;
      INFO = 2;
      ERROR = 3;
      OFF = 4; // Only GRPC logs can be turned off.
    };
    oneof level {
      LogLevel pachyderm = 1;
      LogLevel grpc = 2;
    };
    google.protobuf.Duration duration = 3;
    bool recurse = 4;
}

message SetLogLevelResponse {
    repeated string affected_pods = 1;
    repeated string errored_pods = 2;
}

service Debug {
  rpc Profile(ProfileRequest) returns (stream google.protobuf.BytesValue) {}
  rpc Binary(BinaryRequest) returns (stream google.protobuf.BytesValue) {}
  rpc Dump(DumpRequest) returns (stream google.protobuf.BytesValue) {}
  rpc SetLogLevel(SetLogLevelRequest) returns (SetLogLevelResponse) {}
}

