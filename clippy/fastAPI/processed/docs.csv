,fname,text
0,Overview," Pachyderm is a data science platform that provides data-driven pipelines with version control and autoscaling. It is container-native, allowing developers to use the languages and libraries that are best suited to their needs, and runs across all major cloud providers and on-premises installations.
The platform is built on Kubernetes and integrates with standard tools for CI/CD, logging, authentication, and data APIs, making it scalable and incredibly flexible. Pachyderm‚Äôs data-driven pipelines allow you to automatically trigger data processing based on changes in your data, and the platform‚Äôs autoscaling capabilities ensure that resource utilization is optimized, maximizing developer efficiency.
"
1,Key Features," Key Features and Benefits # The following are the key features of Pachyderm that make it a powerful data processing platform.
Data-driven Pipelines # Automatically trigger pipelines based on changes in the data. Orchestrate batch or real-time data pipelines. Only process dependent changes in the data. Reproducibility and data lineage across all pipelines. Version Control # Track every change to your data automatically. Works with any file type. Supports collaboration through a git-like structure of commits. Autoscaling and Deduplication # Autoscale jobs based on resource demand. Automatically parallelize large data sets. Automatically deduplicate data across repositories. Flexibility and Infrastructure Agnosticism # Use existing cloud or on-premises infrastructure. Process any data type, size, or scale in batch or real-time pipelines. Container-native architecture allows for developer autonomy. Integrates with existing tools and services, including CI/CD, logging, authentication, and data APIs. "
2,Target Audience," Target Audience # Pachyderm is designed for data engineers and data scientists who are managing and processing large amounts of data in a scalable and efficient manner. Pachyderm is ideal for organizations working with big data and require robust, version-controlled, reproducible, and distributed data pipelines.
It is particularly useful for large unstructured data processing jobs, such as dataset curation for computer vision, speech recognition, video analytics, NLP, and many others.
Non-Target Audience # Pachyderm is not intended for users who do not require large-scale data processing and analysis. For instance, data scientists who are just starting with a small project may not need Pachyderm&rsquo;s distributed system. Additionally, users with limited experience with containerization, cloud computing, and distributed systems may find it challenging to use Pachyderm effectively.
"
3,Basic Concepts," Pachyderm File System # The Pachyderm File System (PFS) is the backbone of the Pachyderm data platform, providing a secure, scalable, and efficient way to store and manage large amounts of data. It is a version-controlled data management system that enables users to store any type of data in any format and scale, from a single file to a directory of files. The PFS is built on top of Postgres and S3, ensuring that your data is secure, consistent, and easily accessible. With PFS, users can version their data and work collaboratively with their teams, using branches and commits to manage and track changes over time.
Repositories (Repo) # Pachyderm repositories are version controlled, meaning that they keep track of changes to the data stored within them. Each repository can contain any type of data, including individual files or directories of files, and can handle data of any scale.
Learn more about Repositories
Branches # Branches in Pachyderm are similar to branches in Git. They are pointers to commits that move along a growing chain of commits. This allows you to work with different versions of your data within the same repository.
Learn more about Branches
Commits # A commit in Pachyderm is created automatically whenever data is added to or deleted from a repository. Each commit preserves the state of all files in the repository at the time of the commit, similar to a snapshot. Each commit is uniquely identifiable by a UUID and is immutable, meaning that the source data can never change.
Learn more about Commits
Pachyderm Pipeline System # The Pachyderm Pipeline System (PPS) is a core component of the Pachyderm platform, designed to run robust data pipelines in a scalable and reproducible manner. With PPS, you can define, execute, and monitor complex data transformations using code that is run in Docker containers. The output of each pipeline is version-controlled in a Pachyderm data repository, providing a complete, auditable history of all processing steps. In this way, PPS provides a flexible, data-driven solution for managing your data processing needs, while keeping data and processing results secure, reproducible, and scalable.
Learn more about the PPS
Pipelines # Pachyderm pipelines are used to transform data from Pachyderm repositories. The output data is versioned in a Pachyderm data repository, and the code for the transformation is run in Docker containers. Pipelines are triggered by new commits to a branch, making them data-driven.
Learn more about Pipelines
Jobs # A job in Pachyderm is the execution of a pipeline with a new commit. The data is distributed and parallelized computation is performed across a cluster. Each job is uniquely identified, making it possible to reproduce the results of a specific job.
Learn more about Jobs
Datum # A datum in Pachyderm is a unit of computation for a job. It is used to distribute the processing workloads and to define how data can be split for parallel processing.
Learn more about Datums
"
4,Intro to Data Versioning," Introduction to Data Versioning # On this page we want to give a brief overview of how to use and interact with versioned data inside Pachyderm. Collectively, this is often referred to as the Pachyderm File System (PFS).
Repositories # Data versioning in Pachyderm starts with creating a data repository. Pachyderm data repos are similar to Git repositories in that they provide a place to track changes made to a set of files.
Using the Pachyderm CLI (pachctl) we would create a repository called data with the create repo command.
pachctl create repo data Once a repo is created, data can be added, deleted, or updated to a branch and all changes are versioned with commits.
Commits # In Pachyderm, commits are made to branches of a repo. For example, in the following session if we add a file to our data repository, that file will be captured in a commit.
$ pachctl put file data@master -f my_file.bin $ pachctl list commit images@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master 6806cce 4 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE /my_file.bin file 57.27KiB If we then delete that file, it is removed from the active state of the branch, but the commit still exists.
$ pachctl delete file data@master:/my_file.bin $ pachctl list commit data@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master ff1867a 3 seconds ago 0B USER data master 6806cce 20 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE Then if we add the file back, we&rsquo;ll see a third commit.
$ pachctl create file data@master:/my_file.bin $ pachctl list commit data@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master 0ec029b 20 seconds ago 57.27KiB USER data master ff1867a 3 seconds ago 0B USER data master 6806cce 20 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE /my_file.bin file 57.27KiB Visualizing the commit history for the master branch looks like the following.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; tag: &#34;master&#34; Branches are a critical for tracking commits. The branch functions as a pointer to the most recent commit to the branch. For instance, when we create a new commit on the master branch (pachctl put file data@master -f my_new_file), we would create a new commit and our master branch would point at it.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;master&#34; As we&rsquo;ve already seen, we can reference the HEAD of the branch, with the syntax, data@master.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; type:HIGHLIGHT tag: &#34;HEAD&#34; Navigating Commits # Here we&rsquo;ll introduce the basics of how to navigate commits. Navigating these commits is an important aspect of working with PFS, and allows you to easily manage the history and evolution of your data.
One useful feature for navigating commits in PFS is the ability to refer to a previous commit using ancestry syntax. This syntax allows you to specify a commit relative to the current one, making it easy to compare and manipulate different versions of your data.
This makes it simple to switch between different versions of your data, and to perform operations like diffing, branching, and merging.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master^&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; type:HIGHLIGHT commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; To refer to the commit 2 before the HEAD:
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master^^&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; type:HIGHLIGHT commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; Similarly, we can abbreviate this with the following syntax:
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master^2&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; type:HIGHLIGHT commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; We can reference the commits in numerical order using .n, where n is the commit number.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master.1&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; type:HIGHLIGHT commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; %%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master.-1&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; type:HIGHLIGHT commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; Branches # In Pachyderm, branches are used to track changes in a repository. You can think of a branch as a tag on a specific commit. Branches are associated with a particular commit and are updated as new commits are made (moving the HEAD of that branch to its most recent commit). This also means that at any time, you can change the commit that a branch is associated with, affecting branch history.
Here&rsquo;s an example of a repo with three branches, each with its own history of commits:
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit commit branch v1.0 commit commit commit branch v1.1 commit commit commit tag:&#34;v1.1:HEAD&#34; checkout v1.0 commit tag:&#34;v1.0:HEAD&#34; checkout master commit tag:&#34;master:HEAD&#34; &ldquo;Merging&rdquo; Branches # The concept of merging binary data from different commits is complex. Ultimately, there are too many edge cases to do it reliably for every type of binary data, because computing a diff between two commits is ultimately meaningless unless you know how to compare the data. For example, we know that text files can be compared line-by-line or a bitmap image pixel by pixel, but how would we compute a diff for, say, binary model files?
Additionally, the output of a merge is usually a master copy, the official set of files desired. We rarely combine multiple pieces of image data to make one image, and if we are, we have usually created a technique for doing so. In the end, some files will be deleted, some updated, and some added.
Instead, merging data, means creating a new commit with the desired combination of files and pointing our branch at that commit. In order to maintain a proper history, we would also want to make sure that the parent of that commit is relevant to what we want as well.
For example, in this situation, we have created a branch, dev, based on the 1-2833cd3 commit. We have committed multiple times to the dev branch, but nothing to master.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit id:&#34;0-96e9b89&#34; commit id:&#34;1-2833cd3&#34; tag:&#34;master:HEAD&#34; branch dev commit id:&#34;2-25a8daf&#34; commit id:&#34;3-6413afc&#34; commit id:&#34;4-41a750b&#34; tag:&#34;dev:HEAD&#34; In this case it is simple to simply move the master branch to follow the most recent commit on dev, 4-41a750b.
pachctl create branch data@master --head 41a750b
Which would look like this:
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit id:&#34;0-96e9b89&#34; commit id:&#34;1-2833cd3&#34; branch dev commit id:&#34;2-25a8daf&#34; commit id:&#34;3-6413afc&#34; commit id:&#34;4-41a750b&#34; tag:&#34;master:HEAD, dev:HEAD&#34; Or from the history perspective of the respective branches:
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit id:&#34;0-96e9b89&#34; commit id:&#34;1-2833cd3&#34; branch dev commit id:&#34;2-25a8daf&#34; commit id:&#34;3-6413afc&#34; checkout dev commit tag:&#34;dev:HEAD&#34; id:&#34;4-41a750b&#34; checkout master commit id:&#34;2-25a8daf &#34; commit id:&#34;3-6413afc &#34; commit tag:&#34;master:HEAD&#34; id:&#34;4-41a750b &#34; Branches are useful for many reasons, but in Pachyderm they also form the foundation of the pipeline system. New commits on branches can be used to trigger pipelines to run, resulting in one of the key differentiators, data-driven pipelines.
"
5,Intro to Pipelines," Introduction to Pipelines # The Pachyderm Pipeline System (PPS) is a powerful tool for automating data transformations. With PPS, pipelines can be automatically triggered whenever input data changes, meaning that data transformations happen automatically in response to changes in your data, without the need for manual intervention.
Pipelines in Pachyderm are defined by a pipeline specification and run on Kubernetes. The output of a pipeline is stored in a versioned data repository, which allows you to reproduce any transformation that occurs in Pachyderm.
Pipelines can be combined into a computational DAG (directed acyclic graph), with each pipeline being triggered when an upstream commit is finished. This allows you to build complex workflows that can process large amounts of data efficiently and with minimal manual intervention.
Pipeline Specification # This is a Pachyderm pipeline definition in YAML. It describes a pipeline called transform that takes data from the data repository and transforms it using a Python script my_transform_code.py.
pipeline: name: transform input: pfs: repo: data glob: &#34;/*&#34; transform: image: my-transform-image:v1.0 cmd: - python - &#34;/my_transform_code.py&#34; - &#34;--input&#34; - &#34;/pfs/data/&#34; - &#34;--output&#34; - &#34;/pfs/out/&#34; Here&rsquo;s a breakdown of the different sections of the pipeline definition:
pipeline specifies the name of the pipeline (in this case, it&rsquo;s transform). This name will also be used as the name for the output data repository. input specifies the input for the pipeline. In this case, the input is taken from the data repository in Pachyderm. glob is used to specify how the files from the repository map to datums for processing. In this case, /* is used to specify all files in the repository can be processed individually. transform specifies the code and image to use for processing the input data. The image field specifies the Docker image to use for the pipeline. In this example, the image is named my-transform-image with a tag of v1.0. The cmd field specifies the command to run inside the container. In this example, the command is python /my_transform_code.py, which runs a Python script named my_transform_code.py. The script is passed the --input flag pointing to the input data directory, and the --output flag pointing to the output data directory. /pfs/data/ and /pfs/out/ are directories created by Pachyderm. The input directory will contain an individual datum when the job is running, and anything put into the output directory will be committed to the output repositories when the job is complete. So, in summary, this pipeline definition defines a pipeline called transform that takes all files in the data repository, runs a Python script to transform them, and outputs the results to the out repository.
Datums and Jobs # Pipelines can distribute work across a cluster to parallelize computation. Each time data is committed to a Pachyderm repository, a job is created for each pipeline with that repo as an input to process the data.
To determine how to distribute data and computational work, datums are used. A datum is an indivisible unit of data required by the pipeline, defined according to the pipeline spec. The datums will be distributed across the cluster to be processed by workers.
‚ÑπÔ∏è Only one job per pipeline will be created per commit, but there may be many datums per job.
For example, say you have a bunch of images that you want to normalize to a single size. You could iterate through each image and use opencv to change the size of it. No image depends on any other image, so this task can be parallelized by treating each image as an individual unit of work, a datum.
Next, let‚Äôs say you want to create a collage from those images. Now, we need to consider all of the images together to combine them. In this case, the collection of images would be a single datum, since they are all required for the process.
Pachyderm input specifications can handle both of these situations with the glob section of the Pipeline Specification.
Basic Glob Patterns # In this section we&rsquo;ll introduce glob patterns and datums in a couple of examples.
In the basic glob pattern example below, the input glob pattern is /*. This pattern matches each image at the top level of the images@master branch as an individual unit of work.
pipeline: name: resize description: A pipeline that resizes an image. input: pfs: glob: /* repo: images transform: cmd: - python - resize.py - --input - /pfs/images/* - --output - /pfs/out/ image: pachyderm/opencv When the pipeline is executed, it retrieves the datums defined in the input specification. For each datum, the worker downloads the necessary files into the Docker container at the start of its execution, and then performs the transform. Once the execution is complete, the output for each execution is combined into a commit and written to the output data repository.
In this example, the input glob pattern is /. This pattern matches everything at the top level of the images@master branch as an individual unit of work.
pipeline: name: collage description: A pipeline that creates a collage for a collection of images. input: pfs: glob: / repo: images transform: cmd: - python - collage.py - --input - /pfs/images/* - --output - /pfs/out/ image: pachyderm/opencv When this pipeline runs, it retrieves a single datum from the input specification. The job runs the single datum, downloading all the files from the images@master into the Docker container, and performs the transform. The result is then committed to the output data repository.
Advanced Glob Patterns # Datums can also be created from advanced operations, such as Join, Cross, Group, Union, and others to combine glob patterns from multiple data repositories. This allows us to create complex datum definitions, enabling sophisticated data processing pipelines.
Pipeline Communication (Advanced) # A much more detailed look at how Pachyderm actually triggers pipelines is shown in the sequence diagram below. This is a much more advanced level of detail, but knowing how the different pieces of the platform interact can be useful.
Before we look at the diagram, it may be helpful to provide a brief recap of the main participants involved:
User: The user is the person interacting with Pachyderm, typically through the command line interface (CLI) or one of the client libraries. PFS (Pachyderm File System): PFS is the underlying file system that stores all of the data in Pachyderm. It provides version control and lineage tracking for all data inside it. PPS (Pachyderm Pipeline System): PPS is how code gets applied to the data in Pachyderm. It manages the computational graph, which describes the dependencies between different steps of the data processing pipeline. Worker: Workers are Kubernetes pods that executes the jobs defined by PPS. Each worker runs a container image that contains the code for a specific pipeline. The worker will iterate through the datums it is given and apply user code to it. sequenceDiagram participant User participant PPS participant PFS participant Worker User-&gt;&gt;PFS: pachctl create repo foo activate PFS Note over PFS: create branch foo@master deactivate PFS User-&gt;&gt;PPS: pachctl create pipeline bar activate PPS PPS-&gt;&gt;PFS: create branch bar@master &lt;br&gt; (provenant on foo@master) PPS-&gt;&gt;Worker: create pipeline worker master Worker-&gt;&gt;PFS: subscribe to bar@master &lt;br&gt; (because it&#39;s subvenant on foo@master) deactivate PPS User-&gt;&gt;PFS: pachctl put file -f foo@master data.txt activate PFS Note over PFS: start commit PFS-&gt;&gt;PFS: propagate commit &lt;br&gt; (start downstream commits) Note over PFS: copy data.txt to open commit Note over PFS: finish commit PFS--&gt;&gt;Worker: subscribed commit returns deactivate PFS Note over Worker: Pipeline Triggered activate Worker Worker-&gt;&gt;PPS: Create job Worker-&gt;&gt;PFS: request datums for commit PFS--&gt;&gt;Worker: Datum list loop Each datum PFS-&gt;&gt;Worker: download datum Note over Worker: Process datum with user code Worker-&gt;&gt;PFS: copy data to open output commit end Worker-&gt;&gt;PFS: Finish commit Worker-&gt;&gt;PPS: Finish job deactivate Worker This diagram illustrates the data flow and interaction between the user, the Pachyderm Pipeline System (PPS), the Pachyderm File System (PFS), and a worker node when creating and running a Pachyderm pipeline. Note, this is simplified for the single worker case. The multi-worker and autoscaling mechanisms are more complex.
The sequence of events begins with the user creating a PFS repo called foo and a PPS pipeline called bar with the foo repo as its input. When the pipeline is created, PPS creates a branch called bar@master, which is provenant on the foo@master branch in PFS. A worker pod is then created in the Kubernetes cluster by PPS, which subscribes to the bar@master branch.
When the user puts a file named data.txt into the foo@master branch, PFS starts a new commit and propagates the commit, opening downstream commits for anything impacted. The worker receives the subscribed commit and when it finishes, triggers the pipeline.
The triggered pipeline creates a job for the pipeline, requesting datums for the output commit. For each datum, the worker downloads the data, processes it with the user&rsquo;s code, and writes the output to an open output commit in PFS. Once all datums have been processed, the worker finishes the output commit and the job is marked as complete.
"
6,Get Started," What is Pachyderm? # Pachyderm is a data-centric pipeline and data versioning application written in go that runs on top of a Kubernetes cluster.
Local vs Cloud Installation # Local Cloud Used for learning &amp; testing. Used in production environments. Allocates your local machine&rsquo;s resources to spin up a K8s cluster. Uses a cloud provider (AWS, Azure, GCP) to to spin up K8s clusters. Uses Docker Desktop, Minikube, or Kind. Uses EKS, GKE, or AKS Free. Metered by cloud provider. "
7,Local Getting Started Guides," What is a Local Installation? # A local installation means that you will allocate resources from your local machine (e.g., your laptop) to spin up a Kubernetes cluster to run Pachyderm. This installation method is not for a production setup, but is great for personal use, testing, and product exploration.
Which Guide Should I Use? # Both the Docker Desktop and Minikube installation guides support MacOS, Windows, and Linux. If this is your first time using Kubernetes, try Docker Desktop &mdash; if you are experienced with Kubernetes, you can deploy using a variety of solutions not listed here (KinD, Rancher Desktop, Podman, etc.).
üí° Binary Files (Advanced Users)
You can download the latest binary files from GitHub for a direct installation of pachctl and the mount-server.
"
8,Docker Desktop," Before You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:
Open a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each of the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; All installation steps after 1. Install Docker Desktop must be run through the WSL terminal (Ubuntu) and not in Powershell.
You are now ready to continue to Step 1.
You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; 1. Install Docker Desktop # Install Docker Desktop for your machine. Navigate to Settings for Mac, Windows, or Linux. Adjust your resources (~4 CPUs and ~12GB Memory) Enable Kubernetes Select Apply &amp; Restart. 2. Install Pachctl CLI # Operating System: MacOs, Windows, &amp; Darwin Debian brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.5 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.5.1/pachctl_2.5.1_amd64.deb &amp;&amp; sudo dpkg -i /tmp/pachctl.deb 3. Install &amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install PachD: Version: Community Edition Enterprise helm install pachd pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:
helm install pachd pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --set pachd.enterpriseLicenseKey=$(cat license.txt) --set ingress.host=localhost This unlocks Enterprise features but also requires user authentication to access Console. A mock user is created by default to get you started, with the username: admin and password: password.
This may take several minutes to complete.
4. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 5. Connect to Cluster # pachctl connect grpc://localhost:80 ‚ÑπÔ∏è If the connection commands did not work together, run each separately.
Optionally open your browser and navigate to the Console UI.
üí° You can check your Pachyderm version and connection to pachd at any time with the following command:
pachctl version COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
9,Minikube,"Minikube is a tool that quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. It&rsquo;s a great solution for trying out Pachyderm locally.
Before You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have Docker Desktop installed with Kubernetes enabled. You must have Docker Desktop installed with Kubernetes enabled. You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:
Open a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; All installation steps after this point must be run through the WSL terminal (Ubuntu) and not in Powershell.
You are now ready to continue to Step 1.
You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have Docker Desktop installed. 1. Install Docker # brew install docker See the official Docker getting started guide for the most up-to-date installation steps.
2. Install &amp; Start Minikube # Install # brew install minikube See the official Minikube getting started guide for the most up-to-date installation steps.
Start # Launch Docker Desktop. Start Minikube: minikube start 3. Install Pachctl CLI # brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.5 4. Install &amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install PachD: Version: Community Edition Enterprise helm install pachd pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:
helm install pachd pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --set pachd.enterpriseLicenseKey=$(cat license.txt) --set ingress.host=localhost This unlocks Enterprise features but also requires user authentication to access Console. A mock user is created by default to get you started, with the username: admin and password: password.
This may take several minutes to complete.
5. Verify Installation # Run the following command in a new terminal to check the status of your pods: kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE default console-6b9bb8766d-f2gm4 1/1 Running 0 41m default etcd-0 1/1 Running 0 41m default pachd-76896d6b5d-kmfvw 1/1 Running 0 41m default pachd-loki-0 1/1 Running 0 41m default pachd-promtail-rm5ss 1/1 Running 0 41m default pachyderm-kube-event-tail-b9b554fb6-dpcsr 1/1 Running 0 41m default pg-bouncer-5c9494c678-z57qh 1/1 Running 0 41m default postgres-0 1/1 Running 0 41m kube-system coredns-6d4b75cb6d-jnl5f 1/1 Running 3 (42m ago) 97d kube-system etcd-minikube 1/1 Running 4 (42m ago) 97d kube-system kube-apiserver-minikube 1/1 Running 3 (42m ago) 97d kube-system kube-controller-manager-minikube 1/1 Running 4 (42m ago) 97d kube-system kube-proxy-bngzv 1/1 Running 3 (42m ago) 97d kube-system kube-scheduler-minikube 1/1 Running 3 (42m ago) 97d kube-system storage-provisioner 1/1 Running 5 (42m ago) 97d kubernetes-dashboard dashboard-metrics-scraper-78dbd9dbf5-swttf 1/1 Running 3 (42m ago) 97d kubernetes-dashboard kubernetes-dashboard-5fd5574d9f-c7ptx 1/1 Running 4 (42m ago) 97d Re-run this command after a few minutes if pachd is not ready. 6. Connect to Cluster # pachctl connect grpc://localhost:80 ‚ÑπÔ∏è If the connection commands did not work together, run each separately.
Optionally open your browser and navigate to the Console UI.
üí° You can check your Pachyderm version and connection to pachd at any time with the following command:
pachctl version COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
10,AWS + Pachyderm," Before You Start # This guide assumes that you have already tried Pachyderm locally and have all of the following installed:
Kubectl Pachctl Helm AWS CLI Eksctl 1. Create an EKS Cluster # Use the eksctl tool to deploy an EKS Cluster: eksctl create cluster --name pachyderm-cluster --region &lt;region&gt; -profile &lt;your named profile&gt; Verify deployment: kubectl get all 2. Create an S3 Bucket # Run the following command: aws s3api create-bucket --bucket ${BUCKET_NAME} --region ${AWS_REGION} Verify. aws s3 ls 3. Enable Persistent Volumes Creation # Create an IAM OIDC provider for your cluster. Install the Amazon EBS Container Storage Interface (CSI) driver on your cluster. Create a gp3 storage class manifest file (e.g., gp3-storageclass.yaml) kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp3 annotations: storageclass.kubernetes.io/is-default-class: &#34;true&#34; provisioner: kubernetes.io/aws-ebs parameters: type: gp3 fsType: ext4 Set gp3 to your default storage class. kubectl apply -f gp3-storageclass.yaml Verify that it has been set as your default. kubectl get storageclass 4. Create a Values.yaml # Version: Community Edition Enterprise deployTarget: &#34;AMAZON&#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: amazon: bucket: &#34;bucket_name&#34; # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; region: &#34;us-east-2&#34; externalService: enabled: true console: enabled: true deployTarget: &#34;AMAZON&#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: amazon: bucket: &#34;bucket_name&#34; # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; region: &#34;us-east-2&#34; # pachyderm enterprise key enterpriseLicenseKey: &#34;YOUR_ENTERPRISE_TOKEN&#34; console: enabled: true 5. Configure Helm # Run the following to add the Pachyderm repo to Helm:
helm repo add pach https://helm.pachyderm.com helm repo update helm install pachd pach/pachyderm -f my_pachyderm_values.yaml 6. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 7. Connect to Cluster # pachctl connect grpc://localhost:80 ‚ÑπÔ∏è If the connection commands did not work together, run each separately.
Optionally open your browser and navigate to the Console UI.
üí° You can check your Pachyderm version and connection to pachd at any time with the following command:
pachctl version COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
11,Azure + Pachyderm," Before You Start # This guide assumes that you have already tried Pachyderm locally and have all of the following installed:
Kubectl Pachctl Helm Azure CLI. 1. Create an AKS Cluster # You can deploy Kubernetes on Azure by following the official Azure Kubernetes Service documentation, use the quickstart walkthrough, or follow the steps in this section.
At a minimum, you will need to specify the parameters below:
Variable Description RESOURCE_GROUP A unique name for the resource group where Pachyderm is deployed. For example, pach-resource-group. LOCATION An Azure availability zone where AKS is available. For example, centralus. NODE_SIZE The size of the Kubernetes virtual machine (VM) instances. To avoid performance issues, Pachyderm recommends that you set this value to at least Standard_DS4_v2 which gives you 8 CPUs, 28 Gib of Memory, 56 Gib SSD.
In any case, use VMs that support premium storage. See Azure VM sizes for details around which sizes support Premium storage. CLUSTER_NAME A unique name for the Pachyderm cluster. For example, pach-aks-cluster. You can choose to follow the guided steps in Azure Service Portal&rsquo;s Kubernetes Services or use Azure CLI.
Log in to Azure:
az login This command opens a browser window. Log in with your Azure credentials. Resources can now be provisioned on the Azure subscription linked to your account.
Create an Azure resource group or retrieve an existing group.
az group create --name ${RESOURCE_GROUP} --location ${LOCATION} Example:
az group create --name test-group --location centralus System Response:
{ &#34;id&#34;: &#34;/subscriptions/6c9f2e1e-0eba-4421-b4cc-172f959ee110/resourceGroups/pach-resource-group&#34;, &#34;location&#34;: &#34;centralus&#34;, &#34;managedBy&#34;: null, &#34;name&#34;: &#34;pach-resource-group&#34;, &#34;properties&#34;: { &#34;provisioningState&#34;: &#34;Succeeded&#34; }, &#34;tags&#34;: null, &#34;type&#34;: null } Create an AKS cluster in the resource group/location:
For more configuration options: Find the list of all available flags of the az aks create command.
az aks create --resource-group ${RESOURCE_GROUP} --name ${CLUSTER_NAME} --node-vm-size ${NODE_SIZE} --node-count &lt;node_pool_count&gt; --location ${LOCATION} Example:
az aks create --resource-group test-group --name test-cluster --generate-ssh-keys --node-vm-size Standard_DS4_v2 --location centralus Confirm the version of the Kubernetes server by running kubectl version.
‚ÑπÔ∏è &ldquo;See Also:&rdquo; - Azure Virtual Machine sizes
Once your Kubernetes cluster is up, and your infrastructure configured, you are ready to prepare for the installation of Pachyderm. Some of the steps below will require you to keep updating the values.yaml started during the setup of the recommended infrastructure:
2. Create a Storage Container # Pachyderm needs an Azure Storage Container (Object store) to store your data.
To access your data, Pachyderm uses a Storage Account with permissioned access to your desired container. You can either use an existing account or create a new one in your default subscription, then use the JSON key associated with the account and pass it on to Pachyderm.
Set up the following variables:
STORAGE_ACCOUNT: The name of the storage account where you store your data. CONTAINER_NAME: The name of the Azure blob container where you store your data. Create an Azure storage account:
az storage account create \ --resource-group=&#34;${RESOURCE_GROUP}&#34; \ --location=&#34;${LOCATION}&#34; \ --sku=Premium_LRS \ --name=&#34;${STORAGE_ACCOUNT}&#34; \ --kind=BlockBlobStorage System response:
{ &#34;accessTier&#34;: null, &#34;creationTime&#34;: &#34;2019-06-20T16:05:55.616832+00:00&#34;, &#34;customDomain&#34;: null, &#34;enableAzureFilesAadIntegration&#34;: null, &#34;enableHttpsTrafficOnly&#34;: false, &#34;encryption&#34;: { &#34;keySource&#34;: &#34;Microsoft.Storage&#34;, &#34;keyVaultProperties&#34;: null, &#34;services&#34;: { &#34;blob&#34;: { &#34;enabled&#34;: true, ... Make sure that you set Stock Keeping Unit (SKU) to Premium_LRS and the kind parameter is set to BlockBlobStorage. This configuration results in a storage that uses SSDs rather than standard Hard Disk Drives (HDD). If you set this parameter to an HDD-based storage option, your Pachyderm cluster will be too slow and might malfunction.
Verify that your storage account has been successfully created:
az storage account list Obtain the key for the storage account (STORAGE_ACCOUNT) and the resource group to be used to deploy Pachyderm:
STORAGE_KEY=&#34;$(az storage account keys list \ --account-name=&#34;${STORAGE_ACCOUNT}&#34; \ --resource-group=&#34;${RESOURCE_GROUP}&#34; \ --output=json \ | jq &#39;.[0].value&#39; -r )&#34; ‚ÑπÔ∏è Find the generated key in the Storage accounts &gt; Access keys section in the Azure Portal or by running the following command az storage account keys list --account-name=${STORAGE_ACCOUNT}.
Create a new storage container within your storage account:
az storage container create --name ${CONTAINER_NAME} \ --account-name ${STORAGE_ACCOUNT} \ --account-key &#34;${STORAGE_KEY}&#34; 3. Create a Values.yaml # Version: Community Edition Enterprise deployTarget: &#34;MICROSOFT&#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: microsoft: # storage container name container: &#34;blah&#34; # storage account name id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # storage account key secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; externalService: enabled: true console: enabled: true deployTarget: &#34;MICROSOFT&#34; proxy: enabled: true service: type: LoadBalancer ingress: host: &lt;insert-external-ip-address-or-dns-name&gt; pachd: storage: microsoft: # storage container name container: &#34;blah&#34; # storage account name id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # storage account key secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; # pachyderm enterprise key enterpriseLicenseKey: &#34;YOUR_ENTERPRISE_TOKEN&#34; console: enabled: true 4. Configure Helm # Run the following to add the Pachyderm repo to Helm:
helm repo add pach https://helm.pachyderm.com helm repo update helm install pachd pach/pachyderm -f my_pachyderm_values.yaml 5. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 6. Connect to Cluster # pachctl connect grpc://localhost:80 ‚ÑπÔ∏è If the connection commands did not work together, run each separately.
Optionally open your browser and navigate to the Console UI.
üí° You can check your Pachyderm version and connection to pachd at any time with the following command:
pachctl version COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
12,GCP + Pachyderm," Before You Start # This guide assumes that:
You have already tried Pachyderm locally and have some familiarity with Kubectl, Helm, Google Cloud SDK and jq. You have access to a Google Cloud account linked to an active billing account. ‚ö†Ô∏è This is not a production-level setup guide; see the Google Cloud Platform deploy guide for in-depth setup.
1. Create a New Project # Log in to Google Cloud Console. Create a new project (e.g.,pachyderm-quickstart-project). Enable the Compute Engine API. You are now ready to create a GKE Cluster.
2. Run Setup Script # You can run this setup script either through the Cloud Shell or in a local terminal via the gcloud cli. Running this script creates all of the following:
One GKE cluster Workload identity service accounts Permissions A static IP address The cloud SQL instance and databases One cloud storage bucket One file called ${NAME}.values.yaml in the current directory It also installs Pachyderm into the cluster.
3. Connect to Cluster # Run the following: pachctl config import-kube local --overwrite pachctl config set active-context local pachctl port-forward Open your browser at localhost:4000. üí° You can also connect to Console via Google&rsquo;s Cloud Shell:
"
13,Install Pachctl Auto-completion,"Pachyderm autocompletion allows you to automatically finish partially typed commands by pressing TAB. Autocompletion needs to be installed separately when pachctl is already available on your client machine.
Pachyderm autocompletion is supported for bash and zsh shells. You must have either of them preinstalled before installing Pachyderm autocompletion.
üí° Type pachctl completion --help to display help information about the command.
Command Shell: Zsh Bash Verify that bash-completion is installed on your machine. For example, if you have installed bash completion by using Homebrew, type:
brew info bash-completion This command returns information about the directory in which bash-completion and bash completion scripts are installed. For example, /usr/local/etc/bash_completion.d/. You need to specify the path to bash_completion.d as the path to which install pachctl autocompletion. Also, the output of the info command might have a suggestion to include the path to bash-completion into your ~/.bash_profile file.
Install pachctl autocompletion:
pachctl completion bash --install --path &lt;path/to/bash-completion&gt; For example, if you specify the path to bash-completion as /usr/local/etc/bash_completion.d/pachctl, your system response looks like this:
System response:
Bash completions installed in /usr/local/etc/bash_completion.d/pachctl, you must restart bash to enable completions. Restart your terminal.
pachctl autocomplete should now be enabled in your system.
Verify that zsh-completions are installed on your machine. For example, if you have installed zsh completion by using Homebrew, type:
brew info zsh-completions You should see the directory in which zsh-completions are installed and instructions to add the correct path in the ~/.zshrc file. Make sure you add the required path. If you do not have the ~/.zshrc file on your computer, create one. For more information about setting up zsh completions, see zsh-completions.
Install pachctl autocompletion for zsh:
pachctl completion zsh --install --path &lt;path/to/zsh-completions&gt; Example:
pachctl completion zsh --install --path /usr/local/share/zsh-completions/_pachctl System response:
Completions installed in &#34;_pachctl&#34;, you must restart your terminal to enable them. Restart your terminal.
pachctl autocomplete should now be enabled in your system.
"
14,Beginner Tutorial," Before You Start # Install Pachyderm either locally our within the cloud. Install Pachyderm Shell. Join our Slack Community so you can ask any questions you may have! Context # Pachyderm creates a Kubernetes cluster that you interact with using either the pachctl CLI or through Console, a GUI.
pachctl is great for users already experienced with using a CLI. Console is great for beginners and helps with visualizing relationships between projects, repos, and pipelines. Within the cluster, you can create projects that contain repos and pipelines. Pipelines can be single-stage or multi-stage; multi-stage pipelines are commonly referred to as DAGs.
Tutorial: Image processing with OpenCV # In this tutorial you&rsquo;ll create an image edge detection pipeline that processes new data as it is added and outputs the results.
1. Create a Project # To keep our work organized, we&rsquo;re going to create a project named openCV and set it to our currently active context.
pachctl create project openCV pachctl config update context --project openCV You can always check to confirm which project has been set to your context by running the following commands:
# prints current context name (local) pachctl config get active-context # prints local&#39;s context details pachctl config get context local # { # &#34;source&#34;: &#34;IMPORTED&#34;, # &#34;cluster_name&#34;: &#34;docker-desktop&#34;, # &#34;auth_info&#34;: &#34;docker-desktop&#34;, # &#34;cluster_deployment_id&#34;: &#34;dev&#34;, # &#34;project&#34;: &#34;openCV&#34; # } 2. Create a Repo # Repos should be dedicated to a single source of data such as log messages from a particular service, a users table, or training data.
pachctl create repo images You can verify that the repository was created by running the following command:
pachctl list repo # NAME CREATED SIZE (MASTER) ACCESS LEVEL # images 4 seconds ago ‚â§ 0B [repoOwner] 3. Add Data # In Pachyderm, you write data to an explicit commit. Commits are immutable snapshots of your data which give Pachyderm its version control properties. You can add, remove, or update files in a given commit.
Upload an Image File # We&rsquo;re going to use the pachctl put file command, along with the -f flag, to upload an image.
pachctl put file images@master:liberty.png -f http://imgur.com/46Q8nDz.png pachctl put file automatically starts and finishes a commit for you so you can add files more easily.
üí° If you want to add many files over a period of time, you can do pachctl start commit and pachctl finish commit yourself.
You can confirm the commit using the following command:
pachctl list commit images # REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION # openCV/images master 37559e89ed0c4a0cb354649524050851 10 seconds ago 57.27KiB USER You can also view the filename in the commit using the following command:
pachctl list file images@master # NAME TYPE SIZE # /liberty.png file 57.27KiB View Image # In Terminal # Operating System: MacOS Linux pachctl get file images@master:liberty.png | open -f -a Preview.app pachctl get file images@master:liberty.png | display In Console # In your Console, click on the images repo to visualize its commit and inspect its file:
4. Create a Pipeline # Now that you have some data in your repo, it is time to do something with it using a pipeline.
Pipelines process data and are defined using a JSON pipeline specification. For this tutorial, we&rsquo;ve already created the spec for you.
Review Pipeline Spec # Take a moment to review the details of the provided pipeline spec so that you&rsquo;ll know how to create one on your own in the future.
{ // The `pipeline` section contains a `name` for identification; this name is also used to create a corresponding output repo. &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;description&#34;: &#34;A pipeline that performs image edge detection by using the OpenCV library.&#34;, // The `transform` section allows you to specify the docker `image` you want to use (`pachyderm/opencv:1.0`)and the `cmd` that defines the entry point (`edges.py`). &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/edges.py&#34; ], &#34;image&#34;: &#34;pachyderm/opencv:1.0&#34; }, // The input section specifies repos visible to the running pipeline, and how to process the data from the repos. // Commits to these repos trigger the pipeline to create new processing jobs. In this case, `images` is the repo, and `/*` is the glob pattern. &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;images&#34;, // The glob pattern defines how the input data will be transformed into datum if you want to distribute computation. `/*` means that each file can be processed individually. &#34;glob&#34;: &#34;/*&#34; } } } The following extract is the Python code run in this pipeline:
import cv2 import numpy as np from matplotlib import pyplot as plt import os # make_edges reads an image from /pfs/images and outputs the result of running # edge detection on that image to /pfs/out. Note that /pfs/images and # /pfs/out are special directories that Pachyderm injects into the container. def make_edges(image): img = cv2.imread(image) tail = os.path.split(image)[1] edges = cv2.Canny(img,100,200) plt.imsave(os.path.join(&#34;/pfs/out&#34;, os.path.splitext(tail)[0]+&#39;.png&#39;), edges, cmap = &#39;gray&#39;) # walk /pfs/images and call make_edges on every file found for dirpath, dirs, files in os.walk(&#34;/pfs/images&#34;): for file in files: make_edges(os.path.join(dirpath, file)) The code simply walks over all the images in /pfs/images, performs edge detection, and writes the result to /pfs/out.
/pfs/images and /pfs/out are special local directories that Pachyderm creates within the container automatically. Input data is stored in /pfs/&lt;input_repo_name&gt;. ‚ÑπÔ∏è Your code must write out to /pfs/out (see the function make_edges(image) above). Pachyderm gathers data written to /pfs/out, versions it, and maps it to the pipeline&rsquo;s output repo of the same name.
Now, let&rsquo;s create the pipeline in Pachyderm:
pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/2.5.x/examples/opencv/edges.json Again, check the end result in your Console: What Happens When You Create a Pipeline # When you create a pipeline, Pachyderm transforms all current and future data added to your input repo using your user code. This process is known as a job. The initial job downloads the specified Docker image that is used for all future jobs.
View the job: pachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # 23378d899d3d45738f55df3809841145 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 5 seconds ago 5 seconds ago Check the state of your pipeline: pachctl list pipeline # NAME VERSION INPUT CREATED STATE / LAST JOB DESCRIPTION # edges 1 images:/* 2 minutes ago running / success A pipeline that performs image edge detection by using the OpenCV library. List your repositories: pachctl list repo # NAME CREATED SIZE (MASTER) ACCESS LEVEL # edges 10 minutes ago ‚â§ 22.22KiB [repoOwner] Output repo for pipeline edges. # images 3 hours ago ‚â§ 57.27KiB [repoOwner] Reading the Output # We can view the output data from the edges repo in the same fashion that we viewed the input data.
Operating System: MacOS Linux pachctl get file edges@master:liberty.png | open -f -a Preview.app pachctl get file edges@master:liberty.png | display Processing More Data # Create two new commits: pachctl put file images@master:AT-AT.png -f http://imgur.com/8MN9Kg0.png pachctl put file images@master:kitten.png -f http://imgur.com/g2QnNqa.png View the list of jobs that have started: pachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # 1c1a9d7d36944eabb4f6f14ebca25bf1 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 31 seconds ago 31 seconds ago # fe5c4f70ac4347fd9c5934f0a9c44651 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 47 seconds ago 47 seconds ago # 23378d899d3d45738f55df3809841145 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 12 minutes ago 12 minutes ago View the output data: Operating System: MacOS Linux pachctl get file edges@master:AT-AT.png | open -f -a Preview.app pachctl get file edges@master:kitten.png | open -f -a Preview.app pachctl get file edges@master:AT-AT.png | display pachctl get file edges@master:kitten.png | display 5. Create a DAG # Currently, we&rsquo;ve only set up a single-stage pipeline. Let&rsquo;s create a multi-stage pipeline (also known as a DAG) by adding a montage pipeline that takes our both original and edge-detected images and arranges them into a single montage of images:
Below is the pipeline spec for this new pipeline:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;montage&#34; }, &#34;description&#34;: &#34;A pipeline that combines images from the `images` and `edges` repositories into a montage.&#34;, &#34;input&#34;: { &#34;cross&#34;: [ { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/&#34;, &#34;repo&#34;: &#34;images&#34; } }, { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/&#34;, &#34;repo&#34;: &#34;edges&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;sh&#34; ], &#34;image&#34;: &#34;v4tech/imagemagick&#34;, &#34;stdin&#34;: [ &#34;montage -shadow -background SkyBlue -geometry 300x300+2+2 $(find /pfs -type f | sort) /pfs/out/montage.png&#34; ] } } This montage pipeline spec is similar to our edges pipeline except for the following differences:
We are using a different Docker image that has imagemagick installed. We are executing a sh command with stdin instead of a python script in the pipeline&rsquo;s transform section. We have multiple input data repositories (images and edges). In the montage pipeline we are combining our multiple input data repositories using a cross pattern. This cross pattern creates a single pairing of our input images with our edge detected images.
Create the montage pipeline: pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/2.5.x/examples/opencv/montage.json View the triggered jobs: pachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # 01e0c8040e18429daf7f67ce34c3a5d7 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 11 seconds ago 11 seconds ago # 1c1a9d7d36944eabb4f6f14ebca25bf1 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 12 minutes ago 12 minutes ago # fe5c4f70ac4347fd9c5934f0a9c44651 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 12 minutes ago 12 minutes ago # 23378d899d3d45738f55df3809841145 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 24 minutes ago 24 minutes ago View the generated montage image: Operating System: MacOS Linux pachctl get file montage@master:montage.png | open -f -a Preview.app pachctl get file montage@master:montage.png | display "
15,Concepts,"This section details the foundational concepts of Pachyderm&rsquo;s data versioning and pipeline semantics broken down into two main components:
Pachyderm File System (PFS) manages Pachyderm&rsquo;s data and versioning system. Pachyderm Pipeline System (PPS) enables you to perform various transformations on your data. "
16,Deferred Processing,"While a Pachyderm pipeline is running, it processes any new data that you commit to its input branch. However, in some cases, you want to commit data more frequently than you want to process it.
Because Pachyderm pipelines do not reprocess the data that has already been processed, in most cases, this is not an issue. But, some pipelines might need to process everything from scratch. For example, you might want to commit data every hour, but only want to retrain a machine learning model on that data daily because it needs to train on all the data from scratch.
In these cases, you can leverage a massive performance benefit from deferred processing. This section covers how to achieve that and control what gets processed.
Pachyderm controls what is being processed by using the filesystem, rather than at the pipeline level. Although pipelines are inflexible, they are simple and always try to process the data at the heads of their input branches. In contrast, the filesystem is very flexible and gives you the ability to commit data in different places and then efficiently move and rename the data so that it gets processed when you want.
Configure a Staging Branch in an Input repository # When you want to load data into Pachyderm without triggering a pipeline, you can upload it to a staging branch and then submit accumulated changes in one batch by re-pointing the HEAD of your master branch to a commit in the staging branch.
Although, in this section, the branch in which you consolidate changes is called staging, you can name it as you like.
‚ÑπÔ∏è You can have multiple staging branches. For example, dev1, dev2, staging&hellip;
In the example below, we first create a repository called data on which we configure a staging branch:
üí° A simple pipeline subscribes to the master branch of the repo data:
{ &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*&#34;, } } Create a repository. For example, data.
pachctl create repo data Create a master branch.
pachctl create branch data@master View the created branch:
pachctl list commit data REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION data master 8090bfb4d4fe44158eac12199c37a591 About a minute ago 0B AUTO Pachyderm automatically created an empty HEAD commit on the new branch, as you can see from the zero-byte size and AUTO commit origin. When you commit data to the master branch, the pipeline immediately starts a job to process it. However, if you want to commit something without immediately processing it, you need to commit it to a different branch.
Commit a file to the staging branch:
pachctl put file data@staging -f &lt;file&gt; Pachyderm automatically creates the staging branch. Your repo now has 2 branches, staging and master. In this example, the staging name is used, but you can name the branch as you want.
Verify that the branches were created:
pachctl list branch data BRANCH HEAD TRIGGER staging f3506f0fab6e483e8338754081109e69 - master 8090bfb4d4fe44158eac12199c37a591 - The master branch still has the same HEAD commit. No jobs have started to process the new file, because there are no pipelines that take staging as inputs. You can continue to commit to staging to add new data to the branch, and the pipeline will not process anything.
When you are ready to process the data, update the master branch to point it to the head of the staging branch:
pachctl create branch data@master --head staging List your branches to verify that the master branch&rsquo;s HEAD commit has changed:
pachctl list branch data staging f3506f0fab6e483e8338754081109e69 master f3506f0fab6e483e8338754081109e69 The master and staging branches now have the same HEAD commit. This means that your pipeline has data to process.
Verify that the pipeline has new jobs:
pachctl list job test@f3506f0fab6e483e8338754081109e69 ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE f3506f0fab6e483e8338754081109e69 test 32 seconds ago Less than a second 0 6 + 0 / 6 108B 24B success You should see one job that Pachyderm created for all the changes you have submitted to the staging branch, with the same ID. While the commits to the staging branch are ancestors of the current HEAD in master, they were never the actual HEAD of master themselves, so they do not get processed. This behavior works for most of the use cases because commits in Pachyderm are generally additive, so processing the HEAD commit also processes data from previous commits.
Process Specific Commits # Sometimes you want to process specific intermediary commits that are not in the HEAD of the branch. To do this, you need to set master to have these commits as HEAD. For example, if you submitted ten commits in the staging branch and you want to process the seventh, third, and most recent commits, you need to run the following commands respectively:
pachctl create branch data@master --head staging^7 pachctl create branch data@master --head staging^3 pachctl create branch data@master --head staging When you run the commands above, Pachyderm creates a job for each of the commands one after another. Therefore, when one job is completed, Pachyderm starts the next one. To verify that Pachyderm created jobs for these commands, run pachctl list job -p &lt;pipeline_name&gt; --history all.
Change the HEAD of your Branch # You can move backward to previous commits as easily as advancing to the latest commits. For example, if you want to change the final output to be the result of processing staging^1, you can roll back your HEAD commit by running the following command:
pachctl create branch data@master --head staging^1 This command starts a new job to process staging^1. The HEAD commit on your output repo will be the result of processing staging^1 instead of staging.
Copy Files from One Branch to Another # Using a staging branch allows you to defer processing. To use this functionality you need to know your input commits in advance. However, sometimes you want to be able to commit data in an ad-hoc, disorganized manner and then organize it later. Instead of pointing your master branch to a commit in a staging branch, you can copy individual files from staging to master. When you run copy file, Pachyderm only copies references to the files and does not move the actual data for the files around.
To copy files from one branch to another, complete the following steps:
Start a commit:
pachctl start commit data@master Copy files:
pachctl copy file data@staging:file1 data@master:file1 pachctl copy file data@staging:file2 data@master:file2 ... Close the commit:
pachctl finish commit data@master ‚ÑπÔ∏è While the commit is open, you can run pachctl delete file if you want to remove something from the parent commit or pachctl put file if you want to upload something that is not in a repo yet.
Deferred Processing in Output Repositories # You can perform the same deferred processing operations with data in output repositories. To do so, rather than committing to a staging branch, configure the output_branch field in your pipeline specification.
To configure deferred processing in an output repository, complete the following steps:
In the pipeline specification, add the output_branch field with the name of the branch in which you want to accumulate your data before processing:
&#34;output_branch&#34;: &#34;staging&#34; When you want to process data, run:
pachctl create branch pipeline@master --head staging Automate Deferred Processing With Branch Triggers # Typically, repointing from one branch to another happens when a certain condition is met. For example, you might want to repoint your branch when you have a specific number of commits, or when the amount of unprocessed data reaches a certain size, or at a specific time interval, such as daily, or other. This can be automated using branch triggers. A trigger is a relationship between two branches, such as master and staging in the examples above, that says: when the head commit of staging meets a certain condition it should trigger master to update its head to that same commit. In other words it does pachctl create branch data@master --head staging automatically when the trigger condition is met.
Building on the example above, to make master automatically trigger when there&rsquo;s 1 Megabyte of new data on staging, run:
pachctl create branch data@master --trigger staging --trigger-size 1MB pachctl list branch data BRANCH HEAD TRIGGER staging 8b5f3eb8dc4346dcbd1a547f537982a6 - master 8090bfb4d4fe44158eac12199c37a591 staging on Size(1MB) When you run that command, it may or may not set the head of master. It depends on the difference between the size of the head of staging and the existing head of master, or 0 if it doesn&rsquo;t exist. Notice that in the example above staging had an existing head with less than a MB of data in it so master is still empty. If you don&rsquo;t see staging when you list branch that&rsquo;s ok, triggers can point to branches that don&rsquo;t exist yet. The head of master will update if you add a MB of new data to staging:
dd if=/dev/urandom bs=1MiB count=1 | pachctl put file data@staging:/file pachctl list branch data BRANCH HEAD TRIGGER staging 64b70e6aeda84845858c42d755023673 - master 64b70e6aeda84845858c42d755023673 staging on Size(1MB) Triggers automate deferred processing, but they don&rsquo;t prevent manually updating the head of a branch. If you ever want to trigger master even though the trigger condition hasn&rsquo;t been met you can run:
pachctl create branch data@master --head staging Notice that you don&rsquo;t need to re-specify the trigger when you call create branch to change the head. If you do want to clear the trigger delete the branch and recreate it.
There are three conditions on which you can trigger the repointing of a branch.
time, using a cron specification (&ndash;trigger-cron) size (&ndash;trigger-size) number of commits (&ndash;trigger-commits) When more than one is specified, a branch repoint will be triggered when any of the conditions is met. To guarantee that they all must be met, add &ndash;trigger-all.
To experiment further, see the full triggers example.
Embed Triggers in Pipelines # Triggers can also be specified in the pipeline spec and automatically created when the pipeline is created. For example, this is the edges pipeline from our our OpenCV demo modified to only trigger when there is a 1 Megabyte of new images:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;description&#34;: &#34;A pipeline that performs image edge detection by using the OpenCV library.&#34;, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;images&#34;, &#34;trigger&#34;: { &#34;size&#34;: &#34;1MB&#34; } } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/edges.py&#34; ], &#34;image&#34;: &#34;pachyderm/opencv&#34; } } When you create this pipeline, Pachyderm will also create a branch in the input repo that specifies the trigger and the pipeline will use that branch as its input. The name of the branch is auto-generated with the form &lt;pipeline-name&gt;-trigger-n. You can manually update the heads of these branches to trigger processing just like in the previous example.
‚ÑπÔ∏è Deleting or updating a pipeline will not clean up the trigger branch that it has created. In fact, the trigger branch has a lifetime that is not tied to the pipeline&rsquo;s lifetime. There is no guarantee that other pipelines are not using that trigger branch. A trigger branch can, however, be deleted manually (pachctl delete branch &lt;repo&gt;@&lt;branch&gt;).
More advanced automation # More advanced use cases might not be covered by the trigger methods above. For those, you need to create a Kubernetes application that uses Pachyderm APIs and watches the repositories for the specified condition. When the condition is met, the application switches the Pachyderm branch from staging to master.
"
17,Distributed Computing,"Distributing your computations across multiple workers is a fundamental part of any big data processing. When you build production-scale pipelines, you need to adjust the number of workers and resources that are allocated to each job to optimize throughput.
A Pachyderm worker is an identical Kubernetes pod that runs the Docker image that you specified in the pipeline spec. Your analysis code does not affect how Pachyderm distributes the workload among workers. Instead, Pachyderm spreads out the data that needs to be processed across the various workers and makes that data available for your code.
When you create a pipeline, Pachyderm spins up worker pods that continuously run in the cluster waiting for new data to be available for processing. You can change this behavior by setting &quot;autoscaling&quot; :true. Therefore, you do not need to recreate and schedule workers for every new job.
For each job, all the datums are queued up and then distributed across the available workers. When a worker finishes processing its datum, it grabs a new datum from the queue until all the datums complete processing. If a worker pod crashes, its datums are redistributed to other workers for maximum fault tolerance.
The following animation shows how distributed computing works:
In the diagram above, you have three Pachyderm worker pods that process your data. When a pod finishes processing a datum, it automatically takes another datum from the queue to process it. Datums might be different in size and, therefore, some of them might be processed faster than others.
Each datum goes through the following processing phases inside a Pachyderm worker pod:
Phase Description Downloading The Pachyderm worker pod downloads the datum contents into Pachyderm. Processing The Pachyderm worker pod runs the contents of the datum against your code. Uploading The Pachyderm worker pod uploads the results of processing into an output repository. When a datum completes a phase, the Pachyderm worker moves it to the next one while another datum from the queue takes its place in the processing sequence.
The following animation displays what happens inside a pod during the datum processing:
Parallelism # You can control the number of worker pods that Pachyderm runs in a pipeline by defining the parallelism parameter in the pipeline specification.
example # &#34;parallelism_spec&#34;: { // Exactly one of these two fields should be set &#34;constant&#34;: int Pachyderm has the following parallelism strategies that you can set in the pipeline spec:
Strategy Description constant Pachyderm starts the specified number of workers. For example, if you set &quot;constant&quot;:10, Pachyderm spreads the computation workload among ten workers. By default, Pachyderm sets parallelism to ‚Äúconstant&quot;: 1, which means that it spawns one worker per Kubernetes node for this pipeline.
Autoscaling # Pipelines that will not have a constant flow of data to process should use the autoscaling feature by setting &quot;autoscaling&quot;: true in the pipeline spec.
Doing so will cause the pipeline to go into standby when there is nothing for the workers to do. In standby a pipeline will have no workers and will consume no resources; it will just wait for data to come in for it to process.
When data does come in, the pipeline will exit its standby status and spin up workers to process the new data. Initially, a single worker will spin up and layout a distributed processing plan for the job. Then it will start working on the job, and if there is more work that could happen in parallel, it will spin up more workers to run in parallel, up to the limit defined by the parallelism_spec.
Multiple jobs can run in parallel and cause new workers to spin up. For example, if a job comes in with a single datum, it will cause a single worker to spin up. If another job with a single datum comes in while the first job is still running, another worker will spin up to work on the second job. Again this is bounded by the limit defined in the parallelism_spec.
One limitation of autoscaling is that it cannot dynamically scale down. Suppose a job with many datums is near completion, only one worker is still working while the others are idle. Pachyderm does not yet have a way for the idle workers to steal work, and there are a few issues that prevent us from spinning down the idle workers. Kubernetes does not have a good way to scale down a controller and specify which pods should be killed, so scaling down may kill the worker pod that is still doing work. This means another worker will have to restart that work from scratch, and the job will take longer. Additionally, we want to keep the workers around to participate in the distributed merge process at the end of the job.
‚ÑπÔ∏è See Also:
Glob Pattern Pipeline Specification "
18,Global Identifier," Definition # Pachyderm provides users with a simple way to follow a change throughout their DAG (i.e., traverse Provenance and Subvenance).
Pachyderm associates a commit ID to each new commit. You can quickly check this new commit by running pachctl list commit repo@branch. All resulting downstream commits and jobs in your DAG will then share that same ID (Global Identifier).
üìñ The commits and jobs sharing the same ID represent a logically-related set of objects. The ID of a commit is also:
the ID of any commits created along due to provenance relationships, and the ID of any jobs triggered by the creation of those commits. This ability to track down related commits and jobs with one global identifier brought the need to introduce a new scope to our original concepts of job and commit. The nuance in the scope of a commit or a job ( &ldquo;Global&rdquo; or &ldquo;Local&rdquo;) gives the term two possible meanings.
CONCEPT SCOPE DEFINITION Commit Global A commit with global scope (global commit) represents the set of all provenance-dependent commits sharing the same ID. You can retrieve a global commit by running pachctl list commit &lt;commitID&gt;. Commit Local Repo The same term of commit, applied to the more focused scope of a repo (pachctl list commit &lt;repo&gt;@&lt;commitID&gt; or pachctl list commit &lt;repo&gt;@&lt;branch&gt;=&lt;commitID&gt;), represents &ldquo;the Git-like&rdquo; record of one commit in a single branch of a repository&rsquo;s file system. Job Global A job with global scope (global job) is the set of jobs triggered due to commits in a global commit.
You can retrieve a global job by running pachctl list job &lt;commitID&gt;. Job Local Pipeline Narrowing down the scope to a single pipeline (pachctl list job &lt;pipeline&gt;@&lt;commitID&gt;) shifts the meaning to the execution of a given job in a pipeline of your DAG. List All Global Commits And Global Jobs # You can list all global commits by running the following command:
pachctl list commit Each global commit displays how many (sub) commits it is made of.
ID SUBCOMMITS PROGRESS CREATED MODIFIED 1035715e796f45caae7a1d3ffd1f93ca 7 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 7 seconds ago 7 seconds ago 28363be08a8f4786b6dd0d3b142edd56 6 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 24 seconds ago 24 seconds ago e050771b5c6f4082aed48a059e1ac203 4 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 24 seconds ago 24 seconds ago Similarly, if you run the equivalent command for global jobs:
pachctl list job you will notice that the job IDs are shared with the global commit IDs.
ID SUBJOBS PROGRESS CREATED MODIFIED 1035715e796f45caae7a1d3ffd1f93ca 2 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 55 seconds ago 55 seconds ago 28363be08a8f4786b6dd0d3b142edd56 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá About a minute ago About a minute ago e050771b5c6f4082aed48a059e1ac203 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá About a minute ago About a minute ago For example, in this example, 7 commits and 2 jobs are involved in the changes occured in the global commit ID 1035715e796f45caae7a1d3ffd1f93ca.
‚ÑπÔ∏è The progress bar is equally divided to the number of steps, or pipelines, you have in your DAG. In the example above,1035715e796f45caae7a1d3ffd1f93ca is two steps. If one of the sub-jobs fails, you will see the progress bar turn red for that pipeline step. To troubleshoot, look into that particular pipeline execution.
List All Commits And Jobs With A Global ID # To list all (sub) commits involved in a global commit:
pachctl list commit 1035715e796f45caae7a1d3ffd1f93ca REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION images master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 238.3KiB USER edges.spec master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 244B ALIAS montage.spec master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 405B ALIAS montage.meta master 1035715e796f45caae7a1d3ffd1f93ca 4 minutes ago 1.656MiB AUTO edges master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 133.6KiB AUTO edges.meta master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 373.9KiB AUTO montage master 1035715e796f45caae7a1d3ffd1f93ca 4 minutes ago 1.292MiB AUTO Similarly, change commit in job to list all (sub) jobs linked to your global job ID.
pachctl list job 1035715e796f45caae7a1d3ffd1f93ca ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE 1035715e796f45caae7a1d3ffd1f93ca montage 5 minutes ago 4 seconds 0 1 + 0 / 1 79.49KiB 381.1KiB success 1035715e796f45caae7a1d3ffd1f93ca edges 5 minutes ago 2 seconds 0 1 + 0 / 1 57.27KiB 22.22KiB success For each pipeline execution (sub job) within this global job, Pachyderm shows the time since each sub job started and its duration, the number of datums in the PROGRESS section, and other information. The format of the progress column is DATUMS PROCESSED + DATUMS SKIPPED / TOTAL DATUMS.
For more information, see Datum Processing States.
‚ÑπÔ∏è The global commit and global job above are the result of a pachctl put file images@master -i images.txt in the images repo of the open cv example.
The following diagram illustrates the global commit and its various components: Let&rsquo;s take a look at the origin of each commit.
‚ÑπÔ∏è Check the list of all commit origins in the Commit page.
Inspect the commit ID 1035715e796f45caae7a1d3ffd1f93ca in the images repo, the repo in which our change (put file) has originated:
pachctl inspect commit images@1035715e796f45caae7a1d3ffd1f93ca --raw Note that this original commit is of USER origin (i.e., the result of a user change).
&#34;origin&#34;: { &#34;kind&#34;: &#34;USER&#34; }, Inspect the following commit 1035715e796f45caae7a1d3ffd1f93ca produced in the output repos of the edges pipeline:
pachctl inspect commit edges@1035715e796f45caae7a1d3ffd1f93ca --raw { &#34;commit&#34;: { &#34;branch&#34;: { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; }, &#34;id&#34;: &#34;1035715e796f45caae7a1d3ffd1f93ca&#34; }, &#34;origin&#34;: { &#34;kind&#34;: &#34;AUTO&#34; }, &#34;parent_commit&#34;: { &#34;branch&#34;: { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; }, &#34;id&#34;: &#34;28363be08a8f4786b6dd0d3b142edd56&#34; }, &#34;started&#34;: &#34;2021-07-07T13:52:34.140584032Z&#34;, &#34;finished&#34;: &#34;2021-07-07T13:52:36.507625440Z&#34;, &#34;direct_provenance&#34;: [ { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;spec&#34; }, &#34;name&#34;: &#34;master&#34; }, { &#34;repo&#34;: { &#34;name&#34;: &#34;images&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; } ], &#34;details&#34;: { &#34;size_bytes&#34;: &#34;22754&#34; } } Note that the origin of the commit is of kind AUTO as it has been trigerred by the arrival of a commit in the upstream repo images.
&#34;origin&#34;: { &#34;kind&#34;: &#34;AUTO&#34; }, The same origin (AUTO ) applies to the commits sharing that same ID in the montage output repo as well as edges.meta and montage.meta system repos.
üìñ Check the list of all types of repos in the Repo page.
Besides the USER and AUTO commits, notice a set of ALIAS commits in edges.spec and montage.spec: pachctl inspect commit edges.spec@336f02bdbbbb446e91ba27d2d2b516c6 --raw The version of each pipeline within their respective .spec repos are neither the result of a user change, nor of an automatic change. They have, however, contributed to the creation of the previous AUTO commits. To make sure that we have a complete view of all the data and pipeline versions involved in all the commits resulting from the initial put file, their version is kept as ALIAS commits under the same global ID.
For a full view of GlobalID in action, take a look at our GlobalID illustration.
Track Provenance Downstream # Pachyderm provides the wait commit &lt;commitID&gt; command that enables you to track your commits downstream as they are produced.
Unlike the list commit &lt;commitID&gt;, each line is printed as soon as a new (sub) commit of your global commit finishes.
Change commit in job to list the jobs related to your global job as they finish processing a commit.
Squash And Delete Commit # See squash commit and delete commit in the Delete a Commit / Delete Data page of the How-Tos section of this Documentation.
"
19,Pipeline Concepts,"Pachyderm Pipeline System (PPS) is the computational component of the Pachyderm platform that enables you to perform various transformations on your data.
"
20,Datum," ‚ÑπÔ∏è TLDR: Datums define what input data is seen by your code. It can be all data at once, each directory independently, individual files one by one, or combined data from multiple inputs together.
Definition # A datum is the smallest indivisible unit of computation within a job. A job can have one, many or no datums. Each datum is processed independently with a single execution of the user code on one of the pipeline worker pods. The files output by all of the datums are then combined together to create the final output commit.
Zero-Datum Jobs # A &ldquo;zero-datum&rdquo; job is a job that is successfully executed but has no matching files to transform with the provided user code.
Data distribution # Think of datums as a way to divide your input data and distribute processing workloads. They are instrumental in optimizing your pipeline performance.
You define how your data is spread among workers by specifying pipeline inputs for your pipeline in its pipeline specification file.
Based on this specification file, the data in the input of your pipeline is turned into datums, each of which can contain 1-to-many files. Pachyderm provides a wide variety of ways to define the granularity of each datum.
For example, you can configure a whole branch of an input repository to be one datum, each top-level filesystem object of a given branch to be a separate datum, specific paths on a given branch can be datums, etc&hellip; You can also create a combination of the above by aggregating multiple input.
Pipeline Inputs # This section details the tools at your disposal to &ldquo;break down&rdquo; your data and fit your specific use case.
PFS Input and Glob Pattern # The most primitive input of a pipeline is a PFS input, defined, at a minimum, by:
a repo containing the data you want your pipeline to consider a branch to watch for commits and a glob pattern to determine how the input data is partitioned. A pipeline input can have one or multiple PFS inputs. In the latter case, Pachyderm provides a variety of options to aggregate several PFS inputs together.
"
21,Cross & Union Inputs,"Pachyderm enables you to combine multiple PFS inputs by using the union and cross operators in the pipeline specification.
You can think of union as a disjoint union binary operator and cross as a cartesian product binary operator.
This section describes how to use cross and union in your pipelines and how you can optimize your code when you work with them.
Union Input # The union input combines each of the datums in the input repos as one set of datums. The number of datums that are processed is the sum of all the datums in each repo.
For example, you have two input repos, A and B. Each of these repositories contain three files with the following names.
Repository A has the following structure:
A ‚îú‚îÄ‚îÄ 1.txt ‚îú‚îÄ‚îÄ 2.txt ‚îî‚îÄ‚îÄ 3.txt Repository B has the following structure:
B ‚îú‚îÄ‚îÄ 4.txt ‚îú‚îÄ‚îÄ 5.txt ‚îî‚îÄ‚îÄ 6.txt If you want your pipeline to process each file independently as a separate datum, use a glob pattern of /*. Each glob is applied to each input independently. The input section in the pipeline spec might have the following structure:
&#34;input&#34;: { &#34;union&#34;: [ { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;A&#34; } }, { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;B&#34; } } ] } In this example, each Pachyderm repository has those three files in the root directory, so three datums from each input. Therefore, the union of A and B has six datums in total. Your pipeline processes the following datums without any specific order:
/pfs/A/1.txt /pfs/A/2.txt /pfs/A/3.txt /pfs/B/4.txt /pfs/B/5.txt /pfs/B/6.txt ‚ÑπÔ∏è Each datum in a pipeline is processed independently by a single execution of your code. In this example, your code runs six times, and each datum is available to it one at a time. For example, your code processes pfs/A/1.txt in one of the runs and pfs/B/5.txt in a different run, and so on. In a union, two or more datums are never available to your code at the same time. You can simplify your union code by using the name property as described below.
Simplifying the Union Pipelines Code # In the example above, your code needs to read into the pfs/A or pfs/B directory because only one of them is present in any given datum. To simplify your code, you can add the name field to the pfs object and give the same name to each of the input repos. For example, you can add, the name field with the value C to the input repositories A and B:
&#34;input&#34;: { &#34;union&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: &#34;C&#34;, &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;A&#34; } }, { &#34;pfs&#34;: { &#34;name&#34;: &#34;C&#34;, &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;B&#34; } } ] } Then, in the pipeline, all datums appear in the same directory.
/pfs/C/1.txt # from A /pfs/C/2.txt # from A /pfs/C/3.txt # from A /pfs/C/4.txt # from B /pfs/C/5.txt # from B /pfs/C/6.txt # from B Cross Input # In a cross input, Pachyderm exposes every combination of datums, or a cross-product, from each of your input repositories to your code in a single run.
In other words, a cross input pairs every datum in one repository with each datum in another, creating sets of datums. Your transformation code is provided one of these sets at the time to process.
For example, you have repositories A and B with three datums, each with the following structure:
‚ÑπÔ∏è For this example, the glob pattern is set to /*.
Repository A has three files at the top level:
A ‚îú‚îÄ‚îÄ 1.txt ‚îú‚îÄ‚îÄ 2.txt ‚îî‚îÄ‚îÄ 3.txt Repository B has three files at the top level:
B ‚îú‚îÄ‚îÄ 4.txt ‚îú‚îÄ‚îÄ 5.txt ‚îî‚îÄ‚îÄ 6.txt Because you have three datums in each repo, Pachyderm exposes a total of nine combinations of datums to your code.
üí° In cross pipelines, both pfs/A and pfs/B directories are visible during each code run.
Run 1: /pfs/A/1.txt /pfs/B/4.txt Run 2: /pfs/A/1.txt /pfs/B/5.txt ... Run 9: /pfs/A/3.txt /pfs/B/6.txt ‚ÑπÔ∏è In cross inputs, if you use the name field, your two inputs cannot have the same name. This could cause file system collisions.
‚ÑπÔ∏è See Also:
Cross Input in a pipeline specification Union Input in a pipeline specification Distributed hyperparameter tuning example "
22,Datum Processing,"A datum is a Pachyderm abstraction that helps in optimizing pipeline processing. Datums exist only as a pipeline processing property and are not filesystem objects. You can never copy a datum. They are a representation of a unit of work.
Job Processing and Datums # When new data comes in (in the form of commit(s) in its input repo(s)), a Pachyderm pipeline automatically starts a new job. Each Pachyderm job consists of the following stages:
1: Creation of input datums # In this stage, Pachyderm creates datums based on the input data according to the pipeline input(s) set in the pipeline specification file.
2: Transformation # The pipeline uses your code to process the datums.
3: Creation of output files # Your code writes output file(s) in the pfs/out output directory that Pachyderm creates automatically for each pipeline&rsquo;s job.
4: Final commit in the pipeline&rsquo;s output repo # ‚ÑπÔ∏è The output produced by a pipeline&rsquo;s job is written to an output repo of the same name (i.e., output repo name = pipeline name).
The content of all /pfs/out is combined in a commit to the pipeline&rsquo;s output repo. This generally means unioning all the files together.
üí° The Single Datum Provenance Rule.
If two outputted files have the same name (i.e., two datums wrote to the same output file, creating a conflict), then an error is raised, resulting in your pipeline failure.
Avoid this anti-pattern from the start by having each datum write in separate files. Pachyderm provides an environment variable PACH_DATUM_ID that stores the datum ID. This variable is available in the pipeline&rsquo;s user code. To ensure that each datum outputs distinct file paths, you can use this variable in the name of your outputted files.
5. Next: Add a Reduce (Merge) pipeline # If you need files from different datums merged into single files in a particular way:
add a pipeline that groups the files from the previous output repo into single datums using the appropriate glob pattern. then merge them as intended using your code. The example that follows illustrates this two steps approach.
Example: Two Steps Map/Reduce Pattern and Single Datum Provenance Rule # In this example, we highlight a two pipelines pattern where a first pipeline&rsquo;s glob pattern splits an incoming commit into three datums (called &ldquo;Datum1&rdquo; (Red), &ldquo;Datum2&rdquo; (Blue), &ldquo;Datum3&rdquo; (Purple)), each producing two files each. The files can then be further appended or overwritten with other files to create the final result. Below, a second pipeline appends the content of all files in each directory into one final document.
‚ÑπÔ∏è In the example, the files are named after the datum itself. Depending on your use case, there might be more logical ways to name the files produced by a datum. However, in any case, make sure that this name is unique for each datum to avoid duplicate files with the same file path. Each file is put in specific directories. This directory structure has been thought to facilitate the aggregation of the content in the following pipeline. Think about your directory structure so that the next glob pattern will aggregate your data as needed. Let&rsquo;s now create a new commit and overwrite a file in datum 2, Pachyderm detects three datums. However, because datum 1 and datum 3 are unchanged, it skips processing these datums. Pachyderm detects that something has changed in datum 2. It is unaware of any details of the change; therefore, it processes the whole datum 2(') (here in yellow) and outputs 3 files. Then, the following pipeline aggregates these data to create the final result.
Incrementality # In Pachyderm, unchanged datums are never re-processed. For example, if you have multiple datums, and only one datum was modified; Pachyderm processes that datum only and skips processing other datums. This incremental behavior ensures efficient resource utilization.
Overwriting files # By default, Pachyderm overwrites new data. For example, if you have a file foo in the repository A and add the same file foo to that repository again by using the pachctl put file command, Pachyderm will overwrite that file in the repo.
For more information, and learn how to change this behavior, see File.
Note: Data persistence between datums # Pachyderm only controls and wipes the /pfs directories between datums. If scratch/temp space is used during execution, the user needs to be careful to clean that up. Not cleaning temporary directories may cause unexpected bugs where one datum accesses temporary files that were previously used by another datum!
"
23,Datum Processing States,"When a pipeline runs, it processes your datums. Some of them get processed successfully and some might be skipped or even fail. Generally, processed datums fall into either successful or failure state category.
The following table describes the processing states of datums that can occur in Pachyderm:
Successful States
State Description Success The datum has been successfully processed in this job. Skipped The datum has been successfully processed in a previous job, has not changed since then, and therefore, it was skipped in the current job. Failure States
State Description Failed The datum failed to be processed. Any failed datum in a job fails the whole job. Recovered The datum failed, but was recovered by the user&rsquo;s error handling code. Although the datum is marked as recovered, Pachyderm does not process it in the downstream pipelines. A recovered datum does not fail the whole job. Just like failed datums, recovered datums are retried on the next run of the pipeline. You can view the information about datum processing states in the output of the pachctl list job &lt;jobID&gt; command:
‚ÑπÔ∏è Datums that failed are still included in the total, but not shown in the progress indicator.
"
24,Glob Pattern,"Defining how your data is spread among workers is one of the most important aspects of distributed computation.
Pachyderm uses glob patterns to provide flexibility to define data distribution.
‚ÑπÔ∏è Pachyderm&rsquo;s concept of glob patterns is similar to Unix glob patterns. For example, the ls *.md command matches all files with the .md file extension.
The glob pattern applies to all of the directories/files in the branch specified by the pfs section of the pipeline specification (referred to as PFS inputs). The directories/files that match are the datums that will be processed by the worker(s) that run your pipeline code.
‚ö†Ô∏è You must configure a glob pattern for each PFS input of a pipeline specification.
We have listed some commonly used glob patterns. We will later illustrate their use in an example:
Glob Pattern Datum created / Pachyderm denotes the whole repository as a single datum and sends all input data to a single worker node to be processed together. /* Pachyderm defines each top-level files / directories in the input repo, as a separate datum. For example, if you have a repository with ten files and no directory structure, Pachyderm identifies each file as a single datum and processes them independently. /*/* Pachyderm processes each file / directory in each subdirectories as a separate datum. /** Pachyderm processes each file in all directories and subdirectories as a separate datum. Glob patterns also let you take only a particular subset of the files / directories, a specific branch&hellip; as an input instead of the whole repo. We will elaborate on this more in the following example.
If you have more than one input repo in your pipeline, or want to consider more than one branch in a repo, or any combination of the above, you can define a different glob pattern for each PFS input. You can additionally combine the resulting datums by using the cross, union, join, or group operator to create the final datums that your code processes. For more information, see Cross and Union, Join, Group.
Example of Defining Datums # Let&rsquo;s consider an input repo with the following structure where each top-level directory represents a US state with a json file for each city in that state:
/California /San-Francisco.json /Los-Angeles.json ... /Colorado /Denver.json /Boulder.json ... /Washington /Seattle.json /Vancouver.json Now let&rsquo;s consider what the following glob patterns would match respectively:
Glob Pattern Corresponding match Example / This pattern matches /, the root directory itself, meaning all the data would be one large datum. All changes in any of the files and directories trigger Pachyderm to process the whole repository contents as a single datum. If you add a new file Sacramento.json to the California/ directory, Pachyderm processes all changed files and directories in the repo as a single datum. /* This pattern matches everything under the root directory. It defines one datum per state, which means that all the cities for a given state are processed together by a single worker, but each state is processed independently. If you add a new file Sacramento.json to the California/ directory, Pachyderm processes the California/ datum only. /Colorado/* This pattern matches files only under the /Colorado directory. It defines one datum per city. If you add a new file Alamosa.json to the Colorado/ directory and Sacramento.json to the California/ directory, Pachyderm processes the Alamosa.json datum only. /C* This pattern matches all files under the root directory that start with the character C. In the example, the California and Colorado directories will each define a datum. /*/* This pattern matches everything that&rsquo;s two levels deep relative to the root. If we add County sub-directories to our states, /California/LosAngeles/LosAngeles.json, /California/LosAngeles/Malibu.json and /California/SanDiego/LaMosa.json for example, then this pattern would match each of those 3 .json files individually. /** The match is applied at all levels of your directory structure. This is a recursive glob pattern. Let&rsquo;s look at the additional example below for more detail. Example # The case of the /** glob pattern
Say we have the following repo structure:
/nope1.txt /test1.txt /foo-1 /nope2.txt /test2.txt /foo-2 /foo-2_1 /nope3.txt /test3.txt /anothertest.txt &hellip;and apply the following pattern to our input repo:
&#34;glob&#34;: &#34;/**test*.txt&#34; We are recursively matching all .txt files containing test starting from our input repo&rsquo;s root directory. In this case, the resulting datums will be:
- /test1.txt - /foo-1/test2.txt - /foo-2/foo-2_1/test3.txt - /foo-2/foo-2_1/anothertest.txt üìñ To understand how Pachyderm scales, read Distributed Computing. To learn about Datums&rsquo; incremental processing, read our Datum Processing section. Test a Glob pattern # You can use the pachctl glob file command to preview which filesystem objects a pipeline defines as datums. This command helps you to test various glob patterns before you use them in a pipeline.
If you set the glob property to /, Pachyderm detects all top-level filesystem objects in the train repository as one datum:
Example # pachctl glob file train@master:/ System Response:
NAME TYPE SIZE / dir 15.11KiB If you set the glob property to /*, Pachyderm detects each top-level filesystem object in the train repository as a separate datum:
Example # pachctl glob file train@master:/* System Response:
NAME TYPE SIZE /IssueSummarization.py file 1.224KiB /requirements.txt file 74B /seq2seq_utils.py file 13.81KiB Test your Datums # The granularity of your datums defines how your data will be distributed across the available workers allocated to a job. Pachyderm allows you to check those datums:
for a pipeline currently being developed for a past job Testing your glob pattern before creating a pipeline # You can use the pachctl list datum -f &lt;my_pipeline_spec.json&gt; command to preview the datums defined by a pipeline given its specification file.
‚ÑπÔ∏è The pipeline does not need to have been created for the command to return the list of datums. This &ldquo;dry run&rdquo; helps you adjust your glob pattern when creating your pipeline.
Example # pachctl list datum -f edges.json System Response:
ID FILES STATUS TIME - images@b8687e9720f04b7ab53ae8c64541003b:/46Q8nDz.jpg - - - images@b8687e9720f04b7ab53ae8c64541003b:/8MN9Kg0.jpg - - - images@b8687e9720f04b7ab53ae8c64541003b:/Togu2RY.jpg - - - images@b8687e9720f04b7ab53ae8c64541003b:/g2QnNqa.jpg - - - images@b8687e9720f04b7ab53ae8c64541003b:/w7RVTsv.jpg - - Running list datum on a past job # You can use the pachctl list datum &lt;pipeline&gt;@&lt;job_ID&gt; command to check the datums processed by a given job.
Example # pachctl list datum edges@b8687e9720f04b7ab53ae8c64541003b System Response:
ID FILES STATUS TIME a4149cd1907145f982e0eb49c50af3f1d4d8fecaa8647d62f2d9d93e30578df8 images@b8687e9720f04b7ab53ae8c64541003b:/w7RVTsv.jpg success Less than a second e2b4628dd88b179051ba0576e06fac12ae2e4d16165296212d0e98de501d17df images@b8687e9720f04b7ab53ae8c64541003b:/Togu2RY.jpg success Less than a second 353b6d2a5ac78f56facc7979e190affbb8f75c6f74da84b758216a8df77db473 images@7856142de8714c11b004610ea7af2378:/8MN9Kg0.jpg skipped Less than a second b751702850acad5502dc51c3e7e7a1ac10ba2199fdb839989cd0c5430ee10b84 images@fc9a12ee149a4499a1a7da0a31971b37:/46Q8nDz.jpg skipped Less than a second de9e3703322eff2ab90e89ff01a18c448af9870f17e78438c5b0f56588af9c44 images@7856142de8714c11b004610ea7af2378:/g2QnNqa.jpg skipped Less than a second In this example, you can see that the job b8687e9720f04b7ab53ae8c64541003b only processed 2 datums from the images input repo. The rest was skipped as it had been processed by previous jobs already. Notice that the ID of the datums is now showing.
üìñ Stats and Datum Metadata
Running list datum on a given job execution of a pipeline allows you to additionally display the STATUS (running, failed, success) and TIME of each datum.
You might want to follow up with inspect datum pipeline@job_number datum ID to detail the files that a specific datum includes.
pachctl inspect datum edges@b8687e9720f04b7ab53ae8c64541003b a4149cd1907145f982e0eb49c50af3f1d4d8fecaa8647d62f2d9d93e30578df8 System Response:
ID	a4149cd1907145f982e0eb49c50af3f1d4d8fecaa8647d62f2d9d93e30578df8 Job ID	b8687e9720f04b7ab53ae8c64541003b State	SUCCESS Data Downloaded	606.3KiB Data Uploaded	26.96KiB Total Time	582.000134ms Download Time	40.062075ms Process Time	535.387088ms Upload Time	6.550971ms PFS State: REPO COMMIT PATH edges.meta b8687e9720f04b7ab53ae8c64541003b /pfs/a4149cd1907145f982e0eb49c50af3f1d4d8fecaa8647d62f2d9d93e30578df8 Inputs: REPO COMMIT PATH images b8687e9720f04b7ab53ae8c64541003b /w7RVTsv.jpg Add --raw for a full JSON version of the datum&rsquo;s metadata.
"
25,Group Input,"A group is a special type of pipeline input that enables you to aggregate files that reside in one or separate Pachyderm repositories and match a particular naming pattern. The group operator must be used in combination with a glob pattern that reflects a specific naming convention.
By analogy, a Pachyderm group is similar to a database group-by, but it matches on file paths only, not the content of the files.
Unlike the join datum that will always contain a single match (even partial) from each input repo, a group creates one datum for each set of matching files accross its input repos. You can use group to aggregate data that is not adequately captured by your directory structure or to control the granularity of your datums through file name-matching.
When you configure a group input, you must specify a glob pattern that includes a capture group. The capture group defines the specific string in the file path that is used to match files in other grouped repos. Capture groups work analogously to the regex capture group. You define the capture group inside parenthesis. Capture groups are numbered from left to right and can also be nested within each other. Numbering for nested capture groups is based on their opening parenthesis.
Below you can find a few examples of applying a glob pattern with a capture group to a file path. For example, if you have the following file path:
/foo/bar-123/ABC.txt The following glob patterns in a joint input create the following capture groups:
Regular expression Capture groups /(*) foo /*/bar-(*) 123 /(*)/*/(??)*.txt Capture group 1: foo, capture group 2: AB. /*/(bar-(123))/* Capture group 1: bar-123, capture group 2: 123. Also, groups require you to specify a replacement group in the group_by parameter to define which capture groups you want to try to match.
For example, $1 indicates that you want Pachyderm to match based on capture group 1. Similarly, $2 matches the capture group 2. $1$2 means that it must match both capture groups 1 and 2.
If Pachyderm does not find any matching files, you get a zero-datum job.
You can test your glob pattern and capture groups by using the pachctl list datum -f &lt;your_pipeline_spec.json&gt; command as described in List Datum.
üí° The content of the capture group defined in the group_by parameter is available to your pipeline&rsquo;s code in an environment variable: PACH_DATUM_&lt;input.name&gt;_GROUP_BY.
Example # For example, a repository labresults contains the lab results of patients. The files at the root of your repository have the following naming convention. You want to group your lab results by patientID.
labresults repo:
‚îú‚îÄ‚îÄ LIPID-patientID1-labID1.txt (1) ‚îú‚îÄ‚îÄ LIPID-patientID2-labID1.txt (2) ‚îú‚îÄ‚îÄ LIPID-patientID1-labID2.txt (3) ‚îú‚îÄ‚îÄ LIPID-patientID3-labID3.txt (4) ‚îú‚îÄ‚îÄ LIPID-patientID1-labID3.txt (5) ‚îú‚îÄ‚îÄ LIPID-patientID2-labID3.txt (6) Pachyderm runs your code on the set of files that match the glob pattern and capture groups.
The following example shows how you can use group to aggregate all the lab results of each patient.
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;group&#34; }, &#34;input&#34;: { &#34;group&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;labresults&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*-(*)-lab*.txt&#34;, &#34;group_by&#34;: &#34;$1&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;bash&#34; ], &#34;stdin&#34;: [ &#34;wc&#34; ,&#34;-l&#34; ,&#34;/pfs/labresults/*&#34; ] } } } The glob pattern for the labresults repository, /*-(*)-lab*.txt, selects all files with a patientID match in the root directory.
The pipeline will process 3 datums for this job.
all files containing patientID1 (1, 3, 5) are grouped in one datum, a second datum will be made of (2, 6) for patientID2 and a third with (4) for patientID3 The pachctl list datum -f &lt;your_pipeline_spec.json&gt; command is a useful tool to check your datums:
ID FILES STATUS TIME - labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID1-labID1.txt, labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID1-labID3.txt, labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID1-labID2.txt - - - labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID2-labID1.txt, labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID2-labID3.txt - - - labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID3-labID3.txt To experiment further, see the full group example.
"
26,Join Input,"A join is a special type of pipeline input that enables you to combine files that reside in separate Pachyderm repositories and match a particular naming pattern. The join operator must be used in combination with a glob pattern that reflects a specific naming convention. Note that in Pachyderm, matches are made on file paths only, not the files&rsquo; content.
Pachyderm supports two types of joins:
A default join setting, similar to a database equi-join or inner-join operation. Unlike the cross input, which creates datums from every combination of files in each input repository, inner joins only create datums where there is a match. You can use inner joins to combine data from different Pachyderm repositories and ensure that only specific files from each repo are processed together. If Pachyderm does not find any matching files, you get a zero-datum job. Pachyderm also supports a join close to a database outer-join, allowing you to create datums for all files in a repo, even if there is no match. The outer-join behavior can be set on any repository in your join. When you configure a join input (inner or outer), you must specify a glob pattern that includes a capture group. The capture group defines the specific string in the file path that is used to match files in other joined repos. Capture groups work analogously to the regex capture group. You define the capture group inside parenthesis. Capture groups are numbered from left to right and can also be nested within each other. Numbering for nested capture groups is based on their opening parenthesis.
Below you can find a few examples of applying a glob pattern with a capture group to a file path. For example, if you have the following file path:
/foo/bar-123/ABC.txt The following glob patterns in a joint input create the following capture groups:
Regular expression Capture groups /(*) foo /*/bar-(*) 123 /(*)/*/(??)*.txt Capture group 1: foo, capture group 2: AB. /*/(bar-(123))/* Capture group 1: bar-123, capture group 2: 123. Also, joins require you to specify a replacement group in the join_on parameter to define which capture groups you want to try to match.
For example, $1 indicates that you want Pachyderm to match based on capture group 1. Similarly, $2 matches the capture group 2. $1$2 means that it must match both capture groups 1 and 2.
See the full join input configuration in the pipeline specification.
You can test your glob pattern and capture groups by using the pachctl list datum -f &lt;your_pipeline_spec.json&gt; command as described in List Datum.
üí° The content of the capture group defined in the join_on parameter is available to your pipeline&rsquo;s code in an environment variable: PACH_DATUM_&lt;input.name&gt;_JOIN_ON.
Inner Join # Per default, a join input has an inner-join behavior.
Inner Join Example # For example, you have two repositories. One with sensor readings and the other with parameters. The repositories have the following structures:
readings repo:
‚îú‚îÄ‚îÄ ID1234 ‚îú‚îÄ‚îÄ file1.txt ‚îú‚îÄ‚îÄ file2.txt ‚îú‚îÄ‚îÄ file3.txt ‚îú‚îÄ‚îÄ file4.txt ‚îú‚îÄ‚îÄ file5.txt parameters repo:
‚îú‚îÄ‚îÄ file1.txt ‚îú‚îÄ‚îÄ file2.txt ‚îú‚îÄ‚îÄ file3.txt ‚îú‚îÄ‚îÄ file4.txt ‚îú‚îÄ‚îÄ file5.txt ‚îú‚îÄ‚îÄ file6.txt ‚îú‚îÄ‚îÄ file7.txt ‚îú‚îÄ‚îÄ file8.txt Pachyderm runs your code only on the pairs of files that match the glob pattern and capture groups.
The following example shows how you can use joins to group matching IDs:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;joins&#34; }, &#34;input&#34;: { &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;readings&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;parameters&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/joins.py&#34;], &#34;image&#34;: &#34;joins-example&#34; } } The glob pattern for the readings repository, /*/(*).txt, indicates all matching files in the ID sub-directory. In the parameters repository, the glob pattern /(*).txt selects all the matching files in the root directory. All files with indices from 1 to 5 match. The files with indices from 6 to 8 do not match. Therefore, you only get five datums for this job.
To experiment further, see the full joins example.
Outer Join # Pachyderm also supports outer joins. Outer joins include everything an inner join does plus the files that didn&rsquo;t match anything. Inputs can be set to outer semantics independently. So while there isn&rsquo;t an explicit notion of &ldquo;left&rdquo; or &ldquo;right&rdquo; outer joins, you can still get those semantics, and even extend them to multiway joins.
Outer Join Example # Building off the example above, notice that there are 3 files in the parameters repo, file6.txt, file7.txt and file8.txt, which don&rsquo;t match any files in the readings repo. In an inner join, those files are omitted. If you still want to see the files without a match, you can use an outer join. The change to the pipeline spec is simple:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;joins&#34; }, &#34;input&#34;: { &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;readings&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;parameters&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34;, &#34;outer_join&#34;: true } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/joins.py&#34;], &#34;image&#34;: &#34;joins-example&#34; } } Your code will see the joined pairs that it saw before. In addition to those five datums, your code will also see three new ones: one for each of the files in parameters that didn&rsquo;t match. Note that this means that your code needs to handle (not crash) the case where input files are missing from /pfs/readings.
To experiment further, see the full join example.
"
27,Metadata," Datum Statistics # Pachyderm stores information about each datum that a pipeline processes, including timing information, size information, and /pfs snapshots. You can view these statistics by running the pachctl inspect datum command (or its language client equivalents).
In particular, Pachyderm provides the following information for each datum processed by your pipelines:
The amount of data that was uploaded and downloaded The time spend uploading and downloading data The total time spend processing Success/failure information, including any error encountered for failed datums The directory structure of input data that was seen by the job. Use the pachctl list datum &lt;pipeline&gt;@&lt;job ID&gt; to retrieve the list of datums processed by a given job, and pick the datum ID you want to inspect. That information can be useful when troubleshooting a failed job.
Meta Repo # Once a pipeline has finished a job, you can access additional execution metadata about the datums processed in the associated meta system repo. Note that all the inspect datum information above is stored in this meta repo, along with a couple more. For example, you can find the reason in meta/&lt;datumID&gt;/meta: the error message when the datum failed.
See the detail of the meta repo structure below.
üìñ A meta repo contains 2 directories:
/meta/: The meta directory holds datums&rsquo; statistics /pfs: The pfs directory holds the input data of datums, and their resulting output data pachctl list file edges.meta@master System response:
NAME TAG TYPE SIZE /meta/ dir 1.956KiB /pfs/ dir 371.9KiB Meta directory # The meta directory holds each datum&rsquo;s JSON metadata, and can be accessed using a get file:
Example # pachctl get file edges.meta@master:/meta/002f991aa9db9f0c44a92a30dff8ab22e788f86cc851bec80d5a74e05ad12868/meta | jq System response:
{ &#34;job&#34;: { &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;id&#34;: &#34;efca9595bdde4c0ba46a444a5877fdfe&#34; }, &#34;inputs&#34;: [ { &#34;fileInfo&#34;: { ... } ], &#34;hash&#34;: &#34;28e6675faba53383ac84b899d853bb0781c6b13a90686758ce5b3644af28cb62f763&#34;, &#34;stats&#34;: { &#34;downloadTime&#34;: &#34;0.103591200s&#34;, &#34;processTime&#34;: &#34;0.374824700s&#34;, &#34;uploadTime&#34;: &#34;0.001807800s&#34;, &#34;downloadBytes&#34;: &#34;80588&#34;, &#34;uploadBytes&#34;: &#34;38046&#34; }, &#34;index&#34;: &#34;1&#34; } Usepachctl list file edges.meta@master:/meta/ to list the files in the meta directory.
Pfs Directory # The pfs directory has both the input files of datums, and the resulting output files that were committed to the output repo:
Example # pachctl list file montage.meta@master:/pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/ System response:
NAME TAG TYPE SIZE /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/edges/ dir 133.6KiB /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/images/ dir 238.3KiB /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/out/ dir 1.292MiB Use pachctl list file montage.meta@master:/pfs/ to list the files in the pfs directory.
"
28,Job," ‚ö†Ô∏è Note that Pachyderm uses two different scopes when referring to a job.
A &ldquo;global&rdquo; scope tracking down your entire provenance chain. Refer to GlobalID for more details. And a &ldquo;local&rdquo; scope in which a job (also referred to as sub job) is an execution of one particular pipeline. The following page details the latter.
Definition # A Pachyderm job is an execution of a pipeline that triggers when new data is detected in an input repository.
‚ÑπÔ∏è When a commit is made to the input repo of a pipeline, jobs are created for all of the downstream pipelines of a DAG. Those jobs are not running yet; each one is waiting until the prior pipeline(s) that it depends on in your DAG produces their output, which then becomes the input for the waiting pipeline.
Each job runs your code against the current commit in a &lt;repo&gt;@&lt;branch&gt; and then submits the results to the output repository of the pipeline as a single output commit. A pipeline triggers a new job every time you submit new changes, a commit, into your input source.
Each job has an alphanumeric identifier (ID) that you can reference in the &lt;pipeline&gt;@&lt;jobID&gt; format.
You can obtain information about all jobs sharing the same ID (Global ID) by running list jobs &lt;jobID&gt; or restrict to a particular pipeline list jobs -p &lt;pipeline&gt;, or inspect jobs &lt;pipeline&gt;@&lt;jobID&gt; --raw.
Job Statuses # Find a list of all possible job stages below and a state diagram detailing how a job transitions from one state to another.
Stage Description CREATED An input commit exists, but the job has not been started by a worker yet. STARTING The worker has allocated resources for the job (that is, the job counts towards parallelism), but it is still waiting on the inputs to be ready. UNRUNNABLE The job could not be run, because one or more of its inputs is the result of a failed or unrunnable job. As a simple example, say that pipelines Y and Z both depend on the output from pipeline X. If pipeline X fails, both pipeline Y and Z will pass from STARTING to UNRUNNABLE to signify that they had to be cancelled because of upstream failures. RUNNING The worker is processing datums. EGRESS The worker has completed all the datums and is uploading the output to the egress endpoint. FINISHING After all of the datum processing and egress (if any) is done, the job transitions to a finishing state where all of the post-processing tasks such as compaction are performed. FAILURE The worker encountered too many errors when processing a datum. KILLED The job timed out, or a user called StopJob SUCCESS None of the bad stuff happened. Below, the state transition diagram of a job:
List Jobs # They are various ways to list jobs in Pachyderm, depending on the expected outcome:
The pachctl list jobs command returns the list of all global jobs.
The pachctl list jobs &lt;jobID&gt; command returns the list of all jobs sharing the same &lt;jobID&gt;.
Note that you can also track your jobs downstream as they complete by running pachctl wait jobs &lt;jobID&gt;.
The pachctl list jobs -p &lt;pipeline&gt; command returns the list of all the jobs run in a given pipeline.
Example # pachctl list jobs -p edges System Response:
ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE fd9454d06d8e4fa38a75c8cd20b39538 edges 20 hours ago 5 seconds 0 2 + 1 / 3 181.1KiB 111.4KiB success 5a78358d4b53494cbba4550428f2fe98 edges 20 hours ago 2 seconds 0 1 + 0 / 1 57.27KiB 22.22KiB success 7dcd77a2f7f34ff384a6096d1139e922 edges 20 hours ago Less than a second 0 0 + 0 / 0 0B 0B success For each (sub) job, Pachyderm shows the time the pipeline started, its duration, data downloaded and uploaded, the STATE of the pipeline execution, and the number of datums in the PROGRESS section. The format of the progress column is DATUMS PROCESSED + DATUMS SKIPPED / TOTAL DATUMS.
See Datum Processing States for details on Datum states.
Inspect Job # The pachctl inspect jobs &lt;pipeline&gt;@&lt;jobID&gt; command enables you to view detailed information about a specific (sub)job in a given pipeline (state, number of datums processed/failed/skipped, data downloaded, uploaded, process time, image:tag used to transform your data, etc&hellip;). Along with checking the logs, it is especially useful when troubleshooting a failed job.
Example # Add a --raw flag to output a detailed JSON version of the job.
pachctl inspect jobs edges@fd9454d06d8e4fa38a75c8cd20b39538 --raw System Response:
{ &#34;job&#34;: { &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;id&#34;: &#34;fd9454d06d8e4fa38a75c8cd20b39538&#34; }, &#34;pipeline_version&#34;: &#34;1&#34;, &#34;output_commit&#34;: { &#34;branch&#34;: { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; }, &#34;id&#34;: &#34;fd9454d06d8e4fa38a75c8cd20b39538&#34; }, &#34;data_processed&#34;: &#34;2&#34;, &#34;data_skipped&#34;: &#34;1&#34;, &#34;data_total&#34;: &#34;3&#34;, &#34;stats&#34;: { &#34;download_time&#34;: &#34;0.113263653s&#34;, &#34;process_time&#34;: &#34;1.020472976s&#34;, &#34;upload_time&#34;: &#34;0.010323995s&#34;, &#34;download_bytes&#34;: &#34;185424&#34;, &#34;upload_bytes&#34;: &#34;114041&#34; }, &#34;state&#34;: &#34;JOB_SUCCESS&#34;, &#34;created&#34;: &#34;2021-08-02T20:13:10.461841493Z&#34;, &#34;started&#34;: &#34;2021-08-02T20:13:32.870023561Z&#34;, &#34;finished&#34;: &#34;2021-08-02T20:13:38.691891860Z&#34;, &#34;details&#34;: { &#34;transform&#34;: { &#34;image&#34;: &#34;pachyderm/opencv:1.0&#34;, &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/edges.py&#34; ] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;name&#34;: &#34;images&#34;, &#34;repo&#34;: &#34;images&#34;, &#34;repo_type&#34;: &#34;user&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;commit&#34;: &#34;fd9454d06d8e4fa38a75c8cd20b39538&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;salt&#34;: &#34;27bbe39ccae54cc2976e3f960a2e1f94&#34;, &#34;datum_tries&#34;: &#34;3&#34; } } "
29,Pipeline,"A pipeline is a Pachyderm primitive that is responsible for reading data from a specified source, such as a Pachyderm repo, transforming it according to the pipeline configuration, and writing the result to an output repo.
A pipeline subscribes to a branch in one or more input repositories. Every time the branch has a new commit, the pipeline executes a job that runs your code to completion and writes the results to a commit in the output repository. Every pipeline automatically creates an output repository by the same name as the pipeline. For example, a pipeline named model writes all results to the model output repo.
In Pachyderm, a Pipeline is an individual execution step. You can chain multiple pipelines together to create a directed acyclic graph (DAG).
You define a pipeline declaratively, using a JSON or YAML file. Pipeline specification files follow Pachyderm&rsquo;s pipeline reference specification file.
A minimum pipeline specification must include the following parameters:
name ‚Äî The name of your data pipeline. Set a meaningful name for your pipeline, such as the name of the transformation that the pipeline performs. For example, split or edges. Pachyderm automatically creates an output repository with the same name. A pipeline name must be an alphanumeric string that is less than 63 characters long and can include dashes and underscores. No other special characters allowed.
input ‚Äî A location of the data that you want to process, such as a Pachyderm repository. You can specify multiple input repositories and set up the data to be combined in various ways. For more information, see Cross and Union, Join, Group. One very important property that is defined in the input field is the glob pattern that specifies how Pachyderm breaks the data into individual processing units, called Datums. For more information, see Datum.
transform ‚Äî Specifies the code that you want to run against your data. The transform section must include an image field that defines the Docker image that you want to run, as well as a cmd field for the specific code within the container that you want to execute, such as a Python script.
example # { &#34;pipeline&#34;: { &#34;name&#34;: &#34;wordcount&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;python3&#34;, &#34;/my_python_code.py&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } } } "
30,Cron Pipeline,"A Cron pipeline is triggered by a set time interval instead of whenever new changes appear in the input repository.
About Cron Pipelines # Use Cases # Cron pipelines are great for tasks like:
Scraping websites Making API calls Querying a database Retrieving a file from a location accessible through an S3 protocol or a File Transfer Protocol (FTP). Behavior # When you create a Cron pipeline, Pachyderm creates a new input data repository that corresponds to the cron input Then, Pachyderm automatically commits a timestamp file to the cron input repository at your determined interval, which triggers the pipeline.
By default, each cron trigger adds a new tick file to the cron input repository, accumulating more datums over time. Optionally, you can set the overwrite flag to true to overwrite the timestamp file on each tick. To learn more about overwriting commits in Pachyderm, see Datum processing.
Required Parameters # At minimum, a Cron pipeline must include all of the following parameters:
Parameters Description &quot;name&quot; A descriptive name of the cron pipeline. &quot;spec&quot; The interval between scheduled cron jobs; accepts RFC 3339 inputs, Predefined Schedules (@daily), and Intervals (@every 1h30m20s) Callouts # Avoid using intervals faster than 1-5 minutes You can use never during development and manually trigger the pipeline If using jsonnet, you can pass arguments like: --arg cronSpec=&quot;@every 5m&quot; You cannot update a cron pipeline after it has been created; instead, you must delete the pipeline and build a new one. Examples # Every 60 Seconds # &#34;input&#34;: { &#34;cron&#34;: { &#34;name&#34;: &#34;tick&#34;, &#34;spec&#34;: &#34;@every 60s&#34; } } Daily with Overwrites # &#34;input&#34;: { &#34;cron&#34;: { &#34;name&#34;: &#34;tick&#34;, &#34;spec&#34;: &#34;@daily&#34;, &#34;overwrite&#34;: true } } SQL Ingest with Jsonnet # pachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.5.x/src/templates/sql_ingest_cron.jsonnet \ --arg name=myingest \ --arg url=&#34;mysql://root@mysql:3306/test_db&#34; \ --arg query=&#34;SELECT * FROM test_data&#34; \ --arg hasHeader=false \ --arg cronSpec=&#34;@every 60s&#34; \ --arg secretName=&#34;mysql-creds&#34; \ --arg format=json üìñ See Also: Periodic Ingress from MongoDB
"
31,Service,"Service is a special type of pipeline that does not process data but provides a capability to expose it to the outside world. For example, you can use a service to serve a machine learning model as an API that has the most up-to-date version of your data.
The following pipeline spec extract is an example of how you can expose your Jupyter notebook as a service by adding a service field:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;notebook&#34; }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/&#34;, &#34;repo&#34;: &#34;input&#34; } }, &#34;service&#34;: { &#34;external_port&#34;: 30888, &#34;internal_port&#34;: 8888 }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;start-notebook.sh&#34; ], &#34;image&#34;: &#34;jupyter/datascience-notebook&#34; } } The service section specifies the following parameters:
Parameter Description &quot;internal_port&quot; The port that the code running inside the container binds to. &quot;external_port&quot; The port that is exposed outside of the container. You must set this value in the range of 30000 ‚Äî 32767. You can access the service from any Kubernetes node through the following address: http://&lt;kubernetes-host&gt;:&lt;external_port&gt;. üìñ The Service starts running at the first commit in the input repo.
‚ÑπÔ∏è See Also
Service "
32,Spout,"A spout is a type of pipeline that ingests streaming data from an outside source (message queue, database transactions logs, event notifications&hellip; ) as schematized in the diagram below.
Generally, spout pipelines are ideal for situations when the frequency of new incoming data is sporadic, and the latency requirement to start the processing is short.
In these workloads, a regular pipeline with a cron input that polls for new data at a consistent time interval might not be an optimal solution.
A spout pipeline differs from regular pipelines in many ways:
its code runs continuously, waiting for new events. it does not take an input repo. Instead, it consumes data from an outside source. the pipeline&rsquo;s output repo, pfs/out is not directly accessible. To write into the output repo, you will need to use the put file API call via any of the following: the CLI (pachctl put file) one of the Pachyderm&rsquo;s SDKs (for golang or Python ) or your own API client. Pachyderm CLI (pachctl) is packaged in the base image of your spout as well as your authentication information. As a result, the authentication is seamless when using pachctl. ‚ÑπÔ∏è Support for a transparent authentication in our SDKs is coming soon. In the meantime, check our Spout 101 example at the end of this page to learn how to retrieve and inject your authentication token into your API client.
To create a spout pipeline, you will need:
A source of streaming data. A Docker container with your spout code that connects to, reads, transforms, and pushes data from the data source to your output repo. A spout pipeline specification file that uses your container. ‚ÑπÔ∏è It is important to remember that you will need to use a put file API call from a client of your choice to push your data into the pipeline output repository. Having the entire Pachyderm API available to you allows you to package data into commits and transactions at the granularity your problem requires.
A minimum spout specification must include the following parameters in the pipeline specification:
Parameter Description name The name of your data pipeline and the output repository. You can set an arbitrary name that is meaningful to the code you want to run. spout This attribute can be left empty. Optional: Add a service field to expose your spout as a service. transform Specifies the command that you want to call to ingest your data and the Docker image it is packaged in. xs Here is an example of a minimum spout pipeline specification:
‚ÑπÔ∏è The env property is an optional argument. You can define your data stream source from within the container in which you run your script. For simplicity, in this example, env specifies the source of the Kafka host.
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;my-spout&#34; }, &#34;spout&#34;: { }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;go&#34;, &#34;run&#34;, &#34;./main.go&#34; ], &#34;image&#34;: &#34;myaccount/myimage:0.1&#34;, &#34;env&#34;: { &#34;HOST&#34;: &#34;kafkahost&#34;, &#34;TOPIC&#34;: &#34;mytopic&#34;, &#34;PORT&#34;: &#34;9092&#34; } } } Example # For a first overview of how spouts work, see our spout101 example.
"
33,Versioned Data Concepts,"Pachyderm data concepts describe version-control primitives that you interact with when you use Pachyderm.
These ideas are conceptually similar to the Git version-control system with a few notable exceptions. Because Pachyderm deals not only with plain text but also with binary files and large datasets, it does not process the data in the same way as Git. When you use Git, you store a copy of the repository on your local machine. You work with that copy, apply your changes, and then send the changes to the upstream master copy of the repository where it gets merged.
Pachyderm version control works slightly differently. In Pachyderm, only a centralized repository exists and you do not store any local copies of that repository. Therefore, the merge, in the traditional Git meaning, does not occur.
Instead, your data can be continuously updated in the master branch of your repo, while you can experiment with specific data commits in a separate branch or branches. Because of this behavior, you cannot run into a merge conflict with Pachyderm.
"
34,Branch,"A Pachyderm branch is a pointer to a commit that moves along with new commits as they are submitted. By default, when you create a repository, Pachyderm does not create any branches. Most users prefer to create a master branch by initiating the first commit and specifying the master branch in the put file command.
Branches enable collaboration between teams of data scientists. However, many users find it sufficient to use the master branch for all their work. Although the concept of a branch is similar to Git branches, in most cases, branches are not used as extensively as in source code version-control systems.
A branch also stores information about provenance, the other branches it uses as input and which rely on its output. These branch relationships are how Pachyderm knows which data each pipeline relies on, and which branches and repos should be included in each commit.
Each branch has a HEAD which references the latest commit in the branch. Pachyderm pipelines look at the HEAD of the branch for changes and, if they detect new changes, trigger a job. When you commit a new change, the HEAD of the branch moves to the latest commit.
You can create additional branches to experiment with the data (pachctl create branch &lt;myrepo&gt;@&lt;branchname&gt;. Optionally, you can add --head &lt;myrepo&gt;@&lt;master&gt; for the head of the new branch to reference the head commit on master).
To view a list of branches in a repo, run the pachctl list branch &lt;myrepo&gt; command.
example # pachctl list branch images System Response:
BRANCH HEAD master c32879ae0e6f4b629a43429b7ec10ccc ‚ö†Ô∏è Deleting a branch (pachctl delete branch &lt;myrepo&gt;@&lt;branchname&gt;) does not delete the commits on it. All branches must have a head commit. "
35,Commit," ‚ÑπÔ∏è Note that Pachyderm uses the term commit at two different levels. A global level (check GlobalID for more details) and commits that occur on the given branch of a repository. The following page details the latter.
Definition # In Pachyderm, commits are atomic operations that snapshot and preserve the state of the files and directories in a repository at a point in time. Unlike Git commits, Pachyderm commits are centralized and transactional. You can start a commit by running the pachctl start commit command with reference to a specific repository. After you&rsquo;re done making changes to the repository (put file, delete file, &hellip;), you can finish your modifications by running the pachctl finish commit command. This command saves your changes and closes that repository&rsquo;s commit, indicating the data is ready for processing by downstream pipelines.
‚ö†Ô∏è start commit can only be used on input repos without provenance. Such repos are the entry points of a DAG. You cannot manually start a commit from a pipeline output or meta repo.
When you create a new commit, the previous commit on which the new commit is based becomes the parent of the new commit. Your repo history consists of those parent-child relationships between your data commits.
‚ÑπÔ∏è An initial commit has &lt;none&gt; as a parent.
Additionally, commits have an &ldquo;origin&rdquo;. You can see an origin as the answer to: &ldquo;What triggered the production of this commit?&rdquo;.
That origin can be of 3 types:
USER: The commit is the result of a user change (put file, update pipeline, delete file&hellip;) üìñ Every USER change is an initial commit.
AUTO: Pachyderm&rsquo;s pipelines are data-driven. A data commit to a data repository may trigger downstream processing jobs in your pipeline(s). The output commits from triggered jobs will be of type AUTO. ALIAS: Neither USER nor AUTO - ALIAS commits are essentially placeholder commits. They have the same content as their parent commit and are mainly used for global IDs. ‚ÑπÔ∏è To track provenance, Pachyderm requires all commits to belong to exactly one branch. When moving a commit from one branch to another, Pachyderm creates an ALIAS commit on the other branch.
Each commit has an alphanumeric identifier (ID) that you can reference in the &lt;repo&gt;@&lt;commitID&gt; format (or &lt;repo&gt;@&lt;branch&gt;=&lt;commitID&gt; if the commit has multiple branches from the same repo) .
You can obtain information about all commits with a given ID by running pachctl list commit &lt;commitID&gt; or restrict to a particular repository pachctl list commit &lt;repo&gt;, pachctl list commit &lt;repo&gt;@&lt;branch&gt;, or pachctl inspect commit &lt;repo&gt;@&lt;commitID&gt; --raw.
List Commits # The pachctl list commit command returns list of all global commits. This command is detailed in this section of Global ID.
The pachctl list commit &lt;commitID&gt; commands returns the list of all commits sharing the same &lt;commitID&gt;. This command is detailed in this section of Global ID.
Note that you can also track your commits downstream as they complete by running pachctl wait commit &lt;commitID&gt;.
The pachctl list commit &lt;repo&gt;@&lt;branch&gt; command returns the commits in the given branch of a repo.
example # pachctl list commit images@master System Response:
REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION images master c6d7be4a13614f2baec2cb52d14310d0 33 minutes ago 5.121MiB USER images master 385b70f90c3247e69e4bdadff12e44b2 2 hours ago 2.561MiB USER list commit &lt;repo&gt;, without mention of a branch, displays results from all branches of the specified repository. Inspect Commit # The pachctl inspect commit &lt;repo&gt;@&lt;commitID&gt; command enables you to view detailed information about a commit in a given repo (size, parent, the branch it belongs to, how long ago the commit was started and finished&hellip;).
The --full-timestamps flag will give you the exact date and time of when the commit was opened and finished. If you specify a branch instead of a specific commit (pachctl inspect commit &lt;repo&gt;@&lt;branch&gt;), Pachyderm displays the information about the HEAD of the branch. Example # Add a --raw flag to output a detailed JSON version of the commit.
pachctl inspect commit images@c6d7be4a13614f2baec2cb52d14310d0 --raw System Response:
{ &#34;commit&#34;: { &#34;branch&#34;: { &#34;repo&#34;: { &#34;name&#34;: &#34;images&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; }, &#34;id&#34;: &#34;c6d7be4a13614f2baec2cb52d14310d0&#34; }, &#34;origin&#34;: { &#34;kind&#34;: &#34;USER&#34; }, &#34;parent_commit&#34;: { &#34;branch&#34;: { &#34;repo&#34;: { &#34;name&#34;: &#34;images&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; }, &#34;id&#34;: &#34;385b70f90c3247e69e4bdadff12e44b2&#34; }, &#34;started&#34;: &#34;2021-08-02T20:13:10.393036120Z&#34;, &#34;finishing&#34;: &#34;2021-08-02T20:13:10.393036120Z&#34;, &#34;finished&#34;: &#34;2021-08-02T20:13:11.851931210Z&#34;, &#34;size_bytes_upper_bound&#34;: &#34;244068&#34;, &#34;details&#34;: { &#34;size_bytes&#34;: &#34;244068&#34; } } Squash And Delete Commit # See squash commit and delete commit in the Delete a Commit / Delete Data page of the How-Tos section of this Documentation.
"
36,File,"A file is a Unix filesystem object, which is a directory or file, that stores data. Unlike source code version-control systems that are most suitable for storing plain text files, you can store any type of file in Pachyderm, including binary files. Often, data scientists operate with comma-separated values (CSV), JavaScript Object Notation (JSON), images, and other plain text and binary file formats. Pachyderm supports all file sizes and formats and applies storage optimization techniques, such as deduplication, in the background.
To upload your files to a Pachyderm repository, run the pachctl put file command. By using the pachctl put file command, you can put both files and directories into a Pachyderm repository.
‚ö†Ô∏è It is important to note that directories are implied from the paths of the files. Directories are not stored and will not exist unless they contain files. Do not use regex metacharacters in a path or a file name. File Processing Strategies # Pachyderm provides the following file processing strategies:
Overwriting Files # By default, when you put a file into a Pachyderm repository and a file by the same name already exists in the repo, Pachyderm overwrites the existing file with the new data. For example, you have an A.csv file in a repository. If you upload the same file to that repository, Pachyderm overwrites the existing file with the data, which results in the A.csv file having only data from the most recent upload.
Example # View the list of files:
pachctl list file images@master System Response:
NAME TYPE SIZE /A.csv file 258B Add the A.csv file once again:
pachctl put file images@master -f A.csv Verify that the file size has not changed:
pachctl list file images@master System Response:
NAME TYPE SIZE /A.csv file 258B Appending to files # When you enable the append mode by using the --append flag or -a, the new files are appended to existing ones instead of overwriting them. For example, you have an A.csv file in the images repository. If you upload the same file to that repository with the --append flag, Pachyderm appends to the file.
Example # View the list of files:
pachctl list file images@master System Response:
NAME TYPE SIZE /A.csv file 258B Add the A.csv file once again:
pachctl put file -a images@master -f A.csv Verify that the file size has doubled:
pachctl list file images@master System Response:
NAME TYPE SIZE /A.csv file 516B "
37,History,"Pachyderm implements rich version-control and history semantics. This section describes the core concepts and architecture of Pachyderm&rsquo;s version control and the various ways to use the system to access historical data.
The following abstractions store the history of your data:
Commits
In Pachyderm, commits are the core version-control primitive that is similar to Git commits. Commits represent an immutable snapshot of a filesystem and can be accessed with an ID. They have a parentage structure, where new commits inherit content from their parents. You can think of this parentage structure as of a linked list or a chain of commits.
Branches
Branches are pointers to commits that are similar to Git branches. Typically, branches have semantically meaningful names such as master and staging. Branches are mutable, and they move along a growing chain of commits as you commit to the branch, and can even be reassigned to any commit within the repo by using the pachctl create branch command. The commit that a branch points to is referred to as the branches head, and the head&rsquo;s ancestors are referred to as on the branch. Branches can be substituted for commits in Pachyderm&rsquo;s API and behave as if the head of the branch were passed.
Ancestry Syntax # Pachyderm&rsquo;s commits and branches support a familiar Git syntax for referencing their history. A commit or branch parent can be referenced by adding a ^ to the end of the commit or branch. Similar to how master resolves to the head of master, master^ resolves to the parent of the head. You can add multiple ^s. For example, master^^ resolves to the parent of the parent of the head master, and so on. Similarly, master^3 has the same meaning as master^^^.
Git supports two characters for ancestor references ‚Äî^ and ~‚Äî with slightly different meanings. Pachyderm supports both characters as well, but their meaning is identical.
Also, Pachyderm supports a type of ancestor reference that Git does not: the forward reference, using the special character .. It resolves to commits on the beginning of commit chains. For example, master.1 is the first (oldest) commit on the master branch, master.2 is the second commit, and so on.
‚ÑπÔ∏è Resolving ancestry syntax requires traversing chains of commits high numbers passed to ^ and low numbers passed to .. These operations might take a long time. If you plan to repeatedly access an ancestor, you might want to resolve that ancestor with pachctl inspect commit &lt;repo&gt;@&lt;branch or commitID&gt;.
View the Pipeline History # Pipelines are the main processing primitive in Pachyderm. However, they expose version-control and history semantics similar to filesystem objects. This is largely because, under the hood, they are implemented in terms of filesystem objects. You can access previous versions of a pipeline by using the same ancestry syntax that works for commits and branches. For example, pachctl inspect pipeline foo^ gives you the previous version of the pipeline foo. The pachctl inspect pipeline foo.1 command returns the first ever version of that same pipeline. You can use this syntax in all operations and scripts that accept pipeline names.
To view historical versions of a pipeline use the --history flag with the pachctl list pipeline command:
pachctl list pipeline --history all System Response:
NAME VERSION INPUT CREATED STATE / LAST JOB Pipeline2 1 input2:/* 4 hours ago running / success Pipeline1 3 input1:/* 4 hours ago running / success Pipeline1 2 input1:/* 4 hours ago running / success Pipeline1 1 input1:/* 4 hours ago running / success View the Job History # Jobs do not have versioning semantics associated with them. However, they are strongly associated with the pipelines that created them. Therefore, they inherit some of their versioning semantics. You can use the -p &lt;pipeline&gt; flag with the pachctl list job command to list all the jobs that were run for the latest version of the pipeline.
Furthermore, you can get jobs from multiple versions of a pipeline by passing the --history flag. For example, pachctl list job -p edges --history all returns all jobs from all versions of the pipeline edges.
"
38,Provenance,"Data versioning (History) enables Pachyderm users to go back in time and see the state of a dataset or repository at a particular moment.
Data provenance (from the French noun provenance which means the place of origin), also known as data lineage, tracks the dependencies and relationships between datasets. It answers the question &ldquo;Where does the data come from?&rdquo;, but also &ldquo;How was the data transformed along the way?&rdquo;.
Pachyderm enables its users to have both: track all revisions of their data and understand the connection between the data stored in one repository and the results in the other repository.
It automatically maintains a complete audit trail, allowing all results to be fully reproducible.
The following diagram is an illustration of how provenance works:
In the diagram above, two input repositories (model and test_data) feed a scoring pipeline. The model repository continuously collects model artifacts from an outside source. The pipeline combines the data from these two repositories and scores each model against the validation dataset.
Global ID is an easy tool to help you track down your entire provenance chain and understand what data and transformation processes were involved in a specific version of a dataset. Here, the ID1 is shared by all commits and jobs involved in creating the final scoring v1. A simple pachctl list commit ID1 (pachctl list job ID1) will return that list at once.
Tracking Direct Provenance in Pachyderm # Pachyderm provides the pachctl inspect command that enables you to track the direct provenance of your commits and learn where the data in the repository originates in.
Example # pachctl inspect commit edges@71c791f3252c492a8f8ad9a51e5a5cd5 --raw System Response:
{ &#34;commit&#34;: { &#34;branch&#34;: { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; }, &#34;id&#34;: &#34;71c791f3252c492a8f8ad9a51e5a5cd5&#34; }, &#34;origin&#34;: { &#34;kind&#34;: &#34;AUTO&#34; }, &#34;parent_commit&#34;: { &#34;branch&#34;: { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; }, &#34;id&#34;: &#34;b6fc0ab2d8d04972b0c31b0e35133323&#34; }, &#34;started&#34;: &#34;2021-07-07T19:53:17.242981574Z&#34;, &#34;finished&#34;: &#34;2021-07-07T19:53:19.598729672Z&#34;, &#34;direct_provenance&#34;: [ { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;spec&#34; }, &#34;name&#34;: &#34;master&#34; }, { &#34;repo&#34;: { &#34;name&#34;: &#34;images&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; } ], &#34;details&#34;: { &#34;size_bytes&#34;: &#34;22754&#34; } } Provenance information: In the example above, you can see that the commit 71c791f3252c492a8f8ad9a51e5a5cd5 on the master branch of the edges repo was automatically produced (origin.kind = AUTO) from a user input on the master branch of the images repo processed by the edges pipeline (direct_provenance).
History: Additionally, the parent of the commit 71c791f3252c492a8f8ad9a51e5a5cd5 in the edges repo has the id b6fc0ab2d8d04972b0c31b0e35133323.
Traversing Provenance and Subvenance # In Pachyderm, all the related steps in a DAG share the same identifier, making it easy to traverse the provenance and subvenance in any commit.
All it takes is to run pachctl list commit &lt;commitID&gt; to get the full list of all the branches with commits created along the way due to provenance relationships.
Visit the Global ID Page for more details.
"
39,Repository," Definition # A Pachyderm repository is a location where you store your data inside Pachyderm. It is a top-level data object that contains files and folders.
Similar to Git, a Pachyderm repository tracks all changes to the data and creates a history of data modifications that you can access and review.
üìñ You can store any type of file in a Pachyderm repo, including binary and plain text files.
Unlike a Git repository that stores history in a .git file in your copy of a Git repo, Pachyderm stores the history of your commits in a centralized location. Because of that, you do not run into merge conflicts as you often do with Git commits when you try to merge your .git history with the master copy of the repo. With large datatsets resolving a merge conflict might not be possible.
A Pachyderm repository is the first entity that you configure when you want to add data to Pachyderm. You can create a repository with the pachctl create repo command, or by using one of Pachyderm&rsquo;s client API. After creating the repository, add your data by using the pachctl put file command.
‚ö†Ô∏è A Pachyderm repo name can include alphanumeric characters, dashes, and underscores, and should be no more than 63 characters long.
Pachyderm&rsquo;s repositories are divided into two categories:
User repositories
User repositories keep track of your data one commit at a time. They further split into:
Source repositories
Users or external applications outside of Pachyderm can add data to the source repositories for further processing.
Output repositories
Pachyderm automatically creates an output repository at the end of a pipeline for the pipeline to write the results of its transformations into. An output repository might serve as input for another pipeline.
System repositories
System repositories hold certain auxiliary information about pipelines. They are hidden by default in the output of most commands. Along with an output repo, the creation of a pipeline also creates one spec and one meta repo.
spec repositories hold pipeline specification files meta repositories hold metadata related to datum processing (also called &ldquo;stats&rdquo; in this documentation) Pipelines generally manage their own system repos, but if necessary, the system repos for a pipeline named edges can be referenced using edges.meta and edges.spec wherever you would usually put a repo name. Deleting a user repo deletes any associated system repos.
List Your Repos # You can view the list of all user repositories in your Pachyderm cluster by running the pachctl list repo command.
Example # pachctl list repo System Response:
NAME CREATED SIZE (MASTER) ACCESS LEVEL montage 19 hours ago 1.664MiB [repoOwner] Output repo for pipeline montage. edges 19 hours ago 133.6KiB [repoOwner] Output repo for pipeline edges. images 19 hours ago 238.3KiB [repoOwner] Additionally, pachctl list repo --all will let you see all repos of all types, and pachctl list repo --type=spec or pachctl list repo --type=meta will filter the spec or meta repos only.
Inspect a Repo # The pachctl inspect repo command provides a more detailed overview of a specified repository.
Example # pachctl inspect repo raw_data System Response:
Name: raw_data Description: A raw data repository Created: 6 hours ago Size of HEAD on master: 5.121MiB Add a --raw flag to output a more detailed JSON version of the repo&rsquo;s metadata.
Delete a Repo # If you need to delete a repository, you can run the pachctl delete repo command. This command deletes all data and the information about the specified repository, such as commit history. The delete operation is irreversible and results in a complete cleanup of your Pachyderm cluster. If you run the delete command with the --all flag, all repositories will be deleted.
üí° See Also: Pipeline
"
40,How Tos,"This section includes how-tos that describe best practices of data operations in Pachyderm.
"
41,Analyze Time-Windowed Data," üìñ Before you read this section, make sure that you understand the concepts described in the following sections:
Datum Distributed Computing Developer Workflow If you are analyzing data that is changing over time, you might need to analyze historical data. For example, you might need to examine the last two weeks of data, January&rsquo;s data, or some other moving or static time window of data.
Pachyderm provides the following approaches to this task:
Fixed time windows - for rigid, fixed time windows, such as months (Jan, Feb, and so on) or days‚Äî01-01-17, 01-02-17, and so on).
Moving time windows
for rolling time windows of data, such as three-day windows or two-week windows. Fixed Time Windows # Datum is the basic unit of data partitioning in Pachyderm. The glob pattern property in the pipeline specification defines a datum. When you analyze data within fixed time windows, such as the data that corresponds to fixed calendar dates, Pachyderm recommends that you organize your data repositories so that each of the time windows that you plan to analyze corresponds to a separate file or directory in your repository, and therefore, Pachyderm processes it as a separate datum.
Organizing your repository as described above, enables you to do the following:
Analyze each time window in parallel. Only re-process data within a time window when that data, or a corresponding data pipeline, changes. For example, if you have monthly time windows of sales data stored in JSON format that needs to be analyzed, you can create a sales data repository with the following data:
sales ‚îú‚îÄ‚îÄ January | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ February | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îî‚îÄ‚îÄ ... ‚îî‚îÄ‚îÄ March ‚îú‚îÄ‚îÄ 01-01-17.json ‚îú‚îÄ‚îÄ 01-02-17.json ‚îî‚îÄ‚îÄ ... When you run a pipeline with sales as an input repository and a glob pattern of /*, Pachyderm processes each month&rsquo;s worth of sales data in parallel if workers are available. When you add new data into a subset of the months or add data into a new month, for example, May, Pachyderm processes only these updated datums.
More generally, this structure enables you to create the following types of pipelines:
Pipelines that aggregate or otherwise process daily data on a monthly basis by using the /* glob pattern. Pipelines that only analyze a particular month&rsquo;s data by using a /subdir/* or /subdir/ glob pattern. For example, /January/* or /January/. Pipelines that process data on daily by using the /*/* glob pattern. Any combination of the above. Moving Time Windows # In some cases, you need to run analyses for moving or rolling time windows that do not correspond to certain calendar months or days. For example, you might need to analyze the last three days of data, the three days of data before that, or similar. In other words, you need to run an analysis for every rolling length of time.
For rolling or moving time windows, there are a couple of recommended patterns:
Bin your data in repository folders for each of the moving time windows.
Maintain a time-windowed set of data that corresponds to the latest of the moving time windows.
Bin Data into Moving Time Windows # In this method of processing rolling time windows, you create the following two-pipeline DAGs to analyze time windows efficiently:
Pipeline Description Pipeline 1 Reads in data, determines to which bins the data corresponds, and writes the data into those bins. Pipeline 2 Read in and analyze the binned data. By splitting this analysis into two pipelines, you can benefit from using parallelism at the file level. In other words, Pipeline 1 can be easily parallelized for each file, and Pipeline 2 can be parallelized per bin. This structure enables easy pipeline scaling as the number of files increases.
For example, you have three-day moving time windows, and you want to analyze three-day moving windows of sales data. In the first repo, called sales, you commit data for the first day of sales:
sales ‚îî‚îÄ‚îÄ 01-01-17.json In the first pipeline, you specify to bin this data into a directory that corresponds to the first rolling time window from 01-01-17 to 01-03-17:
binned_sales ‚îî‚îÄ‚îÄ 01-01-17_to_01-03-17 ‚îî‚îÄ‚îÄ 01-01-17.json When the next day&rsquo;s worth of sales is committed, that data lands in the sales repository:
sales ‚îú‚îÄ‚îÄ 01-01-17.json ‚îî‚îÄ‚îÄ 01-02-17.json Then, the first pipeline executes again to bin the 01-02-17 data into relevant bins. In this case, the data is placed in the previously created bin named 01-01-17 to 01-03-17. However, the data also goes to the bin that stores the data that is received starting on 01-02-17:
binned_sales ‚îú‚îÄ‚îÄ 01-01-17_to_01-03-17 | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îî‚îÄ‚îÄ 01-02-17.json ‚îî‚îÄ‚îÄ 01-02-17_to_01-04-17 ‚îî‚îÄ‚îÄ 01-02-17.json As more and more daily data is added, your repository structure starting to looks as follows:
binned_sales ‚îú‚îÄ‚îÄ 01-01-17_to_01-03-17 | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îî‚îÄ‚îÄ 01-03-17.json ‚îú‚îÄ‚îÄ 01-02-17_to_01-04-17 | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îú‚îÄ‚îÄ 01-03-17.json | ‚îî‚îÄ‚îÄ 01-04-17.json ‚îú‚îÄ‚îÄ 01-03-17_to_01-05-17 | ‚îú‚îÄ‚îÄ 01-03-17.json | ‚îú‚îÄ‚îÄ 01-04-17.json | ‚îî‚îÄ‚îÄ 01-05-17.json ‚îî‚îÄ‚îÄ ... The following diagram describes how data accumulates in the repository over time:
Your second pipeline can then process these bins in parallel according to the glob pattern of /* or as described further. Both pipelines can be easily parallelized.
In the above directory structure, it might seem that data is duplicated. However, under the hood, Pachyderm deduplicates all of these files and maintains a space-efficient representation of your data. The binning of the data is merely a structural re-arrangement to enable you to process these types of moving time windows.
It might also seem as if Pachyderm performs unnecessary data transfers over the network to bin files. However, Pachyderm ensures that these data operations do not require transferring data over the network.
Maintaining a Single Time-Windowed Data Set # The advantage of the binning pattern above is that any of the moving time windows are available for processing. They can be compared, aggregated, and combined in any way, and any results or aggregations are kept in sync with updates to the bins. However, you do need to create a process to maintain the binning directory structure.
There is another pattern for moving time windows that avoids the binning of the above approach and maintains an up-to-date version of a moving time-windowed data set. This approach involves the creation of the following pipelines:
Pipeline Description Pipeline 1 Reads in data, determines which files belong in your moving time window, and writes the relevant files into an updated
version of the moving time-windowed data set. Pipeline 2 Reads in and analyzes the moving time-windowed data set. For example, you have three-day moving time windows, and you want to analyze three-day moving windows of sales data. The input data is stored in the sales repository:
sales ‚îú‚îÄ‚îÄ 01-01-17.json ‚îú‚îÄ‚îÄ 01-02-17.json ‚îú‚îÄ‚îÄ 01-03-17.json ‚îî‚îÄ‚îÄ 01-04-17.json When the January 4th file, 01-04-17.json, is committed, the first pipeline pulls out the last three days of data and arranges it in the following order:
last_three_days ‚îú‚îÄ‚îÄ 01-02-17.json ‚îú‚îÄ‚îÄ 01-03-17.json ‚îî‚îÄ‚îÄ 01-04-17.json When the January 5th file, 01-05-17.json, is committed into the sales repository:
sales ‚îú‚îÄ‚îÄ 01-01-17.json ‚îú‚îÄ‚îÄ 01-02-17.json ‚îú‚îÄ‚îÄ 01-03-17.json ‚îú‚îÄ‚îÄ 01-04-17.json ‚îî‚îÄ‚îÄ 01-05-17.json the first pipeline updates the moving window:
last_three_days ‚îú‚îÄ‚îÄ 01-03-17.json ‚îú‚îÄ‚îÄ 01-04-17.json ‚îî‚îÄ‚îÄ 01-05-17.json The analysis that you need to run on the moving windowed dataset in moving_sales_window can use the / or /* glob pattern, depending on whether you need to process all of the time-windowed files together or if they can be processed in parallel.
‚ö†Ô∏è When you create this type of moving time-windowed data set, the concept of now or today is relative. You must define the time based on your use case. For example, by configuring to use UTC. Do not use functions such as time.now() to determine the current time. The actual time when this pipeline runs might vary.
"
42,Manage Secrets,"Pachyderm uses Kubernetes&rsquo; Secrets to store and manage sensitive data, such as passwords, OAuth tokens, or ssh keys. You can use any of Kubernetes&rsquo; types of Secrets that match your use case. Namely, generic (or Opaque), tls, or docker-registry.
‚ö†Ô∏è As of today, Pachyderm only supports the JSON format for Kubernetes&rsquo; Secrets files.
To use a Secret in Pachyderm, you need to:
Create it. Reference it in your pipeline&rsquo;s specification file. Create A Secret # The creation of a Secret in Pachyderm requires a JSON configuration file.
A good way to create this file is:
To generate it by calling a dry-run of the kubectl create secret ... --dry-run=client --output=json &gt; myfirstsecret.json command. Then call pachctl create secret -f myfirstsecret.json. üìñ Kubernetes Secrets are, by default, stored as unencrypted base64-encoded strings (i.e., the values for all keys in the data field have to be base64-encoded strings). When using the kubectl create secret command, the encoding is done for you. If you choose to manually create your JSON file, make sure to use your own base 64 encoder.
Generate Your Secret Configuration File # Let&rsquo;s first generate your secret configuration file using the kubectl command. For example:
for a generic authentication secret: kubectl create secret generic mysecretname --from-literal=username=&lt;myusername&gt; --from-literal=password=&lt;mypassword&gt; --dry-run=client --output=json &gt; myfirstsecret.json for a tls secret: kubectl create secret tls mysecretname --cert=&lt;Path to your certificate&gt; --key=&lt;Path to your SSH key&gt; --dry-run=client --output=json &gt; myfirstsecret.json for a docker registry secret: kubectl create secret docker-registry mysecretname --dry-run=client --docker-server=&lt;DOCKER_REGISTRY_SERVER&gt; --docker-username=&lt;DOCKER_USER&gt; --docker-password=&lt;DOCKER_PASSWORD&gt; --output=json &gt; myfirstsecret.json Generic Secret Example # { &#34;apiVersion&#34;: &#34;v1&#34;, &#34;kind&#34;: &#34;Secret&#34;, &#34;metadata&#34;: { &#34;name&#34;: &#34;clearml&#34; }, &#34;type&#34;: &#34;Opaque&#34;, &#34;stringData&#34;: { &#34;access&#34;: &#34;&lt;CLEARML_API_ACCESS_KEY&gt;&#34;, &#34;secret&#34;: &#34;&lt;CLEARML_API_SECRET_KEY&gt;&#34; } } Find more detailed information on the creation of Secrets in Kubernetes documentation.
Create your Secret in Pachyderm # Next, run the following to actually create the secret in the Pachyderm Kubernetes cluster:
pachctl create secret -f myfirstsecret.json You can run pachctl list secret to verify that your secret has been properly created. You should see an output that looks like the following:
NAME TYPE CREATED mysecret kubernetes.io/dockerconfigjson 11 seconds ago ‚ÑπÔ∏è Use pachctl delete secret to delete a secret given its name, pachctl inspect secret to list a secret given its name.
You can now edit your pipeline specification file as follow.
Reference a Secret in Pachyderm&rsquo;s specification file # Now that your secret is created on Pachyderm cluster, you will need to notify your pipeline by updating your pipeline specification file. In Pachyderm, a Secret can be used in three different ways:
As a container environment variable:
In this case, in Pachyderm&rsquo;s pipeline specification file, you need to reference Kubernetes&rsquo; Secret by its:
name and specify an environment variable named env_var that the value of your key should be bound to. This makes for easy access to your Secret&rsquo;s data in your pipeline&rsquo;s code. For example, this is useful for passing the password to a third-party system to your pipeline&rsquo;s code.
&#34;transform&#34;: { &#34;image&#34;: &#34;string&#34;, &#34;cmd&#34;: [ string ], ... &#34;secrets&#34;: [ { &#34;name&#34;: &#34;string&#34;, &#34;env_var&#34;: &#34;string&#34;, &#34;key&#34;: string }] } Example # Example of a pipeline specification file assigning a Secret&rsquo;s values to environment variables.
Look at the pipeline specification in this example and see how we used the &quot;env_var&quot; to pass CLEARML API credentials to the pipeline code.
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;mnist&#34; }, &#34;description&#34;: &#34;MNIST example logging to ClearML&#34;, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;/bin/sh&#34; ], &#34;stdin&#34;: [ &#34;python pytorch_mnist.py --lr 0.2 --save-location /pfs/out&#34; ], &#34;image&#34;: &#34;pachyderm/clearml_mnist:dev0.11&#34;, &#34;secrets&#34;: [ { &#34;name&#34;: &#34;clearml&#34;, &#34;env_var&#34;: &#34;CLEARML_API_ACCESS_KEY&#34;, &#34;key&#34;: &#34;access&#34; }, { &#34;name&#34;: &#34;clearml&#34;, &#34;env_var&#34;: &#34;CLEARML_API_SECRET_KEY&#34;, &#34;key&#34;: &#34;secret&#34; } ] } } As a file in a volume mounted on a container:
In this case, in Pachyderm&rsquo;s pipeline specification file, you need to reference Kubernetes&rsquo; Secret by its:
name and specify the mount point (mount_path) to the secret (ex: &quot;/var/my-app-secret&quot;). Pachyderm mounts all of the keys in the secret with file names corresponding to the keys. This is useful for secure configuration files.
&#34;transform&#34;: { &#34;image&#34;: &#34;string&#34;, &#34;cmd&#34;: [ string ], ... &#34;secrets&#34;: [ { &#34;name&#34;: &#34;string&#34;, &#34;mount_path&#34;: string }] } When pulling images:
Image pull Secrets are a different kind of secret used to store access credentials to your private image registry.
You reference Image Pull Secrets (or Docker Registry Secrets) by setting the image_pull_secrets field of your pipeline specification file to the secret&rsquo;s name you created (ex: &quot;mysecretname&quot;).
&#34;transform&#34;: { &#34;image&#34;: &#34;string&#34;, &#34;cmd&#34;: [ string ], ... &#34;image_pull_secrets&#34;: [ string ] } "
43,Mount a Volume in a Pipeline,"You may have a local or a network-attached storage that you want your pipeline to write files to. You can mount that folder as a volume in Kubernetes and make it available in your pipeline worker by using the pod_patch pipeline parameter. The pod_patch parameter takes a string that specifies the changes that you want to add to your existing manifest. To create a patch, you need to generate a diff of the original ReplicationController and the one with your changes. You can use one of the online JSON patch utilities, such as JSON Patch Generator to create a diff. A diff for mounting a volume might look like this:
[ { &#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/volumes/-&#34;, &#34;value&#34;: { &#34;name&#34;: &#34;task-pv-storage&#34;, &#34;persistentVolumeClaim&#34;: { &#34;claimName&#34;: &#34;task-pv-claim&#34; } } }, { &#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/containers/0/volumeMounts/-&#34;, &#34;value&#34;: { &#34;mountPath&#34;: &#34;/data&#34;, &#34;name&#34;: &#34;task-pv-volume&#34; } } ] This output needs to be converted into a one-liner and added to the pipeline spec.
We will use the OpenCV example. to demonstrate this functionality.
To mount a volume, complete the following steps:
Create a PersistentVolume and a PersistentVolumeClaim as described in Configure a Pod to Use a PersistentVolume for Storage. Modify mountPath and path as needed.
For testing purposes, you might want to add an index.html file as described in Create an index.html file.
Get the ReplicationController (RC) manifest from your pipeline:
kubectl get rc &lt;rc-pipeline&gt; -o json &gt; &lt;filename&gt;.yaml Example:
kubectl get rc pipeline-edges-v7 -o json &gt; test-rc.yaml Open the generated RC manifest for editing.
Under spec, find the volumeMounts section.
Add your volume in the list of mounts.
Example:
{ &#34;mountPath&#34;: &#34;/data&#34;, &#34;name&#34;: &#34;task-pv-storage&#34; } mountPath is where your volume will be mounted inside of the container.
Find the volumes section.
Add the information about the volume.
Example:
{ &#34;name&#34;: &#34;task-pv-storage&#34;, &#34;persistentVolumeClaim&#34;: { &#34;claimName&#34;: &#34;task-pv-claim&#34; } } In this section, you need to specify the PersistentVolumeClaim you have created in Step 1.
Save these changes to a new file.
Copy the contents of the original RC to the clipboard.
Go to a JSON patch generator, such as JSON Patch Generator, and paste the contents of the original RC manifest to the Source JSON field.
Copy the contents of the modified RC manifest to clipboard as described above.
Paste the contents of the modified RC manifest to the Target JSON field.
Copy the generated JSON Patch.
Go to your terminal and open the pipeline manifest for editing.
For example, if you are modifying the edges pipeline, open the edges.json file.
Add the patch as a one-liner under the pod_patch parameter.
Example:
&#34;pod_patch&#34;: &#34;[{\&#34;op\&#34;: \&#34;add\&#34;,\&#34;path\&#34;: \&#34;/volumes/-\&#34;,\&#34;value\&#34;: {\&#34;name\&#34;: \&#34;task-pv-storage\&#34;,\&#34;persistentVolumeClaim\&#34;: {\&#34;claimName\&#34;: \&#34;task-pv-claim\&#34;}}}, {\&#34;op\&#34;: \&#34;add\&#34;,\&#34;path\&#34;: \&#34;/containers/0/volumeMounts/-\&#34;,\&#34;value\&#34;: {\&#34;mountPath\&#34;: \&#34;/data\&#34;,\&#34;name\&#34;: \&#34;task-pv-storage\&#34;}}]&#34; You need to add a backslash () before every quote (&quot;) sign that is enclosed in square brackets ([]). Also, you might need to modify the path to volumeMounts and volumes by removing the /spec/template/spec/ prefix and replacing the assigned volume number with a dash (-). For example, if a path in the JSON patch is /spec/template/spec/volumes/5, you might need to replace it with /volumes/-. See the example above for details.
After modifying the pipeline spec, update the pipeline:
pachctl update pipeline -f &lt;pipeline-spec.yaml&gt; A new pod and new replication controller should be created with your modified changes.
Verify that your file was mounted by connecting to your pod and listing the directory that you have specified as a mountpoint. In this example, it is /data.
Example:
kubectl exec -it &lt;pipeline-pod&gt; -- /bin/bash ls /data If you have added the index.html file for testing as described in Step 1, you should see that file in the mounted directory.
You might want to adjust your pipeline code to read from or write to the mounted directory. For example, in the aforementioned OpenCV example, the code reads from the /pfs/images directory and writes to the /pfs/out directory. If you want to read or write to the /data directory, you need to change those to /data.
üìñ Pachyderm has no notion of the files stored in the mounted directory before it is mounted to Pachyderm. Moreover, if you have mounted a network share to which you write files from other than Pachyderm sources, Pachyderm does not guarantee the provenance of those changes.
"
44,Skip Failed Datums," ‚ÑπÔ∏è The err_cmd parameter enables you to fail a datum without failing the whole job.
üí° Before you read this section, make sure that you understand such concepts as Datum and Pipeline.
When Pachyderm processes your data, it breaks it up into units of computation called datums. Each datum is processed separately. In a basic pipeline configuration, a failed datum results in a failed job. However, in some cases, you might not need all datums to consider a job successful. If your downstream pipelines can be run on only the successful datums instead of needing all the datums to be successful, Pachyderm can mark some datums as recovered which means that they failed with a non-critical error, but the successful datums will be processed.
To configure a condition under which you want your failed datums not to fail the whole job, you can add your custom error code in err_cmd and err_stdin fields in your pipeline specification.
For example, your DAG consists of two pipelines:
The pipeline 1 cleans the data. The pipeline 2 trains your model by using the data from the first pipeline. That means that the second pipeline takes the results of the first pipeline from its output repository and uses that data to train a model. In some cases, you might not need all the datums in the first pipeline to be successful to run the second pipeline.
The following diagram describes how Pachyderm transformation and error code work:
Here is what is happening in the diagram above:
Pachyderm executes the transformation code that you defined in the cmd field against your datums. If a datum is processed without errors, Pachyderm marks it as processed. If a datum fails, Pachyderm executes your error code (err_cmd) on that datum. If the code in err_cmd successfully runs on the skipped datum, Pachyderm marks the skipped datum as recovered. The datum is in a failed state and, therefore, the pipeline does not put it into the output repository, but successful datums continue onto the next step in your DAG. If the err_cmd code fails on the skipped datum, the datum is marked as failed, and, consequently, the job is marked as failed. You can view the processed, skipped, and recovered datums in the PROGRESS field in the output of the pachctl list job -p &lt;pipeline name&gt; command:
Pachyderm writes only processed datums of successful jobs to the output commit so that these datums can be processed by downstream pipelines. For example, in your first pipeline, Pachyderm processes three datums. If one of the datums is marked as recovered and two others are successfully processed, only these two successful datums are used in the next pipeline.
If you want to let the job proceed with only the successful datums being written to the output, set &quot;err_cmd&quot; : [&quot;true&quot;]. The failed datums, which are &ldquo;recovered&rdquo; by err_cmd in this way, will be retried on the next job, just as failed datums.
‚ÑπÔ∏è See Also: Example err_cmd pipeline
"
45,Transactions," ‚ÑπÔ∏è TL;DR:Use transactions to run multiple Pachyderm commands simultaneously in one job run.
A transaction is a Pachyderm operation that enables you to create a collection of Pachyderm commands and execute them concurrently. Regular Pachyderm operations, that are not in a transaction, are executed one after another. However, when you need to run multiple commands at the same time, you can use transactions. This functionality is useful in particular for pipelines with multiple inputs. If you need to update two or more input repos, you might not want pipeline jobs for each state change. You can issue a transaction to start commits in each of the input repos, which puts them both in the same global commit, creating a single downstream commit in the pipeline repo. After the transaction, you can put files and finish the commits at will, and the pipeline job will run once all the input commits have been finished.
Start and Finish Transaction Demarcations # A transaction demarcation initializes some transactional behavior before the demarcated area begins, then ends that transactional behavior when the demarcated area ends. You should see those demarcations as a declaration of the group of commands that will be treated together as a single coherent operation.
To start a transaction demarcation, run the following command:
pachctl start transaction System Response:
Started new transaction: 7a81eab5e6c6430aa5c01deb06852ca5 This command generates a transaction object in the cluster and saves its ID in the local Pachyderm configuration file. By default, this file is stored at ~/.pachyderm/config.json.
Example # { &#34;user_id&#34;: &#34;b4fe4317-be21-4836-824f-6661c68b8fba&#34;, &#34;v2&#34;: { &#34;active_context&#34;: &#34;local-2&#34;, &#34;contexts&#34;: { &#34;default&#34;: {}, &#34;local-2&#34;: { &#34;source&#34;: 3, &#34;active_transaction&#34;: &#34;7a81eab5e6c6430aa5c01deb06852ca5&#34;, &#34;cluster_name&#34;: &#34;minikube&#34;, &#34;auth_info&#34;: &#34;minikube&#34;, &#34;namespace&#34;: &#34;default&#34; }, After you start a transaction demarcation, you can add supported commands (i.e., transactional commands), such as pachctl start commit, pachctl create branch &hellip;, to the transaction.
All commands that are performed in a transaction are queued up and not executed against the actual cluster until you finish the transaction. When you finish the transaction, all queued command are executed atomically.
To finish a transaction, run:
pachctl finish transaction System Response:
Completed transaction with 1 requests: 7a81eab5e6c6430aa5c01deb06852ca5 üí° As soon as a commit is started (whether through start commit or put file without an open commit, or finishing a transaction that contains a start commit), a new global commit as well as a global job is created. All open commits are in a started state, each of the pipeline jobs created is running, and the workers waiting for the commit(s) to be closed to process the data. In other words, your changes will only be applied when you close the commits.
In the case of a transaction, the workers will wait until all of the input commits are finished to process them in one batch. All of those commits and jobs will be part of the same global commit/job and share the same globalID (Transaction ID). Without a transaction, each commit would trigger its own separate job.
We have used the inner join pipeline in our joins example to illustrate the difference between no transaction and the use a transaction, all other things being equal. Make sure to follow the example README if you want to run those pachctl commands yourself.
‚ÑπÔ∏è Note that in the case with the transaction, the put file and following finish commit are happening after the finish transaction instruction.
You must finish your transaction before putting files in the corresponding repo for the data to be part of the same batch. Running a &lsquo;put file&rsquo; before closing the transaction would result in a commit being created independently from the transaction itself and a job to run on that commit.
Supported Operations # While there is a transaction object in the Pachyderm configuration file, all supported API requests append the request to the transaction instead of running directly. These supported commands include:
create repo delete repo update repo start commit finish commit squash commit create branch delete branch create pipeline update pipeline edit pipeline Each time you add a command to a transaction, Pachyderm validates the transaction against the current state of the cluster metadata and obtains any return values, which is important for such commands as start commit. If validation fails for any reason, Pachyderm does not add the operation to the transaction. If the transaction has been invalidated by changing the cluster state, you must delete the transaction and start over, taking into account the new state of the cluster. From a command-line perspective, these commands work identically within a transaction as without. The only differences are that you do not apply your changes until you run finish transaction, and Pachyderm logs a message to stderr to indicate that the command was placed in a transaction rather than run directly.
Other Transaction Commands # Other supported commands for transactions include:
Command Description pachctl list transaction List all unfinished transactions available in the Pachyderm cluster. pachctl stop transaction Remove the currently active transaction from the local Pachyderm config file. The transaction remains in the Pachyderm cluster and can be resumed later. pachctl resume transaction Set an already-existing transaction as the active transaction in the local Pachyderm config file. pachctl delete transaction Deletes a transaction from the Pachyderm cluster. pachctl inspect transaction Provides detailed information about an existing transaction, including which operations it will perform. By default, displays information about the current transaction. If you specify a transaction ID, displays information about the corresponding transaction. Multiple Opened Transactions # Some systems have a notion of nested transactions. That is when you open transactions within an already opened transaction. In such systems, the operations added to the subsequent transactions are not executed until all the nested transactions and the main transaction are finished.
Pachyderm does not support such behavior. Instead, when you open a transaction, the transaction ID is written to the Pachyderm configuration file. If you begin another transaction while the first one is open, Pachyderm returns an error.
Every time you add a command to a transaction, Pachyderm creates a blueprint of the commit and verifies that the command is valid. However, one transaction can invalidate another. In this case, a transaction that is closed first takes precedence over the other. For example, if two transactions create a repository with the same name, the one that is executed first results in the creation of the repository, and the other results in error.
Use Cases # Pachyderm users implement transactions to their own workflows finding unique ways to benefit from this feature, whether it is a small research team or an enterprise-grade machine learning workflow.
Below are examples of the most commonly employed ways of using transactions.
Commit to Separate Repositories Simultaneously # For example, you have a Pachyderm pipeline with two input repositories. One repository includes training data and the other parameters for your machine learning pipeline. If you need to run specific data against specific parameters, you need to run your pipeline against specific commits in both repositories. To achieve this, you need to commit to these repositories simultaneously.
If you use a regular Pachyderm workflow, the data is uploaded sequentially, each time triggering a separate job instead of one job with both commits of new data. One put file operation commits changes to the data repository and the other updates the parameters repository. The following animation shows the standard Pachyderm workflow without a transaction:
In Pachyderm, a pipeline starts as soon as a new commit lands in a repository. In the diagram above, as soon as commit 1 is added to the data repository, Pachyderm runs a job for commit 1 and commit 0 in the parameters repository. You can also see that Pachyderm runs the second job and processes commit 1 from the data repository with the commit 1 in the parameters repository. In some cases, this is perfectly acceptable solution. But if your job takes many hours and you are only interested in the result of the pipeline run with commit 1 from both repositories, this approach does not work.
With transactions, you can ensure that only one job triggers with both the new data and parameters. The following animation demonstrates how transactions work:
The transaction ensures that a single job runs for the two commits that were started within the transaction. While Pachyderm supports some workflows where you can get the same effect by having both data and parameters in the same repo, often separating them and using transactions is much more efficient for organizational and performance reasons.
Switching from Staging to Master Simultaneously # If you are using deferred processing in your repositories because you want to commit your changes frequently without triggering jobs every time, then transactions can help you manage deferred processing with multiple inputs. You commit your changes to the staging branch and when needed, switch the HEAD of your master branch to a commit in the staging branch. To do this simultaneously, you can use transactions.
For example, you have two repositories data and parameters, both of which have a master and staging branch. You commit your changes to the staging branch while your pipeline is subscribed to the master branch. To switch to these branches simultaneously, you can use transactions like this:
pachctl start transaction System Response:
Started new transaction: 0d6f0bc337a0493696e382034a2a2055 pachctl pachctl create branch data@master --head staging Added to transaction: 0d6f0bc337a0493696e382034a2a2055 pachctl create branch parameters@master --head staging Added to transaction: 0d6f0bc337a0493696e382034a2a2055 pachctl finish transaction Completed transaction with 2 requests: 0d6f0bc337a0493696e382034a2a2055 When you finish the transaction, both repositories switch to the master branch at the same time which triggers one job to process those commits together.
Updating Multiple Pipelines Simultaneously # If you want to change logic or intermediate data formats in your DAG, you may need to change multiple pipelines. Performing these changes together in a transaction can avoid creating jobs with mismatched pipeline versions and potentially wasting work.
To get a better understanding of how transactions work in practice, try Use Transactions with Hyperparameter Tuning.
"
46,Basic Data Operations," üìñ Before you read this section, make sure that you are familiar with Pachyderm&rsquo;s concepts of Data and Pipeline.
There are many strategies available to get data in and out of Pachyderm, whether you want to use its data-driven versioned pipelines as a standalone product or plan a third-party integration. The following diagram gives an high level view of them all.
Click on the one that applies to your use-case to read its documentation.
"
47,Egress To An SQL Database," ‚ö†Ô∏è SQL Egress is an experimental feature.
Pachyderm already implements egress to object storage as an optional egress field in the pipeline specification. Similarly, our SQL egress lets you seamlessly export data from a Pachyderm-powered pipeline output repo to an SQL database.
Specifically, we help you connect to a remote database and push the content of CSV files to interface tables, matching their column names and casting their content into their respective SQL datatype.
Interface tables are intermediate tables used for staging the data being egressed from Pachyderm to your data warehouse. They are the tables your SQL Egress pipeline inserts its data into and should be dedicated tables. The content of your interface tables matches the content of the latest output commit of your pipeline.
‚ÑπÔ∏è Best Practice.
A new output commit will trigger a delete of all data in the interface tables before inserting more recent values. As a best practice, we strongly recommend to create a separate database for Pachyderm Egress.
As of today, we support the following drivers:
postgres and postgresql : connect to Postgresql (or compatible databases such as Redshift). mysql : connect to MySQL (or compatible databases such as MariaDB). snowflake : connect to Snowflake. Use SQL Egress # To egress data from the output commit of a pipeline to an SQL database, you will need to:
On your cluster
Create a secret containing your database password.
In the Specification file of your egress pipeline
Reference your secret by providing its name, provide the connection string to the database and choose the format of the files (CSV for now - we are planning on adding JSON soon) containing the data to insert.
In your user code
Write your data into CSV files placed in root directories named after the table you want to insert them into. You can have multiple directories.
1. Create a Secret # Create a secret containing your database password in the field PACHYDERM_SQL_PASSWORD. This secret is identical to the database secret of Pachyderm SQL Ingest. Refer to the SQL Ingest page for instructions on how to create your secret.
2. Update your Pipeline Spec # Append an egress section to your pipeline specification file, then fill in:
the url: the connection string to your database. Its format is identical to the url in the SQL Ingest.
the file_format type: CSV for now.
the name: the Kubernetes secret name.
the columns: Optional array for egress of CSV files with headers only. The order of the columns in this array must match the order of the schema columns; however, the CSV columns can be any order. So if the array is [&ldquo;foo&rdquo;, &ldquo;bar&rdquo;] and the CSV file is:
bar,foo 1,&#34;string&#34; 2,&#34;text!&#34; The following table will be written to the database:
foo | bar =============== string | 1 text! | 2 Example # { &#34;pipeline&#34;: { &#34;name&#34;: &#34;egress&#34; }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;input_repo&#34;, &#34;glob&#34;: &#34;/&#34;, &#34;name&#34;: &#34;in&#34; } }, &#34;transform&#34;: { ... }, &#34;egress&#34;: { &#34;sql_database&#34;: { &#34;url&#34;: &#34;snowflake://pachyderm@WHMUWUD-CJ80657/PACH_DB/PUBLIC?warehouse=COMPUTE_WH&#34;, &#34;file_format&#34;: { &#34;type&#34;: &#34;CSV&#34;, &#34;columns&#34;: [&#34;foo&#34;, &#34;bar&#34;] }, &#34;secret&#34;: { &#34;name&#34;: &#34;snowflakesecret&#34;, &#34;key&#34;: &#34;PACHYDERM_SQL_PASSWORD&#34; } } } } 3. In your User Code, Write Your Data to Directories Named After Each Table # The user code of your pipeline determines what data should be egressed and to which tables. Data (in the form of CSV files) that the pipeline writes to the output repo is interpreted as tables corresponding to directories.
Each top-level directory is named after the table you want to egress its content to. All of the files reachable in the walk of each root directory are parsed in the given format indicated in the egress section of the pipeline specification file (CSV for now), then inserted in their corresponding table. Find more information on how to format your CSV file depending on your targeted SQL Data Type in our SQL Ingest Formatting section.
‚ö†Ô∏è All interface tables must pre-exist before an insertion. Files in the root produce an error as they do not correspond to a table. The directory structure below the top level does not matter. The first directory in the path is the table; everything else is walked until a file is found. All the data in those files is inserted into the table. The order of the values in each line of a CSV must match the order of the columns in the schema of your interface table unless you were using headers AND specified the &quot;columns&quot;: [&quot;foo&quot;, &quot;bar&quot;], field in your pipeline specification file. Example # &#34;1&#34;,&#34;Tim&#34;,&#34;2017-03-12T21:51:45Z&#34;,&#34;true&#34; &#34;12&#34;,&#34;Tom&#34;,&#34;2017-07-25T21:51:45Z&#34;,&#34;true&#34; &#34;33&#34;,&#34;Tam&#34;,&#34;2017-01-01T21:51:45Z&#34;,&#34;false&#34; &#34;54&#34;,&#34;Pach&#34;,&#34;2017-05-15T21:51:45Z&#34;,&#34;true&#34; ‚ÑπÔ∏è Pachyderm queries the schema of the interface tables before insertion then parses the data into their SQL data types. Each insertion creates a new row in your table. Troubleshooting # You have a pipeline running but do not see any update in your database?
Check your logs:
pachctl list pipeline pachctl logs -p &lt;your-pipeline-name&gt; --master "
48,Export via Egress,"The egress field in the Pachyderm pipeline specification enables you to push the results of a pipeline to an external datastore such as Amazon S3, Google Cloud Storage, or Azure Blob Storage. After the user code has finished running, but before the job is marked as successful, Pachyderm pushes the data to the specified destination.
‚ÑπÔ∏è Make sure that your cluster has been configured to work with your object store.
Pick the egress protocol that applies to your storage:
Cloud Platform Protocol Example Google Cloud Storage gs:// gs://gs-bucket/gs-dir Amazon S3 s3:// s3://s3-endpoint/s3-bucket/s3-dir Azure Blob Storage wasb:// wasb://default-container@storage-account/az-dir Example # &#34;egress&#34;: { &#34;URL&#34;: &#34;s3://bucket/dir&#34; }, "
49,Export via Pachctl,"To export your data with pachctl:
List the files in the given directory:
pachctl list file &lt;repo&gt;@&lt;branch&gt;:&lt;dir&gt; Example:
pachctl list myrepo@master:labresults System Response:
NAME TYPE SIZE /labresults/T1606331395-LIPID-PATID2-CLIA24D9871327.txt file 101B /labresults/T1606707557-LIPID-PATID1-CLIA24D9871327.txt file 101B /labresults/T1606707579-LIPID-PATID3-CLIA24D9871327.txt file 100B /labresults/T1606707597-LIPID-PATID4-CLIA24D9871327.txt file 101B Get the contents of a specific file:
pachctl get file &lt;repo&gt;@&lt;branch&gt;:&lt;path/to/file&gt; Example:
pachctl get file myrepo@master:/labresults/T1606331395-LIPID-PATID2-CLIA24D9871327.txt System Response:
PID|PATID2 ORC|ORD777889 OBX|1|NM|2093-3^Cholesterol|212|mg/dL OBX|2|NM|2571-8^Triglyceride|110|mg/dL ‚ÑπÔ∏è You can view the parent, grandparent, and any previous commit by using the caret (^) symbol followed by a number that corresponds to an ancestor in sequence:
View a parent commit
pachctl list commit &lt;repo&gt;@&lt;branch-or-commit&gt;^:&lt;path/to/file&gt; pachctl get file &lt;repo&gt;@&lt;branch-or-commit&gt;^:&lt;path/to/file&gt; View an &lt;n&gt; parent of a commit
pachctl list commit &lt;repo&gt;@&lt;branch-or-commit&gt;^&lt;n&gt;:&lt;path/to/file&gt; pachctl get file &lt;repo&gt;@&lt;branch-or-commit&gt;^&lt;n&gt;:&lt;path/to/file&gt; Example:
pachctl get file datas@master^4:user_data.csv If the file does not exist in that revision, Pachyderm displays an error message.
"
50,Mount a Repo Locally," ‚ö†Ô∏è Pachyderm uses FUSE to mount repositories as local filesystems. Because Apple has announced phasing out support for macOS kernel extensions, including FUSE, this functionality is no longer stable on macOS Catalina (10.15) or later.
Pachyderm enables you to mount a repository as a local filesystem on your computer by using the pachctl mount command. This command uses the Filesystem in Userspace (FUSE) user interface to export a Pachyderm File System (PFS) to a Unix computer system. This functionality is useful when you want to pull data locally to experiment, review the results of a pipeline, or modify the files in the input repository directly.
You can mount a Pachyderm repo in one of the following modes:
Read-only ‚Äî you can read the mounted files to further experiment with them locally, but cannot modify them. Read-write ‚Äî you can read mounted files, modify their contents, and push them back into your centralized Pachyderm input repositories. Prerequisites # You must have the following configured for this functionality to work:
Unix or Unix-like operating system, such as Ubuntu 16.04 or macOS Yosemite or later.
FUSE for your operating system installed:
On macOS, run:
brew install osxfuse On Ubuntu, run:
sudo apt-get install -y fuse For more information, see:
FUSE for macOS
Mounting Repositories in Read-Only Mode # By default, Pachyderm mounts all repositories in read-only mode. You can access the files through your file browser or enable third-party applications access. Read-only access enables you to explore and experiment with the data, without modifying it. For example, you can mount your repo to a local computer and then open that directory in a Jupyter Notebook for exploration.
‚ÑπÔ∏è The pachctl mount command allows you to mount not only the default branch, typically a master branch, but also other Pachyderm branches. By default, Pachyderm mounts the master branch. However, if you add a branch to the name of the repo, the HEAD of that branch will be mounted.
Example:
pachctl mount images --repos images@staging You can also mount a specific commit, but because commits might be on multiple branches, modifying them might result in data deletion in the HEAD of the branches. Therefore, you can only mount commits in read-only mode. If you want to write to a specific commit that is not the HEAD of a branch, you can create a new branch with that commit as HEAD.
Mounting Repositories in Read-Write Mode # Running the pachctl mount command with the --write flag grants you write access to the mounted repositories, which means that you can open the files for editing and put them back to the Pachyderm repository.
‚ö†Ô∏è Your changes are saved to the Pachyderm repository only after you interrupt the pachctl mount with CTRL+C or with pachctl unmount, unmount /&lt;path-to-mount&gt;, or fusermount -u /&lt;path-to-mount&gt;.
For example, you have the OpenCV example pipeline up and running. If you want to edit files in the images repository, experiment with brightness and contrast settings in liberty.png, and finally have your edges pipeline process those changes. If you do not mount the images repo, you would have to first download the files to your computer, edit them, and then put them back to the repository. The pachctl mount command automates all these steps for you. You can mount just the images repo or all Pachyderm repositories as directories on you machine, edit as needed, and, when done, exit the pachctl mount command. Upon exiting the pachctl mount command, Pachyderm uploads all the changes to the corresponding repository.
If someone else modifies the files while you are working on them locally, their changes will likely be overwritten when you exit pachctl mount. This happens because Therefore, make sure that you do not work on the same files while someone else is working on them.
‚ÑπÔ∏è Use writable mount ONLY when you have sole ownership over the mounted data. Otherwise, merge conflicts or unexpected data overwrites can occur.
Because output repositories are created by the Pachyderm pipelines, they are immutable. Only a pipeline can change and update files in these repositories. If you try to change a file in an output repo, you will get an error message.
How to Mount/Unmount a Pachyderm Repo # To mount a Pachyderm repo on a local computer, complete the following steps:
In a terminal, go to a directory in which you want to mount a Pachyderm repo. It can be any new empty directory on your local computer. For example, mydirectory.
Run pachctl mount for a repository and branch that you want to mount:
pachctl mount &lt;path-on-your-computer&gt; [flags] Example:
If you want to mount all the repositories in your Pachyderm cluster to a mydirectory directory on your computer and give WRITE access to them, run: pachctl mount mydirectory --write If you want to mount the master branch of the images repo and enable file editing in this repository, run: pachctl mount mydirectory --repos images@master+w To give read-only access, omit +w.
System Response:
ro for images: &amp;{Branch:master Write:true} ri: repo:&lt;name:&#34;montage&#34; &gt; created:&lt;seconds:1591812554 nanos:348079652 &gt; size_bytes:1345398 description:&#34;Output repo for pipeline montage.&#34; branches:&lt;repo:&lt;name:&#34;montage&#34; &gt; name:&#34;master&#34; &gt; continue ri: repo:&lt;name:&#34;edges&#34; &gt; created:&lt;seconds:1591812554 nanos:201592492 &gt; size_bytes:136795 description:&#34;Output repo for pipeline edges.&#34; branches:&lt;repo:&lt;name:&#34;edges&#34; &gt; name:&#34;master&#34; &gt; continue ri: repo:&lt;name:&#34;images&#34; &gt; created:&lt;seconds:1591812554 nanos:28450609 &gt; size_bytes:244068 branches:&lt;repo:&lt;name:&#34;images&#34; &gt; name:&#34;master&#34; &gt; MkdirAll /var/folders/jl/mm3wrxqd75l9r1_d0zktphdw0000gn/T/pfs201409498/images The command runs in your terminal until you terminate it by pressing CTRL+C.
Tip Mount multiple repos at once by appending each mount instruction to the same command. For example, the following will mount both repos to the /mydirectory directory. pachctl mount ./mydirectory -r first_repo@master -r second_repo@master You can check that the repo was mounted by running the mount command in your terminal:
mount /dev/disk1s1 on / (apfs, local, read-only, journaled) devfs on /dev (devfs, local, nobrowse) /dev/disk1s2 on /System/Volumes/Data (apfs, local, journaled, nobrowse) /dev/disk1s5 on /private/var/vm (apfs, local, journaled, nobrowse) map auto_home on /System/Volumes/Data/home (autofs, automounted, nobrowse) pachctl@osxfuse0 on /Users/testuser/mydirectory (osxfuse, nodev, nosuid, synchronous, mounted by testuser) Access your mountpoint.
For example, in macOS, open Finder, press CMD + SHIFT + G, and type the mountpoint location. If you have mounted the repo to ~/mydirectory, type ~/mydirectory.
Edit the files as needed.
When ready, add your changes to the Pachyderm repo by stopping the pachctl mount command with CTRL+C or by running pachctl unmount &lt;mountpoint&gt; (or unmount /&lt;path-to-mount&gt;, or fusermount -u /&lt;path-to-mount&gt;).
If you have mounted a writable Pachyderm share, interrupting the pachctl mount command results in the upload of your changes to the corresponding repo and branch, which is equivalent to running the pachctl put file command. You can check that Pachyderm runs a new job for this work by listing current jobs with pachctl list job.
"
51,Ingest Data," pachctl put file # ‚ÑπÔ∏è At any time, run pachctl put file --help for the complete list of flags available to you.
Load your data into Pachyderm by using pachctl requires that one or several input repositories have been created.
pachctl create repo &lt;repo name&gt; Use the pachctl put file command to put your data into the created repository. Select from the following options:
Atomic commit: no open commit exists in your input repo. Pachyderm automatically starts a new commit, adds your data, and finishes the commit. pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file1&gt; -f &lt;file1&gt; Alternatively, you can manually start a new commit, add your data in multiple put file calls, and close the commit by running pachctl finish commit.
Start a commit: pachctl start commit &lt;repo&gt;@&lt;branch&gt; Put your data: pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file1&gt; -f &lt;file1&gt; Put more data: pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file2&gt; -f &lt;file2&gt; Close the commit: pachctl finish commit &lt;repo&gt;@&lt;branch&gt; Filepath Formats # üí° Pachyderm uses *?[]{}!()@+^ as reserved characters for glob patterns. Because of this, you cannot use these characters in your filepath.
In Pachyderm, you specify the path to file by using the -f option. A path to file can be a local path or a URL to an external resource. You can add multiple files or directories by using the -i option. To add contents of a directory, use the -r flag.
The following table provides examples of pachctl put file commands with various filepaths and data sources:
Put data from a URL:
pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file&gt; -f http://url_path Put data from an object store. You can use s3://, gcs://, or as:// in your filepath:
chctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file&gt; -f s3://object_store_url ‚ÑπÔ∏è If you are configuring a local cluster to access an external bucket, make sure that Pachyderm has been given the proper access.
Add multiple files at once by using the -i option or multiple -f flags. In the case of -i, the target file must be a list of files, paths, or URLs that you want to input all at once:
chctl put file &lt;repo&gt;@&lt;branch&gt; -i &lt;file containing list of files, paths, or URLs&gt; Add an entire directory or all of the contents at a particular URL, either HTTP(S) or object store URL, s3://, gcs://, and as://, by using the recursive flag, -r:
pachctl put file &lt;repo&gt;@&lt;branch&gt; -r -f &lt;dir&gt; Loading Your Data Partially # Depending on your use case and the volume of your data, you might decide to keep your dataset in its original source and process only a subset in Pachyderm.
Add a metadata file containing a list of URL/path to your external data to your repo.
Your pipeline code will retrieve the data following their path without the need to preload it all. In this case, Pachyderm will not keep versions of the source file, but it will keep track and provenance of the resulting output commits.
"
52,Manage Commits & Delete Data,"If bad data was committed into a Pachyderm input repository, you might need to delete a commit or surgically delete files from your history. Depending on whether or not the bad data is in the HEAD commit of the branch, you can perform one of the following actions:
Delete the HEAD of a Branch. If the incorrect data was added in the latest commit and provided that the commit does not have children: Follow the steps in this section to fix the HEAD of the corrupted branch. If your changes are relatively recent (see conditions below), you can delete a particular file, then erase it from your history. Additionally, although this is a separate use-case, you have the option to squash non-HEAD commits to rewrite your commit history.
Delete the HEAD of a Branch # To fix a broken HEAD, run the following command:
pachctl delete commit &lt;commit-ID&gt; When you delete a HEAD commit, Pachyderm performs the following actions:
Changes HEADs of all the branches that had the bad commit as their HEAD to their bad commit&rsquo;s parent and deletes the commit. The data in the deleted commit is lost. If the bad commit does not have a parent, Pachyderm sets the branch&rsquo;s HEAD to a new empty commit. Interrupts all running jobs, including not only the jobs that use the bad commit as a direct input but also the ones farther downstream in your DAG. Deletes the output commits from the deleted jobs. All the actions listed above are applied to those commits as well. ‚ö†Ô∏è This command will only succeed if the HEAD commit has no children on any branch. pachctl delete commit will error when attempting to delete a HEAD commit with children.
‚ÑπÔ∏è Are you wondering how a HEAD commit can have children?
A commit can be the head of a branch and still have children. For instance, given a master branch in a repository named repo, if you branch master by running pachctl create branch repo@staging --head repo@master, the master&rsquo;s HEAD will have an alias child on staging.
Squash non-HEAD Commits # If your commit has children, you have the option to use the squash commit command. Squashing is a way to rewrite your commit history; this helps clean up and simplify your commit history before sharing your work with team members. Squashing a commit in Pachyderm means that you are combining all the file changes in the commits of a global commit into their children and then removing the global commit. This behavior is inspired by the squash option in git rebase. No data stored in PFS is removed since they remain in the child commits.
pachctl squash commit &lt;commit-ID&gt; ‚ö†Ô∏è Squashing a global commit on the head of a branch (no children) will fail. Use pachctl delete commit instead. Squash commit only applies to user repositories. For example, you cannot squash a commit that updated a pipeline (Commit that lives in a spec repository). Similarly to pachctl delete commit, pachctl squash commit stops (but does not delete) associated jobs. Example # In the simple example below, we create three successive commits on the master branch of a repo repo:
In commit ID1, we added files A and B. In commit ID2, we added file C. In commit ID3, the latest commit, we altered the content of files A and C. We then run pachctl squash commit ID1, then pachctl squash commit ID2, and look at our branch and remaining commit(s).
A‚Äô and C&rsquo; are altered versions of files A and C. At any moment, pachctl list file repo@master invariably returns the same files A‚Äô, B, C‚Äô. pachctl list commit however, differs in each case, since, by squashing commits, we have deleted them from the branch.
Delete Files from History # üìñ It is important to note that this use case is limited to simple cases where the &ldquo;bad&rdquo; changes were made relatively recently, as any pipeline update since then will make it impossible.
In rare cases, you might need to delete a particular file from a given commit and further choose to delete its complete history. In such a case, you will need to:
Create a new commit in which you surgically remove the problematic file.
Start a new commit:
pachctl start commit &lt;repo&gt;@&lt;branch&gt; Delete all corrupted files from the newly opened commit:
pachctl delete file &lt;repo&gt;@&lt;branch or commitID&gt;:/path/to/files Finish the commit:
pachctl finish commit &lt;repo&gt;@&lt;branch&gt; Optionally, wipe this file from your history by squashing the initial bad commit and all its children up to the newly finished commit.
Unless the subsequent commits overwrote or deleted the bad data, the data might still be present in the children commits. Squashing those commits cleans up your commit history and ensures that the errant data is not available when non-HEAD versions of the data are read.
Example # In the simple example below, we want to delete file C in commit 2. How would we do that?
For now, pachctl list file repo@master returns the files A‚Äô, B, C‚Äô, E, F.
A‚Äô and C&rsquo; are altered versions of files A and C. We create a new commit in which we surgically remove file C:
pachctl start commit repo@master pachctl delete file repo@master:path/to/C pachctl finish commit repo@master At this point, pachctl list file repo@master returns the files A‚Äô, B, E, F. We removed file C. However, it still exists in the commit history.
To remove C from the commit history, we squash the commits in which C appears, all the way down to the last commit.
pachctl squash commitID2 pachctl squash commitID3 It is as if C never existed.
"
53,SQL Ingest," ‚ö†Ô∏è SQL Ingest is an experimental feature.
You can inject database content, collected by your data warehouse, by pulling the result of a given query into Pachyderm and saving it as a CSV or JSON file.
Before You Start # You should be familiar with Jsonnet. You should be familiar with creating Jsonnet pipeline specs in Pachyderm. You should be familiar with managing Kubernetes secrets. How to Set Up SQL Ingest # 1. Create &amp; Upload a Secret # You must generate a secret that contains the password granting user access to the database; you will pass the username details through the database connection string in step 2.
Copy the following: kubectl create secret generic yourSecretName --from-literal=PACHYDERM_SQL_PASSWORD=yourDatabaseUserPassword --dry-run=client --output=json &gt; yourSecretFile.json Swap out yourSecretName, yourDatabaseUserPassword, and yourSecretFile with relevant inputs. Open a terminal and run the command. Copy the following: pachctl create secret -f yourSecretFile.json Swap out yourSecretfile with relevant filename. Run the command. Confirm secret by running pachctl list secret. ‚ÑπÔ∏è Not all secret formats are the same. For a full walkthrough on how to create, edit, and view different types of secrets, see Create and Manage Secrets in Pachyderm.
2. Create a Database Connection String # Pachyderm&rsquo;s SQL Ingest requires a connection string defined as a Jsonnet URL parameter to connect to your database; the URL is structured as follows:
&lt;protocol&gt;://&lt;username&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?&lt;param1&gt;=&lt;value1&gt;&amp;&lt;param2&gt;=&lt;value2&gt; 3. Create a Pipeline Spec # Pachyderm provides a default Jsonnet template that has key parameters built in. To use it, you must pass an argument for each parameter.
Copy the following: pachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.5.x/src/templates/sql_ingest_cron.jsonnet \ --arg name=&lt;pipelineName&gt; \ --arg url=&#34;&lt;connectionStringToDdatabase&gt;&#34; \ --arg query=&#34;&lt;query&gt;&#34; \ --arg hasHeader=&lt;boolean&gt; \ --arg cronSpec=&#34;&lt;pullInterval&gt;&#34; \ --arg secretName=&#34;&lt;youSecretName&gt;&#34; \ --arg format=&lt;CsvOrJson&gt; --arg outputFile=&#39;&lt;fileName&gt;&#39; Swap out all of the parameter values with relevant inputs. Open terminal. Run the command. 4. View Query &amp; Results # To View Query String: pachctl inspect pipeline &lt;pipelineName&gt; To View Output File Name: pachctl list file &lt;pipelineName&gt;@master To View Output File Contents: pachctl get file &lt;pipelineName&gt;@master:/0000 Example: Snowflake # In this example, we are leveraging Snowflake&rsquo;s support for queries traversing semi-structured data (here, JSON).
Create a secret with your password named snowflakeSecret. Create a Snowflake specific database connection URL using the following details: Protocol: snowflake Username: username Host: VCNYTW-MH64356 (account name or locator) Database: SNOWFLAKE_SAMPLE_DATA Schema: WEATHER Warehouse: COMPUTE_WH snowflake://username@VCNYTW-MH64356/SNOWFLAKE_SAMPLE_DATA/WEATHER?warehouse=COMPUTE_WH Build query for the table DAILY_14_TOTAL using information from column V. select T, V:city.name, V:data[0].weather[0].description as morning, V:data[12].weather[0].description as pm FROM DAILY_14_TOTAL LIMIT 1 Define the pipeline spec by populating all of the parameter values: pachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.5.x/src/templates/sql_ingest_cron.jsonnet \ --arg name=mysnowflakeingest \ --arg url=&#34;snowflake://username@VCNYTW-MH64356/SNOWFLAKE_SAMPLE_DATA/WEATHER?warehouse=COMPUTE_WH&#34; \ --arg query=&#34;select T, V:city.name, V:data[0].weather[0].description as morning, V:data[12].weather[0].description as pm FROM DAILY_14_TOTAL LIMIT 1&#34; \ --arg hasHeader=true \ --arg cronSpec=&#34;@every 30s&#34; \ --arg secretName=&#34;snowflakeSecret&#34; \ --arg format=json Run the command. How Does This Work? # SQL Ingest&rsquo;s Jsonnet pipeline spec, sql_ingest_cron.jsonnet, creates all of the following:
1 Input Data Repo: Used to store timestamp files at the cronSpec&rsquo;s set interval rate (--arg cronSpec=&quot;pullInterval&quot; \) to trigger the pipeline. 1 Cron Pipeline: Houses the spec details that define the input type and settings and data transformation. 1 Output Repo: Used to store the data transformed by the cron pipeline; set by the pipeline spec&rsquo;s pipeline.name attribute, which you can define through the Jsonnet parameter --arg name=outputRepoName \. 1 Output File: Used to save the query results (JSON or CSV) and potentially be used as input for a following pipeline. In the default Jsonnet template, the file generated is obtainable from the output repo, outputRepoName@master:/0000. The filename is hardcoded, however you could paramaterize this as well using a custom Jsonnet pipeline spec and passing --arg outputFile='0000'. The file&rsquo;s contents are the result of the query(--arg query=&quot;query&quot;) being ran against the database--arg url=&quot;connectionStringToDdatabase&quot; ; both are defined in the transform.cmd attribute.
About SQL Ingest Pipeline Specs # To create an SQL Ingest Jsonnet Pipeline spec, you must have a .jsonnet file and several parameters:
pachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.5.x/src/templates/sql_ingest_cron.jsonnet \ --arg name=&lt;pipelineName&gt; \ --arg url=&#34;&lt;connectionStringToDdatabase&gt;&#34; \ --arg query=&#34;&lt;query&gt;&#34; \ --arg hasHeader=&lt;boolean&gt; \ --arg cronSpec=&#34;&lt;pullInterval&gt;&#34; \ --arg secretName=&#34;&lt;secretName&gt;&#34; \ --arg format=&lt;CsvOrJson&gt; The name of each pipeline (and their related input/output repos) are derived from the name parameter (--arg name=&lt;pipelineName&gt;). Parameters # Parameter Description name The name of output repo where query results will materialize. url The connection string to the database. query The SQL query to be run against the connected database. hasHeader Adds a header to your CSV file if set to true. Ignored if format=&quot;json&quot; (JSON files always display (header,value) pairs for each returned row). Defaults to false. Pachyderm creates the header after each element of the comma separated list of your SELECT clause or their aliases (if any). For example country.country_name_eng will have country.country_name_eng as header while country.country_name_eng as country_name will have country_name. cronSpec How often to run the query. For example &quot;@every 60s&quot;. format The type of your output file containing the results of your query (either json or csv). secretName The Kubernetes secret name that contains the password to the database. outputFile The name of the file created by your pipeline and stored in your output repo; default 0000 URL Parameter Details # &lt;protocol&gt;://&lt;username&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?&lt;param1&gt;=&lt;value1&gt;&amp;&lt;param2&gt;=&lt;value2&gt; Passwords are not included in the URL; they are retrieved from the secret created in step 1. The additional parameters after ? are optional and needed on a case-by-case bases (for example, Snowflake). Parameter Description protocol The name of the database protocol. As of today, we support: - postgres and postgresql : connect to Postgresql or compatible (for example Redshift).
- mysql : connect to MySQL or compatible (for example MariaDB). - snowflake : connect to Snowflake. username The user used to access the database. host The hostname of your database instance. port The port number your instance is listening on. database The name of the database to connect to. Snowflake # Pachyderm supports two connection URL patterns to query Snowflake:
snowflake://username@&lt;account_identifier&gt;/&lt;db_name&gt;/&lt;schema_name&gt;?warehouse=&lt;warehouse_name&gt; snowflake://username@hostname:port/&lt;db_name&gt;/&lt;schema_name&gt;?account=&lt;account_identifier&gt;&amp;warehouse=&lt;warehouse_name&gt; The account_identifier takes one of the following forms for most URLs:
Option 1 - Account Name:organization_name-account_name. Option 2 - Account Locator: account_locator.region.cloud. Formats &amp; SQL Data Types # The following comments on formatting reflect the state of this release and are subject to change.
Formats # Numeric # All numeric values are converted into strings in your CSV and JSON.
Database CSV JSON 12345 12345 &ldquo;12345&rdquo; 123.45 123.45 &ldquo;123.45&rdquo; ‚ö†Ô∏è Note that infinite (Inf) and not a number (NaN) values will also be stored as strings in JSON files. Use this format #.# for all decimals that you plan to egress back to a database. Date/Timestamps # Type Database CSV JSON Date 2022-05-09 2022-05-09T00:00:00 &ldquo;2022-05-09T00:00:00&rdquo; Timestamp ntz 2022-05-09 16:43:00 2022-05-09T16:43:00 &ldquo;2022-05-09T16:43:00&rdquo; Timestamp tz 2022-05-09 16:43:00-05:00 2022-05-09T16:43:00-05:00 &ldquo;2022-05-09T16:43:00-05:00&rdquo; Strings # Database CSV &ldquo;null&rdquo; null `&quot;&quot;` &quot;&quot;&quot;&quot;&quot;&quot; &quot;&quot; &quot;&quot; nil &quot;my string&quot; &ldquo;&ldquo;&ldquo;my string&rdquo;&rdquo;&rdquo; &ldquo;this will be enclosed in quotes because it has a ,&rdquo; &ldquo;this will be enclosed in quotes because it has a ,&rdquo; üí° When parsing your CSVs in your user code, remember to escape &quot; with &quot;&quot;.
Supported Data Types # Some of the Data Types listed in this section are specific to a particular database.
Dates/Timestamps Varchars Numerics Booleans DATE TIME
TIMESTAMP
TIMESTAMP_LTZ
TIMESTAMP_NTZ
TIMESTAMP_TZ
TIMESTAMPTZ
TIMESTAMP WITH TIME ZONE
TIMESTAMP WITHOUT TIME ZONE VARCHAR
TEXT
CHARACTER VARYING SMALLINT
INT2
INTEGER
INT
INT4
BIGINT
INT8
FLOAT
FLOAT4
FLOAT8
REAL
DOUBLE PRECISION
NUMERIC
DECIMAL
NUMBER BOOL
BOOLEAN "
54,View Project," How to View a Project in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Your project is displayed as a DAG (Directed Acyclic Graph) by default.
"
55,View List," How to View a List of Resources in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select View List. The DAG view is converted into a List view, organized by resource types. Select Repositories to view a list of repositories. Select Pipelines to view a list of pipelines. "
56,View Pipelines," How to View Pipeline Details in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select a Pipeline. Scroll and tab through the side panel to review the pipeline&rsquo;s details. Job Overview: Contains details like the number of datums processed, success status, and runtime. Pipeline Info: Contains details like the pipeline&rsquo;s description, number of tries allowed, output branch, and output repos. Pipeline Spec: Contains the pipeline spec which can be copied or downloaded as json/yaml. "
57,View Jobs, How to View Jobs From a Pipeline in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select Jobs. Scroll through the list of jobs. Select See Details. Select Read Logs. 
58,View Outputs, How to View Output Files in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select an Output. Select View Files. Select See Files. Perform one of the following: Preview Download 
59,View Inputs, How to View Input Files in Console # Authenticate to Pachyderm or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select an Input Repo. Select View Files. Perform one of the following: Preview Download 
60,View Audit Logs," How to View Audit Logs # Open a terminal. Input the following command, replacing user command with pachctl terms: pachctl logs | grep &#39;user command&#39; Review logs. pachctl logs | grep &#39;create project&#39; {&#34;log&#34;:&#34;{\&#34;severity\&#34;:\&#34;info\&#34;,\&#34;time\&#34;:\&#34;2023-01-24T15:46:35.447335088Z\&#34;,\&#34;logger\&#34;:\&#34;grpc.admin_v2.API/InspectCluster\&#34;,\&#34;caller\&#34;:\&#34;logging/interceptor.go:529\&#34;,\&#34;message\&#34;:\&#34;request for admin_v2.API/InspectCluster\&#34;,\&#34;service\&#34;:\&#34;admin_v2.API\&#34;,\&#34;method\&#34;:\&#34;InspectCluster\&#34;,\&#34;x-request-id\&#34;:[\&#34;3384fceb-68a7-4003-92ad-3bc8102d9e72\&#34;],\&#34;command\&#34;:[\&#34;pachctl create project dogs\&#34;],\&#34;peer\&#34;:\&#34;10.1.5.2:50784\&#34;,\&#34;request\&#34;:\&#34;client_version:\u003cmajor:2 minor:5 additional:\\\&#34;-alpha.4\\\&#34; git_commit:\\\&#34;1a252e4f760513c820353d47227009472213713a\\\&#34; git_tree_modified:\\\&#34;true\\\&#34; build_date:\\\&#34;2023-01-19T22:43:41Z\\\&#34; go_version:\\\&#34;go1.19.5\\\&#34; platform:\\\&#34;arm64\\\&#34; \u003e \&#34;}\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:35.447552838Z&#34;} {&#34;log&#34;:&#34;{\&#34;severity\&#34;:\&#34;info\&#34;,\&#34;time\&#34;:\&#34;2023-01-24T15:46:35.447439005Z\&#34;,\&#34;logger\&#34;:\&#34;grpc.admin_v2.API/InspectCluster\&#34;,\&#34;caller\&#34;:\&#34;logging/interceptor.go:529\&#34;,\&#34;message\&#34;:\&#34;response for admin_v2.API/InspectCluster\&#34;,\&#34;service\&#34;:\&#34;admin_v2.API\&#34;,\&#34;method\&#34;:\&#34;InspectCluster\&#34;,\&#34;x-request-id\&#34;:[\&#34;3384fceb-68a7-4003-92ad-3bc8102d9e72\&#34;],\&#34;command\&#34;:[\&#34;pachctl create project dogs\&#34;],\&#34;peer\&#34;:\&#34;10.1.5.2:50784\&#34;,\&#34;response\&#34;:\&#34;id:\\\&#34;9d417960076d4f63969254424a25a651\\\&#34; deployment_id:\\\&#34;LImXqySxvSBudquUR0vlohA1NeHnsTrr\\\&#34; version_warnings_ok:true \&#34;,\&#34;messagesSent\&#34;:1,\&#34;messagesReceived\&#34;:1,\&#34;grpc.code\&#34;:0,\&#34;duration\&#34;:0.000194958}\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:35.448185588Z&#34;} {&#34;log&#34;:&#34;{\&#34;severity\&#34;:\&#34;info\&#34;,\&#34;time\&#34;:\&#34;2023-01-24T15:46:35.452357963Z\&#34;,\&#34;logger\&#34;:\&#34;grpc.pfs_v2.API/CreateProject\&#34;,\&#34;caller\&#34;:\&#34;logging/interceptor.go:529\&#34;,\&#34;message\&#34;:\&#34;request for pfs_v2.API/CreateProject\&#34;,\&#34;service\&#34;:\&#34;pfs_v2.API\&#34;,\&#34;method\&#34;:\&#34;CreateProject\&#34;,\&#34;x-request-id\&#34;:[\&#34;b1fd3d34-7fd1-4ca1-bcb3-6130b3ece8e8\&#34;],\&#34;command\&#34;:[\&#34;pachctl create project dogs\&#34;],\&#34;peer\&#34;:\&#34;10.1.5.2:50784\&#34;,\&#34;request\&#34;:\&#34;project:\u003cname:\\\&#34;dogs\\\&#34; \u003e \&#34;}\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:35.452537213Z&#34;} {&#34;log&#34;:&#34;{\&#34;severity\&#34;:\&#34;info\&#34;,\&#34;time\&#34;:\&#34;2023-01-24T15:46:35.465953046Z\&#34;,\&#34;logger\&#34;:\&#34;grpc.pfs_v2.API/CreateProject\&#34;,\&#34;caller\&#34;:\&#34;logging/interceptor.go:529\&#34;,\&#34;message\&#34;:\&#34;response for pfs_v2.API/CreateProject\&#34;,\&#34;service\&#34;:\&#34;pfs_v2.API\&#34;,\&#34;method\&#34;:\&#34;CreateProject\&#34;,\&#34;x-request-id\&#34;:[\&#34;b1fd3d34-7fd1-4ca1-bcb3-6130b3ece8e8\&#34;],\&#34;command\&#34;:[\&#34;pachctl create project dogs\&#34;],\&#34;peer\&#34;:\&#34;10.1.5.2:50784\&#34;,\&#34;messagesSent\&#34;:1,\&#34;messagesReceived\&#34;:1,\&#34;grpc.code\&#34;:0,\&#34;duration\&#34;:0.013663667}\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:35.46607713Z&#34;} {&#34;log&#34;:&#34;{\&#34;severity\&#34;:\&#34;info\&#34;,\&#34;time\&#34;:\&#34;2023-01-24T15:46:39.028400256Z\&#34;,\&#34;logger\&#34;:\&#34;grpc.admin_v2.API/InspectCluster\&#34;,\&#34;caller\&#34;:\&#34;logging/interceptor.go:529\&#34;,\&#34;message\&#34;:\&#34;request for admin_v2.API/InspectCluster\&#34;,\&#34;service\&#34;:\&#34;admin_v2.API\&#34;,\&#34;method\&#34;:\&#34;InspectCluster\&#34;,\&#34;x-request-id\&#34;:[\&#34;714c04e7-d180-4182-9230-3ec27e554d97\&#34;],\&#34;command\&#34;:[\&#34;pachctl create project cats\&#34;],\&#34;peer\&#34;:\&#34;10.1.5.2:60084\&#34;,\&#34;request\&#34;:\&#34;client_version:\u003cmajor:2 minor:5 additional:\\\&#34;-alpha.4\\\&#34; git_commit:\\\&#34;1a252e4f760513c820353d47227009472213713a\\\&#34; git_tree_modified:\\\&#34;true\\\&#34; build_date:\\\&#34;2023-01-19T22:43:41Z\\\&#34; go_version:\\\&#34;go1.19.5\\\&#34; platform:\\\&#34;arm64\\\&#34; \u003e \&#34;}\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:39.028511715Z&#34;} {&#34;log&#34;:&#34;{\&#34;severity\&#34;:\&#34;info\&#34;,\&#34;time\&#34;:\&#34;2023-01-24T15:46:39.028438673Z\&#34;,\&#34;logger\&#34;:\&#34;grpc.admin_v2.API/InspectCluster\&#34;,\&#34;caller\&#34;:\&#34;logging/interceptor.go:529\&#34;,\&#34;message\&#34;:\&#34;response for admin_v2.API/InspectCluster\&#34;,\&#34;service\&#34;:\&#34;admin_v2.API\&#34;,\&#34;method\&#34;:\&#34;InspectCluster\&#34;,\&#34;x-request-id\&#34;:[\&#34;714c04e7-d180-4182-9230-3ec27e554d97\&#34;],\&#34;command\&#34;:[\&#34;pachctl create project cats\&#34;],\&#34;peer\&#34;:\&#34;10.1.5.2:60084\&#34;,\&#34;response\&#34;:\&#34;id:\\\&#34;9d417960076d4f63969254424a25a651\\\&#34; deployment_id:\\\&#34;LImXqySxvSBudquUR0vlohA1NeHnsTrr\\\&#34; version_warnings_ok:true \&#34;,\&#34;messagesSent\&#34;:1,\&#34;messagesReceived\&#34;:1,\&#34;grpc.code\&#34;:0,\&#34;duration\&#34;:0.000077375}\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:39.028556673Z&#34;} {&#34;log&#34;:&#34;{\&#34;severity\&#34;:\&#34;info\&#34;,\&#34;time\&#34;:\&#34;2023-01-24T15:46:39.030396298Z\&#34;,\&#34;logger\&#34;:\&#34;grpc.pfs_v2.API/CreateProject\&#34;,\&#34;caller\&#34;:\&#34;logging/interceptor.go:529\&#34;,\&#34;message\&#34;:\&#34;request for pfs_v2.API/CreateProject\&#34;,\&#34;service\&#34;:\&#34;pfs_v2.API\&#34;,\&#34;method\&#34;:\&#34;CreateProject\&#34;,\&#34;x-request-id\&#34;:[\&#34;a0beac90-f6ad-4c49-8a39-188076e3d03d\&#34;],\&#34;command\&#34;:[\&#34;pachctl create project cats\&#34;],\&#34;peer\&#34;:\&#34;10.1.5.2:60084\&#34;,\&#34;request\&#34;:\&#34;project:\u003cname:\\\&#34;cats\\\&#34; \u003e \&#34;}\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:39.030474673Z&#34;} {&#34;log&#34;:&#34;{\&#34;severity\&#34;:\&#34;info\&#34;,\&#34;time\&#34;:\&#34;2023-01-24T15:46:39.031706548Z\&#34;,\&#34;logger\&#34;:\&#34;grpc.pfs_v2.API/CreateProject\&#34;,\&#34;caller\&#34;:\&#34;logging/interceptor.go:529\&#34;,\&#34;message\&#34;:\&#34;response for pfs_v2.API/CreateProject\&#34;,\&#34;service\&#34;:\&#34;pfs_v2.API\&#34;,\&#34;method\&#34;:\&#34;CreateProject\&#34;,\&#34;x-request-id\&#34;:[\&#34;a0beac90-f6ad-4c49-8a39-188076e3d03d\&#34;],\&#34;command\&#34;:[\&#34;pachctl create project cats\&#34;],\&#34;peer\&#34;:\&#34;10.1.5.2:60084\&#34;,\&#34;messagesSent\&#34;:1,\&#34;messagesReceived\&#34;:1,\&#34;grpc.code\&#34;:0,\&#34;duration\&#34;:0.001327458}\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:39.031799756Z&#34;} Useful Search Terms # Command delete branch delete commit delete file delete job delete pipeline delete repo delete secret delete transaction edit pipeline "
61,View Kubernetes Logs,"The kube-event-tail pod in your Pachyderm cluster stores Kubernetes logs which are discarded after a certain amount of time. You can view these logs to obtain insights on key events. There are three event types: informational, warning, and error.
How to View Kubernetes Logs # Open a terminal. Input the following command, replacing user command with pachctl terms: pachctl kube-events Review logs. LAST SEEN TYPE REASON OBJECT MESSAGE 20 minutes ago ScalingReplicaSet Deployment/pachd Scaled up replica set pachd-84f599bccd to 1 19 minutes ago ScalingReplicaSet Deployment/pachd Scaled down replica set pachd-65fc687687 to 0 from 1 21 minutes ago SandboxChanged Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Container image &#34;pachyderm/kube-event-tail:v0.0.7&#34; already present on machine 21 minutes ago Created Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Created container kube-event-tail 21 minutes ago Started Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Started container kube-event-tail 21 minutes ago SandboxChanged Pod/pachyderm-loki-0 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-loki-0 Container image &#34;grafana/loki:2.6.1&#34; already present on machine 21 minutes ago Created Pod/pachyderm-loki-0 Created container loki 21 minutes ago Started Pod/pachyderm-loki-0 Started container loki 20 minutes ago Warning Unhealthy Pod/pachyderm-loki-0 Readiness probe failed: HTTP probe failed with statuscode: 503 20 minutes ago Warning Unhealthy Pod/pachyderm-loki-0 Liveness probe failed: HTTP probe failed with statuscode: 503 21 minutes ago SandboxChanged Pod/pachyderm-promtail-b8plv Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-promtail-b8plv Container image &#34;docker.io/grafana/promtail:2.6.1&#34; already present on machine 21 minutes ago Created Pod/pachyderm-promtail-b8plv Created container promtail 21 minutes ago Started Pod/pachyderm-promtail-b8plv Started container promtail 21 minutes ago SandboxChanged Pod/pachyderm-proxy-7d757c85bb-zp5ht Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-proxy-7d757c85bb-zp5ht Container image &#34;envoyproxy/envoy-distroless:v1.24.1&#34; already present on machine 21 minutes ago Created Pod/pachyderm-proxy-7d757c85bb-zp5ht Created container envoy 21 minutes ago Started Pod/pachyderm-proxy-7d757c85bb-zp5ht Started container envoy 21 minutes ago Warning Unhealthy Pod/pachyderm-proxy-7d757c85bb-zp5ht Readiness probe failed: HTTP probe failed with statuscode: 503 21 minutes ago SandboxChanged Pod/pg-bouncer-746bb45867-hgd57 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pg-bouncer-746bb45867-hgd57 Container image &#34;pachyderm/pgbouncer:1.16.2&#34; already present on machine 21 minutes ago Created Pod/pg-bouncer-746bb45867-hgd57 Created container pg-bouncer 21 minutes ago Started Pod/pg-bouncer-746bb45867-hgd57 Started container pg-bouncer 20 minutes ago Warning Unhealthy Pod/pg-bouncer-746bb45867-hgd57 Liveness probe failed: psql: error: FATAL: pgbouncer cannot connect to server 21 minutes ago SandboxChanged Pod/pipeline-joins-inner-join-v1-pfrnh Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pipeline-joins-inner-join-v1-pfrnh Container image &#34;pachyderm/worker:2.5.0-alpha.4&#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-inner-join-v1-pfrnh Created container init 21 minutes ago Started Pod/pipeline-joins-inner-join-v1-pfrnh Started container init 21 minutes ago Pulled Pod/pipeline-joins-inner-join-v1-pfrnh Container image &#34;pachyderm/example-joins-inner-outer:2.1.0&#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-inner-join-v1-pfrnh Created container user 21 minutes ago Started Pod/pipeline-joins-inner-join-v1-pfrnh Started container user 20 minutes ago Pulled Pod/pipeline-joins-inner-join-v1-pfrnh Container image &#34;pachyderm/pachd:2.5.0-alpha.4&#34; already present on machine 20 minutes ago Created Pod/pipeline-joins-inner-join-v1-pfrnh Created container storage 20 minutes ago Started Pod/pipeline-joins-inner-join-v1-pfrnh Started container storage 20 minutes ago Warning BackOff Pod/pipeline-joins-inner-join-v1-pfrnh Back-off restarting failed container 19 minutes ago Killing Pod/pipeline-joins-inner-join-v1-pfrnh Stopping container user 19 minutes ago Killing Pod/pipeline-joins-inner-join-v1-pfrnh Stopping container storage 21 minutes ago SandboxChanged Pod/pipeline-joins-reduce-inner-v1-jjx6w Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pipeline-joins-reduce-inner-v1-jjx6w Container image &#34;pachyderm/worker:2.5.0-alpha.4&#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-reduce-inner-v1-jjx6w Created container init 21 minutes ago Started Pod/pipeline-joins-reduce-inner-v1-jjx6w Started container init 21 minutes ago Pulled Pod/pipeline-joins-reduce-inner-v1-jjx6w Container image &#34;ubuntu:20.04&#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-reduce-inner-v1-jjx6w Created container user 21 minutes ago Started Pod/pipeline-joins-reduce-inner-v1-jjx6w Started container user 20 minutes ago Pulled Pod/pipeline-joins-reduce-inner-v1-jjx6w Container image &#34;pachyderm/pachd:2.5.0-alpha.4&#34; already present on machine 20 minutes ago Created Pod/pipeline-joins-reduce-inner-v1-jjx6w Created container storage LAST SEEN TYPE REASON OBJECT MESSAGE 20 minutes ago Started Pod/pipeline-joins-reduce-inner-v1-jjx6w Started container storage 20 minutes ago Warning BackOff Pod/pipeline-joins-reduce-inner-v1-jjx6w Back-off restarting failed container 19 minutes ago Killing Pod/pipeline-joins-reduce-inner-v1-jjx6w Stopping container user 19 minutes ago Killing Pod/pipeline-joins-reduce-inner-v1-jjx6w Stopping container storage 21 minutes ago SandboxChanged Pod/postgres-0 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/postgres-0 Container image &#34;docker.io/pachyderm/postgresql:13.3.0&#34; already present on machine 21 minutes ago Created Pod/postgres-0 Created container postgres 21 minutes ago Started Pod/postgres-0 Started container postgres Key Events # Event Description CrashLoopBackOff A container in a pod keeps crashing and restarting. This typically indicates that there is an issue with the container that needs to be resolved. FailedScheduling The Kubernetes scheduler is unable to schedule a pod on any node. This typically indicates that there are insufficient resources available in the cluster or that there are constraints set that prevent the pod from being scheduled. OutOfMemory A container in a pod ran out of memory. This typically indicates that the container needs to be reconfigured with more memory or that there is an issue with the application running in the container that is causing it to consume too much memory. FailedCreatePodSandBox The Kubernetes API server failed to create a sandbox for a pod. This typically indicates that there is an issue with the node or the network that is preventing the creation of the sandbox. Evicted A pod is evicted from a node due to resource constraints or other reasons. This typically indicates that the node is running low on resources or that there is an issue with the pod that needs to be resolved. NodeNotReady A node is not ready to accept pods. This can occur due to various reasons such as network connectivity issues or insufficient resources. ImagePullBackOff An image pull operation fails. The container runtime is unable to pull the image from the specified registry or repository. FailedMount A mount operation failed, such as when a volume or configMap failed to mount. This can occur due to incorrect configurations, insufficient permissions, or a missing dependency. ‚ÑπÔ∏è These events are just a few of the many events that can occur in Kubernetes. It&rsquo;s important to monitor your cluster for these and other events to ensure the health and stability of your applications.
"
62,Developer Workflow,"In general, the developer workflow for Pachyderm involves adding data to versioned data repositories, creating pipelines to read from those repositories, executing the pipeline&rsquo;s code, and writing the pipeline&rsquo;s output to other data repositories. Both the data and pipeline can be iterated on independently with Pachyderm handling the code execution according to the pipeline specfication. The workflow steps are shown below.
Data Workflow # Adding data to Pachyderm is the first step towards building data-driven pipelines. There are multiple ways to add data to a Pachyderm repository:
By using the pachctl put file command By using a special type of pipeline, such as a spout or cron By using one of the Pachyderm&rsquo;s language clients By using a compatible S3 client For more information, see Load Your Data Into Pachyderm.
Pipeline Workflow # The fundamental concepts of Pachyderm are very powerful, but the manual build steps mentioned in the pipeline workflow can become cumbersome during rapid-iteration development cycles. We&rsquo;ve created a few helpful developer workflows and tools to automate steps that are error-prone or repetitive:
The push images flag or --push-images is a optional flag that can be passed to the create or update pipeline command. This option is most useful when you need to customize your Docker image or are iterating on the Docker image and code together, since it tags and pushes the image before updating the pipeline. CI/CD Integration provides a way to incorporate Pachyderm functions into the CI process. This is most useful when working with a complex project or for code collaboration. "
63,CI/CD Integration,"Pachyderm is a powerful system for providing data provenance and scalable processing to data scientists and engineers. You can make it even more powerful by integrating it with your existing continuous integration and continuous deployment (CI/CD) workflows and systems. If you are just starting to use Pachyderm and not setting up automation for your Pachyderm build processes, see Working with Pipelines.
The following diagram demonstrates automated Pachyderm development workflow with CI:
Although initial CI setup might require extra effort on your side, in the long run, it brings significant benefits to your team, including the following:
Simplified workflow for data scientists. Data scientists do not need to be aware of the complexity of the underlying containerized infrastructure. They can follow an established Git process, and the CI platform takes care of the Docker build and push process behind the scenes.
Your CI platform can run additional unit tests against the submitted code before creating the build.
Flexibility in tagging Docker images, such as specifying a custom name and tag or using the commit SHA for tagging.
CI Workflow # The CI workflow includes the following steps:
A new commit triggers a Git hook.
Typically, Pachyderm users store the following artifacts in a Git repository:
A Dockerfile that you use to build local images. A pipeline.json specification file that you can use in a Makefile to create local builds, as well as in the CI/CD workflows. The code that performs data transformations. A commit hook in Git for your repository triggers the CI/CD process. It uses the information in your pipeline specification for subsequent steps.
Build an image.
Your CI process automatically starts the build of a Docker container image based on your code and the Dockerfile.
Push the image tagged with commit ID to an image registry.
Your CI process pushes a Docker image created in Step 2 to your preferred image registry. When a data scientist submits their code to Git, a CI process uses the Dockerfile in the repository to build, tag with a Git commit SHA, and push the container to your image registry.
Update the pipeline spec with the tagged image.
In this step, your CI/CD infrastructure uses your updated pipeline.json specification and fills in the Git commit SHA for the version of the image that must be used in this pipeline. Then, it runs the pachctl update pipeline command to push the updated pipeline specification to Pachyderm. After that, Pachyderm pulls a new image from the registry automatically. When the production pipeline is updated with the pipeline.json file that has the correct image tag in it, Pachyderm restarts all pods for this pipeline with the new image automatically.
GitHub Actions # GitHub actions are a convenient way to kick off workflows and perform integration. These can be used to:
Manually trigger a pipeline build, or Automatically build a pipeline from a commit or pull request. In our example, we show how to use the Pachyderm GitHub Action to incorporate Pachyderm functions to run on a Pull Request or at other points during development.
"
64,Create a Machine Learning Workflow,"Because Pachyderm is a language and framework agnostic and platform, and because it easily distributes analysis over large data sets, data scientists can use any tooling for creating machine learning workflows. Even if that tooling is not familiar to the rest of an engineering organization, data scientists can autonomously develop and deploy scalable solutions by using containers. Moreover, Pachyderm‚Äôs pipeline logic paired with data versioning make any results reproducible for debugging purposes or during the development of improvements to a model.
For maximum leverage of Pachyderm&rsquo;s built functionality, Pachyderm recommends that you combine model training processes, persisted models, and model utilization processes, such as making inferences or generating results, into a single Pachyderm pipeline Directed Acyclic Graph (DAG).
Such a pipeline enables you to achieve the following goals:
Keep a rigorous historical record of which models were used on what data to produce which results. Automatically update online ML models when training data or parameterization changes. Easily revert to other versions of an ML model when a new model does not produce an expected result or when bad data is introduced into a training data set. The following diagram demonstrates an ML pipeline:
You can update the training dataset at any time to automatically train a new persisted model. Also, you can use any language or framework, including Apache Spark‚Ñ¢, Tensorflow‚Ñ¢, scikit-learn‚Ñ¢, or other, and output any format of persisted model, such as pickle, XML, POJO, or other. Regardless of the framework, Pachyderm versions the model so that you can track the data that was used to train each model.
Pachyderm processes new data coming into the input repository with the updated model. Also, you can recompute old predictions with the updated model, or test new models on previously input and versioned data. This feature enables you to avoid manual updates to historical results or swapping ML models in production.
For examples of ML workflows in Pachyderm see Machine Learning Examples.
"
65,The Push Images Flag,"The --push-images flag is one way to improve development speed when working with pipelines.
The --push-images flag performs the following steps after you have built your image:
In your local registry, generates a unique tag for the image named after the transform.image field of your pipeline spec. üí° You must build your image with your username as a prefix (example: `pachyderm/example-joins-inner-outer`) - This name must match the one declared in the `transform.image` field of your pipeline spec. Pushes the Docker image, with the tag, to your registry Updates the image tag in the pipeline spec json (on the fly) to match the new image Submits the updated pipeline to the Pachyderm cluster The usage of the flag is shown below:
pachctl update pipeline -f &lt;pipeline name&gt; --push-images --registry &lt;registry&gt; --username &lt;registry user&gt; ‚ÑπÔ∏è For more details on the --push-images flag, see Update a Pipeline.
"
66,Working with Pipelines,"A typical Pachyderm workflow involves multiple iterations of experimenting with your code and pipeline specs.
üìñ Before you read this section, make sure that you understand basic Pachyderm pipeline concepts described in Concepts.
In general, there are five steps to working with a pipeline. The stages can be summarized in the image below.
We will walk through each of the stages in detail.
Step 1: Write Your Analysis Code # Because Pachyderm is completely language-agnostic, the code that is used to process data in Pachyderm can be written in any language and can use any libraries of choice. Whether your code is as simple as a bash command or as complicated as a TensorFlow neural network, it needs to be built with all the required dependencies into a container that can run anywhere, including inside of Pachyderm. See Examples.
Your code does not have to import any special Pachyderm functionality or libraries. However, it must meet the following requirements:
Read files from a local file system. Pachyderm automatically mounts each input data repository as /pfs/&lt;repo_name&gt; in the running containers of your Docker image. Therefore, the code that you write needs to read input data from this directory, similar to any other file system.
Because Pachyderm automatically spreads data across parallel containers, your analysis code does not have to deal with data sharding or parallelization. For example, if you have four containers that run your Python code, Pachyderm automatically supplies 1/4 of the input data to /pfs/&lt;repo_name&gt; in each running container. These workload balancing settings can be adjusted as needed through Pachyderm tunable parameters in the pipeline specification.
Write files into a local file system, such as saving results. Your code must write to the /pfs/out directory that Pachyderm mounts in all of your running containers. Similar to reading data, your code does not have to manage parallelization or sharding.
Step 2: Build Your Docker Image # When you create a Pachyderm pipeline, you need to specify a Docker image that includes the code or binary that you want to run. Therefore, every time you modify your code, you need to build a new Docker image, push it to your image registry, and update the image tag in the pipeline spec. This section describes one way of building Docker images, but if you have your own routine, feel free to apply it.
To build an image, you need to create a Dockerfile. However, do not use the CMD field in your Dockerfile to specify the commands that you want to run. Instead, you add them in the cmd field in your pipeline specification. Pachyderm runs these commands inside the container during the job execution rather than relying on Docker to run them. The reason is that Pachyderm cannot execute your code immediately when your container starts, so it runs a shim process in your container instead, and then, it calls your pipeline specification&rsquo;s cmd from there.
‚ÑπÔ∏è The Dockerfile example below is provided for your reference only. Your Dockerfile might look completely different.
To build a Docker image, complete the following steps:
If you do not have a registry, create one with a preferred provider. If you decide to use DockerHub, follow the Docker Hub Quickstart to create a repository for your project.
Create a Dockerfile for your project. See the OpenCV example.
Build a new image from the Dockerfile by specifying a tag:
docker build -t &lt;image&gt;:&lt;tag&gt; . For more information about building Docker images, see Docker documentation.
Step 3: Push Your Docker Image to a Registry # Once your image is built and tagged, you need to upload the image into a public or private image registry, such as DockerHub.
Alternatively, you can use the Pachyderm&rsquo;s built-in functionality to tag, and push images by running the pachctl update pipeline command with the --push-images flag. For more information, see Update a pipeline.
Log in to an image registry.
If you use DockerHub, run:
docker login --username=&lt;dockerhub-username&gt; --password=&lt;dockerhub-password&gt; &lt;dockerhub-fqdn&gt; Push your image to your image registry.
If you use DockerHub, run:
docker push &lt;image&gt;:tag ‚ÑπÔ∏è Pipelines require a unique tag to ensure the appropriate image is pulled. If a floating tag, such as latest, is used, the Kubernetes cluster may become out of sync with the Docker registry, concluding it already has the latest image.
Step 4: Create/Edit the Pipeline Config # Pachyderm&rsquo;s pipeline specification files store the configuration information about the Docker image and code that Pachyderm should run, the input repo(s) of the pipeline, parallelism settings, GPU usage etc&hellip; Pipeline specifications are stored in JSON or YAML format.
A standard pipeline specification must include the following parameters:
name transform input ‚ÑπÔ∏è Some special types of pipelines, such as a spout pipeline, do not require you to specify all of these parameters. Spout pipelines, for example, do not have input repos.
Check our reference pipeline specification page, for a list of all available fields in a pipeline specification file.
You can store your pipeline specifications locally or in a remote location, such as a GitHub repository.
A simple pipeline specification file in JSON would look like the example below. The pipeline takes its data from the input repo data, runs worker containers with the defined image &lt;image&gt;:&lt;tag&gt; and command, then outputs the resulting processed data in the my-pipeline output repo. During a job execution, each worker sees and reads from the local file system /pfs/data containing only matched data from the glob expression, and writes its output to /pfs/out with standard file system functions; Pachyderm handles the rest.
# my-pipeline.json { &#34;pipeline&#34;: { &#34;name&#34;: &#34;my-pipeline&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;&lt;image&gt;:&lt;tag&gt;&#34;, &#34;cmd&#34;: [&#34;command&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } } } Step 5: Deploy/Update the Pipeline # As soon as you create a pipeline, Pachyderm spins up one or more Kubernetes pods in which the pipeline code runs. By default, after the pipeline finishes running, the pods continue to run while waiting for the new data to be committed into the Pachyderm input repository. You can configure this parameter, as well as many others, in the pipeline specification.
Create a Pachyderm pipeline from the spec:
pachctl create pipeline -f my-pipeline.json You can specify a local file or a file stored in a remote location, such as a GitHub repository. For example, https://raw.githubusercontent.com/pachyderm/pachyderm/2.5.x/examples/opencv/edges.json.
If your pipeline specification changes, you can update the pipeline by running
pachctl update pipeline -f my-pipeline.json ‚ÑπÔ∏è &ldquo;See Also:&rdquo; - Updating Pipelines - Advanced users, parameterize your pipeline specifications with Jsonnet pipeline specification files.
"
67,JupyterLab Mount Extension,"Use the JupyterLab extension to:
Connect your Notebook to a Pachyderm cluster Browse, explore, and analyze data stored in Pachyderm directly from your Notebook Run and test out your pipeline code before creating a Docker image Install the Extension # There are two main ways to install the Jupyter Lab extension:
‚≠ê Via Docker: Fastest implementation! üß™ Locally: Great for development and testing Examples # Make sure to check our data science notebook examples running on Pachyderm, from a market sentiment NLP implementation using a FinBERT model to pipelines training a regression model on the Boston Housing Dataset. You will also find integration examples with open-source products, such as labeling or model serving applications.
"
68,Docker Installation Guide," Install to Existing Docker Image # You can choose between Pachyderm&rsquo;s pre-built image (a custom version of jupyter/scipy-notebook) or add the extension to your own image. Pachyderm&rsquo;s image includes:
The extension jupyterlab-pachyderm FUSE A pre-created /pfs directory that mounts to and grants ownership to the JupyterLab User A mount-server binary Option 1: Pre-Built Image # Open your terminal. Run the following: docker run -it -p 8888:8888 -e GRANT_SUDO=yes --user root --device /dev/fuse --privileged --entrypoint /opt/conda/bin/jupyter pachyderm/notebooks-user:v2.5.1 lab --allow-root Open the UI using the link provided in the terminal following: [I 2023-01-26 19:07:00.245 ServerApp] Jupyter Server 1.16.0 is running at: [I 2023-01-26 19:07:00.245 ServerApp] http://fb66b212ca13:8888/lab?token=013dbb47fc32c0f1ec8277a399e8ccf0e4eb87055942a21d [I 2023-01-26 19:07:00.245 ServerApp] or http://127.0.0.1:8888/lab?token=013dbb47fc32c0f1ec8277a399e8ccf0e4eb87055942a21d Navigate to the connection tab. You will need to provide a link formatted like the following: grpc://&lt;cluster-ip&gt;:&lt;port&gt; 5. Open another terminal and run the following to get the IP address and port number:
kubectl get services | grep -w &#34;pachd &#34; Find the servic/pachd line item and copy the IP address and first port number. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT pachd ClusterIP 10.106.225.116 &lt;none&gt; 30650/TCP,30657/TCP,30658/TCP,30600/TCP,30656/TCP Input the full connection URL (grpc://10.106.225.116:30650). Navigate to the Launcher view in Jupyter and select Terminal. Input the following command: pachctl version If you see a pachctl and pachd version, you are good to go. Option 2: Custom Dockerfile # Replace the following ${PACHCTL_VERSION} with the version of pachctl that matches your cluster&rsquo;s, and update &lt;version&gt; with the release number of the extension.
You can find the latest available version of our Pachyderm Mount Extension in PyPi.
# This runs the following section as root; if adding to an existing Dockerfile, set the user back to whatever you need. USER root # This is the directory files will be mounted to, mirroring how pipelines are run. RUN mkdir -p /pfs # If you are not using &#34;jovyan&#34; as your notebook user, replace the user here. RUN chown $NB_USER /pfs # Fuse is a requirement for the mount extension RUN apt-get clean &amp;&amp; RUN apt-get update &amp;&amp; apt-get -y install curl fuse # Install the mount-server binary RUN curl -f -o mount-server.deb -L https://github.com/pachyderm/pachyderm/releases/download/v${PACHCTL_VERSION}/mount-server_${PACHCTL_VERSION}_amd64.deb RUN dpkg -i mount-server.deb # Optionally Install Pachctl - Set the version of Pachctl that matches your cluster deployment. RUN curl -f -o pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v${PACHCTL_VERSION}/pachctl_${PACHCTL_VERSION}_amd64.deb RUN dpkg -i pachctl.deb # This sets the user back to the notebook user account (i.e., Jovyan) USER $NB_UID # Replace the version here with the version of the extension you would like to install from https://pypi.org/project/jupyterlab-pachyderm/ RUN pip install jupyterlab-pachyderm==&lt;version&gt; Then, build, tag, and push your image.
"
69,Local Installation Guide," Before You Start # You must have a Pachyderm cluster running. Install Jupyter Lab (pip install jupyterlab) Install FUSE ‚ö†Ô∏è Local installation of FUSE requires a reboot to access your Startup Security Utility and enable kernel extensions (kexts) after you have downloaded all of the necessary pre-requisites.
Install jupyterlab pachyderm (pip install jupyterlab-pachyderm) Download mount-server binary Local Installation Steps # Open your terminal. Navigate to your downloads folder. Copy the mount-server binary you downloaded from the pre-requisites into a folder included within your $PATH so that your jupyterlab-pachyderm extension can find it: sudo cp mount-server /usr/local/bin Open your zshrc profile: vim ~/.zshrc Create a /pfs directory to mount your data to. This is the default directory used; alternatively, you can define an empty output folder that PFS should mount by adding export PFS_MOUNT_DIR=/&lt;directory&gt;/&lt;path&gt; to your bash/zshrc profile. Update the source by restarting your computer or executing the following command: source ~/.zshrc Run jupyter lab. If you have an existing pachyderm config file at ~/.pachyderm/config.json, the extension automatically connects to the active context. Otherwise, you must enter the cluster address manually in the extension UI.
"
70,User Guide," Select a Project # You can filter mountable repositories by selecting a project.
Open the JupyterLab UI. Navigate to the Project dropdown. Select an existing project or the default project. Create a Repo &amp; Repo Branch # Open the JupyterLab UI.
Open a Terminal from the launcher.
Input the following:
pachctl create repo demo pachctl create branch demo@master Check the Unmounted Repositories section.
üí° Your repo is created within the project set to your current context.
Mount a Repo Branch # Open the JupyterLab UI. Navigate to the Unmounted Repositories section. Scroll to a repository&rsquo;s row. Select Mount. Mount (and Test) a Datum # You can mount to a specific datum in your repository from the JupyterLab UI using an input spec. This is useful when:
Working on data that is deeply nested within a specific directory of your repository. Testing and exploring viable glob patterns to use for your datums. Open the JupyterLab UI.
Mount to a repo from the Unmounted Repositories section. (e.g., mounting to demo would look like /pfs/demo/ in the file browser).
Navigate to the Mounted Repositories section and select Datum.
You should see the following:
pfs: repo: demo branch: master glob: / Update the glob pattern to match the datums you wish to focus on.
Directory Example # pfs: repo: demo branch: master glob: /images/2022/* Extension Example # pfs: repo: demo branch: master glob: /images/**.png Select Mount Datums.
The file browser updates to display the matching datums.
When you return to the mounted view by selecting Back, the file browser will return to displaying datums that match your default glob pattern.
Explore Directories &amp; Files # At the bottom of the Mounted Repositories tab, you&rsquo;ll find the file browser.
Mounted repositories are nested within the root /pfs (Pachyderm&rsquo;s File System) These repositories are read-only Mounted repositories have a / glob pattern applied to their directories and files Files only downloaded locally when you access them (saving you time) Using the previous example, while the Demo repository is mounted, you can select the demo folder to reveal the example myfile.txt.
"
71,Troubleshooting,"In general, restarting your server should resolve most JupyterLab Mount Extension issues. To restart your server, run the following command from the terminal window in Jupyterlab:
pkill -f &#34;mount-server&#34; The server restarts by itself.
Known Issues # M1 Users With Docker Desktop &lt; 4.6 # A documented issue between qemu and Docker Desktop prevents you from running our pre-built Mount Extension Image in Docker Desktop.
We recommend the following:
Use Podman (See installation instructions) brew install podman podman machine init --disk-size 50 podman machine start podman machine ssh sudo rpm-ostree install qemu-user-static &amp;&amp; sudo systemctl reboot THEN then replace the keyword docker with podman in all the commands above.
Or make sure that your qemu version is &gt; 6.2 "
72,Create a Pipeline,"To create a pipeline, you need to define a pipeline specification in YAML, JSON, or Jsonnet.
Before You Start # A basic pipeline must have all of the following:
pipeline.name: The name of your pipeline. transform.cmd: The command that executes your user code. transform.img: The image that contains your user code. input.pfs.repo: The output repository for the transformed data. input.pfs.glob: The glob pattern used to identify the shape of datums. How to Create a Pipeline # Via Local File # Define a pipeline specification in YAML, JSON, or Jsonnet.
Pass the pipeline configuration to Pachyderm:
pachctl create pipeline -f &lt;pipeline_spec&gt; Via URL # Find a pipeline specification hosted in a public or internal repository. Pass the pipeline configuration to Pachyderm: pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/2.5.x/examples/opencv/edges.json Via Jsonnet # Jsonnet Pipeline specs let you create pipelines while passing a set of parameters dynamically, allowing you to reuse the baseline of a given pipeline while changing the values of chosen fields. You can, for example, create multiple pipelines out of the same jsonnet pipeline spec file while pointing each of them at different input repositories, parameterize a command line in the transform field of your pipelines, or dynamically pass various docker images to train different models on the same dataset.
For illustration purposes, in the following example, we are creating a pipeline named edges-1 and pointing its input repository at the repo &lsquo;images&rsquo;:
pachctl create pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg src=images üìñ You can define multiple pipeline specifications in one file by separating the specs with the following separator: ---. This works in both JSON and YAML files.
Examples # JSON # { &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;description&#34;: &#34;A pipeline that performs image edge detection by using the OpenCV library.&#34;, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/edges.py&#34; ], &#34;image&#34;: &#34;pachyderm/opencv&#34; }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;images&#34;, &#34;glob&#34;: &#34;/*&#34; } } } YAML # pipeline: name: edges description: A pipeline that performs image edge detection by using the OpenCV library. transform: cmd: - python3 - &#34;/edges.py&#34; image: pachyderm/opencv input: pfs: repo: images glob: &#34;/*&#34; Considerations # When you create a pipeline, Pachyderm automatically creates an eponymous output repository. However, if such a repo already exists, your pipeline will take over the master branch. The files that were stored in the repo before will still be in the HEAD of the branch. "
73,Delete a Pipeline,"You can delete a pipeline by running:
pachctl delete pipeline &lt;pipeline_name&gt; To delete all of your pipelines (be careful with this feature), use the additional --all flag.
When you delete a pipeline:
Kubernetes deletes all resources associated with the pipeline - pods (if any), services, and replication controllers. Pachyderm deletes the user output repository with all its data as well as the system meta (stats) and spec (historical versions of the pipeline specification file) repositories. Check our Repositories concept page for more details on repositories types. ‚ÑπÔ∏è If you are using Pachyderm authorization features, only authorized users will be able to delete a given pipeline. In particular, they will have to be repoOwner of the output repo of the pipeline (i.e., have created the pipeline) or clusterAdmin.
You can use the --keep-repo flag to preserve the output repo with all its branches. However, important job metadata will still be deleted (including all historical versions of the pipeline specification file). As a result, you will not be able to recreate the deleted pipeline with the same name unless that repo is deleted first.
Example # For example, if a pipeline &ldquo;xyz&rdquo; exists, then there is an output repo &ldquo;xyz&rdquo;. If a user deletes the pipeline with --keep-repo, the output repo &ldquo;xyz&rdquo; will remain, but the pipeline will be gone. If the user tries to create a new pipeline called &ldquo;xyz&rdquo;, it will fail (there is already an output repo with that name). For the pipeline creation to be successful, the user would have to delete repo &ldquo;xyz&rdquo; first.
‚ÑπÔ∏è You can use the output repo of a pipeline deleted with --keep-repo as an input repo and add more data.
When Pachyderm cannot delete a pipeline with the standard command, you might need to enforce deletion using the --force flag. Because this option can break dependent components in your DAG, use this option withextreme caution.
üí° See Also
Update a Pipeline Create a Pipeline "
74,Jsonnet Pipeline Specifications," ‚ö†Ô∏è Jsonnet pipeline specifications is an experimental feature.
Pachyderm pipeline&rsquo;s specification files are intuitive, simple, and language agnostic. They are, however, very static.
A jsonnet pipeline specification file is a thin wrapping layer atop of your JSON file, allowing you to parameterize a pipeline specification file, thus adding a dynamic component to the creation and update of pipelines.
With jsonnet pipeline specs, you can easily reuse the baseline of a given pipeline spec while experimenting with various values of given fields.
Jsonnet Specs # Pachyderm&rsquo;s Jsonnet pipeline specs are written in the open-source templating language jsonnet. Jsonnet wraps the baseline of a JSON file into a function, allowing the injection of parameters to a pipeline specification file.
All jsonnet pipeline specs have a .jsonnet extension.
As an example, check the file edges.jsonnet below. It is a parameterized version of the edges pipeline spec edges.json in the opencv example, used to inject a name modifier and an input repository name into the original pipeline specifications.
Example 1 # In this snippet of edges.jsonnet, the parameter src sits in place of what would have been the value of the field repo, as a placeholder for any parameter that will be passed to the Jsonnet pipeline spec.
input: { pfs: { name: &#34;images&#34;, glob: &#34;/*&#34;, repo: src, } }, See the full edges.jsonnet here:
//// // Template arguments: // // suffix : An arbitrary suffix appended to the name of this pipeline, for // disambiguation when multiple instances are created. // src : the repo from which this pipeline will read the images to which // it applies edge detection. //// function(suffix, src) { pipeline: { name: &#34;edges-&#34;+suffix }, description: &#34;OpenCV edge detection on &#34;+src, input: { pfs: { name: &#34;images&#34;, glob: &#34;/*&#34;, repo: src, } }, transform: { cmd: [ &#34;python3&#34;, &#34;/edges.py&#34; ], image: &#34;pachyderm/opencv:0.0.1&#34; } } Or check our full &ldquo;jsonnet-ed&rdquo; opencv example.
To create or update a pipeline using a jsonnet pipeline specification file:
add the --jsonnet flag to your pipeline create or pipeline update commands, followed by a local path to your jsonnet file or an url. add --arg &lt;parameter-name&gt;=value for each variable. Example 2 # pachctl create pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg src=images The command above will generate a JSON file named edges-1.json then create a pipeline of the same name taking the repository images as its input.
üìñ Read Jsonnet&rsquo;s complete standard library documentation to learn about all the variables types, string manipulation and mathematical functions, or assertions available to you.
At the minimum, your function should always have a parameter that acts as a name modifier. Pachyderm&rsquo;s pipeline names are unique. You can quickly generate several pipelines from the same jsonnet pipeline specification file by adding a prefix or a suffix to its generic name.
üìñ Your .jsonnet file can create multiple pipelines at once as illustrated in our group example.
Use Cases # Using jsonnet pipeline specifications, you could pass different images to the transform section of an otherwise identical JSON specification file to train multiple models on the same dataset, or switch between one input repo holding test data to another holding production data by parameterizing the input repo field.
During the development phase of a pipeline, it can be helpful to pass the tag of an image as a parameter: each re-build of the pipeline&rsquo;s code requires you to increment your tag value; passing it as a parameter will save you the time to update your JSON specifications. You could also consider preparing a library of ready-made jsonnet pipeline specs for data science teams to instantiate, according to their own set of parameters.
We will let you imagine more use cases in which those jsonnet specs can be helpful to you.
"
75,Update a Pipeline,"While working with your data, you often need to modify an existing pipeline with new transformation code or pipeline parameters.
Use the pachctl update pipeline command to make changes to a pipeline, whether you have re-built a docker image after a code change and/or need to update pipeline parameters in the pipeline specification file.
Alternatively, you can update a pipeline using jsonnet pipeline specification files.
After You Changed Your Specification File # Run the pachctl update pipeline command to apply any change to your pipeline specification JSON file, such as change to the parallelism settings, change of an image tag, change of an input repository, etc&hellip;
By default, a pipeline update does not trigger the reprocessing of the data that has already been processed. Instead, it processes only the new data you submit to the input repo. If you want to run the changes in your pipeline against the data in your input repo&rsquo;s HEAD commit, use the --reprocess flag. The updated pipeline will then continue to process new input data only. Previous results remain accessible through the corresponding commit IDs.
To update a pipeline, run the following command after you have updated your pipeline specification JSON file.
pachctl update pipeline -f pipeline.json ‚ÑπÔ∏è Similar to create pipeline, update pipeline with the -f flag can take a URL if your JSON manifest is hosted on GitHub or other remote location.
Using Jsonnet Pipeline Specification Files # Jsonnet pipeline specs allow you to bypass the &ldquo;update-your -specification-file&rdquo; step and apply your changes at once by running:
pachctl update pipeline --jsonnet &lt;your jsonnet pipeline specs path or URL&gt; --arg &lt;param 1&gt;=&lt;value 1&gt; --arg &lt;param 2&gt;=&lt;value 2&gt; Example # pachctl update pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg tag=1.0.2 Update the Code in a Pipeline # To update the code in your pipeline, complete the following steps:
Make the code changes.
Verify that the Docker daemon is running. Depending on your operating system and the Docker distribution that you use, steps for enabling it might vary:
docker ps If you get an error message similar to the following:
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? enable the Docker daemon (see the Docker documentation for your operating system and platform). For example, if you use minikube on macOS, run the following command:
eval $(minikube docker-env) Then build, tag, and push the new image to your image registry and update the pipeline. This step comes in 3 flavors:
If you prefer to use instructions from your image registry # Build, tag, and push a new image as described in your image registry documentation. For example, if you use DockerHub, see Docker Documentation.
Update the transform.image field of your pipeline spec with your new tag.
üí° Make sure to update your tag every time you re-build. Our pull policy is IfNotPresent (Only pull the image if it does not already exist on the node.). Failing to update your tag will result in your pipeline running on a previous version of your code.
Update the pipeline:
pachctl update pipeline -f &lt;pipeline.json&gt; If you chose to use a jsonnet version of your pipeline specs # Pass the tag of your image to your jsonnet specs.
As an example, see the tag parameter in this jsonnet version of opencv&rsquo;s edges pipeline (edges.jsonnet):
//// // Template arguments: // // suffix : An arbitrary suffix appended to the name of this pipeline, for // disambiguation when multiple instances are created. // src : the repo from which this pipeline will read the images to which // it applies edge detection. //// function(suffix, src) { pipeline: { name: &#34;edges-&#34;+suffix }, description: &#34;OpenCV edge detection on &#34;+src, input: { pfs: { name: &#34;images&#34;, glob: &#34;/*&#34;, repo: src, } }, transform: { cmd: [ &#34;python3&#34;, &#34;/edges.py&#34; ], image: &#34;pachyderm/opencv:0.0.1&#34; } } Once your pipeline code is updated and your image is built, tagged, and pushed, update your pipeline using this command line. In this case, there is no need to edit the pipeline specification file to update the value of your new tag. This command will take care of it:
pachctl update pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg tag=1.0.2 If you use Pachyderm commands # Build your new image using docker build (for example, in a makefile: @docker build --platform linux/amd64 -t $(DOCKER_ACCOUNT)/$(CONTAINER_NAME) .). No tag needed, the folllowing --push-images flag will take care of it.
Run the following command:
pachctl update pipeline -f &lt;pipeline name&gt; --push-images --registry &lt;registry&gt; --username &lt;registry user&gt; If you use DockerHub, omit the --registry flag.
Example:
pachctl update pipeline -f edges.json --push-images --username testuser When prompted, type your image registry password:
Example:
Password for docker.io/testuser: Building pachyderm/opencv:f1e0239fce5441c483b09de425f06b40, this may take a while. "
76,Project Operations,"Projects are logical collections of related work (such as repos and pipelines). Each Pachyderm cluster ships with an initial project named default. PachCTL supports all Project operations, such as adding/removing team members, resources, etc. Pachyderm Console can be used to view and access Projects. Pachyderm&rsquo;s integrations with JupyterLab, Seldon, S3 Gateway, and DeterminedAI also support projects.
Benefits of Projects # Logical Organization of DAGs: Similar to a file system, you can organize your work within a Pachyderm instance.
Standardizable: Resources like repos can have the same name if they belong to different projects, making it easier to create and adhere to project templates in a collaborative environment. For example, ProjectA.Repo1 and ProjectB.Repo1.
Multi-team Enablement: With Enterprise Pachyderm, You can grant access to projects based on roles; projects are hidden from users without access by default.
Example # In the following example there are two projects: DOGS and CATS. They have similarly named repositories and pipelines. With Enterprise Pachyderm, you could scope access to each project by user or user group.
graph TD A[(Project DOGS)] --&gt; B[Picture Repo] A[(Project DOGS)] --&gt; C[Text Repo] A[(Project DOGS)] --&gt; D[Audio Repo] B --&gt; X(Cleanup Pipeline A) C --&gt; Y(Cleanup Pipeline B) D --&gt; Z(Cleanup Pipeline C) X -- from output repo --&gt; 1(Grouping Pipeline) Y -- from output repo --&gt; 1(Grouping Pipeline) Z -- from output repo --&gt; 1(Grouping Pipeline) J[(Project CATS)] --&gt; M[Picture Repo] J[(Project CATS)] --&gt; N[Text Repo] J[(Project CATS)] --&gt; O[Audio Repo] M --&gt; P(Cleanup Pipeline A) N --&gt; Q(Cleanup Pipeline B) O --&gt; R(Cleanup Pipeline C) P -- from output repo --&gt; 2(Grouping Pipeline) Q -- from output repo --&gt; 2(Grouping Pipeline) R -- from output repo --&gt; 2(Grouping Pipeline) "
77,Create a Project," Before You Start # Project names should be less than 51 characters long Project names cannot start with special characters and cannot contain periods (.) at all. Regex example: /^[a-zA-Z0-9-_]+$/. How to Create a Project # 1. Create a Project # Tool: Pachctl CLI Console pachctl create project foo This is not yet available.
2. Verify Creation # You can verify that your project has been created by running pachctl list projects or by opening Console (localhost for non-production personal-machine installations) and viewing the home page.
"
78,Set a Project as Current," Before You Start # Pachyderm ships with an initial project named default that is automatically set to your active context. How to Set a Project to Your Current Context # Tool: Pachctl CLI Console In order to begin working on a project other than the default project, you must assign it to a pachCTL context. This enables you to safely add or update resources such as pipelines and repos without affecting other projects.
pachctl config update context --project foo You can check the details of your active context using the following commands:
pachctl config get active-context # returns contextName pachctl config get context &lt;contextName&gt; # { # &#34;source&#34;: &#34;IMPORTED&#34;, # &#34;cluster_name&#34;: &#34;docker-desktop&#34;, # &#34;auth_info&#34;: &#34;docker-desktop&#34;, # &#34;cluster_deployment_id&#34;: &#34;dev&#34;, # &#34;project&#34;: &#34;foo&#34; # } Open the Console UI. Navigate to the top-level Projects view. Scroll to a project you wish to work on. Select View Project. You can now work within the project from Console.
"
79,Add a Project Resource," Tool: Pachctl CLI Console There are two main ways to add a resource to a project, depending on whether or not the project has been set to your current pachyderm context.
Add Resource to Unset Project:
pachctl create repo bar --project foo Add Resource to Set Project:
pachctl create repo bar Open the Console UI. Scroll to the project you wish to work in. Select View Project. Select Create Repo. For more information about Repos.
"
80,Grant Project Access,"Cluster Admins and Project Owners can grant or revoke user access to projects within a cluster (in addition to the individual resources within the project). View roles and permissions available.
How to Grant Project Access to a User # Tool: Pachctl CLI Console pachctl auth set project foo repoReader,repoWriter user:edna Open the Console UI. Navigate to the project you wish to grant user permissions to. Select Edit Project Permissions. Search for and select the user or user&rsquo;s group. Choose which permissions to grant from the dropdown. Select Add. How to Check Project Access for a User # Tool: Pachctl CLI pachctl auth check project foo user:bob "
81,Delete a Project," ‚ö†Ô∏è Do not delete your project before deleting resources inside of it.
How to Delete a Project # In order to fully and safely delete a project, you must delete the resources inside of it first in the following order:
Delete all pipelines in your project. pachctl delete pipeline &lt;pipeline_name&gt; Delete all repos in your project. pachctl delete repo &lt;repo&gt; Delete the project itself. pachctl delete project &lt;project&gt; If the project you removed was set to your currently active context, make sure to assign a new one:
pachctl config update context --project foo "
82,Create S3 Bucket,"Call the create an S3 bucket command on your S3 client to create a branch in a Pachyderm repository. For example, let&rsquo;s create the master branch of the repo foo in project bar.
Tool: AWS S3 CLI MinIO Create a bucket named foo in project bar. aws --endpoint-url http://localhost:30600/ s3 mb s3://master.foo.bar # make_bucket: master.foo.bar verify that the S3 bucket has been created: aws --endpoint-url http://localhost:30600/ s3 ls # 2022-12-7 22:46:08 master.foo.bar Create a bucket named foo in project bar. mc mb local/master.foo.bar # Bucket created successfully `local/master.foo.bar`. Verify that the S3 bucket has been created: mc ls local # [2021-04-26 22:46:08] 0B master.foo.bar/ "
83,Delete an S3 Object,"You can call the delete an S3 Object command on your S3 client to delete a file from a Pachyderm repository. For example, let&rsquo;s delete the file test.csv from the master branch of the foo repo within the bar project.
Tool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 rm s3://master.foo.bar/test.csv # delete: s3://master.foo.bar/test.csv mc rm local/master.foo.bar/test.csv # Removing `local/master.foo.bar/test.csv`. "
84,Delete Empty S3 Bucket,"You can call the delete an empty S3 bucket command on your S3 client to delete a Pachyderm repository. For example, let&rsquo;s delete the the repo foo in project bar.
Tool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 rb s3://master.foo.bar # remove_bucket: master.foo.bar mc rb local/master.foo.bar # Removed `local/master.foo.bar` successfully. "
85,Get an S3 Object,"You can call the get an S3 object command on your S3 client to download a file by specifying the branch.repo.project it lives in. For example, let&rsquo;s get the test.csv file from master.foo.bar.
Tool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 cp s3://master.foo.bar/test.csv . # download: s3://master.foo.bar/test.csv to ./test.csv mc cp local/master.foo.bar/test.csv . # test.csv: 2.56 MiB / 2.56 MiB ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì 100.00% 1.26 MiB/s 2s Versioning # Most operations act on the HEAD of the given branch. However, if your object store library or tool supports versioning, you can get objects in non-HEAD commits by using the commit ID as the S3 object version ID or use the following syntax --bucket &lt;commit&gt;.&lt;branch&gt;.&lt;repo&gt;.&lt;project&gt;
To retrieve the file file.txt in the commit a5984442ce6b4b998879513ff3da17da on the master branch of the repo foo in project bar:
Get Object By: path id aws s3api get-object --bucket a5984442ce6b4b998879513ff3da17da.master.foo.bar --profile gcp-pf --endpoint http://localhost:30600 --key file.txt export.txt aws s3api get-object --bucket master.foo.bar --profile gcp-pf --endpoint http://localhost:30600 --key file.txt --version-id a5984442ce6b4b998879513ff3da17da export.txt { &#34;AcceptRanges&#34;: &#34;bytes&#34;, &#34;LastModified&#34;: &#34;2021-06-03T01:31:36+00:00&#34;, &#34;ContentLength&#34;: 5, &#34;ETag&#34;: &#34;\&#34;b5fdc0b3557bd4de47045f9c69fa8e54102bcecc36f8743ab88df90f727ff899\&#34;&#34;, &#34;VersionId&#34;: &#34;a5984442ce6b4b998879513ff3da17da&#34;, &#34;ContentType&#34;: &#34;text/plain; charset=utf-8&#34;, &#34;Metadata&#34;: {} } "
86,List S3 Buckets,"You can check the list of filesystem objects in your Pachyderm repository by running an S3 client ls command.
Tool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600 s3 ls # 2021-04-26 15:09:50 master.train.myproject # 2021-04-26 14:58:50 master.pre_process.myproject # 2021-04-26 14:58:09 master.split.myproject # 2021-04-26 14:58:09 stats.split.myproject mc ls local # [2021-04-26 15:09:50 PDT] 0B master.train.myproject/ # [2021-04-26 14:58:50 PDT] 0B master.pre_process.myproject/ # [2021-04-26 14:58:09 PDT] 0B master.split.myproject/ # [2021-04-26 14:58:09 PDT] 0B stats.split.myproject/ "
87,List S3 Objects,"You can list the contents of a given Pachyderm repository using the following commands.
Tool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 ls s3://master.raw_data.myproject # 2021-04-26 11:22:23 2685061 github_issues_medium.csv mc ls local/master.raw_data.myproject # [2021-04-26 12:11:37 PDT] 2.6MiB github_issues_medium.csv "
88,Write an S3 Object,"You can write an S3 object to a Pachyderm repo within a project by performing the following commands:
Tool: AWS S3 CLI MinIO Create the object: aws --endpoint-url http://localhost:30600/ s3 cp test.csv s3://master.foo.bar # upload: ./test.csv to s3://master.foo.bar/test.csv Check that the object was added: aws --endpoint-url http://localhost:30600/ s3 ls s3://master.foo.bar/ # 2021-04-26 12:11:37 2685061 github_issues_medium.csv # 2021-04-26 12:11:37 62 test.csv Create the object: mc cp test.csv local/master.foo.bar/test.csv # test.csv: 62 B / 62 B ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì 100.00% 206 B/s 0s Check that the object was added: mc ls local/master.foo.bar # [2021-04-26 12:11:37 PDT] 2.6MiB github_issues_medium.csv # [2021-04-26 12:11:37 PDT] 62B test.csv ‚ÑπÔ∏è Not all the repositories that you see in the results of the ls command are repositories that can be written to. Some of them might be read-only. Note that you should have writing access to the input repo in order to be able to add files to it.
"
89,Deploy & Manage,"This section describes how to deploy Pachyderm in a production environment. Additionally, you will find information about basic Pachyderm operations, including upgrading to minor and major versions, autoscaling&hellip;
Before you start&hellip; The following high-level architecture diagram lays out Pachyderm&rsquo;s main components. It might help you build a quick mental model of Pachyderm. ‚ö†Ô∏è We are now shipping Pachyderm with an embedded proxy allowing your cluster to expose one single port externally. This deployment setup is optional.
If you choose to deploy Pachyderm with a Proxy, check out our new recommended architecture and deployment instructions.
"
90,Deploy,"Pachyderm runs on Kubernetes, is backed by an object store of your choice, and comes with a bundled version of PostgreSQL (metadata storage) by default.
We recommended disabling the bundled PostgreSQL and using a managed database instance (such as RDS, CloudSQL, or PostgreSQL Server) for production environments.
"
91,AWS Deployment,"This article walks you through deploying a Pachyderm cluster on Amazon Elastic Kubernetes Service (EKS).
Architecture Diagram # Before You Start # Before you can deploy Pachyderm on an EKS cluster, verify that you have the following prerequisites installed and configured:
kubectl AWS CLI eksctl aws-iam-authenticator. pachctl 1. Deploy Kubernetes by using eksctl # ‚ö†Ô∏è Pachyderm requires running your cluster on Kubernetes 1.19.0 and above.
Use the eksctl tool to deploy an EKS cluster in your Amazon AWS environment. The eksctl create cluster command creates a virtual private cloud (VPC), a security group, and an IAM role for Kubernetes to create resources. For detailed instructions, see Amazon documentation.
To deploy an EKS cluster, complete the following steps:
Deploy an EKS cluster:
eksctl create cluster --name &lt;name&gt; --version &lt;version&gt; \ --nodegroup-name &lt;name&gt; --node-type &lt;vm-flavor&gt; \ --nodes &lt;number-of-nodes&gt; --nodes-min &lt;min-number-nodes&gt; \ --nodes-max &lt;max-number-nodes&gt; --node-ami auto Example
eksctl create cluster --name pachyderm-cluster --region us-east-2 --profile &lt;your named profile&gt; Verify the deployment:
kubectl get all System Response:
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.100.0.1 &lt;none&gt; 443/TCP 23h Once your Kubernetes cluster is up, and your infrastructure is configured, you are ready to prepare for the installation of Pachyderm. Some of the steps below will require you to keep updating the values.yaml started during the setup of the recommended infrastructure.
‚ÑπÔ∏è Pachyderm recommends securing and managing your secrets in a Secret Manager. Learn about the set up and configuration of your EKS cluster to retrieve the relevant secrets from AWS Secrets Manager then resume the following installation steps.
2. Create an S3 bucket # Create an S3 object store bucket for data # Pachyderm needs an S3 bucket (Object store) to store your data. You can create the bucket by running the following commands:
‚ö†Ô∏è The S3 bucket name must be globally unique across the entire Amazon region.
Set up the following system variables:
BUCKET_NAME ‚Äî A globally unique S3 bucket name. AWS_REGION ‚Äî The AWS region of your Kubernetes cluster. For example, us-west-2 and not us-west-2a. If you are creating an S3 bucket in the us-east-1 region, run the following command:
aws s3api create-bucket --bucket ${BUCKET_NAME} --region ${AWS_REGION} If you are creating an S3 bucket in any region but the us-east-1 region, run the following command:
aws s3api create-bucket --bucket ${BUCKET_NAME} --region ${AWS_REGION} --create-bucket-configuration LocationConstraint=${AWS_REGION} Verify that the S3 bucket was created:
aws s3 ls You now need to give Pachyderm access to your bucket either by:
Adding a policy to your service account IAM Role (Recommended) OR Passing your AWS credentials (account ID and KEY) to your values.yaml when installing üìñ IAM roles provide finer grained user management and security capabilities than access keys. Pachyderm recommends the use of IAM roles for production deployments.
Add An IAM Role And Policy To Your Service Account # Before you can make sure that the containers in your pods have the right permissions to access your S3 bucket, you will need to Create an IAM OIDC provider for your cluster.
Then follow the steps detailled in Create an IAM Role And Policy for your Service Account.
In short, you will:
Retrieve your OpenID Connect provider URL:
Go to the AWS Management console. Select your cluster instance in Amazon EKS. In the Configuration tab of your EKS cluster, find your OpenID Connect provider URL and save it. You will need it when creating your IAM Role. Create an IAM policy that gives access to your bucket:
Create a new Policy from your IAM Console. Select the JSON tab. Copy/Paste the following text in the JSON tab: { &#34;Version&#34;: &#34;2012-10-17&#34;, &#34;Statement&#34;: [ { &#34;Effect&#34;: &#34;Allow&#34;, &#34;Action&#34;: [ &#34;s3:ListBucket&#34; ], &#34;Resource&#34;: [ &#34;arn:aws:s3:::&lt;your-bucket&gt;&#34; ]},{ &#34;Effect&#34;: &#34;Allow&#34;, &#34;Action&#34;: [ &#34;s3:PutObject&#34;, &#34;s3:GetObject&#34;, &#34;s3:DeleteObject&#34; ], &#34;Resource&#34;: [ &#34;arn:aws:s3:::&lt;your-bucket&gt;/*&#34; ]} ] } Replace &lt;your-bucket&gt; with the name of your S3 bucket.
Create an IAM role as a Web Identity using the cluster OIDC procider as the identity provider.
Create a new Role from your IAM Console. Select the Web identity Tab. In the Identity Provider drop down, select the OpenID Connect provider URL of your EKS and sts.amazonaws.com as the Audience. Attach the newly created permission to the Role. Name it. Retrieve the Role arn. You will need it in your values.yaml annotations when deploying Pachyderm. (Optional) Set Up Bucket Encryption # To set up bucket encryption, see Amazon S3 Default Encryption for S3 Buckets.
3. Enable Your Persistent Volumes Creation # etcd and PostgreSQL (metadata storage) each claim the creation of a persistent volume.
Create an IAM OIDC provider for your cluster. Install the Amazon EBS Container Storage Interface (CSI) driver on your cluster. Create a gp3 storage class manifest file (e.g., gp3-storageclass.yaml) kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp3 annotations: storageclass.kubernetes.io/is-default-class: &#34;true&#34; provisioner: kubernetes.io/aws-ebs parameters: type: gp3 fsType: ext4 Set gp3 to your default storage class. kubectl apply -f gp3-storageclass.yaml Verify that it has been set as your default. kubectl get storageclass üí° If you need to mark a StorageClass as non-default, use the following:
kubectl patch storageclass &lt;storageclass-name&gt; -p &#39;{&#34;metadata&#34;: {&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;false&#34;}}}&#39; 4. Create an AWS Managed PostgreSQL Database # By default, Pachyderm runs with a bundled version of PostgreSQL. For production environments, it is strongly recommended that you disable the bundled version and use an RDS PostgreSQL instance.
‚ö†Ô∏è Note that Aurora Serverless PostgreSQL is not supported and will not work.
Create An RDS Instance # üìñ Find the details of all the steps highlighted below in AWS Documentation: &ldquo;Getting Started&rdquo; hands-on tutorial.
In the RDS console, create a database in the region matching your Pachyderm cluster. Choose the PostgreSQL engine. Select a PostgreSQL version &gt;= 13.3. Configure your DB instance as follows: SETTING Recommended value DB instance identifier Fill in with a unique name across all of your DB instances in the current region. Master username Choose your Admin username. Master password Choose your Admin password. DB instance class The standard default should work. You can change the instance type later on to optimize your performances and costs. Storage type and Allocated storage If you select io1, keep the 100 GiB default size. Read more information on Storage for RDS on Amazon&rsquo;s website. Storage autoscaling If your workload is cyclical or unpredictable, enable storage autoscaling to allow RDS to scale up your storage when needed. Standby instance We highly recommend creating a standby instance for production environments. VPC Select the VPC of your Kubernetes cluster. Attention: After a database is created, you can&rsquo;t change its VPC. Read more on VPCs and RDS on Amazon documentation. Subnet group Pick a Subnet group or Create a new one. Read more about DB Subnet Groups on Amazon documentation. Public access Set the Public access to No for production environments. VPC security group Create a new VPC security group and open the postgreSQL port or use an existing one. Password authentication or Password and IAM database authentication Choose one or the other. Database name In the Database options section, enter Pachyderm&rsquo;s Database name (We are using pachyderm in this example.) and click Create database to create your PostgreSQL service. Your instance is running. Warning: If you do not specify a database name, Amazon RDS does not create a database. If you plan to deploy a standalone cluster (i.e., if you do not plan to register your cluster with a separate enterprise server, you must create a second database named dex in your RDS instance for Pachyderm&rsquo;s authentication service. Read more about dex on PostgreSQL in Dex&rsquo;s documentation.
Additionally, create a new user account and grant it full CRUD permissions to both pachyderm and (when applicable) dex databases. Read about managing PostgreSQL users and roles in this blog. Pachyderm will use the same username to connect to pachyderm as well as to dex.
Update your values.yaml # Once your databases have been created, add the following fields to your Helm values:
global: postgresql: postgresqlUsername: &#34;username&#34; postgresqlPassword: &#34;password&#34; # The name of the database should be Pachyderm&#39;s (&#34;pachyderm&#34; in the example above), not &#34;dex&#34; # See also # postgresqlExistingSecretName: &#34;&lt;yoursecretname&gt;&#34; postgresqlDatabase: &#34;databasename&#34; # The postgresql database host to connect to. Defaults to postgres service in subchart postgresqlHost: &#34;RDS CNAME&#34; # The postgresql database port to connect to. Defaults to postgres server in subchart postgresqlPort: &#34;5432&#34; postgresql: # turns off the install of the bundled postgres. # If not using the built in Postgres, you must specify a Postgresql # database server to connect to in global.postgresql enabled: false 5. Deploy Pachyderm # You have set up your infrastructure, created your S3 bucket and an AWS Managed PostgreSQL instance, and granted your cluster access to both: you can now finalize your values.yaml and deploy Pachyderm.
Update Your Values.yaml # ‚ÑπÔ∏è If you have not created a Managed PostgreSQL RDS instance, replace the Postgresql section below with postgresql:enabled: true in your values.yaml. This setup is not recommended in production environments.
Volume Type: gp3 For gp3 EBS Volumes # Check out our example of values.yaml for gp3 or use our minimal example below.
Gp3 + Service account annotations # deployTarget: AMAZON # This uses GP3 which requires the CSI Driver https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html # And a storageclass configured named gp3 etcd: storageClass: gp3 proxy: enabled: true service: type: LoadBalancer pachd: storage: amazon: bucket: your-bucket-name region: us-east-2 serviceAccount: additionalAnnotations: eks.amazonaws.com/role-arn: arn:aws:iam::&lt;ACCOUNT_ID&gt;:role/pachyderm-bucket-access worker: serviceAccount: additionalAnnotations: eks.amazonaws.com/role-arn: arn:aws:iam::&lt;ACCOUNT_ID&gt;:role/pachyderm-bucket-access global: postgresql: postgresqlUsername: &#34;username&#34; postgresqlPassword: &#34;password&#34; # The name of the database should be Pachyderm&#39;s (&#34;pachyderm&#34; in the example above), not &#34;dex&#34; postgresqlDatabase: &#34;databasename&#34; # The postgresql database host to connect to. Defaults to postgres service in subchart postgresqlHost: &#34;RDS CNAME&#34; # The postgresql database port to connect to. Defaults to postgres server in subchart postgresqlPort: &#34;5432&#34; postgresql: # turns off the install of the bundled postgres. # If not using the built in Postgres, you must specify a Postgresql # database server to connect to in global.postgresql enabled: false Gp3 + AWS Credentials # deployTarget: AMAZON # This uses GP3 which requires the CSI Driver https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html # And a storageclass configured named gp3 etcd: storageClass: gp3 proxy: enabled: true service: type: LoadBalancer pachd: storage: amazon: bucket: blah region: us-east-2 # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html id: AKIAIOSFODNN7EXAMPLE # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html secret: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY global: postgresql: postgresqlUsername: &#34;username&#34; postgresqlPassword: &#34;password&#34; # The name of the database should be Pachyderm&#39;s (&#34;pachyderm&#34; in the example above), not &#34;dex&#34; postgresqlDatabase: &#34;databasename&#34; # The postgresql database host to connect to. Defaults to postgres service in subchart postgresqlHost: &#34;RDS CNAME&#34; # The postgresql database port to connect to. Defaults to postgres server in subchart postgresqlPort: &#34;5432&#34; postgresql: # turns off the install of the bundled postgres. # If not using the built in Postgres, you must specify a Postgresql # database server to connect to in global.postgresql enabled: false üí° Retain (ideally in version control) a copy of the Helm values used to deploy your cluster. It might be useful if you need to restore a cluster from a backup.
Deploy Pachyderm On The Kubernetes Cluster # You can now deploy a Pachyderm cluster by running this command:
helm repo add pach https://helm.pachyderm.com helm repo update helm install pachyderm -f values.yaml pach/pachyderm --version &lt;version-of-the-chart&gt; System Response:
NAME: pachd LAST DEPLOYED: Mon Jul 12 18:28:59 2021 NAMESPACE: default STATUS: deployed REVISION: 1 The deployment takes some time. You can run kubectl get pods periodically to check the status of deployment. When Pachyderm is deployed, the command shows all pods as READY:
kubectl wait --for=condition=ready pod -l app=pachd --timeout=5m System Response
pod/pachd-74c5766c4d-ctj82 condition met Note: If you see a few restarts on the pachd nodes, it means that Kubernetes tried to bring up those pods before etcd was ready. Therefore, Kubernetes restarted those pods. You can safely ignore this message.
Finally, make sure that pachctl talks with your cluster.
6. Have &lsquo;pachctl&rsquo; And Your Cluster Communicate # Exposed Publicly?: yes no Retrieve the external IP address of your TCP load balancer or your domain name:
kubectl get services | grep pachd-lb | awk &#39;{print $4}&#39; Update the context of your cluster with their direct url, using the external IP address/domain name above:
pachctl connect grpc://localhost:80 Check that your are using the right context:
pachctl config get active-context Your cluster context name should show up.
Run the following:
# Background this process because it blocks. pachctl port-forward 7. Check That Your Cluster Is Up And Running # ‚ö†Ô∏è If Authentication is activated (When you deploy with an enterprise key already set, for example), you need to run pachct auth login, then authenticate to Pachyderm with your User, before you use pachctl.
pachctl version System Response:
COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
92,Azure,"The following article walks you through deploying a Pachyderm cluster on Microsoft¬Æ Azure¬Æ Kubernetes Service environment (AKS).
Before You Start # Before you can deploy Pachyderm on an AKS cluster, verify that you have the following prerequisites installed and configured:
Azure CLI 2.0.1 or later jq kubectl pachctl 1. Deploy Kubernetes # You can deploy Kubernetes on Azure by following the official Azure Kubernetes Service documentation, use the quickstart walkthrough, or follow the steps in this section.
At a minimum, you will need to specify the parameters below:
Variable Description RESOURCE_GROUP A unique name for the resource group where Pachyderm is deployed. For example, pach-resource-group. LOCATION An Azure availability zone where AKS is available. For example, centralus. NODE_SIZE The size of the Kubernetes virtual machine (VM) instances. To avoid performance issues, Pachyderm recommends that you set this value to at least Standard_DS4_v2 which gives you 8 CPUs, 28 Gib of Memory, 56 Gib SSD.
In any case, use VMs that support premium storage. See Azure VM sizes for details around which sizes support Premium storage. CLUSTER_NAME A unique name for the Pachyderm cluster. For example, pach-aks-cluster. You can choose to follow the guided steps in Azure Service Portal&rsquo;s Kubernetes Services or use Azure CLI.
Log in to Azure:
az login This command opens a browser window. Log in with your Azure credentials. Resources can now be provisioned on the Azure subscription linked to your account.
Create an Azure resource group or retrieve an existing group.
az group create --name ${RESOURCE_GROUP} --location ${LOCATION} Example:
az group create --name test-group --location centralus System Response:
{ &#34;id&#34;: &#34;/subscriptions/6c9f2e1e-0eba-4421-b4cc-172f959ee110/resourceGroups/pach-resource-group&#34;, &#34;location&#34;: &#34;centralus&#34;, &#34;managedBy&#34;: null, &#34;name&#34;: &#34;pach-resource-group&#34;, &#34;properties&#34;: { &#34;provisioningState&#34;: &#34;Succeeded&#34; }, &#34;tags&#34;: null, &#34;type&#34;: null } Create an AKS cluster in the resource group/location:
For more configuration options: Find the list of all available flags of the az aks create command.
az aks create --resource-group ${RESOURCE_GROUP} --name ${CLUSTER_NAME} --node-vm-size ${NODE_SIZE} --node-count &lt;node_pool_count&gt; --location ${LOCATION} Example:
az aks create --resource-group test-group --name test-cluster --generate-ssh-keys --node-vm-size Standard_DS4_v2 --location centralus Confirm the version of the Kubernetes server by running kubectl version.
2. Create an Azure Storage Container For Your Data # Set up the following variables:
STORAGE_ACCOUNT: The name of the storage account where you store your data. CONTAINER_NAME: The name of the Azure blob container where you store your data. Create an Azure storage account:
az storage account create \ --resource-group=&#34;${RESOURCE_GROUP}&#34; \ --location=&#34;${LOCATION}&#34; \ --sku=Premium_LRS \ --name=&#34;${STORAGE_ACCOUNT}&#34; \ --kind=BlockBlobStorage System response:
{ &#34;accessTier&#34;: null, &#34;creationTime&#34;: &#34;2019-06-20T16:05:55.616832+00:00&#34;, &#34;customDomain&#34;: null, &#34;enableAzureFilesAadIntegration&#34;: null, &#34;enableHttpsTrafficOnly&#34;: false, &#34;encryption&#34;: { &#34;keySource&#34;: &#34;Microsoft.Storage&#34;, &#34;keyVaultProperties&#34;: null, &#34;services&#34;: { &#34;blob&#34;: { &#34;enabled&#34;: true, ... Make sure that you set Stock Keeping Unit (SKU) to Premium_LRS and the kind parameter is set to BlockBlobStorage. This configuration results in a storage that uses SSDs rather than standard Hard Disk Drives (HDD). If you set this parameter to an HDD-based storage option, your Pachyderm cluster will be too slow and might malfunction.
Verify that your storage account has been successfully created:
az storage account list Obtain the key for the storage account (STORAGE_ACCOUNT) and the resource group to be used to deploy Pachyderm:
STORAGE_KEY=&#34;$(az storage account keys list \ --account-name=&#34;${STORAGE_ACCOUNT}&#34; \ --resource-group=&#34;${RESOURCE_GROUP}&#34; \ --output=json \ | jq &#39;.[0].value&#39; -r )&#34; ‚ÑπÔ∏è Find the generated key in the Storage accounts &gt; Access keys section in the Azure Portal or by running the following command az storage account keys list --account-name=${STORAGE_ACCOUNT}.
Create a new storage container within your storage account:
az storage container create --name ${CONTAINER_NAME} \ --account-name ${STORAGE_ACCOUNT} \ --account-key &#34;${STORAGE_KEY}&#34; 3. Persistent Volumes Creation # etcd and PostgreSQL (metadata storage) each claim the creation of a pv.
If you plan to deploy Pachyderm with its default bundled PostgreSQL instance, read the warning below and jump to the deployment section:
‚ö†Ô∏è The metadata service (Persistent disk) generally requires a small persistent volume size (i.e. 10GB) but high IOPS (1500), therefore, depending on your disk choice, you may need to oversize the volume significantly to ensure enough IOPS.
If you plan to deploy a managed PostgreSQL instance (Recommended in production), read the following section.
4. Create an Azure Managed PostgreSQL Server Database # By default, Pachyderm runs with a bundled version of PostgreSQL. For production environments, we strongly recommend that you disable the bundled version and use a PostgreSQL Server instance.
Create A PostgreSQL Server Instance # In the Azure console, choose the Azure Database for PostgreSQL servers service. You will be asked to pick your server type: Single Server or Hyperscale (for multi-tenant applications), then configure your DB instance as follows.
SETTING Recommended value subscription and resource group Pick your existing resource group.
Important Your Cluster and your Database must be deployed in the same resource group. server name Name your instance. location Create a database in the region matching your Pachyderm cluster. compute + storage The standard instance size (GP_Gen5_4 = Gen5 VMs with 4 cores) should work. Remember that Pachyderm&rsquo;s metadata services require high IOPS (1500). Oversize the disk accordingly Master username Choose your Admin username. (&ldquo;postgres&rdquo;) Master password Choose your Admin password. You are ready to create your instance.
Example # az postgres server create \ --resource-group &lt;your_resource_group&gt; \ --name &lt;your_server_name&gt; \ --location westus \ --sku-name GP_Gen5_2 \ --admin-user &lt;server_admin_username&gt; \ --admin-password &lt;server_admin_password&gt; \ --ssl-enforcement Disabled \ --version 11 üìñ For detailed steps, see the official Azure documentation.
‚ö†Ô∏è Make sure that your PostgreSQL version is &gt;= 11 Keep the SSL setting Disabled. Once created, go back to your newly created database, and:
Open the access to your instance: ‚ÑπÔ∏è Azure provides two options for pods running on an AKS worker nodes to access a PostgreSQL DB instance, pick what fit you best:
Create a firewall rule on the Azure DB Server with a range of IP addresses that encompasses all IPs of the AKS Cluster nodes (this can be a very large range if using node auto-scaling). Create a VNet Rule on the Azure DB Server that allows access from the subnet the AKS nodes are in. This is used in conjunction with the Microsoft.Sql VNet Service Endpoint enabled on the cluster subnet. You can also choose the more secure option to deny public access to your PostgreSQL instance then Create a private endpoint in the K8s vnet. Read more about how to configure a private link using CLI on Azure&rsquo;s documentation
Alternativelly, in the Connection Security of your newly created server, Allow access to Azure services (This is equivalent to running az postgres server firewall-rule create --server-name &lt;your_server_name&gt; --resource-group &lt;your_resource_group&gt; --name AllowAllAzureIps --start-ip-address 0.0.0.0 --end-ip-address 0.0.0.0).
In the Essentials page of your instance, find the full server name and admin username that will be required in your values.yaml. Create Your Databases # After your instance is created, you will need to create Pachyderm&rsquo;s database(s).
If you plan to deploy a standalone cluster (i.e., if you do not plan to register your cluster with a separate enterprise server, you will need to create a second database named &ldquo;dex&rdquo; in your PostgreSQL Server instance for Pachyderm&rsquo;s authentication service. Note that the database must be named dex. This second database is not needed when your cluster is managed by an enterprise server.
‚ÑπÔ∏è Read more about dex on PostgreSQL in Dex&rsquo;s documentation.
Pachyderm will use the same user to connect to pachyderm as well as to dex.
Update your yaml values # Once your databases have been created, add the following fields to your Helm values:
global: postgresql: postgresqlUsername: &#34;see admin username above&#34; postgresqlPassword: &#34;password&#34; # The server name of the instance postgresqlDatabase: &#34;pachyderm&#34; # The postgresql database host to connect to. postgresqlHost: &#34;see server name above&#34; # The postgresql database port to connect to. Defaults to postgres server in subchart postgresqlPort: &#34;5432&#34; postgresql: # turns off the install of the bundled postgres. # If not using the built in Postgres, you must specify a Postgresql # database server to connect to in global.postgresql enabled: false 5. Deploy Pachyderm # You have set up your infrastructure, created your data container and a Managed PostgreSQL instance, and granted your cluster access to both: you can now finalize your values.yaml and deploy Pachyderm.
Update Your Values.yaml # ‚ÑπÔ∏è If you have not created a Managed PostgreSQL Server instance, replace the Postgresql section below with postgresql:enabled: true in your values.yaml. This setup is not recommended in production environments.
If you have previously tried to run Pachyderm locally, make sure that you are using the right Kubernetes context first.
Verify cluster context:
kubectl config current-context This command should return the name of your Kubernetes cluster that runs on Azure.
If you have a different context displayed, configure kubectl to use your Azure configuration:
az aks get-credentials --resource-group ${RESOURCE_GROUP} --name ${CLUSTER_NAME} System Response:
Merged &#34;${CLUSTER_NAME}&#34; as current context in /Users/test-user/.kube/config Update your values.yaml
Update your values.yaml with your container name (see example of values.yaml here) or use our minimal example below.
deployTarget: &#34;MICROSOFT&#34; pachd: storage: microsoft: # storage container name container: &#34;container_name&#34; # storage account name id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # storage account key secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; externalService: enabled: true global: postgresql: postgresqlUsername: &#34;see admin username above&#34; postgresqlPassword: &#34;password&#34; # The server name of the instance postgresqlDatabase: &#34;pachyderm&#34; # The postgresql database host to connect to. postgresqlHost: &#34;see server name above&#34; # The postgresql database port to connect to. Defaults to postgres server in subchart postgresqlPort: &#34;5432&#34; postgresql: # turns off the install of the bundled postgres. # If not using the built in Postgres, you must specify a Postgresql # database server to connect to in global.postgresql enabled: false Check the list of all available helm values at your disposal in our reference documentation or on Github.
Deploy Pachyderm On The Kubernetes Cluster # Now you can deploy a Pachyderm cluster by running this command:
helm repo add pach https://helm.pachyderm.com helm repo update helm install pachd -f values.yaml pach/pachyderm --version &lt;version-of-the-chart&gt; System Response:
NAME: pachd LAST DEPLOYED: Mon Jul 12 18:28:59 2021 NAMESPACE: default STATUS: deployed REVISION: 1 Refer to our generic Helm documentation for more information on how to select your chart version.
Pachyderm pulls containers from DockerHub. It might take some time before the pachd pods start. You can check the status of the deployment by periodically running kubectl get all.
When pachyderm is up and running, get the information about the pods:
kubectl get pods Once the pods are up, you should see a pod for pachd running (alongside etcd, pg-bouncer, postgres, or console, depending on your installation).
System Response:
NAME READY STATUS RESTARTS AGE pachd-1971105989-mjn61 1/1 Running 0 54m ... Note: Sometimes Kubernetes tries to start pachd nodes before the etcd nodes are ready which might result in the pachd nodes restarting. You can safely ignore those restarts.
Finally, make sure that pachctl talks with your cluster.
6. Have &lsquo;pachctl&rsquo; And Your Cluster Communicate # Assuming your pachd is running as shown above, make sure that pachctl can talk to the cluster.
If you are exposing your cluster publicly:
Retrieve the external IP address of your TCP load balancer or your domain name:
kubectl get services | grep pachd-lb | awk &#39;{print $4}&#39; Update the context of your cluster with their direct url, using the external IP address/domain name above:
pachctl connect grpc://localhost:80 Check that your are using the right context:
pachctl config get active-context Your cluster context name should show up.
If you&rsquo;re not exposing pachd publicly, you can run:
# Background this process because it blocks. pachctl port-forward 7. Check That Your Cluster Is Up And Running # ‚ö†Ô∏è If Authentication is activated (When you deploy with an enterprise key already set, for example), you need to run pachct auth login, then authenticate to Pachyderm with your User, before you use pachctl.
pachctl version System Response:
COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
93,Configure Environment Variables,"You can define environment variables that handle required configuration. In Pachyderm, you can define the following types of environment variables:
pachd variables: Used for your Pachyderm daemon container.
Pachyderm worker variables: Used by the Kubernetes pods that run your pipeline code.
üí° You can reference environment variables in your code. For example, if your code writes data to an external system and you want to know the current job ID, you can use the PACH_JOB_ID environment variable to refer to the current job ID.
pachd Environment Variables # You can find the list of pachd environment variables in the pachd manifest by running the following command:
kubectl get deploy pachd -o yaml The following tables list all the pachd environment variables.
Global Configuration # Environment Variable Default Value Description ETCD_SERVICE_HOST N/A The host on which the etcd service runs. ETCD_SERVICE_PORT N/A The etcd port number. PPS_WORKER_GRPC_PORT 80 The GRPs port number. PORT 650 The pachd port number. HTTP_PORT 652 The HTTP port number. PEER_PORT 653 The port for pachd-to-pachd communication. NAMESPACE deafult The namespace in which Pachyderm is deployed. pachd Configuration # Environment Variable Default Value Description NUM_SHARDS 32 The max number of pachd pods that can run in a single cluster. STORAGE_BACKEND &quot;&quot; The storage backend defined for the Pachyderm cluster. STORAGE_HOST_PATH &quot;&quot; The host path to storage. KUBERNETES_PORT_443_TCP_ADDR none An IP address that Kubernetes exports automatically for your code to communicate with the Kubernetes API. Read access only. Most variables that have use the PORT_ADDRESS_TCP_ADDR pattern are Kubernetes environment variables. For more information,
see Kubernetes environment variables. METRICS true Defines whether anonymous Pachyderm metrics are being collected or not. BLOCK_CACHE_BYTES 1G The size of the block cache in pachd. WORKER_IMAGE &quot;&quot; The base Docker image that is used to run your pipeline. WORKER_SIDECAR_IMAGE &quot;&quot; The pachd image that is used as a worker sidecar. WORKER_IMAGE_PULL_POLICY IfNotPresent The pull policy that defines how Docker images are pulled. You can set a Kubernetes image pull policy as needed. LOG_LEVEL info Verbosity of the log output. If you want to disable logging, set this variable to 0. Viable Options debug info error
For more information, see Go logrus log levels. IAM_ROLE &quot;&quot; The role that defines permissions for Pachyderm in AWS. IMAGE_PULL_SECRET &quot;&quot; The Kubernetes secret for image pull credentials. EXPOSE_OBJECT_API false Controls access to internal Pachyderm API. WORKER_USES_ROOT true Controls root access in the worker container. S3GATEWAY_PORT 600 The S3 gateway port number DISABLE_COMMIT_PROGRESS_COUNTER false A feature flag that disables commit propagation progress counter. If you have a large DAG, setting this parameter to true might help improve etcd performance. You only need to set this parameter on the pachd pod. Pachyderm passes this parameter to worker containers automatically. Storage Configuration # Environment Variable Default Value Description STORAGE_MEMORY_THRESHOLD N/A Defines the storage memory threshold. STORAGE_SHARD_THRESHOLD N/A Defines the storage shard threshold. Pipeline Worker Environment Variables # Pachyderm defines many environment variables for each Pachyderm worker that runs your pipeline code. You can print the list of environment variables into your Pachyderm logs by including the env command into your pipeline specification. For example, if you have an images repository, you can configure your pipeline specification like this:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;env&#34; }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/&#34;, &#34;repo&#34;: &#34;images&#34; } }, &#34;transform&#34;: { &#34;cmd&#34;: [&#34;sh&#34; ], &#34;stdin&#34;: [&#34;env&#34;], &#34;image&#34;: &#34;ubuntu:14.04&#34; } } Run this pipeline and upon completion you can view the log with variables by running the following command:
pachctl logs --pipeline=env PPS_WORKER_IP=172.17.0.7 DASH_PORT_8081_TCP_PROTO=tcp PACHD_PORT_600_TCP_PORT=600 KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 ... You should see a lengthy list of variables. Many of them define internal networking parameters that most probably you will not need to use.
Most users find the following environment variables particularly useful:
Environment Variable Description PACH_JOB_ID The ID of the current job. For example, PACH_JOB_ID=8991d6e811554b2a8eccaff10ebfb341. PACH_DATUM_ID The ID of the current Datum. PACH_DATUM_&lt;input.name&gt;_JOIN_ON Exposes the join_on match to the pipeline&rsquo;s job. PACH_DATUM_&lt;input.name&gt;_GROUP_BY Expose the group_by match to the pipeline&rsquo;s job. PACH_OUTPUT_COMMIT_ID The ID of the commit in the output repo for the current job. For example, PACH_OUTPUT_COMMIT_ID=a974991ad44d4d37ba5cf33b9ff77394. PPS_NAMESPACE The PPS namespace. For example, PPS_NAMESPACE=default. PPS_SPEC_COMMIT The hash of the pipeline specification commit.
This value is tied to the pipeline version. Therefore, jobs that use the same version of the same pipeline have the same spec commit. For example, PPS_SPEC_COMMIT=3596627865b24c4caea9565fcde29e7d. PPS_POD_NAME The name of the pipeline pod. For example, pipeline-env-v1-zbwm2. PPS_PIPELINE_NAME The name of the pipeline that this pod runs. For example, env. PIPELINE_SERVICE_PORT_PROMETHEUS_METRICS The port that you can use to exposed metrics to Prometheus from within your pipeline. The default value is 9090. HOME The path to the home directory. The default value is /root &lt;input-repo&gt;=&lt;path/to/input/repo&gt; The path to the filesystem that is defined in the input in your pipeline specification. Pachyderm defines such a variable for each input. The path is defined by the glob pattern in the spec. For example, if you have an input images and a glob pattern of /, Pachyderm defines the images=/pfs/images variable. If you have a glob pattern of /*, Pachyderm matches the files in the images repository and, therefore, the path is images=/pfs/images/liberty.png. input_COMMIT The ID of the commit that is used for the input. For example, images_COMMIT=fa765b5454e3475f902eadebf83eac34. S3_ENDPOINT A Pachyderm S3 gateway sidecar container endpoint. If you have an S3 enabled pipeline, this parameter specifies a URL that you can use to access the pipeline&rsquo;s repositories state when a particular job was run. The URL has the following format: http://&lt;job-ID&gt;-s3:600. An example of accessing the data by using AWS CLI looks like this: `echo foo_data In addition to these environment variables, Kubernetes injects others for Services that run inside the cluster. These variables enable you to connect to those outside services, which can be powerful but might also result in processing being retried multiple times.
For example, if your code writes a row to a database, that row might be written multiple times because of retries. Interaction with outside services must be idempotent to prevent unexpected behavior. Furthermore, one of the running services that your code can connect to is Pachyderm itself. This is generally not recommended as very little of the Pachyderm API is idempotent, but in some specific cases it can be a viable approach.
"
94,Configure Tracing with Jaeger,"Pachyderm has the ability to trace requests using Jaeger. This can be useful when diagnosing slow clusters.
Collecting Traces # To use tracing in Pachyderm, complete the following steps:
Run Jaeger in Kubernetes
kubectl apply -f https://raw.githubusercontent.com/pachyderm/pachyderm/v2.5.1/etc/deploy/tracing/jaeger-all-in-one.yaml Point Pachyderm at Jaeger
For pachctl, run:
export JAEGER_ENDPOINT=localhost:14268 kubectl port-forward svc/jaeger-collector 14268 &amp; # Collector service For pachd, run:
kubectl delete po -l suite=pachyderm,app=pachd The port-forward command is necessary because pachctl sends traces to Jaeger (it actually initiates every trace), and reads the JAEGER_ENDPOINT environment variable for the address to which it will send the trace info.
Restarting the pachd pod is necessary because pachd also sends trace information to Jaeger, but it reads the environment variables corresponding to the Jaeger service[1] on startup to find Jaeger (the Jaeger service is created by the jaeger-all-in-one.yaml manifest). Killing the pods restarts them, which causes them to connect to Jaeger.
Send Pachyderm a traced request by setting the PACH_TRACE environment variable to &ldquo;true&rdquo; before running any pachctl command (note that JAEGER_ENDPOINT must also be set/exported):
PACH_TRACE=true pachctl list job # for example Pachyderm does not recommend exporting PACH_TRACE because tracing calls can slow them down and make interesting traces hard to find in Jaeger. Therefore, you might want to set this variable for the specific calls you want to trace.
However, Pachyderm&rsquo;s client library reads this variable and implements the relevant tracing, so any binary that uses Pachyderm&rsquo;s go client library can trace calls if these variables are set.
View Traces # To view traces, run:
kubectl port-forward svc/jaeger-query 16686:80 &amp; # UI service Then, connect to localhost:16686 in your browser, and you should see all collected traces.
‚ÑπÔ∏è See Also: Kubernetes Service Environment Variables
Troubleshooting # If you see &lt;trace-without-root-span&gt;, this likely means that pachd has connected to Jaeger, but pachctl has not. Make sure that the JAEGER_ENDPOINT environment variable is set on your local machine, and that kubectl port-forward &quot;po/${jaeger_pod}&quot; 14268 is running.
If you see a trace appear in Jaeger with no subtraces, like so:
This might mean that pachd has not connected to Jaeger, but pachctl has. Restart the pachd pods after creating the Jaeger service in Kubernetes.
"
95,Console,"Pachyderm Console is a complete web UI for visualizing running pipelines and exploring your data. By clicking on individual pipeline segments, users can check their jobs&rsquo; status, visualize their commits&rsquo; content, access logs, and much more! It is a valuable companion when troubleshooting pipelines.
Enterprise Edition # Pachyderm Community Edition comes with Console per default. Upon upgrading to Pachyderm Enterprise, you will be able to:
Benefit from our Authentication/Authorization features and control which users, groups, or roles have access to specific Pachyderm resources. Lift all CE scaling limits. ‚ÑπÔ∏è Request an Enterprise trial token directly from Console CE by hitting the &ldquo;Upgrade to Enterprise&rdquo; button at the bottom right of your Console, fill in this form, or get in touch with us at sales@pachyderm.io.
Console States # Before diving into Console installation steps, please look at Console&rsquo;s various states, from the Community Edition to Enterprise. It should give you a mental model of the various paths to upgrade your Community Edition and what happens when your Enterprise token expires.
"
96,Cloud Deployment," Before You Start # üí° Taking Pachyderm for a test drive? Try our Quick Cloud Installation for non-production deployment testing.
You must have Pachyderm installed following one of these guides: AWS GCP Azure Deploy # Set up your Ingress and DNS and point your browser to: http://&lt;external-IP-address-or-domain-name&gt;:80 or, https://&lt;external-IP-address-or-domain-name&gt;:443 if TLS is enabled Set up your IDP during deployment. ‚ÑπÔ∏è You can use the mock user (username:admin, password: password) to login to Console when authentication is enabled but no Identity provider was wired (Enterprise).
Configure your Identity Provider As Part of Helm: To configure your Identity Provider as a part of helm install, see examples for the oidc.upstreamIDPs value in the helm chart values specification and read our IDP Configuration page for a better understanding of each field. Manually via Values.yaml: You can manually update your values.yaml with oidc.mockIDP = false then set up an Identity Provider by using pachctl. You are all set! You should land on the Projects page of Console.
Enterprise + Helm # When Enterprise is enabled through Helm, Auth is automatically activated. This means that you do not need to run pachctl auth activate; a pachyderm-auth Kubernetes secret is created which contains a rootToken key. Use {{&quot;kubectl get secret pachyderm-auth -o go-template='{{.data.rootToken | base64decode }}'&quot;}} to retrieve it and save it where you see fit.
Considerations # If you run pachctl auth activate, the secret is not updated. Instead, the rootToken is printed in your STDOUT for you to save; the same behavior applies if you activate enterprise manually (pachctl license activate) then activate authentication (pachctl auth activate). You can set the helm value pachd.activateAuth to false to prevent the automatic bootstrap of auth on the cluster. "
97,Local Deployment," Before You Start # üìñ A local installation helps you learn some of the Pachyderm basics and experiment with the product. It is not designed to be a production environment.
You must have Pachyderm installed locally (pachd and pachctl) You must have a local Kubernetes cluster running. Deploy # Open the terminal. Verify your Kubernetes cluster is running via pachctl version. If your cluster is up but the context is stale, run the following: pachctl config import-kube local --overwrite pachctl config set active-context local Connect to Console by running the following command: pachctl port-forward Open your browser and navigate to the localhost console service port number; typically 4000. "
98,"Deploy Pachyderm with TSL (SSL, HTTPS)","Secure internet browser connections and transactions via data encryption by deploying Pachyderm with Transport Layer Security (TLS).
Before You Start # You must have admin control over the domain you wish to use. You must obtain a certificate from a trusted Certificate Authority (CA) such as: Let&rsquo;s Encrypt HashiCorp Vault Venafi The .crt file you are using must contain the full certificate chain (root, intermediates, and leaf). üí° You can simplify this process by using a tool like Cert-Manager, which is a certificate controller for Kubernetes that obtains certificates from Issuers, ensures the certificates are valid, and attempts to renew certificates at a configured time before expiry.
You can install Cert-Manager using the following command:
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.10.1/cert-manager.yaml How to Deploy With TLS # 1. Create a TLS Secret # Open a terminal and navigate to the location of your generated .key and .crt files. Run the following command: kubectl create secret tls &lt;name&gt; --key=tls.key --cert=tls.crt Verify your certificate: kubectl get certificate 2. Enable TLS in Your Helm Chart Values. # Reference the certificate object in your helm chart by setting your TLS secret name in the proper TLS section.
Setup Type: With Proxy Without Proxy proxy: tls: enabled: true secretName: &#34;&lt;the-secret-name-in-your-certificate-resource&gt;&#34; pachd: tls: enabled: true secretName: &#34;&lt;the-secret-name-in-your-certificate-resource&gt;&#34; For the Cert Manager users, the secret name should match the name set in your certificate resource.
Self-Signed &amp; Custom Certificates # When using self signed certificates or custom certificate authority (instead of Lets Encrypt, HashiCorp Vault, or Venafi), you must set global.customCaCerts to true to add Pachyderm&rsquo;s certificate and CA to the list of trusted authorities for console and enterprise.
If you are using a custom ca-signed cert, you must include the full certificate chain in the root.crt file.
3. Connect to Pachyderm via SSL # After you deploy Pachyderm, to connect through pachctl by using a trusted certificate, you will need to set the pachd_address in the Pachyderm context with the cluster IP address that starts with grpcs://. You can do so by running the following command:
Setup Type: With Proxy Without Proxy pachctl connect grpc://localhost:80 pachd: tls: enabled: true secretName: &#34;&lt;the-secret-name-in-your-certificate-resource&gt;&#34; "
99,Enable Logs Aggregation With Loki," Shipping logs to Loki # Loki retrieves logs from pods in Kubernetes through an agent service called Promtail. Promtail runs on each node and sends logs from Kubernetes pods to the Loki API Server, tagging each log entry with information about the pod that produced it.
You need to configure Promtail for your environment to ship logs to your Loki instance. If you are running multiple nodes, then you will need to install and configure Promtail for each node shipping logs to Loki.
Fetching logs # While installing Loki will enable the collection of logs, commands such as pachctl logs will not fetch logs directly from Loki until the LOKI_LOGGING environment variable on the pachd container is true.
This is controlled by the helm value pachd.lokiLogging, which can be set by adding the following to your values.yaml file:
pachd: lokiLogging: true Pachyderm reads logs from the Loki API Server with a particular set of tags. The URI at which Pachyderm reads from the Loki API Server is determined by the LOKI_SERVICE_HOST and LOKI_SERVICE_PORT environment values automatically added by Loki Kubernetes service.
If Loki is deployed after the pachd container, the pachd container will need to be redeployed to receive these connection parameters.
‚ÑπÔ∏è If you are not running Promtail on the node where your Pachyderm pods are located, you will be unable to get logs for pipelines running on that node via pachctl logs -p pipelineName.
Default Loki Bundle # Per default, Pachyderm ships with an embedded version of Loki that can be deployed by adding the lokiDeploy: true next to the existing lokiLogging: true.
pachd: lokiDeploy: true lokiLogging: true In such case, add the following section to your value.yaml:
loki-stack: loki: persistence: enabled: true accessModes: - ReadWriteOnce size: 5Gi storageClassName: standard annotations: {} grafana: enabled: true ‚ÑπÔ∏è Grafana Users:
To use Grafana, deploy with loki-stack.grafana.enabled: true.
To access Grafana, run port-forward with kubectl port-forward svc/pachyderm-grafana 4001:80. Change the port 4001 to what suits you best.
Login to localhost:4001 with the username admin, and the password found with running kubectl get secret pachyderm-grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 -d. If enterprise is activated, you will be able to inspect containers logs in your console.
Using Loki in Another Namespace # Instead of deploying a local loki instance in your pachyderm namespace, you can configure pachyderm to use a loki running in another namespace. To do so, you must set lokiHost and lokiPort. You should also set lokiDeploy: false to prevent the chart from deploying a local loki instance.:
pachd: lokiDeploy: false lokiHost: &#34;&lt;loki-namespace&gt;.&lt;loki-service-name&gt;.svc.cluster.local.&#34; lokiPort: 3100 References # Loki Documentation - https://grafana.com/docs/loki/latest/ Promtail Documentation - https://grafana.com/docs/loki/latest/clients/promtail/ Operating Loki - https://grafana.com/docs/loki/latest/operations/ "
100,Google Cloud Platform,"This article walks you through deploying a Pachyderm cluster on Google Kubernetes Engine (GKE).
Before You Start # Install the following clients:
Google Cloud SDK &gt;= 124.0.0 kubectl pachctl jq Create a new Project or retrieve the ID of an existing Project you want to deploy your cluster on.
Set your gcloud config to automatically select your project:
PROJECT_ID=&lt;your project ID&gt; gcloud config set project ${PROJECT_ID} Enable the GKE API on your project if you have not done so already.
Enable the CloudSQL Admin API to administer your instance.
1. Deploy Kubernetes # ‚ö†Ô∏è Pachyderm recommends running your cluster on Kubernetes 1.19.0 and above.
To create a new Kubernetes cluster by using GKE, run:
CLUSTER_NAME=&lt;any unique name, e.g. &#34;pach-cluster&#34;&gt; GCP_ZONE=&lt;a GCP availability zone. e.g. &#34;us-west1-a&#34;&gt; gcloud config set compute/zone ${GCP_ZONE} gcloud config set container/cluster ${CLUSTER_NAME} MACHINE_TYPE=&lt;machine type for the k8s nodes, we recommend &#34;n1-standard-4&#34; or larger&gt; ‚ÑπÔ∏è Adding --scopes storage-rw to the gcloud container clusters create ${CLUSTER_NAME} --machine-type ${MACHINE_TYPE} command below will grant the rw scope to whatever service account is on the cluster, which, if you don‚Äôt provide it, is the default compute service account for the project with Editor permissions. While this is not recommended in any production settings, this option can be useful for a quick setup in development. In that scenario, you do not need any service account or additional GCP Bucket permission (see below).
# By default the following command spins up a 3-node cluster. You can change the default with `--num-nodes VAL`. gcloud container clusters create ${CLUSTER_NAME} \ --machine-type=${MACHINE_TYPE} \ --workload-pool=${PROJECT_ID}.svc.id.goog \ --enable-ip-alias \ --create-subnetwork=&#34;&#34; \ --enable-stackdriver-kubernetes \ --enable-dataplane-v2 \ --enable-shielded-nodes \ --release-channel=&#34;regular&#34; \ --workload-metadata=&#34;GKE_METADATA&#34; \ --enable-autorepair \ --enable-autoupgrade \ --disk-type=&#34;pd-ssd&#34; \ --image-type=&#34;COS_CONTAINERD&#34; # By default, GKE clusters have RBAC enabled. To allow the &#39;helm install&#39; to give the &#39;pachyderm&#39; service account # the requisite privileges via clusterrolebindings, you will need to grant *your user account* the privileges # needed to create those clusterrolebindings. # # Note that this command is simple and concise, but gives your user account more privileges than necessary. See # https://docs.pachyderm.io/en/latest/deploy-manage/deploy/rbac/ for the complete list of privileges that the # pachyderm serviceaccount needs. kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account) This might take a few minutes to start up. You can check the status on the GCP Console. A kubeconfig entry is automatically generated and set as the current context. As a sanity check, make sure your cluster is up and running by running the following kubectl command:
# List all pods in the kube-system namespace. kubectl get pods -n kube-system System Response:
NAME READY STATUS RESTARTS AGE event-exporter-gke-67986489c8-j4jr8 2/2 Running 0 3m21s fluentbit-gke-499hn 2/2 Running 0 3m6s fluentbit-gke-7xp2f 2/2 Running 0 3m6s fluentbit-gke-jx7wt 2/2 Running 0 3m6s gke-metrics-agent-jmqsl 1/1 Running 0 3m6s gke-metrics-agent-rd5pr 1/1 Running 0 3m6s gke-metrics-agent-xxl52 1/1 Running 0 3m6s kube-dns-6c7b8dc9f9-ff4bz 4/4 Running 0 3m16s kube-dns-6c7b8dc9f9-mfjrt 4/4 Running 0 2m27s kube-dns-autoscaler-58cbd4f75c-rl2br 1/1 Running 0 3m16s kube-proxy-gke-nad-cluster-default-pool-2e5710dd-38wz 1/1 Running 0 105s kube-proxy-gke-nad-cluster-default-pool-2e5710dd-4b7j 1/1 Running 0 3m6s kube-proxy-gke-nad-cluster-default-pool-2e5710dd-zmzh 1/1 Running 0 3m5s l7-default-backend-66579f5d7-2q64d 1/1 Running 0 3m21s metrics-server-v0.3.6-6c47ffd7d7-k2hmc 2/2 Running 0 2m38s pdcsi-node-7dtbc 2/2 Running 0 3m6s pdcsi-node-bcbcl 2/2 Running 0 3m6s pdcsi-node-jl8hl 2/2 Running 0 3m6s stackdriver-metadata-agent-cluster-level-85d6d797b4-4l457 2/2 Running 0 2m14s If you don&rsquo;t see something similar to the above output, you can point kubectl to the new cluster manually by running the following command:
# Update your kubeconfig to point at your newly created cluster. gcloud container clusters get-credentials ${CLUSTER_NAME} Once your Kubernetes cluster is up, and your infrastructure configured, you are ready to prepare for the installation of Pachyderm. Some of the steps below will require you to keep updating the values.yaml started during the setup of the recommended infrastructure:
2. Create a GCS Bucket # Create an GCS object store bucket for your data # Pachyderm needs a GCS bucket (Object store) to store your data. You can create the bucket by running the following commands:
Set up the following system variables:
BUCKET_NAME ‚Äî A globally unique GCP bucket name where your data will be stored. GCP_REGION ‚Äî The GCP region of your Kubernetes cluster e.g. &ldquo;us-west1&rdquo;. Create the bucket:
gsutil mb -l ${GCP_REGION} gs://${BUCKET_NAME} Check that everything has been set up correctly:
gsutil ls # You should see the bucket you created. You now need to give Pachyderm access to your GCP resources.
Set Up Your GCP Service Account # To access your GCP resources, Pachyderm uses a GCP Project Service Account with permissioned access to your desired resources.
You can create a Service Account with Google Cloud Console:
GSA_NAME=&lt;Your Google Service Account Name&gt; gcloud iam service-accounts create ${GSA_NAME} More information about the creation and management of a Service account on GCP documentation.
Configure Your Service Account Permissions # For Pachyderm to access your Google Cloud Resources, run the following:
Create the following set of variables
SERVICE_ACCOUNT=&#34;${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&#34; # &#34;default&#34; or the namespace in which your cluster was deployed K8S_NAMESPACE=&#34;default&#34; PACH_WI=&#34;serviceAccount:${PROJECT_ID}.svc.id.goog[${K8S_NAMESPACE}/pachyderm]&#34; SIDECAR_WI=&#34;serviceAccount:${PROJECT_ID}.svc.id.goog[${K8S_NAMESPACE}/pachyderm-worker]&#34; CLOUDSQLAUTHPROXY_WI=&#34;serviceAccount:${PROJECT_ID}.svc.id.goog[${K8S_NAMESPACE}/k8s-cloudsql-auth-proxy]&#34; Grant access to cloudSQL and your bucket to the Service Account
# Grant access to cloudSQL to the Service Account gcloud projects add-iam-policy-binding ${PROJECT_ID} \ --member=&#34;serviceAccount:${SERVICE_ACCOUNT}&#34; \ --role=&#34;roles/cloudsql.client&#34; # Grant access to storage (bucket + volumes) to the Service Account gcloud projects add-iam-policy-binding ${PROJECT_ID} \ --member=&#34;serviceAccount:${SERVICE_ACCOUNT}&#34; \ --role=&#34;roles/storage.admin&#34; Use Workload Identity to run Pachyderm Services as the Service Account
Workload Identity is the recommended way to access Google Cloud services from applications running within GKE.
gcloud iam service-accounts add-iam-policy-binding ${SERVICE_ACCOUNT} \ --role roles/iam.workloadIdentityUser \ --member &#34;${PACH_WI}&#34; gcloud iam service-accounts add-iam-policy-binding ${SERVICE_ACCOUNT} \ --role roles/iam.workloadIdentityUser \ --member &#34;${SIDECAR_WI}&#34; gcloud iam service-accounts add-iam-policy-binding ${SERVICE_ACCOUNT} \ --role roles/iam.workloadIdentityUser \ --member &#34;${CLOUDSQLAUTHPROXY_WI}&#34; For a set of standard roles, read the GCP IAM permissions documentation.
3. Persistent Volumes Creation # etcd and PostgreSQL (metadata storage) each claim the creation of a persistent disk.
If you plan to deploy Pachyderm with its default bundled PostgreSQL instance, read the warning below, and jump to the deployment section:
üìñ When deploying Pachyderm on GCP, your persistent volumes are automatically created and assigned the default disk size of 50 GBs. Note that StatefulSets is a default as well.
‚ö†Ô∏è Each persistent disk generally requires a small persistent volume size but high IOPS (1500). If you choose to overwrite the default disk size, depending on your disk choice, you may need to oversize the volume significantly to ensure enough IOPS. For reference, 1GB should work fine for 1000 commits on 1000 files. 10GB is often a sufficient starting size, though we recommend provisioning at least 1500 write IOPS, which requires at least 50GB of space on SSD-based PDs and 1TB of space on Standard PDs.
If you plan to deploy a managed PostgreSQL instance (CloudSQL), read the following section. Note that this is the recommended setup in production.
4. Create a GCP Managed PostgreSQL Database # By default, Pachyderm runs with a bundled version of PostgreSQL. For production environments, it is strongly recommended that you disable the bundled version and use a CloudSQL instance.
Create A CloudSQL Instance # üí° Find the details of the steps and available parameters to create a CloudSQL instance in GCP Documentation: &ldquo;Create instances: CloudSQL for PostgreSQL&rdquo;.
Set up the following system variable:
INSTANCE_NAME ‚Äî Your Cloud SQL instance name. See the illustrative example below:
gcloud sql instances create ${INSTANCE_NAME} \ --database-version=POSTGRES_13 \ --cpu=2 \ --memory=7680MB \ --zone=${GCP_ZONE} \ --availability-type=ZONAL \ --storage-size=50GB \ --storage-type=SSD \ --storage-auto-increase \ --root-password=&#34;&lt;InstanceRootPassword&gt;&#34; When you create a new Cloud SQL for PostgreSQL instance, a default admin user Username: &quot;postgres&quot; is created. It will later be used by Pachyderm to access its databases. Note that the --root-password flag above sets the password for this user.
Check out Google documentation for more information on how to Create and Manage PostgreSQL Users.
Create Your Databases # After your instance is created, you will need to create Pachyderm&rsquo;s database(s).
If you plan to deploy a standalone cluster (i.e., if you do not plan to register your cluster with a separate enterprise server), you will need to create a second database named &ldquo;dex&rdquo; in your Cloud SQL instance for Pachyderm&rsquo;s authentication service. Note that the database must be named dex. This second database is not needed when your cluster is managed by an enterprise server.
‚ÑπÔ∏è Read more about dex on PostgreSQL in Dex&rsquo;s documentation.
Run the first or both commands depending on your use case.
gcloud sql databases create pachyderm -i ${INSTANCE_NAME} gcloud sql databases create dex -i ${INSTANCE_NAME} Pachyderm will use the same user &ldquo;postgres&rdquo; to connect to pachyderm as well as to dex.
Update your values.yaml # Once your databases have been created, add the following fields to your Helm values:
‚ÑπÔ∏è To identify a Cloud SQL instance, you can find the INSTANCE_NAME on the Overview page for your instance in the Google Cloud Console, or by running the following command: gcloud sql instances describe INSTANCE_NAME For example: myproject:myregion:myinstance.
You will need to retrieve the name of your Cloud SQL connection:
CLOUDSQL_CONNECTION_NAME=$(gcloud sql instances describe ${INSTANCE_NAME} --format=json | jq .&#34;connectionName&#34;) cloudsqlAuthProxy: enabled: true connectionName: &#34;&lt;CLOUDSQL_CONNECTION_NAME&gt;&#34; serviceAccount: &#34;&lt;SERVICE_ACCOUNT&gt;&#34; resources: requests: memory: &#34;500Mi&#34; cpu: &#34;250m&#34; postgresql: # turns off the install of the bundled postgres. # If not using the built in Postgres, you must specify a Postgresql # database server to connect to in global.postgresql enabled: false global: postgresql: # The postgresql database host to connect to. Defaults to postgres service in subchart postgresqlHost: &#34;cloudsql-auth-proxy.default.svc.cluster.local.&#34; # The postgresql database port to connect to. Defaults to postgres server in subchart postgresqlPort: &#34;5432&#34; postgresqlSSL: &#34;disable&#34; postgresqlUsername: &#34;postgres&#34; postgresqlPassword: &#34;&lt;InstanceRootPassword&gt;&#34; 5. Deploy Pachyderm # You have set up your infrastructure, created your GCP bucket and a CloudSQL instance, and granted your cluster access to both: you can now finalize your values.yaml and deploy Pachyderm.
Update Your Values.yaml # See an example of values.yaml here.
You might want to create a static IP address to access your cluster externally. Refer to our infrastructure documentation for more details or check the example below:
STATIC_IP_NAME=&lt;your address name&gt; gcloud compute addresses create ${STATIC_IP_NAME} --region=${GCP_REGION} STATIC_IP_ADDR=$(gcloud compute addresses describe ${STATIC_IP_NAME} --region=${GCP_REGION} --format=json --flatten=address | jq &#39;.[]&#39; ) ‚ÑπÔ∏è If you have not created a Managed CloudSQL instance, replace the Postgresql section below with postgresql:enabled: true in your values.yaml and remove the cloudsqlAuthProxy fields. This setup is not recommended in production environments.
Retrieve these additional variables, then fill in their values in the YAML file below:
echo $BUCKET_NAME echo $SERVICE_ACCOUNT echo $CLOUDSQL_CONNECTION_NAME deployTarget: GOOGLE proxy: enabled: true service: type: LoadBalancer pachd: enabled: true storage: google: bucket: &#34;&lt;BUCKET_NAME&#34; serviceAccount: additionalAnnotations: iam.gke.io/gcp-service-account: &#34;&lt;SERVICE_ACCOUNT&gt;&#34; name: &#34;pachyderm&#34; worker: serviceAccount: additionalAnnotations: iam.gke.io/gcp-service-account: &#34;&lt;SERVICE_ACCOUNT&gt;&#34; name: &#34;pachyderm-worker&#34; cloudsqlAuthProxy: enabled: true connectionName: &#34;&lt;CLOUDSQL_CONNECTION_NAME&gt;&#34; serviceAccount: &#34;&lt;SERVICE_ACCOUNT&gt;&#34; resources: requests: memory: &#34;500Mi&#34; cpu: &#34;250m&#34; postgresql: enabled: false global: postgresql: postgresqlHost: &#34;cloudsql-auth-proxy.default.svc.cluster.local.&#34; postgresqlPort: &#34;5432&#34; postgresqlSSL: &#34;disable&#34; postgresqlUsername: &#34;postgres&#34; postgresqlPassword: &#34;&lt;InstanceRootPassword&gt;&#34; Deploy Pachyderm on the Kubernetes cluster # You can now deploy a Pachyderm cluster by running this command:
helm repo add pach https://helm.pachyderm.com helm repo update helm install pachyderm -f my_values.yaml pach/pachyderm System Response:
NAME: pachyderm LAST DEPLOYED: Mon Nov 8 16:48:49 2021 NAMESPACE: default STATUS: deployed REVISION: 1 ‚ö†Ô∏è If RBAC authorization is a requirement or you run into any RBAC errors see Configure RBAC.
It may take a few minutes for the pachd nodes to be running because Pachyderm pulls containers from DockerHub. You can see the cluster status with kubectl, which should output the following when Pachyderm is up and running:
kubectl get pods Once the pods are up, you should see a pod for pachd running (alongside etcd, pg-bouncer or postgres, console, depending on your installation).
System Response:
NAME READY STATUS RESTARTS AGE etcd-0 1/1 Running 0 4m50s pachd-5db79fb9dd-b2gdq 1/1 Running 2 4m49s postgres-0 1/1 Running 0 4m50s If you see a few restarts on the pachd pod, you can safely ignore them. That simply means that Kubernetes tried to bring up those containers before other components were ready, so it restarted them.
Finally, make sure that pachctl talks with your cluster 6. Have &lsquo;pachctl&rsquo; and your Cluster Communicate # Assuming your pachd is running as shown above, make sure that pachctl can talk to the cluster.
Exposed Publicly?: yes no Retrieve the external IP address of your TCP load balancer or your domain name:
kubectl get services | grep pachd-lb | awk &#39;{print $4}&#39; Update the context of your cluster with their direct url, using the external IP address/domain name above:
pachctl connect grpc://localhost:80 Check that your are using the right context:
pachctl config get active-context Your cluster context name should show up.
Run the following:
# Background this process because it blocks. pachctl port-forward 7. Check That Your Cluster Is Up And Running # You are done! You can make sure that your cluster is working by running pachctl version or creating a new repo.
‚ö†Ô∏è If Authentication is activated (When you deploy with an enterprise key, for example), you will need to run pachct auth login, then authenticate to Pachyderm with your User, before you use pachctl.
pachctl version System Response:
COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
101,Helm Deployment,"The package manager Helm is the authoritative deployment method for Pachyderm.
‚ÑπÔ∏è Pachyderm services are exposed on the cluster internal IP (ClusterIP) instead of each node‚Äôs IP (Nodeport) except for LOCAL Helm installations (i.e. Services are still accessible through Nodeports on Local installations).
This page gives a high level view of the steps to follow to install Pachyderm using Helm. Find our chart on Artifacthub or in our GitHub repository.
‚ö†Ô∏è We are now shipping Pachyderm with an optional embedded proxy allowing your cluster to expose one single port externally. This deployment setup is optional.
If you choose to deploy Pachyderm with a Proxy, check out our new recommended architecture and deployment instructions as they alter some of the following instructions.
‚ö†Ô∏è Before your start your installation process.
Refer to this generic page for more information on how to install and get started with Helm.
Read our infrastructure recommendations. You will find instructions on setting up an ingress controller, a TCP load balancer, or connecting an Identity Provider for access control.
If you are planning to use Pachyderm UI with authentication, read our Console deployment instructions. Note that, unless your deployment is LOCAL (i.e., on a local machine for development only, for example, on Minikube or Docker Desktop), the deployment of Console requires the set up of a DNS, an Ingress, and the activation of authentication.
Install # Prerequisites # Install Helm.
Install pachctl, the command-line utility for interacting with a Pachyderm cluster.
Choose the deployment guidelines that apply to you:
Find the deployment page that applies to your Cloud provider (or custom deployment, or on-premises deployment). It will help list the various installation prerequisites, and deployment instructions (Kubernetes, PostgreSQL, Object Store, IdP etc&hellip;) that fit your own use case.
For example, if your Cloud provider is Google Cloud Platform, follow the Prerequisites and Deploy Kubernetes sections of the deployment on Google Cloud Platform page.
Additionally, those instructions will help identify the configuration parameters needed. Those parameter values will be set in your YAML configuration file as follows.
Edit a Values.yaml File # Create a personalized my_pachyderm_values.yaml out of this example repository. Pick the example that fits your target deployment and update the relevant values according to the parameters gathered in the previous step.
Refer to this section to understand Pachyderm&rsquo;s main configuration values (License Key, IdP configuration, etc&hellip;) and how you can set them.
See the reference values.yaml for the list of all available helm values at your disposal.
‚ö†Ô∏è No default k8s CPU and memory requests and limits are created for pachd. If you don&rsquo;t provide values in the values.yaml file, then those requests and limits are simply not set.
For Production deployments, Pachyderm strongly recommends that you create your values.yaml file with CPU and memory requests and limits for both pachd and etcd set to values appropriate to your specific environment. For reference, 1 CPU and 2 GB memory for each is a sensible default.
Install Pachyderm&rsquo;s Helm Chart # Get your Helm Repo Info
helm repo add pach https://helm.pachyderm.com helm repo update Install Pachyderm
You are ready to deploy Pachyderm on the environment of your choice.
helm install pachd -f my_pachyderm_values.yaml pach/pachyderm --version &lt;your_chart_version&gt; üìñ To choose a specific helm chart version
Each chart version is associated with a given version of Pachyderm. You will find the list of all available chart versions and their associated version of Pachyderm on Artifacthub.
You can choose a specific helm chart version by adding a --version flag (for example, --version 0.3.0) to your helm install. No additional flag will install the latest GA release of Pachyderm by default. You can choose the latest pre-release version of the chart by using the flag --devel (pre-releases are versions of the chart that correspond to releases of Pachyderm that don&rsquo;t have the GA status yet). For example: When the 2.0 version of Pachyderm was a release candidate, using the flag --devel would let you install the latest RC of 2.0 while no flag would retrieve the newest GA (1.13.4).
Check your deployment
kubectl get pods Once the pods are up, you should see a pod for pachd running (alongside etcd, pg-bouncer or postgres, console, depending on your installation).
System Response:
NAME READY STATUS RESTARTS AGE etcd-0 1/1 Running 0 18h pachd-5db79fb9dd-b2gdq 1/1 Running 2 18h postgres-0 1/1 Running 0 18h Have &lsquo;pachctl&rsquo; and your Cluster Communicate # Assuming your pachd is running as shown above, make sure that pachctl can talk to the cluster.
If you are exposing your cluster publicly:
Retrieve the external IP address of your TCP load balancer or your domain name: kubectl get services | grep pachd-lb | awk &#39;{print $4}&#39; Update the context of your cluster with their direct url, using the external IP address/domain name above:
pachctl connect grpc://localhost:80 Check that your are using the right context:
pachctl config get active-context Your cluster context name should show up.
If you&rsquo;re not exposing pachd publicly, you can run:
# Background this process because it blocks. pachctl port-forward Verify that pachctl and your cluster are connected:
pachctl version System Response:
COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 Uninstall Pachyderm&rsquo;s Helm Chart # Helm uninstall a release by running:
helm uninstall pachd We recommend making sure that everything is properly removed following a helm uninstall:
The uninstall leaves your persistent volumes. To clean them up, run kubectl get pvc and delete the claims data-postgres-0 and etcd-storage-etcd-0. ‚ö†Ô∏è Deleting pvs will result in the loss of your data.
All other resources should have been removed by Helm. Run kubectl get all | grep &quot;etcd\|\pachd\|postgres\|pg-bouncer&quot; to make sure of it and delete any remaining resources where necessary.
If your uninstall failed, there might be config jobs still running. Run kubectl get jobs.batch | grep pachyderm and delete any remaining job.
Upgrade Pachyderm&rsquo;s Helm Chart # When a new version of Pachyderm&rsquo;s chart is released, or when you want to update some configuration parameters on your cluster, use the helm upgrade command:
helm upgrade pachd -f my_new_pachyderm_values.yaml pach/pachyderm --version &lt;your_chart_version&gt; READ BEFORE ANY INSTALL OR UPGRADE: Pachyderm Configuration Values and Platform Secrets # In this section, you will find a complete list of configuration values you can set when installing/upgrading a cluster as well as options to provide them. Refer to your deployment instructions to identify which are needed.
Values Used By Pachyderm At Installation/Upgrade Time # We have grouped those fields in two categories:
Values that you will need to provide:
pachd.enterpriseLicenseKey: Your enterprise license (When installing Enterprise) oidc.upstreamIDPs : The list of dex connectors, each containing Oauth client info connecting to an upstream IDP (When setting up the Authentication). If you are not planning to use Enterprise or plug an IdP, then no other value should be needed
Values autogenerated when left blank. Can be any value:
pachd.rootToken: Password of your root user. pachd.enterpriseSecret: Needed if you connect to an enterprise server. pachd.oauthClientSecret: Pachd oidc config to connect to dex. console.config.oauthClientSecret: Oauth client secret for Console. Required if you set Console Enterprise. global.postgresqlPassword: Password to your postgreSQL. pachd.enterpriseServerToken: Provide this token when installing a cluster that is under an Enterprise Server&rsquo;s umbrella. Those configuration values can be provided to Pachyderm in various ways:
A - Hard-coded in your values.yaml B - In a secret, by referencing a secret name in your values.yaml A - Provide credentials directly in your values.yaml # You can provide credentials and configuration values directly in your values.yaml or set them with a --set argument during the installation/upgrade. The second table below (Column B) lists the names of the fields in which you can set your values.
A-1 - Provide the few required values and let Pachyderm generate the rest
Provide your Enterprise License (For Enterprise users only). Optionally, your IDPs configuration (For Enterprise users using the Authentication feature) The rest of the values will be generated for you.
‚ÑπÔ∏è In the case where you have activated Enterprise and configured your IdP, note that you have the option to set roles at the Cluster level when deploying.
A-2 - Choose and set your values rather than rely on autogeneration
For example, by using:
For a root token: openssl rand -hex 16 f438329fc98302779be65eef226d32c1 For other values: openssl rand -base64 42 tJkHm0+8niOtP1F8lAPryO9dGwMV7SL/u/uCZQi24kFuRj+7VYvtj01q ‚ÑπÔ∏è Note that those values will be injected into platform secrets at the time of the installation.
B - Use Secret(s) # If your organization uses tools like ArgoCD for Gitops, you might want to create secrets ahead of time then provide their names in the secretName field of your values.yaml.
Find the secret name field that references your secret in your values.yaml (Column A) and its corresponding Secret Key (First column) in the second table below.
Pachyderm Platform Secrets # Pachyderm inject the values hard coded in your values.yaml into &ldquo;platform secrets&rdquo; at the time of the deployment or upgrade (those values can be Postgresql admin login username and password, OAuth information to set up your IdP, or your enterprise license key).
Find the complete list of secrets in the table below:
Secret Name Key Description pachyderm-auth - root-token - auth-config - cluster-role-bindings - &ldquo;root&rdquo; user password of your cluster. - Pachd oidc config to connect to dex. - Role Based Access declaration at the cluster level. Used to define access control at the cluster level when deploying. For example: Give a specific group ClusterAdmin access at once, or give an entire company a default RepoReader access to all repos on this cluster. pachyderm-console-secret OAUTH_CLIENT_SECRET Oauth client secret for Console. Required if you set Console Enterprise. pachyderm-deployment-id-secret CLUSTER_DEPLOYMENT_ID Internal Cluster identifier. pachyderm-enterprise enterprise-secret For internal use. Used as a shared secret between an Enterprise Server and a Cluster to communicate. Always present when enterprise is on but used only when an Enterprise Server is set. pachyderm-identity upstream-idps The list of dex connectors, each containing Oauth client info connecting to an upstream IDP. pachyderm-license enterprise-license-key Your enterprise license. pachyderm-storage-secret This content depends on what object store backs your installation of Pachyderm. Credentials for Pachyderm to access your object store. postgres postgresql-password Password for Pachyderm to Access Postgres. Secrets in bold do not need to be set by users.
Mapping External Secrets Fields, Values Fields, and Pachyderm Platform Secrets # In the following table, you will find the complete list of:
the secret keys and the secret name fields needed to reference a secret in a values.yaml. the secret values fields if you chose to hard code your values rather than pass them in a secret. Pachyderm&rsquo;s platform secrets and keys those values will be injected into in the latter case. üí° Order of operations.
Note that if no secret name is provided for the fields mentioned in A (see table above), Pachyderm will retrieve the dedicated plain-text secret values in the helm values (Column B) and populate (or autogenerate when left blank) its own platform secrets at the time of the installation/upgrade (Column C).
Secret KEY name Description A - Create your secrets ahead of your cluster creation B - Pass credentials in values.yaml C - Corresponding (Platform Secret, Key) in which the values provided in A or B will be injected. root-token Root user Password pachd.rootTokenSecretName pachd.rootToken (pachyderm-auth, rootToken) auth-config Oauth client secret for pachd pachd.oauthClientSecretSecretName pachd.oauthClientSecret (pachyderm-auth, auth-config) cluster-role-bindings Role Based Access declaration at the cluster level. No specific secret to pass role based access information. Use plain text in your values.yaml (see pachAuthClusterRoleBindings) pachd.pachAuthClusterRoleBindings (pachyderm-auth, cluster-role-bindings) enterprise-license-key Your enterprise license pachd.enterpriseLicenseKeySecretName pachd.enterpriseLicenseKey (pachyderm-license, enterprise-license-key) postgresql-password Password to your database global.postgresql.postgresqlExistingSecretName global.postgresql.postgresqlPassword (postgres, postgresql-password) OAUTH_CLIENT_SECRET Oauth client secret for Console Required if you set Console console.config.oauthClientSecretSecretName console.config.oauthClientSecret (pachyderm-console-secret, OAUTH_CLIENT_SECRET) upstream-idps The list of dex connectors, each containing Oauth client info connecting to an upstream IDP oidc.upstreamIDPsSecretName oidc.upstreamIDPs (pachyderm-identity, upstream-idps) enterprise-server-token Users set this value when they install a cluster that is under an Enterprise Server&rsquo;s umbrella. Pachyderm (pachd) uses this token to instruct the enterprise server to add it to its registry. This token must be tied to a user on the enterprise server that has the clusterAdmin role. pachd.enterpriseServerTokenSecretName pachd.enterpriseServerToken Not injected into any platform secret. It is passed into the deployment manifest if set as plaintext. enterprise-secret Needed if you connect to an enterprise server pachd.enterpriseSecretSecretName pachd.enterpriseSecret (pachyderm-enterprise, enterprise-secret) "
102,Import a Kubernetes Context,"After you have deployed Pachyderm with Helm, the Pachyderm context is not created. Therefore, you need to manually create a new Pachyderm context with the embedded current Kubernetes context and activate that context.
To import a Kubernetes context, complete the following steps:
Verify that the cluster was successfully deployed:
kubectl get pods You should see a pod for pachd running (alongside etcd, pg-bouncer or postgres, console, depending on your installation).
System Response:
NAME READY STATUS RESTARTS AGE console-6c989c8d56-ftxk7 1/1 Running 0 3d18h etcd-0 1/1 Running 0 3d18h pachd-f9fd5b6fc-8d774 1/1 Running 0 3d18h pg-bouncer-794d8f68f-sjbbh 1/1 Running 0 3d18h Create a new Pachyderm context with the embedded Kubernetes context:
pachctl config import-kube &lt;new-pachyderm-context-name&gt; -k `kubectl config current-context` Verify that the context was successfully created and view the context parameters:
Example:
pachctl config get context &lt;new-pachyderm-context-name&gt; System Response:
{ &#34;source&#34;: &#34;IMPORTED&#34;, &#34;cluster_name&#34;: &#34;minikube&#34;, &#34;auth_info&#34;: &#34;minikube&#34;, &#34;namespace&#34;: &#34;default&#34; } Activate the new Pachyderm context:
pachctl config set active-context &lt;new-pachyderm-context-name&gt; Verify that the new context has been activated:
pachctl config get active-context "
103,Infrastructure Recommendations,"In the simplest case, such as running a Pachyderm cluster locally, implicit and explicit port-forwarding enables you to communicate with pachd, the Pachyderm API pod, and console, the Pachyderm UI. Port-forwarding can be used in cloud environments as well, but a production environment might require you to define additional inbound connection rules.
Before we dive into the delivery of external traffic to Pachyderm, read the following recommendations to set up your infrastructure in production.
‚ÑπÔ∏è Refer to our generic &ldquo;Helm Install&rdquo; page for more information on how to install and get started with Helm.
Pachyderm Infrastructure Recommendations # ‚ö†Ô∏è We are now shipping Pachyderm with an embedded proxy allowing your cluster to expose one single port externally. This deployment setup is optional.
If you choose to deploy Pachyderm with a Proxy, our new recommended architecture and deployment instructions overwrite the following instructions.
For production deployments, we recommend that you:
Use a secure connection
Make sure that you have Transport Layer Security (TLS) enabled for Ingress connections. You can deploy pachd and console with different certificates if required. Self-signed certificates might require additional configuration. For instructions on deployment with TLS, see Deploy Pachyderm with TLS.
Use Pachyderm authentication/authorization
Pachyderm authentication is an additional security layer to protect your data from unauthorized access. See the authentication and authorization section to activate access control and set up an IdP (Identity Provider).
Add an Ingress Controller to your cluster for HTTP/HTTPS incoming traffic.
Provision a TCP load balancer for gRPC incoming traffic. Provision a TCP load balancer with port 30650 (gRPC port) and 30600 (s3gateway port) forwarding to pachd.
Configure access to your external IP addresses through firewalls or your Cloud Provider Network Security.
(Optional) Create a DNS entry for each public IP (each Load Balancer)
Once you have your networking infrastructure setup, check the deployment page of your cloud provider. The following section comes back to the setup of an Ingress and a TCP Load Balancer in details.
Deliver External Traffic To Pachyderm # Pachyderm provides multiple ways to deliver external traffic to services.
However, we recommend to set up the following resources in a production environment:
An Ingress Controller to manage HTTP/HTTPS external access to the Console and authentication services (oidc and identity services). A TCP Load Balancer to manage gRPC external access to pachd. The diagram below gives a quick overview of those recommendations on AWS EKS: NodePort # By default, the local deployment of Pachyderm deploys the pachd service as type:NodePort. However, NodePort is a limited solution that is not recommended in production deployments. Therefore, Pachyderm services are otherwise exposed on the cluster internal IP (ClusterIP) instead of each node‚Äôs IP (Nodeport).
Ingress # An Ingress exposes HTTP and HTTPS routes from outside the cluster to services in the cluster such as Console or Authentication services.
To configure the Ingress, enable the ingress field in your values.yaml, and chose one of the following:
deploy your preferred Ingress Controller (Traefik, NGINX). or, provide any specific Kubernetes Ingress annotations to customize your ingress controller behavior. If your ingress is enabled:
Cloud providers may provision a Load balancer automatically. For example, AWS will provision an Application Load Balancer (ALB) in front of Console. The deployment of Pachyderm (Check our Helm documentation) automatically creates the following set of rules: - host: &lt;your_domain_name&gt; http: paths: - path: &#34;/dex&#34; backend: serviceName: &#34;pachd&#34; servicePort: &#34;identity-port&#34; - path: &#34;/authorization-code/callback&#34; backend: serviceName: &#34;pachd&#34; servicePort: &#34;oidc-port&#34; - path: &#34;/*&#34; backend: serviceName: &#34;console&#34; servicePort: &#34;console-http&#34; See our reference values.yaml for all available fields.
üìñ You might choose to deploy your preferred Ingress Controller (Traefik, NGINX). Read about the installation and configuration of Traefik on a cluster.
‚ö†Ô∏è To have the ingress routes use the https protocol without enabling the cert secret configuration, set ingress.uriHttpsProtoOverride to true in your values.yaml.
Example on AWS EKS # In the example below, we are opening the HTTPS port and enabling TLS.
ingress: enabled: true annotations: alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:region:account-id:certificate/aaaa-bbbb-cccc alb.ingress.kubernetes.io/group.name: pachyderm # lets multiple ingress resources be configured into one load balancer alb.ingress.kubernetes.io/listen-ports: &#39;[{&#34;HTTPS&#34;: 443}]&#39; alb.ingress.kubernetes.io/scheme: internal alb.ingress.kubernetes.io/security-groups: sg-aaaa alb.ingress.kubernetes.io/subnets: subnet-aaaa, subnet-bbbb, subnet-cccc alb.ingress.kubernetes.io/target-type: ip kubernetes.io/ingress.class: alb host: &#34;your_domain_name&#34; tls: enabled: true secretName: &#34;pach-tls&#34; Example on GCP GKE # In the example below using the ingress controller Traefik, we are opening the HTTPS port and enabling TLS.
ingress: enabled: true annotations: kubernetes.io/ingress.clas: traefik host: &#34;your_domain_name&#34; tls: enabled: true secretName: &#34;pach-tls&#34; Example on Azure AKS # In the example below, we are using the ingress controller Nginx, and opening the HTTP port.
ingress: enabled: true annotations: kubernetes.io/ingress.class: &#34;nginx&#34; host: &#34;your_domain_name&#34; tls: enabled: true secretName: &#34;pach-tls&#34; ATTENTION: You must use TLS when deploying on Azure.
As of today, few Ingress Controller offer full support of the gRPC protocol. To access pachd over gRPC (for example, when using pachctl or the s3Gateway, we recommend using a Load Balancer instead.
üìñ See Also:
Kubernetes Ingress. Kubernetes Ingress Controller. LoadBalancer # You should load balance all gRPC and S3 incoming traffic to a TCP LB (load balanced at L4 of the OSI model) deployed in front of the pachd service. To automatically provision an external load balancer in your current cloud (if supported), enable the externalService field of the pachd service in your values.yaml as follow:
# If enabled, External service creates a service which is safe to # be exposed externally pachd: externalService: enabled: true apiGRPCPort: 30650 s3GatewayPort: 30600 annotations: {see example below} See our reference values.yaml for all available fields.
‚ÑπÔ∏è When externalService is enabled, Pachyderm creates a corresponding pachd-lb service of type:LoadBalancer allowing your cloud platform (AWS, GKE&hellip;) to provision a TCP Load Balancer automatically.
Add the appropriate annotations to attach any Load Balancer configuration information to the metadata of your service.
Example on AWS EKS # In the following example, we deploy an NLB and enable TLS on AWS EKS:
pachd: externalService: enabled: true apiGRPCPort: 30650 s3GatewayPort: 30600 annotations: service.beta.kubernetes.io/aws-load-balancer-type: &#34;external&#34; service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: &#34;ip&#34; service.beta.kubernetes.io/aws-load-balancer-scheme: &#34;internal&#34; service.beta.kubernetes.io/aws-load-balancer-subnets: &#34;subnet-aaaaa,subnet-bbbbb,subnet-ccccc&#34; service.beta.kubernetes.io/aws-load-balancer-ssl-cert: &#34;arn:aws:acm:region:account-id:certificate/aaa-bbb-cccc&#34; service.beta.kubernetes.io/aws-load-balancer-ssl-ports: &#34;30600,30650,30657,30658&#34; Example on GCP GKE # In the following example, we pre created a static IP by running gcloud compute addresses create ADDRESS_NAME --global --ip-version IPV4, then passed this external IP to the values.yaml as follow:
pachd: externalService: enabled: true apiGRPCPort: 30650 s3GatewayPort: 30600 loadBalancerIP: ${ADDRESS_NAME} Example on Azure AKS # This example is identical to the example on Google GKE.
pachd: externalService: enabled: true apiGRPCPort: 30650 s3GatewayPort: 30600 loadBalancerIP: ${ADDRESS_NAME} Next: Find the deployment page that matches your cloud provider
"
104,Setup Ingress with Traefik,"Before completing the following steps, read the Infrastructure Recommendation page.
‚ö†Ô∏è We are now shipping Pachyderm with an embedded proxy allowing your cluster to expose one single port externally. This deployment setup is optional.
If you choose to deploy Pachyderm with a Proxy, our new recommended architecture and deployment instructions overwrite the following instructions.
This section provides an example of how to route cluster-external HTTP/HTTPS requests to cluster-internal services (here Pachyderm UI console service and authentication services using the ingress controller Traefik.
Traefik ingress controller on Pachyderm UI&rsquo;s cluster in one diagram # Here is a quick high-level view of the various components at play. ‚ö†Ô∏è The following installation steps are for Informational Purposes ONLY. Please refer to your full Traefik documentation for further installation details and any troubleshooting advice.
Traefik installation and Ingress resource Definition # Helm install Traefik:
Get Repo Info helm repo add traefik https://helm.traefik.io/traefik helm repo update Install the Traefik helm chart (helm v3) helm install traefik traefik/traefik Run a quick check: kubectl get all You should see your Traefik pod, service, deployments.apps, and replicaset.app.
You can now access your Traefik Dashboard at http://127.0.0.1:9000/dashboard/ following the port-forward instructions (You can choose to apply your own Ingress resource instead.):
kubectl port-forward $(kubectl get pods --selector &#34;app.kubernetes.io/name=traefik&#34; --output=name) 9000:9000 Configure the Ingress in the helm chart. You will need to configure any specific annotations your ingress controller requires.
my_pachyderm_values.yaml
ingress: enabled: false annotations: kubernetes.io/ingress.class: &#34;traefik&#34; traefik.ingress.kubernetes.io/router.tls: &#34;true&#34; host: &#34;&lt;your_domain_name&gt;&#34; For a list of all available annotations, read the Traefik &amp; Kubernetes documentation.
At a minimum, you will need to specify the host field: match the hostname header of the http request (domain).
Check the list of all available helm values at your disposal in our reference documentation.
Install Pachyderm and Console using the Helm Chart
Once you have your networking infrastructure set up, apply a helm values file such as the one specified in the example file below to wire up routing through an Ingress, and set up TLS.
ingress: enabled: true host: &lt;DNS-ENTRY-A&gt; annotations: ## annotations specific to integrate with your ingress-controller traefik.ingress.kubernetes.io/router.tls: &#34;true&#34; kubernetes.io/ingress.class: &#34;traefik&#34; tls: enabled: true secretName: &#34;pach-tls&#34; pachd: tls: enabled: true secretName: &#34;pach-tls&#34; externalService: enabled: true loadBalancerIP: &lt;DNS-ENTRY-B&gt; console: enabled: true helm install pachd -f my_pachyderm_values.yaml pach/pachyderm The deployment of Pachyderm automatically creates the required set of rules.
Check your new rules by running kubectl describe ingress console: s kubectl describe ingress console Name: console Namespace: default Address: Default backend: default-http-backend:80 Rules: Host Path Backends console.localhost / console:console-http (10.1.0.7:4000) Annotations: kubernetes.io/ingress.class: traefik /dex pachd:identity-port (10.1.0.8:1658) Annotations: kubernetes.io/ingress.class: traefik / pachd:oidc-port (10.1.0.8:1657) Annotations: kubernetes.io/ingress.class: traefik Events: &lt;none&gt;
Check the Traefik Dashboard again (http://127.0.0.1:9000/dashboard/), your new set of rules should now be visible.
Browse # Connect to your Console (Pachyderm UI): https://&lt;external-IP-address-or-domain-name&gt;:443/ (if TLS is enabled) or http://&lt;external-IP-address-or-domain-name&gt;:80/. You are all set!
References # Traefik documentation. "
105,Monitor with Prometheus," ‚ÑπÔ∏è To monitor a Pachyderm cluster with Prometheus, a Pachyderm Enterprise License is required.
Pachyderm&rsquo;s deployment manifest exposes Prometheus metrics, allowing an easy set up of the monitoring of your cluster. Only available for self-managed deployments today.
‚ö†Ô∏è These installation steps are for Informational Purposes ONLY. Please refer to your full Prometheus documentation for further installation details and any troubleshooting advice.
Prometheus installation and Service Monitor creation # Helm install kube-prometheus-stack, Prometheus&rsquo; Kubernetes cluster monitoring using the Prometheus Operator:
Get Repo Info helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Install the Prometheus-operator helm chart helm install &lt;a-release-name&gt; prometheus-community/kube-prometheus-stack Create a ServiceMonitor for Pachyderm in Kubernetes:
Create a myprometheusservice.yaml
apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: pachyderm-scraper labels: release: &lt;a-release-name&gt; spec: selector: matchLabels: suite: pachyderm namespaceSelector: matchNames: - default endpoints: - port: prom-metrics interval: 30s Create a ServiceMonitor looking to scrape metrics from suite: pachyderm:
kubectl create -f myprometheusservice.yaml The prometheus-operator will search for the pods based on the label selector &lt;a-release-name&gt; and creates a prometheus target so prometheus will scrape the metrics endpoint prom-metrics.
In this case, it looks for anything with the label suite: pachyderm - which is by default associated with all Pachyderm resources.
‚ÑπÔ∏è Our Service Monitor `pachyderm-scraper` above maps the endpoint port `prom-metrics` to a corresponding `prom-metrics` port described in Pachyderm's deployment manifest. Let's take a quick look at this file: ```s kubectl -o json get service/pachd ``` In the json file, find: ```json { &quot;name&quot;: &quot;prom-metrics&quot;, &quot;port&quot;: 1656, &quot;protocol&quot;: &quot;TCP&quot;, &quot;targetPort&quot;: &quot;prom-metrics&quot; } ``` Port-Forward # One last step before you can collect your metrics: If you followed the instruction above, you can connect to Prometheus by using kubectl port-forward.
kubectl port-forward pod/prometheus-&lt;a-release-name&gt;-kube-prometheus-stack-prometheus-0 9090 If you have an existing Prometheus deployment, please navigate to your Prometheus GUI.
Browse # You can now browse your targets (http://localhost:9090/targets). Run a pipeline of your choice. The pachyderm-scraper should be visible:
In the ClassicUI tab, you should be able to see the new pachyderm metrics.
References # &lt;! &ndash;* Find the full list of Pachyderm metrics here: - Pachd metrics - Pipeline metrics -&gt;
Kube Prometheus Stack documentation. Prometheus documentation. "
106,Job Metrics,"pachyderm_worker_datum_count pachyderm_worker_datum_download_bytes_count pachyderm_worker_datum_download_seconds_count pachyderm_worker_datum_download_size_bucket pachyderm_worker_datum_download_size_count pachyderm_worker_datum_download_size_sum pachyderm_worker_datum_download_time_bucket pachyderm_worker_datum_download_time_count pachyderm_worker_datum_download_time_sum pachyderm_worker_datum_proc_seconds_count pachyderm_worker_datum_proc_time_bucket pachyderm_worker_datum_proc_time_count pachyderm_worker_datum_proc_time_sum pachyderm_worker_datum_upload_bytes_count pachyderm_worker_datum_upload_seconds_count pachyderm_worker_datum_upload_size_bucket pachyderm_worker_datum_upload_size_count pachyderm_worker_datum_upload_size_sum pachyderm_worker_datum_upload_time_bucket pachyderm_worker_datum_upload_time_count pachyderm_worker_datum_upload_time_sum
"
107,Pachd Metrics,"pachyderm_pachd_auth_log_req_seconds_count pachyderm_pachd_cache_object_cache_hits_gauge pachyderm_pachd_cache_object_gets_gauge pachyderm_pachd_cache_object_info_cache_hits_gauge pachyderm_pachd_cache_object_info_gets_gauge pachyderm_pachd_cache_object_info_loads_deduped_gauge pachyderm_pachd_cache_object_info_loads_gauge pachyderm_pachd_cache_object_info_local_load_errs_gauge pachyderm_pachd_cache_object_info_local_loads_gauge pachyderm_pachd_cache_object_info_peer_errors_gauge pachyderm_pachd_cache_object_info_peer_loads_gauge pachyderm_pachd_cache_object_info_server_requests_gauge pachyderm_pachd_cache_object_loads_deduped_gauge pachyderm_pachd_cache_object_loads_gauge pachyderm_pachd_cache_object_local_load_errs_gauge pachyderm_pachd_cache_object_local_loads_gauge pachyderm_pachd_cache_object_peer_errors_gauge pachyderm_pachd_cache_object_peer_loads_gauge pachyderm_pachd_cache_object_server_requests_gauge pachyderm_pachd_cache_tag_cache_hits_gauge pachyderm_pachd_cache_tag_gets_gauge pachyderm_pachd_cache_tag_loads_deduped_gauge pachyderm_pachd_cache_tag_loads_gauge pachyderm_pachd_cache_tag_local_load_errs_gauge pachyderm_pachd_cache_tag_local_loads_gauge pachyderm_pachd_cache_tag_peer_errors_gauge pachyderm_pachd_cache_tag_peer_loads_gauge pachyderm_pachd_cache_tag_server_requests_gauge pachyderm_pachd_enterprise_activate_seconds_count pachyderm_pachd_enterprise_activate_time_bucket pachyderm_pachd_enterprise_activate_time_count pachyderm_pachd_enterprise_activate_time_sum pachyderm_pachd_enterprise_log_req_seconds_count pachyderm_pachd_pfs_check_object_seconds_count pachyderm_pachd_pfs_check_object_time_bucket pachyderm_pachd_pfs_check_object_time_count pachyderm_pachd_pfs_check_object_time_sum pachyderm_pachd_pfs_create_repo_seconds_count pachyderm_pachd_pfs_create_repo_time_bucket pachyderm_pachd_pfs_create_repo_time_count pachyderm_pachd_pfs_create_repo_time_sum pachyderm_pachd_pfs_delete_all_seconds_count pachyderm_pachd_pfs_delete_all_time_bucket pachyderm_pachd_pfs_delete_all_time_count pachyderm_pachd_pfs_delete_all_time_sum pachyderm_pachd_pfs_delete_branch_seconds_count pachyderm_pachd_pfs_delete_branch_time_bucket pachyderm_pachd_pfs_delete_branch_time_count pachyderm_pachd_pfs_delete_branch_time_sum pachyderm_pachd_pfs_delete_repo_seconds_count pachyderm_pachd_pfs_delete_repo_time_bucket pachyderm_pachd_pfs_delete_repo_time_count pachyderm_pachd_pfs_delete_repo_time_sum pachyderm_pachd_pfs_finish_commit_seconds_count pachyderm_pachd_pfs_finish_commit_time_bucket pachyderm_pachd_pfs_finish_commit_time_count pachyderm_pachd_pfs_finish_commit_time_sum pachyderm_pachd_pfs_func_1_seconds_count pachyderm_pachd_pfs_get_file_seconds_count pachyderm_pachd_pfs_get_file_time_bucket pachyderm_pachd_pfs_get_file_time_count pachyderm_pachd_pfs_get_file_time_sum pachyderm_pachd_pfs_get_object_seconds_count pachyderm_pachd_pfs_get_object_time_bucket pachyderm_pachd_pfs_get_object_time_count pachyderm_pachd_pfs_get_object_time_sum pachyderm_pachd_pfs_get_objects_seconds_count pachyderm_pachd_pfs_get_objects_time_bucket pachyderm_pachd_pfs_get_objects_time_count pachyderm_pachd_pfs_get_objects_time_sum pachyderm_pachd_pfs_inspect_branch_seconds_count pachyderm_pachd_pfs_inspect_branch_time_bucket pachyderm_pachd_pfs_inspect_branch_time_count pachyderm_pachd_pfs_inspect_branch_time_sum pachyderm_pachd_pfs_inspect_object_seconds_count pachyderm_pachd_pfs_inspect_object_time_bucket pachyderm_pachd_pfs_inspect_object_time_count pachyderm_pachd_pfs_inspect_object_time_sum pachyderm_pachd_pfs_inspect_repo_seconds_count pachyderm_pachd_pfs_inspect_repo_time_bucket pachyderm_pachd_pfs_inspect_repo_time_count pachyderm_pachd_pfs_inspect_repo_time_sum pachyderm_pachd_pfs_list_file_stream_seconds_count pachyderm_pachd_pfs_list_file_stream_time_bucket pachyderm_pachd_pfs_list_file_stream_time_count pachyderm_pachd_pfs_list_file_stream_time_sum pachyderm_pachd_pfs_list_repo_seconds_count pachyderm_pachd_pfs_list_repo_time_bucket pachyderm_pachd_pfs_list_repo_time_count pachyderm_pachd_pfs_list_repo_time_sum pachyderm_pachd_pfs_put_file_seconds_count pachyderm_pachd_pfs_put_file_time_bucket pachyderm_pachd_pfs_put_file_time_count pachyderm_pachd_pfs_put_file_time_sum pachyderm_pachd_pfs_put_object_seconds_count pachyderm_pachd_pfs_put_object_split_seconds_count pachyderm_pachd_pfs_put_object_split_time_bucket pachyderm_pachd_pfs_put_object_split_time_count pachyderm_pachd_pfs_put_object_split_time_sum pachyderm_pachd_pfs_put_object_time_bucket pachyderm_pachd_pfs_put_object_time_count pachyderm_pachd_pfs_put_object_time_sum pachyderm_pachd_pfs_start_commit_seconds_count pachyderm_pachd_pfs_start_commit_time_bucket pachyderm_pachd_pfs_start_commit_time_count pachyderm_pachd_pfs_start_commit_time_sum pachyderm_pachd_pps_create_pipeline_seconds_count pachyderm_pachd_pps_create_pipeline_time_bucket pachyderm_pachd_pps_create_pipeline_time_count pachyderm_pachd_pps_create_pipeline_time_sum pachyderm_pachd_pps_delete_all_seconds_count pachyderm_pachd_pps_delete_all_time_bucket pachyderm_pachd_pps_delete_all_time_count pachyderm_pachd_pps_delete_all_time_sum pachyderm_pachd_pps_delete_job_seconds_count pachyderm_pachd_pps_delete_job_time_bucket pachyderm_pachd_pps_delete_job_time_count pachyderm_pachd_pps_delete_job_time_sum pachyderm_pachd_pps_delete_pipeline_seconds_count pachyderm_pachd_pps_delete_pipeline_time_bucket pachyderm_pachd_pps_delete_pipeline_time_count pachyderm_pachd_pps_delete_pipeline_time_sum pachyderm_pachd_pps_func_1_seconds_count pachyderm_pachd_pps_list_pipeline_seconds_count pachyderm_pachd_pps_list_pipeline_time_bucket pachyderm_pachd_pps_list_pipeline_time_count pachyderm_pachd_pps_list_pipeline_time_sum pachyderm_pachd_report_metric pachyderm_pachd_transaction_delete_all_seconds_count pachyderm_pachd_transaction_delete_all_time_bucket pachyderm_pachd_transaction_delete_all_time_count pachyderm_pachd_transaction_delete_all_time_sum pachyderm_pachd_transaction_func_1_seconds_count
"
108,Non-Default Namespaces,"Often, production deploys of Pachyderm involve deploying Pachyderm to a non-default namespace. This helps administrators of the cluster more easily manage Pachyderm components alongside other things that might be running inside of Kubernetes (DataDog, TensorFlow Serving, etc.).
To deploy Pachyderm to a non-default namespace, you need to add the -n or --namespace flag when deploying. If the namespace does not already exist, you can have Helm create it with --create-namespace.
helm install &lt;args&gt; --namespace pachyderm --create-namespace To talk to your Pachyderm cluster:
You can either modify an existing pachctl context
pachctl config update context --namespace pachyderm or import one from Kubernetes:
"
109,On Premises,"This page walks you through the fundamentals of what you need to know about Kubernetes, persistent volumes, and object stores to deploy Pachyderm on-premises.
üìñ Read our infrastructure recommendations. You will find instructions on how to set up an ingress controller, a load balancer, or connect an Identity Provider for access control. If you are planning to install Pachyderm UI. Read our Console deployment instructions. Note that, unless your deployment is LOCAL (i.e., on a local machine for development only, for example, on Minikube or Docker Desktop), the deployment of Console requires, at a minimum, the set up of an Ingress. Troubleshooting a deployment? Check out Troubleshooting Deployments. üí° We are now shipping Pachyderm with an optional embedded proxy allowing your cluster to expose one single port externally. This deployment setup is optional.
If you choose to deploy Pachyderm with a Proxy, check out our new recommended architecture and deployment instructions.
Introduction # Deploying Pachyderm successfully on-premises requires a few prerequisites. Pachyderm is built on Kubernetes. Before you can deploy Pachyderm, you will need to perform the following actions:
Deploy Kubernetes on-premises. Deploy two Kubernetes persistent volumes that Pachyderm will use to store its metadata. Deploy an on-premises object store using a storage provider like MinIO, EMC&rsquo;s ECS, or SwiftStack to provide S3-compatible access to your data storage. Finally, Deploy Pachyderm using Helm by running the helm install command with the appropriate values configured in your values.yaml. We recommend reading these generic deployment steps if you are unfamiliar with Helm. Prerequisites # Before you start, you will need the following clients installed:
kubectl pachctl Setting Up To Deploy On-Premises # Deploying Kubernetes # The Kubernetes docs have instructions for deploying Kubernetes in a variety of on-premise scenarios. We recommend following one of these guides to get Kubernetes running.
üí° Pachyderm recommends running your cluster on Kubernetes 1.19.0 and above.
Storage Classes # Once you deploy Kubernetes, you will also need to configure storage classes to consume persistent volumes for etcd and postgresql.
‚ö†Ô∏è The database and metadata service (Persistent disks) generally requires a small persistent volume size (i.e. 10GB) but high IOPS (1500), therefore, depending on your storage provider, you may need to oversize the volume significantly to ensure enough IOPS.
Once you have determined the name of the storage classes you are going to use and the sizes, you can add them to your helm values file, specifically:
etcd: storageClass: MyStorageClass size: 10Gi postgresql: persistence: storageClass: MyStorageClass size: 10Gi Deploying An Object Store # An object store is used by Pachyderm&rsquo;s pachd for storing all your data. The object store you use must be accessible via a low-latency, high-bandwidth connection.
‚ÑπÔ∏è For an on-premises deployment, it is not advisable to use a cloud-based storage mechanism. Do not deploy an on-premises Pachyderm cluster against cloud-based object stores (such as S3, GCS, Azure Blob Storage).
You will, however, access your Object Store using the S3 protocol.
Storage providers like MinIO (the most common and officially supported option), EMC&rsquo;s ECS, Ceph, or SwiftStack provide S3-compatible access to enterprise storage for on-premises deployment.
Sizing And Configuring The Object Store # Start with a large multiple of your current data set size.
You will need four items to configure the object store. We are prefixing each item with how we will refer to it in the helm values file.
endpoint: The access endpoint. For example, MinIO&rsquo;s endpoints are usually something like minio-server:9000.
Do not begin it with the protocol; it is an endpoint, not an url. Also, check if your object store (e.g. MinIO) is using SSL/TLS. If not, disable it using secure: false.
bucket: The bucket name you are dedicating to Pachyderm. Pachyderm will need exclusive access to this bucket.
id: The access key id for the object store.
secret: The secret key for the object store.
pachd: storage: backend: minio minio: bucket: &#34;&#34; endpoint: &#34;&#34; id: &#34;&#34; secret: &#34;&#34; secure: &#34;&#34; Next Step: Proceed to your Helm installation # Once you have Kubernetes deployed, your storage classes setup, and your object store configured, follow those steps to Helm install Pachyderm on your cluster.
"
110,Production Recommendations,"To deploy in production, we recommend setting up the following pieces of networking infrastructure: A load balancer, a kubernetes ingress controller, and a DNS pointing to the load balancer. In addition we recommend using a managed database instance (such as RDS for AWS).
‚ö†Ô∏è Interested in deploying with an embedded proxy and expose one single external port?
We are now shipping Pachyderm with an optional embedded proxy allowing your cluster to expose one single port externally. This deployment setup is optional.
If you choose to deploy Pachyderm with a Proxy, check out our new recommended architecture and deployment instructions.
Deploying with a proxy presents a couple of advantages:
You only need to set up one TCP Load Balancer (No more Ingress in front of Console). You will need one DNS only. It simplifies the deployment of Console. No more port-forward. Once you have your networking infrastructure set up, apply a helm values file such as the one specified in the example file below to wire up routing through an Ingress, and set up TLS. We recommend using a certificate manager such as cert-manager to refresh certificates and inject them as kubernetes secrets into your cluster for the ingress and load balancer to use.
‚ÑπÔ∏è This example uses Traefik as an Ingress controller. To configure other ingress controllers, apply their annotations in .Values.console.annotations.
values.yaml with enterprise license &amp; authentication # ingress: enabled: true host: &lt;DNS-ENTRY&gt; tls: enabled: true secretName: &#34;pach-tls&#34; pachd: tls: enabled: true secretName: &#34;pach-tls&#34; externalService: enabled: true loadBalancerIP: &lt;LOAD-BALANCER-IP&gt; console: enabled: true annotations: ## annotations specific to integrate with your ingress-controller ## the example below is a provided configuration specific to traefik as an ingress-controller traefik.ingress.kubernetes.io/router.tls: &#34;true&#34; kubernetes.io/ingress.class: &#34;traefik&#34; "
111,Quickstart,"On this page, you will find simplified deployment instructions and Helm values to get you started with the latest release of Pachyderm on the Kubernetes Engine of your choice (AWS (EKS), Google (GKS), and Azure (AKS)).
For each cloud provider, we will give you the option to &ldquo;quick deploy&rdquo; Pachyderm with or without an enterprise key. A quick deployment allows you to experiment with Pachyderm without having to go through any infrastructure setup. In particular, you do not need to set up any object store or PostgreSQL instance.
üí° The deployment steps highlighted in this document are not intended for production. For production settings, please read our infrastructure recommendations. In particular, we recommend:
the use of a managed PostgreSQL server (RDS, CloudSQL, or PostgreSQL Server) rather than Pachyderm&rsquo;s default bundled PostgreSQL. the setup of a TCP Load Balancer in front of your pachd service. the setup of an Ingress Controller in front of Console. Then find your targeted Cloud provider in the Deploy and Manage section of this documentation.
‚ö†Ô∏è We are now shipping Pachyderm with an optional embedded proxy allowing your cluster to expose one single port externally. This deployment setup is optional.
If you choose to deploy Pachyderm with a Proxy, check out our new recommended architecture and deployment instructions.
Deploying with a proxy presents a couple of advantages:
You only need to set up one TCP Load Balancer (No more Ingress in front of Console). You will need one DNS only. It simplifies the deployment of Console. No more port-forward. 1. Prerequisites # Pachyderm is deployed on a Kubernetes Cluster.
Install the following clients on your machine before you start creating your cluster. Use the latest available version of the components listed below.
kubectl: the cli to interact with your cluster. pachctl: the cli to interact with Pachyderm. Install Helm for your deployment. ‚ö†Ô∏è Get a Pachyderm Enterprise key
To get a free-trial token, fill in this form, get in touch with us at sales@pachyderm.io, or on our Slack.
Select your favorite cloud provider.
üí° Note that we often use the acronym CE for Community Edition.
2. Create Your Values.yaml # ‚ÑπÔ∏è Pachyderm comes with a Web UI (Console) per default.
AWS # Additional client installation: Install AWS CLI
Create an EKS cluster
Create an S3 bucket for your data
Create a values.yaml
Deploy Pachyderm CE (includes Console CE) # deployTarget: &#34;AMAZON&#34; pachd: storage: amazon: bucket: &#34;bucket_name&#34; # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; region: &#34;us-east-2&#34; externalService: enabled: true console: enabled: true Deploy Pachyderm Enterprise with Console # Note that when deploying Pachyderm Enterprise with Console, we create a default mock user (username:admin, password: password) to authenticate yourself to Console so you don&rsquo;t have to connect an Identity Provider to make things work. The mock user is a Cluster Admin per default.
deployTarget: &#34;AMAZON&#34; pachd: storage: amazon: bucket: &#34;bucket_name&#34; # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; region: &#34;us-east-2&#34; # pachyderm enterprise key enterpriseLicenseKey: &#34;YOUR_ENTERPRISE_TOKEN&#34; console: enabled: true Jump to Helm install
Google # Additional client installation: Install Google Cloud SDK
Create a GKE cluster Note: Add --scopes storage-rw to your gcloud container clusters create command.
Create a GCS Bucket for your data
Create a values.yaml
Deploy Pachyderm CE (includes Console CE) # deployTarget: &#34;GOOGLE&#34; pachd: storage: google: bucket: &#34;bucket_name&#34; cred: | INSERT JSON CONTENT HERE externalService: enabled: true console: enabled: true Deploy Pachyderm Enterprise with Console # Note that when deploying Pachyderm Enterprise with Console, we create a default mock user (username:admin, password: password) to authenticate yourself to Console so you don&rsquo;t have to connect an Identity Provider to make things work. The mock user is a Cluster Admin per default.
deployTarget: &#34;GOOGLE&#34; pachd: storage: google: bucket: &#34;bucket_name&#34; cred: | INSERT JSON CONTENT HERE # pachyderm enterprise key enterpriseLicenseKey: &#34;YOUR_ENTERPRISE_TOKEN&#34; console: enabled: true Jump to Helm install
Azure # ‚ÑπÔ∏è This section assumes that you have an Azure Subscription. Additional client installation: Install Azure CLI 2.0.1 or later.
Create an AKS cluster
Create a Storage Container for your data
Create a values.yaml
Deploy Pachyderm CE (includes Console CE) # deployTarget: &#34;MICROSOFT&#34; pachd: storage: microsoft: # storage container name container: &#34;blah&#34; # storage account name id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # storage account key secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; externalService: enabled: true console: enabled: true Deploy Pachyderm Enterprise with Console # Note that when deploying Pachyderm Enterprise with Console, we create a default mock user (username:admin, password: password) to authenticate yourself to Console so you don&rsquo;t have to connect an Identity Provider to make things work. The mock user is a Cluster Admin per default.
deployTarget: &#34;MICROSOFT&#34; pachd: storage: microsoft: # storage container name container: &#34;blah&#34; # storage account name id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # storage account key secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; # pachyderm enterprise key enterpriseLicenseKey: &#34;YOUR_ENTERPRISE_TOKEN&#34; console: enabled: true Jump to Helm install
3. Helm Install # You will be deploying the latest GA release of Pachyderm:
helm repo add pach https://helm.pachyderm.com helm repo update helm install pachd pach/pachyderm -f my_pachyderm_values.yaml Check your deployment:
kubectl get pods The deployment takes some time. You can run kubectl get pods periodically to check the status of your deployment.
Once all the pods are up, you should see a pod for pachd running (alongside etcd, pg-bouncer or postgres, console, depending on your installation). If you are curious about the architecture of Pachyderm, take a look at our high-level architecture diagram.
System Response:
NAME READY STATUS RESTARTS AGE console-7b69ddf66d-bxmg5 1/1 Running 0 18h etcd-0 1/1 Running 0 18h pachd-5db79fb9dd-b2gdq 1/1 Running 2 18h pg-bouncer-55d9c86768-g8lx7 1/1 Running 0 18h postgres-0 1/1 Running 0 18h 4. Have &lsquo;pachctl&rsquo; And Your Cluster Communicate # You have deployed Pachyderm without Console # Retrieve the external IP address of pachd service:
kubectl get services | grep pachd-lb | awk &#39;{print $4}&#39; Then update your context for pachctl to point at your cluster:
pachctl connect grpc://localhost:80 If Authentication is activated (When you deploy with an enterprise key already set, for example), you need to run pachct auth login, then authenticate to Pachyderm with your mock User (username:admin, password: password), before you use pachctl.
You have deployed Pachyderm with Console # To connect to your new Pachyderm instance, run:
pachctl config import-kube local --overwrite pachctl config set active-context local Then run pachctl port-forward (Background this process in a new tab of your terminal).
Check that your cluster is up and running # pachctl version System Response:
COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 5. Connect to Console # To connect to your Console (Pachyderm UI):
Point your browser to http://localhost:4000 If Authentication is activated (When you deploy with an enterprise key already set, for example), you you will be prompted to authenticate: Use your mock User (username:admin, password: password). You are all set!
6. Try our beginner tutorial. # 7. NOTEBOOKS USERS: Install Pachyderm JupyterLab Mount Extension # Once your cluster is up and running, you can helm install JupyterHub on your Pachyderm cluster and experiment with your data in Pachyderm from your Notebook cells.
Check out our JupyterHub and Pachyderm Mount Extension page for installation instructions.
Use Pachyderm&rsquo;s default image and values.yaml jupyterhub-ext-values.yaml or follow the instructions to update your own.
‚ÑπÔ∏è Make sure to check our data science notebook examples running on Pachyderm, from a market sentiment NLP implementation using a FinBERT model to pipelines training a regression model on the Boston Housing Dataset.
"
112,RBAC,"Pachyderm has support for Kubernetes Role-Based Access Controls (RBAC), which is a default part of all Pachyderm deployments. In most use cases, Pachyderm sets all the RBAC permissions automatically. However, if you are deploying Pachyderm on a cluster that your company owns, security policies might not allow certain RBAC permissions by default. Therefore, you need to contact your Kubernetes administrator and provide the following list of required permissions:
Rules: []rbacv1.PolicyRule{{ APIGroups: []string{&#34;&#34;}, Verbs: []string{&#34;get&#34;, &#34;list&#34;, &#34;watch&#34;}, Resources: []string{&#34;nodes&#34;, &#34;pods&#34;, &#34;pods/log&#34;, &#34;endpoints&#34;}, }, { APIGroups: []string{&#34;&#34;}, Verbs: []string{&#34;get&#34;, &#34;list&#34;, &#34;watch&#34;, &#34;create&#34;, &#34;update&#34;, &#34;delete&#34;}, Resources: []string{&#34;replicationcontrollers&#34;, &#34;services&#34;}, }, { APIGroups: []string{&#34;&#34;}, Verbs: []string{&#34;get&#34;, &#34;list&#34;, &#34;watch&#34;, &#34;create&#34;, &#34;update&#34;, &#34;delete&#34;}, Resources: []string{&#34;secrets&#34;}, ResourceNames: []string{client.StorageSecretName}, }}, The following table explains how Pachyderm uses those permissions:
Permission Description Access to nodes Used for metrics reporting, disabling should not affect Pachyderm&rsquo;s operation. Access to pods, replica controllers, and services Pachyderm uses this permission to monitor the created pipelines. The permissions related to replicationcontrollers and services are used in the setup and deletion of pipelines. Each pipeline has its own RC and service in addition to the pods. Access to secrets Required to give various kinds of credentials to pipelines, including storage credentials to access S3 or other object storage backends, Docker credentials to pull from a private registry, and others. RBAC and DNS # In older Kubernetes versions, kube-dns did not work properly with RBAC. To check if your cluster is affected by this issue, run:
kubectl get all --namespace=kube-system System response:
NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 1 1 1 0 3m NAME DESIRED CURRENT READY AGE rs/kube-dns-86f6f55dd5 1 1 0 3m NAME READY STATUS RESTARTS AGE po/kube-addon-manager-oryx 1/1 Running 0 3m po/kube-dns-86f6f55dd5-xksnb 2/3 Running 4 3m po/kubernetes-console-bzjjh 1/1 Running 0 3m po/storage-provisioner 1/1 Running 0 3m NAME DESIRED CURRENT READY AGE rc/kubernetes-console 1 1 1 3m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP 3m svc/kubernetes-console NodePort 10.97.194.16 &lt;none&gt; 80:30000/TCP 3m In the output above, po/kubernetes-console-bzjjh has only two out of three pods ready and has restarted four times. To fix this issue, run:
kubectl -n kube-system create sa kube-dns kubectl -n kube-system patch deploy/kube-dns -p &#39;{&#34;spec&#34;: {&#34;template&#34;: {&#34;spec&#34;: {&#34;serviceAccountName&#34;: &#34;kube-dns&#34;}}}}&#39; These commands enforce kube-dns to use the appropriate ServiceAccount. Kubernetes has created the ServiceAccount, but does not use it until you run the above commands.
Resolving RBAC Permissions on GKE # When you deploy Pachyderm on GKE, you might see the following error:
Error from server (Forbidden): error when creating &#34;STDIN&#34;: clusterroles.rbac.authorization.k8s.io &#34;pachyderm&#34; is forbidden: attempt to grant extra privileges: To fix this issue, run the following command and redeploy Pachyderm:
kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account) "
113,Set Up AWS Secret Manager,"For production environments, we highly recommend securing and centralizing the storage and management of your secrets (database access, root token, enterprise key, etc&hellip;) in AWS Secrets Manager, then allow your EKS cluster to retrieve those secrets using fine-grained IAM policies.
This section will walk you through the steps to enable your EKS cluster to retrieve secrets from AWS Secrets Manager.
1. Prerequisites # Note that the following steps start right after installing your EKS cluster. For informations on how to set your cluster up in production, refer to the deploy Kubernetes section of our deployment instructions on AWS.
2. Install The AWS Secrets and Configuration Provider (ASCP) # To retrieve your secrets through your workloads running on your cluster, you will first need to install:
A Secrets Store CSI driver AWS Secrets Manager and Config Provider ‚ö†Ô∏è The ASCP works with Amazon Elastic Kubernetes Service (Amazon EKS) 1.17+.
Install the Secrets Store CSI Driver # Deploy the Secrets Store CSI driver by following the installation steps.
üí° Make sure to enable the Sync as Kubernetes Secret feature explicitly by setting the helm parameter syncSecret.enabled to true.
‚ÑπÔ∏è helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts helm install csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver --namespace kube-system --set syncSecret.enabled=true Install the AWS Provider # AWS provider for the Secrets Store CSI Driver allows you to make secrets stored in Secrets Manager appear as files mounted in Kubernetes pods.
‚ÑπÔ∏è kubectl apply -f https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml 3. Store Pachyderm&rsquo;s Secrets in Secrets Manager # In your Secret Manager Console, click on Store a new secret, select the Other type of Secret (for generic secrets), provide the following Key/Value pairs, then choose a secret name.
Secret Key Description Value root_token Root clusterAdmin of your cluster Any postgresql_password Password to your database Any OAUTH_CLIENT_SECRET Oauth client secret for Console Required if you set an Enterprise key Any enterprise_license_key Your enterprise license Your enterprise License key pachd_oauth_client_secret Oauth client secret for pachd Any enterprise_secret Needed if you connect to an enterprise server Any Create your secret, then retrieve its arn. It will be needed in the next phase.
4- Grant Your EKS Cluster Access To Your Secrets Manager # Your cluster has an OpenID Connect issuer URL associated with it. To use IAM roles for service accounts, an IAM OIDC provider must exist for your cluster.
Create an IAM OIDC Provider # Before granting your EKS pods the proper permissions to access your secrets, you need to create an IAM OIDC provider for your cluster or retrieve the arn of your provider if you already have one created.
Follow the steps in AWS user guide
Example # eksctl utils associate-iam-oidc-provider --cluster=&#34;&lt;cluster-name&gt;&#34; Create An IAM Policy That Grants Read Access To Your Secret # Create a new Policy from your IAM Console Select the JSON tab. Copy/Paste the following text in the JSON tab { &#34;Version&#34;: &#34;2012-10-17&#34;, &#34;Statement&#34;: [ { &#34;Effect&#34;: &#34;Allow&#34;, &#34;Action&#34;: [ &#34;secretsmanager:GetSecretValue&#34;, &#34;secretsmanager:DescribeSecret&#34;, ], &#34;Resource&#34;: [ &lt;!-- Copy the arn of your secret HERE - see example below&gt; &#34;arn:aws:secretsmanager:&lt;region&gt;:&lt;account&gt;„äôÔ∏è&lt;your secret name&gt;&#34; ] } ] } This policy limits the access to the secrets that your EKS cluster needs to access.
Attach Your Policy To An IAM Role and The Role To Your Service Account # Create an IAM role and attach the IAM policy that you specified to it. The role is associated with a Kubernetes service account created in the namespace that you specify (your cluster&rsquo;s) and annotated with eks.amazonaws.com/role-arn:arn:aws:iam::111122223333:role/my-role-name.
Example # eksctl create iamserviceaccount \ --name &#34;&lt;my-service-account&gt;&#34; \ --cluster &#34;&lt;my-cluster&gt;&#34; \ --attach-policy-arn \ &#34;&lt;Copy the arn of your policy HERE&gt;&#34; \ --approve \ --override-existing-serviceaccounts 5. Mount Your Secrets In Your EKS Cluster # To show secrets in EKS as though they are files on the filesystem, you need to create a SecretProviderClass YAML file that contains information about your secrets as well as information on how to display them in the EKS pod. Use the file provided below and run kubectl apply -f yoursecretclass.yaml.
The SecretProviderClass must be in the same namespace as the EKS cluster.
metadata: # Insert your secret name name: pach-secrets spec: provider: aws parameters: objects: | - objectName: &#34;pach-secrets&#34; objectType: &#34;secretsmanager&#34; jmesPath: - path: root_token objectAlias: root-token - path: postgresql_password objectAlias: postgresql-password - path: OAUTH_CLIENT_SECRET objectAlias: OAUTH_CLIENT_SECRET - path: enterprise_license_key objectAlias: enterprise-license-key - path: pachd_oauth_client_secret	objectAlias: pachd-oauth-client-secret - path: enterprise_secret objectAlias: enterprise-secret secretObjects: - data: - key: root-token objectName: root-token secretName: root-token type: Opaque - data: - key: postgresql-password objectName: postgresql-password secretName: postgresql-password type: Opaque - data: - key: OAUTH_CLIENT_SECRET objectName: OAUTH_CLIENT_SECRET secretName: console-oauth-client-secret type: Opaque - data: - key: enterprise-license-key objectName: enterprise-license-key secretName: enterprise-license-key type: Opaque - data: - key: pachd-oauth-client-secret	objectName: pachd-oauth-client-secret	secretName: pachd-oauth-client-secret type: Opaque - data: - key: enterprise-secret objectName: enterprise-secret secretName: enterprise-secret type: Opaque 6. Create A Syncer Pod # Once your secret class is configured, a pod needs to request the class to trigger the CSI driver and retrieve the secrets in Kubernetes. Update the file below with your serviceAccountName and secretProviderClass before you run a kubectl apply -f syncerpod.yaml
apiVersion: v1 kind: Pod metadata: name: secret-syncer spec: containers: - name: secret-syncer image: k8s.gcr.io/pause volumeMounts: - name: secrets-store-inline mountPath: &#34;/mnt/secrets-store&#34; readOnly: true terminationGracePeriodSeconds: 3 serviceAccountName: &#34;&lt;Insert your service account name&gt;&#34; volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: # Insert the name of your Secret Provider secretProviderClass: &#34;pach-secrets&#34; Run a quick kubectl get all to check on your new pod.
8. Update Your Secrets In Pachyderm Values.YAML # Finally, using the secretName(s) of your SecretProviderClass above, update Pachyderm&rsquo;s values.YAML with the list of secrets you will be needing.
Choose the ones that apply to your use case.
deployTarget: LOCAL global: postgresql: postgresqlExistingSecretName: postgresql-password postgresqlExistingSecretKey: postgresql-password console: enabled: true config: oauthClientSecretSecretName: console-oauth-client-secret pachd: rootTokenSecretName: root-token enterpriseSecretSecretName: enterprise-secret oauthClientSecretSecretName: pachd-oauth-client-secret	enterpriseLicenseKeySecretName: enterprise-license-key activateEnterprise: true Your Secrets Manager is now configured to provide credential values to your cluster, you can go back to your installation of Pachyderm instructions.
"
114,Upgrade to Embedded Proxy,"Pachyderm ships with an embedded proxy that exposes one external TCP port (:80 or :443 if TLS is enabled) for all access, including: GRPCs, HTTP/S, s3 Gateway, OIDC, and Dex. Switching to using the embedded proxy improves your deployment&rsquo;s security posture because the proxy is hardened against malicious traffic and writes out extensive audit logs for all requests to Pachyderm.
Before You Start # This guide assumes that:
You have Pachyderm already set up with Enterprise Server, Authentication and an IdP Connector. You are upgrading to from &lt; 2.5.0 to 2.5.0 or newer. You are currently using pachd.externalService. You have a DNS set up or Load Balance IP Address for the proxy.host attribute. How to Upgrade to Embedded Proxy # Update your helm values.yaml file to include the following proxy settings: proxy: enabled: true # host can be &#34;http://&lt;Enterprise-server-external-IP-or-DNS&gt;&#34; or the value of proxy.service.type.loadBalancerIP host: 192.168.1.70 service: # type can also be NodePort type: LoadBalancer # loadBalancerIP can be left blank if you don&#39;t know the provisioned IP. loadBalancerIP: # legacyPorts are only needed for compatibility with your existing configuration. This is not needed for a fresh install where proxy is enabled. legacyPorts: grpc: 30650 s3gateway: 30650 oidcPort: 0 identityPort: 0 Remove the pachd.externalService section. Upgrade your cluster: helm repo update helm upgrade pachyderm pachyderm/pachyderm -f values.yml Connect to your cluster: pachctl connect grpc://localhost:80 "
115,Manage,"This section describes main managing operations and administrative tasks that you might need to perform throughout the lifetime of your cluster.
"
116,Backup & Restore Your Cluster,"This page will walk you through the main steps required to manually back up and restore the state of a Pachyderm cluster in production.
Details on how to perform those steps might vary depending on your infrastructure and cloud provider / on-premises setup.
Refer to your provider&rsquo;s documentation.
Overview # Pachyderm state is stored in two main places (See our high-level architecture diagram):
an object-store holding Pachyderm&rsquo;s data. a PostgreSQL instance made up of one or two databases: pachyderm holding Pachyderm&rsquo;s metadata and dex holding authentication data. Backing up a Pachyderm cluster involves snapshotting both the object store and the PostgreSQL database(s) (see above), in a consistent state, at a given point in time.
Restoring it involves re-populating the database(s) and the object store using those backups, then recreating a Pachyderm cluster.
‚ÑπÔ∏è Make sure that you have a bucket for backup use, separate from the object store used by your cluster. Depending on the reasons behind your cluster recovery, you might choose to use an existing vs. a new instance of PostgreSQL and/or the object store. Manual Back Up Of A Pachyderm Cluster # Before any manual backup:
Make sure to retain a copy of the Helm values used to deploy your cluster. Then, suspend any state-mutating operations. ‚ÑπÔ∏è Backups incur downtime until operations are resumed. Operational best practices include notifying Pachyderm users of the outage and providing an estimated time when downtime will cease. Downtime duration is a function of the size of the data be to backed up and the networks involved; Testing before going into production and monitoring backup times on an ongoing basis might help make accurate predictions. Suspend Operations # Pause any external automated process ingressing data to Pachyderm input repos, or queue/divert those as they will fail to connect to the cluster while the backup occurs.
Suspend all mutation of state by scaling pachd and the worker pods down:
‚ö†Ô∏è Before starting, make sure that your context points to the server you want to pause by running pachctl config get active-context. Find more information on how to set your context in our deployment section.
To pause Pachyderm:
If you are an Enterprise user: Run the pachctl enterprise pause command.
Alternatively, you can use kubectl:
Before starting, make sure that kubectl points to the right cluster. Run kubectl config get-contexts to list all available clusters and contexts (the current context is marked with a *), then kubectl config use-context &lt;your-context-name&gt; to set the proper active context.
kubectl scale deployment pachd --replicas 0 kubectl scale rc --replicas 0 -l suite=pachyderm,component=worker Note that it takes some time for scaling down to take effect;
Run the watch command to monitor the state of pachd and worker pods terminating:
watch -n 5 kubectl get pods Back Up The Databases And The Object Store # This step is specific to your database and object store hosting.
If your PostgreSQL instance is solely dedicated to Pachyderm, you can use PostgreSQL&rsquo;s tools, like pg_dumpall, to dump your entire PostgreSQL state.
Alternatively, you can use targeted pg_dump commands to dump the pachyderm and dex databases, or use your Cloud Provider&rsquo;s backup product.
In any case, make sure to use TLS. Note that if you are using a cloud provider, you might choose to use the provider‚Äôs method of making PostgreSQL backups.
‚ö†Ô∏è A production setting of Pachyderm implies that you are running a managed PostgreSQL instance.
üìñ PostgreSQL on AWS RDS backup GCP Cloud SQL backup Azure Database for PostgreSQL backup For on-premises Kubernetes deployments, check the vendor documentation for your on-premises PostgreSQL for details on backing up and restoring your databases.
To back up the object store, you can either download all objects or use the object store provider‚Äôs backup method.
The latter is preferable since it will typically not incur egress costs. üìñ AWS backup for S3 GCP Cloud storage bucket backup Azure blob backup For on-premises Kubernetes deployments, check the vendor documentation for your on-premises object store for details on backing up and restoring a bucket.
Resuming operations # Once your backup is completed, resume your normal operations by scaling pachd back up. It will take care of restoring the worker pods:
Enterprise users: run pachctl enterprise unpause.
Alternatively, if you used kubectl:
kubectl scale deployment pachd --replicas 1 Restore Pachyderm # There are two primary use cases for restoring a cluster:
Your data have been corrupted, preventing your cluster from functioning correctly. You want the same version of Pachyderm re-installed on the latest uncorrupted data set. You have upgraded a cluster and are encountering problems. You decide to uninstall the current version and restore the latest backup of a previous version of Pachyderm. Depending on your scenario, pick all or a subset of the following steps:
Populate new pachyderm and dex (if required) databases on your PostgreSQL instance Populate a new bucket or use the backed-up object-store (note that, in that case, it will no longer be a backup) Create a new empty Kubernetes cluster and give it access to your databases and bucket Deploy Pachyderm into your new cluster üìñ Find the detailed installations instructions of your PostgreSQL instance, bucket, Kubernetes cluster, permissions setup, and Pachyderm deployment for each Cloud Provider in the Deploy section of our Documentation
Restore The Databases And Objects # Restore PostgreSQL backups into your new databases using the appropriate method (this is most straightforward when using a cloud provider). Copy the objects from the backed-up object store to your new bucket or re-use your backup. Deploy Pachyderm Into The New Cluster # Finally, update the copy of your original Helm values to point Pachyderm to the new databases and the new object store, then use Helm to install Pachyderm into the new cluster.
üìñ The values needing an update and deployment instructions are detailed in the Chapter 6 of all our cloud installation pages. For example, in the case of GCP, check the deploy Pachyderm chapter
Connect &lsquo;pachctl&rsquo; To Your Restored Cluster # &hellip;and check that your cluster is up and running.
Backup/Restore A Stand-Alone Enterprise Server # Backing up / restoring an Enterprise Server is similar to the back up / restore of a regular cluster (see above), with two slight variations:
The name of its Kubernetes deployment is pach-enterprise versus pachd in the case of a regular cluster. The Enterprise Server does not use an Object Store. An Enterprise server only requires a dex database. Backup A Standalone Enterprise Server # ‚ö†Ô∏è Make sure that pachctl and kubectl are pointing to the right cluster. Check your Enterprise Server context: pachctl config get active-enterprise-context, or pachctl config set active-enterprise-context &lt;my-enterprise-context-name&gt; --overwrite to set it.
Pause the Enterprise Server like you would pause a regular cluster by running pachctl enterprise pause (Enterprise users), or using kubectl. ‚ÑπÔ∏è kubectl users: There is a difference with the pause of a regular cluster. The deployment of the enterprise server is named pach-enterprise; therefore, the first command should be:
kubectl scale deployment pach-enterprise --replicas 0 There is no need to pause all the Pachyderm clusters registered to the Enterprise Server to backup the enterprise server; however, pausing the Enterprise server will result in your clusters becoming unavailable.
As a reminder, the Enterprise Server does not use any object-store. Therefore, the backup of the Enterprise Server only consists in backing up the database dex.
Resume the operations on your Enterprise Server by running pachctl enterprise unpause (Enterprise users) to scale the pach-enterprise deployment back up. Alternatively, if you used kubectl, run:
kubectl scale deployment pach-enterprise --replicas 1 Restore An Enterprise Server # Follow the steps above while skipping all tasks related to creating and populating a new object-store.
Once your cluster is up and running, check that all your clusters are automatically registered with your new Enterprise Server.
Additional Info # For additional questions about backup / restore, you can post them in the community #help channel on Slack, or reach out to your TAM if you are an Enterprise customer.
"
117,Disable Usage Metrics,"Pachyderm automatically collects and reports anonymous usage metrics. These metrics help the Pachyderm team understand how people use Pachyderm to make it better. If you want opt out of anonymous metrics collection, disable them by setting the METRICS environment variable to false in the pachd container.
"
118,Manage Cluster Access,"Pachyderm contexts enable you to store configuration parameters for multiple Pachyderm clusters in a single configuration file saved at ~/.pachyderm/config.json. This file stores the information about all Pachyderm clusters that you have deployed from your machine locally or on a remote server.
For example, if you have a cluster that is deployed locally in minikube and another one deployed on Amazon EKS, configurations for these clusters are stored in that config.json file. By default, all local cluster configurations have the local prefix. If you have multiple local clusters, Pachyderm adds a consecutive number to the local prefix of each cluster.
The following text is an example of a Pachyderm config.json file:
{ &#34;user_id&#34;: &#34;b4fe4317-be21-4836-824f-6661c68b8fba&#34;, &#34;v2&#34;: { &#34;active_context&#34;: &#34;local-1&#34;, &#34;contexts&#34;: { &#34;default&#34;: {}, &#34;local&#34;: {}, &#34;local-1&#34;: {}, }, &#34;metrics&#34;: true } } View the Active Context # When you have multiple Pachyderm clusters, you can switch between them by setting the current context. The active context is the cluster that you interact with when you run pachctl commands.
To view active context, type:
View the active context:
pachctl config get active-context System response:
local-1 List all contexts and view the current context:
pachctl config list context System response:
ACTIVE NAME default local * local-1 The active context is marked with an asterisk.
Change the Active Context # To change the active context, type pachctl config set active-context &lt;name&gt;.
Also, you can set the PACH_CONTEXT environmental variable that overrides the active context.
Example:
export PACH_CONTEXT=local1 Create a New Context # When you deploy a new Pachyderm cluster, a new context that points to the new cluster is created automatically.
In addition, you can create a new context by providing your parameters through the standard input stream (stdin) in your terminal. Specify the parameters as a comma-separated list enclosed in curly brackets.
‚ÑπÔ∏è By default, the pachd port is 30650.
To create a new context with specific parameters, complete the following steps:
Create a new Pachyderm context with a specific pachd IP address and a client certificate:
echo &#39;{&#34;pachd_address&#34;:&#34;10.10.10.130:650&#34;, &#34;server_cas&#34;:&#34;insert your base 64 encoded key.pem&#34;}&#39; | pachctl config set context new-local System response:
Reading from stdin Verify your configuration by running the following command:
pachctl config get context new-local { &#34;pachd_address&#34;: &#34;10.10.10.130:650&#34;, &#34;server_cas&#34;: &#34;insert your base 64 encoded key.pem&#34; } Update an Existing Context # You can update an existing context with new parameters, such as a Pachyderm IP address, certificate authority (CA), and others. For the list of parameters, see Pachyderm Config Specification.
To update the Active Context, run the following commands:
Update the context with a new pachd address:
pachctl config update context local-1 --pachd-address 10.10.10.131 The pachctl config update command supports the --pachd-address flag only.
Verify that the context has been updated:
pachctl config get context local-1 System response:
{ &#34;pachd_address&#34;: &#34;10.10.10.131&#34; } Alternatively, you can update multiple properties by using an echo script:
echo &#39;{&#34;pachd_address&#34;:&#34;10.10.10.132&#34;, &#34;server_cas&#34;:&#34;insert your base 64 encoded key.pem&#34;}&#39; | pachctl config set context local-1 --overwrite System response:
Reading from stdin. Verify that the changes were applied:
pachctl config get context local-1 System response:
{ &#34;pachd_address&#34;: &#34;10.10.10.132&#34;, &#34;server_cas&#34;: &#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVEakNDQXZhZ0F3SUJBZ0lERDkyc01BMEdDU3FHU0liM0RRRUJDd1VBTUVVeEN6QUpCZ05WQkFZVEFrUkYKTVJVd0V3WURWUVFLREF4RUxWUnlkWE4wSUVkdFlrZ3hIekFkQmdOVkJBTU1Ga1F0VkZKVlUxUWdVbTl2ZENCRApRU0F6SURJd01UTXdIaGNOTVRNd09USXdNRGd5TlRVeFdoY05Namd3T1RJd01EZ3lOVFV4V2pCRk1Rc3dDUVlEClZRUUdFd0pFUlRFVk1CTUdBMVVFQ2d3TVJDMVVjblZ6ZENCSGJXSklNUjh3SFFZRFZRUUREQlpFTFZSU1ZWTlUKSUZKdmIzUWdRMEVnTXlBeU1ERXpNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQQp4SHRDa29JZjdPMVVtSTRTd01vSjM1TnVPcE5jRytRUWQ1NU9hWWhzOXVGcDh2YWJvbUd4dlFjZ2RKaGw4WXdtCkNNMm9OY3FBTnRGamJlaEVlb0xEYkY3ZXUrZzIwc1JvTm95Zk1yMkVJdURjd3U0UVJqbHRyNU01cm9mbXc3d0oKeVN4cloxdlptM1oxVEF2Z3U4WFh2RDU1OGwrKzBaQlgrYTcyWmw4eHY5TnRqNmU2U3ZNalpidTM3Nk1sMXdycQpXTGJ2aVByNmViSlNXTlh3ckl5aFVYUXBsYXBSTzVBeUE1OGNjblNRM2ozdFlkTGw0LzFrUitXNXQwcXA5eCt1CmxvWUVyQy9qcElGM3Qxb1cvOWdQUC9hM2VNeWtyL3BiUEJKYnFGS0pjdStJODlWRWdZYVZJNTk3M2J6Wk5POTgKbER5cXdFSEM0NTFRR3NEa0dTTDhzd0lEQVFBQm80SUJCVENDQVFFd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVVQNURJZmNjVmIvTWtqNm5ETDB1aUR5R3lMK2N3RGdZRFZSMFBBUUgvQkFRREFnRUdNSUcrCkJnTlZIUjhFZ2JZd2diTXdkS0J5b0hDR2JteGtZWEE2THk5a2FYSmxZM1J2Y25rdVpDMTBjblZ6ZEM1dVpYUXYKUTA0OVJDMVVVbFZUVkNVeU1GSnZiM1FsTWpCRFFTVXlNRE1sTWpBeU1ERXpMRTg5UkMxVWNuVnpkQ1V5TUVkdApZa2dzUXoxRVJUOWpaWEowYVdacFkyRjBaWEpsZG05allYUnBiMjVzYVhOME1EdWdPYUEzaGpWb2RIUndPaTh2ClkzSnNMbVF0ZEhKMWMzUXVibVYwTDJOeWJDOWtMWFJ5ZFhOMFgzSnZiM1JmWTJGZk0xOHlNREV6TG1OeWJEQU4KQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBRGxrT1dPUjBTQ05FenpRaHRad1VHcTJhUzdlemlHMWNxUmR3OENxZgpqWHY1ZTRYNnh6bm9FQWl3TlN0Znp3TFMwNXpJQ3g3dUJWU3VONU1FQ1gxc2o4SjB2UGdjbEw0eEFVQXQ4eVFnCnQ0UlZMRnpJOVhSS0VCbUxvOGZ0TmRZSlNOTU93TG81cUxCR0FyRGJ4b2had3I3OGU3RXJ6MzVpaDFXV3pBRnYKbTJjaGxUV0wrQkQ4Y1J1M1N6ZHBwanZXN0l2dXdiRHpKY21Qa24yaDZzUEtSTDhtcFhTU25PTjA2NTEwMmN0TgpoOWo4dEdsc2k2QkRCMkI0bCtuWmszekNScnliTjFLajdZbzhFNmw3VTB0Sm1oRUZMQXR1VnF3ZkxvSnM0R2xuCnRRNXRMZG5rd0JYeFAvb1ljdUVWYlNkYkxUQW9LNTlJbW1Rcm1lL3lkVWxmWEE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==&#34; } "
119,S3 Gateway,"Use the embedded S3 Gateway to send or receive data through the S3 protocol using object storage tooling such as Minio, boto3, or AWS s3 CLI. Operations available are similar to those officially documented for S3.
S3 Gateway Syntax # The S3 gateway presents each branch from every Pachyderm repository as an S3 bucket. Buckets are represented via [&lt;commit&gt;.]&lt;branch&gt;.&lt;repo&gt;.&lt;project&gt;, with the commit being optional.
The master.foo.bar bucket corresponds to the master branch of the repo foo within the bar project. The be97b64f110643389f171eb64697d4e1.master.foo.bar bucket corresponds to the commit be97b64f110643389f171eb64697d4e1 on the master branch of the foo repo within the bar project. If auth is enabled, credentials must be passed with each S3 gateway endpoint as mentioned in the S3 Client configuration steps.
Command Examples # The following command examples assume that you have upgraded to use the embedded proxy, which will become mandatory in future releases.
Put Data Into Pachyderm Repo # Tool: S3 Client Pachctl CLI aws --endpoint-url &lt;pachyderm-address&gt; s3 cp myfile.csv s3://master.foo.bar pachctl put file data@master:/ -f myfile.csv --project bar Retrieve Data From Pachyderm Repo # Tool: S3 Client Pachctl CLI aws --endpoint-url &lt;pachyderm-address&gt; s3 cp s3://master.foo.bar/myfile.csv pachctl get file data@master:/myfile.csv --project bar Port Forwarding # You can pachctl port-forward to access the s3 gateway through the localhost:30600 endpoint, however, the Kubernetes port forwarder incurs substantial overhead and does not recover well from broken connections.
"
120,AWS CLI," Before You Start # You must have the AWS CLI installed Configuration Steps # Install the AWS CLI as described in the AWS documentation.
Verify that the AWS CLI is installed:
aws --version Configure AWS CLI. Use the aws configure command to configure your credentials file: aws configure --profile &lt;name-your-profile&gt; # AWS Access Key ID: YOUR-PACHYDERM-AUTH-TOKEN # AWS Secret Access Key: YOUR-PACHYDERM-AUTH-TOKEN # Default region name: # Default output format [None]: ‚ÑπÔ∏è Note that the --profile flag (named profiles) is optional. If not used, your access information will be stored in the default profile.
To reference a given profile when using the S3 client, append --profile &lt;name-your-profile&gt; at the end of your command.
Verify Setup # To verify your setup, you can check the list of filesystem objects on the master branch of your repository.
aws --endpoint-url http://&lt;localhost_or_externalIP&gt;:30600/ s3 ls s3://master.&lt;repo&gt;.&lt;project&gt; "
121,Boto3," Before You Start # Before using Boto3, you need to set up authentication credentials for your AWS account using the AWS CLI as mentioned previously.
Configuration Steps # Then follow the Using boto documentation starting with importing boto3 in your python file and creating your S3 resources.
üìñ Find boto3 full documentation here.
"
122,Credentials," Before You Start # You must configure an S3 Client (Boto3, AWS, MinIO) You must have authentication enabled. How to Set Your Credentials # Auth: Enabled Disabled Run the following command: more ~/.pachyderm/config.json Search for your session token: &quot;session_token&quot;: &quot;your-session-token-value&quot;. Make sure to fill both fields Access Key ID and Secret Access Key with that same value. You must set ACCESS_KEY_ID and SECRET_ACCESS_KEY to any matching, non-empty string.
For example, you could set both values to &quot;x&quot;.
Robot Users # Depending on your use case, it might make sense to pass the credentials of a robot-user or another type of user altogether. Refer to the authentication section of the documentation for more RBAC information.
"
123,MinIO," Before You Start # Configuration Steps # Install the MinIO client as described on the MinIO download page.
Verify that MinIO components are successfully installed by running the following command:
minio version mc version System Response:
Version: 2019-07-11T19:31:28Z Release-tag: RELEASE.2019-07-11T19-31-28Z Commit-id: 31e5ac02bdbdbaf20a87683925041f406307cfb9 Set up the MinIO configuration file to use the S3 Gateway port 30600 for your host:
vi ~/.mc/config.json You should see a configuration similar to the following. For a minikube deployment, verify the local configuration:
&#34;local&#34;: { &#34;url&#34;: &#34;http://localhost:30600&#34;, &#34;accessKey&#34;: &#34;YOUR-PACHYDERM-AUTH-TOKEN&#34;, &#34;secretKey&#34;: &#34;YOUR-PACHYDERM-AUTH-TOKEN&#34;, &#34;api&#34;: &#34;S3v4&#34;, &#34;lookup&#34;: &#34;auto&#34; }, Both the access key and secret key should be set as mentioned in the # Set Your Credentails section of this page.
Verify Setup # Check the list of filesystem objects on the master branch of the repository raw_data.
mc ls local/master.&lt;repo&gt;.&lt;project&gt; "
124,Unsupported Operations,"Some of the S3 operations are not yet supported by Pachyderm. If you run any of these operations, Pachyderm returns a standard S3 NotImplemented error.
The S3 Gateway does not support the following S3 operations:
Accelerate Analytics Object copying. PFS supports this functionality through gRPC. CORS configuration Encryption HTML form uploads Inventory Legal holds Lifecycles Logging Metrics Notifications Object locks Payment requests Policies Public access blocks Regions Replication Retention policies Tagging Torrents Website configuration "
125,Sidecar S3 Gateway,"You can interact with input/output data through the S3 protocol using Pachyderm&rsquo;s S3-protocol-enabled pipelines.
About # Pachyderm&rsquo;s S3-protocol-enabled pipelines run a separate S3 gateway instance in a sidecar container within the pipeline-worker pod. Using this approach enables maintaining data provenance since the external code (e.g., within a Kubeflow pod) is executed in (and associated with) a Pachyderm job.
When enabled, input and output repositories are exposed as S3 Buckets via the S3 gateway sidecar instance.
Input Address: s3://&lt;input_repo_name&gt;. Output Address: s3://out Example with Kubeflow Pod # The following diagram shows communication between the S3 gateway deployed in a sidecar and the Kubeflow pod.
Configure an S3-enabled Pipeline # Open your pipeline spec. Add &quot;s3&quot;: true to input.pfs. Add &quot;s3_out&quot;: true to pipeline. Save your spec. Update your pipeline. Example Pipeline Spec # The following spec example reads files in the input bucket labresults and copies them in the pipeline&rsquo;s output bucket:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;s3_protocol_enabled_pipeline&#34; }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/&#34;, &#34;repo&#34;: &#34;labresults&#34;, &#34;name&#34;: &#34;labresults&#34;, &#34;s3&#34;: true } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;sh&#34; ], &#34;stdin&#34;: [ &#34;set -x &amp;&amp; mkdir -p /tmp/result &amp;&amp; aws --endpoint-url $S3_ENDPOINT s3 ls &amp;&amp; aws --endpoint-url $S3_ENDPOINT s3 cp s3://labresults/ /tmp/result/ --recursive &amp;&amp; aws --endpoint-url $S3_ENDPOINT s3 cp /tmp/result/ s3://out --recursive&#34; ], &#34;image&#34;: &#34;pachyderm/ubuntu-with-s3-clients:v0.0.1&#34; }, &#34;s3_out&#34;: true } User Code Requirements # Your user code is responsible for:
Providing its own S3 client package as part of the image (boto3) reading and writing in the S3 Buckets exposed to the pipeline Accessing the Sidecar # Use the S3_ENDPOINT environment variable to access the sidecar. No authentication is needed; you can only read the input bucket and write in the output bucket.
aws --endpoint-url $S3_ENDPOINT s3 cp /tmp/result/ s3://out --recursive If Authentication is Enabled # If auth is enabled on the Pachyderm cluster, credentials must be passed with each S3 gateway endpoint.
‚ö†Ô∏è The Access Key must equal the Secret Key.
Constraints # All files are processed as a single datum, meaning: The glob field in the pipeline must be set to &quot;glob&quot;: &quot;/&quot;. Already processed datums are not skipped. Only cross inputs are supported; join, group, and union are not supported. You can create a cross of an S3-enabled input with a non-S3 input; For a non-S3 input in such a cross, you can still specify a glob pattern. Input bucket(s) are read-only, and the output bucket is initially empty and writable. üìñ See Also:
Configure Environment Variables Pachyderm S3 Gateway Supported Operations Complete S3 Gateway API reference Pipeline Specification "
126,Storage Use Optimization,"This section discusses best practices for minimizing the space needed to store your Pachyderm data, increasing the performance of your data processing as related to data organization, and general good ideas when you are using Pachyderm to version/process your data.
Setting a root volume size # When planning and configuring your Pachyderm deployment, you need to make sure that each node&rsquo;s root volume is big enough to accommodate your total processing bandwidth. Specifically, you should calculate the bandwidth for your expected running jobs as follows:
(storage needed per datum) x (number of datums being processed simultaneously) / (number of nodes) Here, the storage needed per datum must be the storage needed for the largest datum you expect to process anywhere on your DAG plus the size of the output files that will be written for that datum. If your root volume size is not large enough, pipelines might fail when downloading the input. The pod would get evicted and rescheduled to a different node, where the same thing might happen (assuming that node had a similar volume).
‚ÑπÔ∏è See Also: Troubleshoot a pipeline
"
127,Uninstall Pachyderm, Uninstall Pachyderm # helm uninstall pachd kubectl delete pvc -l suite=pachyderm Uninstall Pachctl # brew uninstall @&lt;major&gt;.&lt;minor&gt; 
128,Upgrade Pachyderm,"Learn how to upgrade Pachyderm to access new features and performance enhancements.
Before You Start # Check the release notes before ugprading Back up your cluster Update your Helm chart values if applicable How to Upgrade Pachyderm # Run the following brew command or download &amp; install the latest release assets: brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.5 Upgrade Helm. Deploy Method: Production Local (Personal Machine) Note that the repo name input (pachyderm) must match the name you provided upon first install.
helm repo update helm upgrade pachyderm pachyderm/pachyderm -f my_pachyderm_values.yaml --set proxy.enabled=true --set proxy.service.type=LoadBalancer Note that the repo name input (pachyderm) must match the name you provided upon first install.
helm repo update helm upgrade pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Verify that the installation was successful by running pachctl version: pachctl version # COMPONENT VERSION # pachctl 2.5.1 # pachd 2.5.1 "
129,Use GPUs," To install Pachyderm on an NVIDIA DGX A100 box, skip to Pachyderm on NVIDIA DGX A100. If you already have a GPU enabled Kubernetes cluster, skip to Configure GPUs in Pipelines. Otherwise, read the following section. Set up a GPU enabled Kubernetes Cluster # Pachyderm leverages Kubernetes Device Plugins to let Kubernetes Pods access specialized hardware such as GPUs. For instructions on how to set up a GPU-enabled Kubernetes cluster through device plugins, see the Kubernetes documentation.
Pachyderm on NVIDIA DGX A100 # Let‚Äôs walk through the main steps allowing Pachyderm to leverage the AI performance of your DGX A100 GPUs.
üìñ Read about NVIDIA DGX A100&rsquo;s full userguide.
üí° Support for scheduling GPU workloads in Kubernetes requires a fair amount of trial and effort. To ease the process:
This setup page will walk you through very detailed installation steps to prepare your Kubernetes cluster. Take advantage of a user&rsquo;s past experience in this blog. Here is a quick recap of what will be needed:
Have a working Kubernetes control plane and worker nodes attached to your cluster. Install the DGX system in a hosting environment. Add the DGX to your K8s API server as a worker node. Now that the DGX is added to your API server, you can then proceed to:
Enable the GPU worker node in the Kubernetes cluster by installing NVIDIA&rsquo;s dependencies:
Dependencies packages and deployment methods may vary. The following list is not exhaustive and is intended to serve as a general guideline.
NVIDIA drivers
For complete instructions on setting up NVIDIA drivers, visit this quickstart guide or check this summary of the steps.
NVIDIA Container Toolkit (nvidia-docker2)
You may need to use different packages depending on your container engine.
NVIDIA Kubernetes Device Plugin
To use GPUs in Kubernetes, the NVIDIA Device Plugin is required. The NVIDIA Device Plugin is a daemonset that enumerates the number of GPUs on each node of the cluster and allows pods to be run on GPUs. Follow those steps to deploy the device plugin as a daemonset using helm.
Checkpoint: Run NVIDIA System Management Interface (nvidia-smi) on the CLI. It should return the list of NVIDIA GPUs.
Test a sample container with GPU:
To test whether CUDA jobs can be deployed, run a sample CUDA (vectorAdd) application.
For reference, find the pod spec below:
apiVersion: v1 kind: Pod metadata: name: gpu-test spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: &#34;nvidia/samples:vectoradd-cuda10.2&#34; resources: limits: nvidia.com/gpu: 1 Save it as gpu-pod.yaml then deploy the application:
kubectl apply -f gpu-pod.yaml Check the logs to make sure that the app completed successfully:
kubectl get pods gpu-test If the container above is scheduled successfully: install Pachyderm. You are ready to start leveraging NVIDIA&rsquo;s GPUs in your Pachyderm pipelines.
üí° Note that you have the option to use GPUs for compute-intensive workloads on:
Google Container Engine (GKE). Amazon Elastic Kubernetes Service (EKS). Azure Kubermnetes Service (AKS). Configure GPUs in Pipelines # Once your GPU-enabled Kubernetes cluster is set, you can request a GPU tier in your pipeline specifications by setting up GPU resource limits, along with its type and number of GPUs.
üí° By default, Pachyderm workers are spun up and wait for new input. That works great for pipelines that are processing a lot of new incoming commits. However, for lower volume of input commits, you could have your pipeline workers &rsquo;taking&rsquo; the GPU resource as far as k8s is concerned, but &lsquo;idling&rsquo; as far as you are concerned.
Make sure to set the autoscaling field to true so that if your pipeline is not getting used, the worker pods get spun down and the GPU resource freed. Additionally, specify how much of GPU your pipeline worker will need via the resource_requests fields in your pipeline specification with resource_requests &lt;= resource_limits. Below is an example of a pipeline spec for a GPU-enabled pipeline from our market sentiment analysis example:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;train_model&#34; }, &#34;description&#34;: &#34;Fine tune a BERT model for sentiment analysis on financial data.&#34;, &#34;input&#34;: { &#34;cross&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;dataset&#34;, &#34;glob&#34;: &#34;/&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;language_model&#34;, &#34;glob&#34;: &#34;/&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python&#34;, &#34;finbert_training.py&#34;, &#34;--lm_path&#34;, &#34;/pfs/language_model/&#34;, &#34;--cl_path&#34;, &#34;/pfs/out&#34;, &#34;--cl_data_path&#34;, &#34;/pfs/dataset/&#34; ], &#34;image&#34;: &#34;pachyderm/market_sentiment:dev0.25&#34; }, &#34;resource_limits&#34;: { &#34;gpu&#34;: { &#34;type&#34;: &#34;nvidia.com/gpu&#34;, &#34;number&#34;: 1 } }, &#34;resource_requests&#34;: { &#34;memory&#34;: &#34;4G&#34;, &#34;cpu&#34;: 1 } } "
130,Using the Pachyderm Shell,"The Pachyderm Shell is a special-purpose shell for Pachyderm that provides auto-suggesting as you type. New Pachyderm users will find this user-friendly shell especially appealing as it helps to learn pachctl, type commands faster, and displays useful information about the objects you are interacting with. This new shell does not supersede the classic use of pachctl shell in your standard terminal, but is a compelling convenience for power users and beginners alike. If you prefer to use just pachctl, you can continue to do so.
To enter the Pachyderm Shell, type:
pachctl shell When you enter pachctl shell, your prompt changes to display your current Pachyderm context, as well as displays a list of available commands in a drop-down list.
To scroll through the list, press TAB and then use arrows to move up or down. Press SPACE to select a command.
When in the Pachyderm Shell, you do not need to prepend your commands with pachctl because Pachyderm does that for you automatically behind the scenes. For example, instead of running pachctl list repo, run list repo:
With nested commands, pachctl shell can do even more. For example, if you type list file &lt;repo&gt;@&lt;branch&gt;/, you can preview and select files from that branch:
Similarly, you can select a commit:
Exit the Pachyderm Shell # To exit the Pachyderm Shell, press CTRL-D or type exit.
Clearing Cached Completions # To optimize performance and achieve faster response time, the Pachyderm Shell caches completion results. You can clear this cache by pressing F5 forcing the Pachyderm Shell to send requests to the server for new completions.
Clearing the screen # To clear the screen, press CTRL-L.
Limitations # The Pachyderm Shell does not support standard UNIX commands or kubectl commands. To run them, exit the Pachyderm Shell or run the commands in a different terminal window.
"
131,Reference,"This section includes references to several important aspects of Pachyderm, including: the pachctl CLI, pipeline specification attributes, and helm chart configuration settings.
"
132,Pachctl auth," pachctl auth # Auth commands manage access to data in a Pachyderm cluster
Synopsis # Auth commands manage access to data in a Pachyderm cluster
Options # -h, --help help for auth Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
133,Pachctl auth activate," pachctl auth activate # Activate Pachyderm&rsquo;s auth system
Synopsis # Activate Pachyderm&rsquo;s auth system, and restrict access to existing data to the root user
pachctl auth activate [flags] Options # --client-id string The client ID for this pachd (default &#34;pachd&#34;) --enterprise Activate auth on the active enterprise context -h, --help help for activate --issuer string The issuer for the OIDC service (default &#34;http://pachd:1658/&#34;) --only-activate Activate auth without configuring the OIDC service --redirect string The redirect URL for the OIDC service (default &#34;http://localhost:30657/authorization-code/callback&#34;) --scopes strings Comma-separated list of scopes to request (default [email,profile,groups,openid]) --supply-root-token Prompt the user to input a root token on stdin, rather than generating a random one. --trusted-peers strings Comma-separated list of OIDC client IDs to trust Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
134,Pachctl auth check," pachctl auth check # Check whether a subject has a permission on a resource
Synopsis # Check whether a subject has a permission on a resource
Options # -h, --help help for check Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
135,Pachctl auth check project," pachctl auth check project # Check the permissions a user has on &lsquo;project&rsquo;
Synopsis # Check the permissions a user has on &lsquo;project&rsquo;
pachctl auth check project &lt;project&gt; [user] [flags] Options # -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
136,Pachctl auth check repo," pachctl auth check repo # Check the permissions a user has on &lsquo;repo&rsquo;
Synopsis # Check the permissions a user has on &lsquo;repo&rsquo;
pachctl auth check repo &lt;repo&gt; [&lt;user&gt;] [flags] Options # -h, --help help for repo --project string The project containing the repo. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
137,Pachctl auth deactivate," pachctl auth deactivate # Delete all ACLs, tokens, admins, IDP integrations and OIDC clients, and deactivate Pachyderm auth
Synopsis # Deactivate Pachyderm&rsquo;s auth and identity systems, which will delete ALL auth tokens, ACLs and admins, IDP integrations and OIDC clients, and expose all data in the cluster to any user with cluster access. Use with caution.
pachctl auth deactivate [flags] Options # --enterprise Deactivate auth on the active enterprise context -h, --help help for deactivate Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
138,Pachctl auth get config," pachctl auth get-config # Retrieve Pachyderm&rsquo;s current auth configuration
Synopsis # Retrieve Pachyderm&rsquo;s current auth configuration
pachctl auth get-config [flags] Options # --enterprise Get auth config for the active enterprise context -h, --help help for get-config -o, --output-format string output format (&#34;json&#34; or &#34;yaml&#34;) (default &#34;json&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
139,Pachctl auth get groups," pachctl auth get-groups # Get the list of groups a user belongs to
Synopsis # Get the list of groups a user belongs to. If no user is specified, the current user&rsquo;s groups are listed.
pachctl auth get-groups [username] [flags] Options # --enterprise Get group membership info from the enterprise server -h, --help help for get-groups Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
140,Pachctl auth get robot token," pachctl auth get-robot-token # Get an auth token for a robot user with the specified name.
Synopsis # Get an auth token for a robot user with the specified name.
pachctl auth get-robot-token [username] [flags] Options # --enterprise Get a robot token for the enterprise context -h, --help help for get-robot-token -q, --quiet if set, only print the resulting token (if successful). This is useful for scripting, as the output can be piped to use-auth-token --ttl string if set, the resulting auth token will have the given lifetime. If not set, the token does not expire. This flag should be a golang duration (e.g. &#34;30s&#34; or &#34;1h2m3s&#34;). Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
141,Pachctl auth get," pachctl auth get # Get the role bindings for a resource
Synopsis # Get the role bindings for a resource
Options # -h, --help help for get Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
142,Pachctl auth get cluster," pachctl auth get cluster # Get the role bindings for &lsquo;cluster&rsquo;
Synopsis # Get the role bindings for &lsquo;cluster&rsquo;
pachctl auth get cluster [flags] Options # -h, --help help for cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
143,Pachctl auth get enterprise," pachctl auth get enterprise # Get the role bindings for the enterprise server
Synopsis # Get the role bindings for the enterprise server
pachctl auth get enterprise [flags] Options # -h, --help help for enterprise Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
144,Pachctl auth get project," pachctl auth get project # Get the role bindings for &lsquo;project&rsquo;
Synopsis # Get the role bindings for &lsquo;project&rsquo;
pachctl auth get project &lt;project&gt; [flags] Options # -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
145,Pachctl auth get repo," pachctl auth get repo # Get the role bindings for &lsquo;repo&rsquo;
Synopsis # Get the role bindings for &lsquo;repo&rsquo;
pachctl auth get repo &lt;repo&gt; [flags] Options # -h, --help help for repo --project string The project containing the repo. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
146,Pachctl auth login," pachctl auth login # Log in to Pachyderm
Synopsis # Login to Pachyderm. Any resources that have been restricted to the account you have with your ID provider (e.g. GitHub, Okta) account will subsequently be accessible.
pachctl auth login [flags] Options # --enterprise Login for the active enterprise context -h, --help help for login -t, --id-token If set, read an ID token on stdin to authenticate the user -b, --no-browser If set, don&#39;t try to open a web browser Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
147,Pachctl auth logout," pachctl auth logout # Log out of Pachyderm by deleting your local credential
Synopsis # Log out of Pachyderm by deleting your local credential. Note that it&rsquo;s not necessary to log out before logging in with another account (simply run &lsquo;pachctl auth login&rsquo; twice) but &rsquo;logout&rsquo; can be useful on shared workstations.
pachctl auth logout [flags] Options # --enterprise Log out of the active enterprise context -h, --help help for logout Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
148,Pachctl auth revoke," pachctl auth revoke # Revoke a Pachyderm auth token
Synopsis # Revoke a Pachyderm auth token.
pachctl auth revoke [flags] Options # --enterprise Revoke an auth token (or all auth tokens minted for one user) on the enterprise server -h, --help help for revoke --token string Pachyderm auth token that should be revoked (one of --token or --user must be set) --user string User whose Pachyderm auth tokens should be revoked (one of --token or --user must be set) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
149,Pachctl auth roles for permission," pachctl auth roles-for-permission # List roles that grant the given permission
Synopsis # List roles that grant the given permission
pachctl auth roles-for-permission &lt;permission&gt; [flags] Options # -h, --help help for roles-for-permission Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
150,Pachctl auth rotate root token," pachctl auth rotate-root-token # Rotate the root user&rsquo;s auth token
Synopsis # Rotate the root user&rsquo;s auth token
pachctl auth rotate-root-token [flags] Options # -h, --help help for rotate-root-token --supply-token string An auth token to rotate to. If left blank, one will be auto-generated. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
151,Pachctl auth set config," pachctl auth set-config # Set Pachyderm&rsquo;s current auth configuration
Synopsis # Set Pachyderm&rsquo;s current auth configuration
pachctl auth set-config [flags] Options # --enterprise Set auth config for the active enterprise context -f, --file string input file (to use as the new config (default &#34;-&#34;) -h, --help help for set-config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
152,Pachctl auth set," pachctl auth set # Set the role bindings for a resource
Synopsis # Set the role bindings for a resource
Options # -h, --help help for set Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
153,Pachctl auth set cluster," pachctl auth set cluster # Set the roles that &lsquo;subject&rsquo; has on the &lsquo;cluster&rsquo;
Synopsis # Set the roles that &lsquo;subject&rsquo; has on the &lsquo;cluster&rsquo;
pachctl auth set cluster [role1,role2 | none ] subject [flags] Options # -h, --help help for cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
154,Pachctl auth set enterprise," pachctl auth set enterprise # Set the roles that &lsquo;subject&rsquo; has on the enterprise server
Synopsis # Set the roles that &lsquo;subject&rsquo; has on the enterprise server
pachctl auth set enterprise [role1,role2 | none ] subject [flags] Options # -h, --help help for enterprise Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
155,Pachctl auth set project," pachctl auth set project # Set the roles that &lsquo;subject&rsquo; has on &lsquo;project&rsquo;
Synopsis # Set the roles that &lsquo;subject&rsquo; has on &lsquo;project&rsquo;
pachctl auth set project &lt;project&gt; [role1,role2 | none ] &lt;subject&gt; [flags] Options # -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
156,Pachctl auth set repo," pachctl auth set repo # Set the roles that &lsquo;subject&rsquo; has on &lsquo;repo&rsquo;
Synopsis # Set the roles that &lsquo;subject&rsquo; has on &lsquo;repo&rsquo;
pachctl auth set repo &lt;repo&gt; [role1,role2 | none ] &lt;subject&gt; [flags] Options # -h, --help help for repo --project string The project containing the repo. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
157,Pachctl auth use auth token," pachctl auth use-auth-token # Read a Pachyderm auth token from stdin, and write it to the current user&rsquo;s Pachyderm config file
Synopsis # Read a Pachyderm auth token from stdin, and write it to the current user&rsquo;s Pachyderm config file
pachctl auth use-auth-token [flags] Options # --enterprise Use the token for the enterprise context -h, --help help for use-auth-token Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
158,Pachctl auth whoami," pachctl auth whoami # Print your Pachyderm identity
Synopsis # Print your Pachyderm identity.
pachctl auth whoami [flags] Options # --enterprise -h, --help help for whoami Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
159,Pachctl buildinfo," pachctl buildinfo # Print go buildinfo.
Synopsis # Print information about the build environment.
pachctl buildinfo [flags] Options # -h, --help help for buildinfo Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
160,Pachctl completion," pachctl completion # Print or install terminal completion code.
Synopsis # Print or install terminal completion code.
Options # -h, --help help for completion Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
161,Pachctl completion bash," pachctl completion bash # Print or install the bash completion code.
Synopsis # Print or install the bash completion code.
pachctl completion bash [flags] Options # -h, --help help for bash --install Install the completion. --path string Path to install the completions to. (default &#34;/etc/bash_completion.d/pachctl&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
162,Pachctl completion zsh," pachctl completion zsh # Print or install the zsh completion code.
Synopsis # Print or install the zsh completion code.
pachctl completion zsh [flags] Options # -h, --help help for zsh --install Install the completion. --path string Path to install the completions to. (default &#34;_pachctl&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
163,Pachctl config," pachctl config # Manages the pachyderm config.
Synopsis # Gets/sets pachyderm config values.
Options # -h, --help help for config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
164,Pachctl config delete," pachctl config delete # Commands for deleting pachyderm config values
Synopsis # Commands for deleting pachyderm config values
Options # -h, --help help for delete Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
165,Pachctl config delete context," pachctl config delete context # Deletes a context.
Synopsis # Deletes a context.
pachctl config delete context &lt;context&gt; [flags] Options # -h, --help help for context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
166,Pachctl config get," pachctl config get # Commands for getting pachyderm config values
Synopsis # Commands for getting pachyderm config values
Options # -h, --help help for get Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
167,Pachctl config get active context," pachctl config get active-context # Gets the currently active context.
Synopsis # Gets the currently active context.
pachctl config get active-context [flags] Options # -h, --help help for active-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
168,Pachctl config get active enterprise context," pachctl config get active-enterprise-context # Gets the currently active enterprise context.
Synopsis # Gets the currently active enterprise context.
pachctl config get active-enterprise-context [flags] Options # -h, --help help for active-enterprise-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
169,Pachctl config get context," pachctl config get context # Gets a context.
Synopsis # Gets the config of a context by its name.
pachctl config get context &lt;context&gt; [flags] Options # -h, --help help for context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
170,Pachctl config get metrics," pachctl config get metrics # Gets whether metrics are enabled.
Synopsis # Gets whether metrics are enabled.
pachctl config get metrics [flags] Options # -h, --help help for metrics Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
171,Pachctl config import kube," pachctl config import-kube # Import a kubernetes context as a Pachyderm context, and set the active Pachyderm context.
Synopsis # Import a kubernetes context as a Pachyderm context. By default the current kubernetes context is used.
pachctl config import-kube &lt;context&gt; [flags] Options # -e, --enterprise Configure an enterprise server context. -h, --help help for import-kube -k, --kubernetes string Specify the kubernetes context&#39;s values to import. -n, --namespace string Specify a namespace where Pachyderm is deployed. -o, --overwrite Overwrite a context if it already exists. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
172,Pachctl config list," pachctl config list # Commands for listing pachyderm config values
Synopsis # Commands for listing pachyderm config values
Options # -h, --help help for list Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
173,Pachctl config list context," pachctl config list context # Lists contexts.
Synopsis # Lists contexts.
pachctl config list context [flags] Options # -h, --help help for context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
174,Pachctl config set," pachctl config set # Commands for setting pachyderm config values
Synopsis # Commands for setting pachyderm config values
Options # -h, --help help for set Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
175,Pachctl config set active context," pachctl config set active-context # Sets the currently active context.
Synopsis # Sets the currently active context.
pachctl config set active-context &lt;context&gt; [flags] Options # -h, --help help for active-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
176,Pachctl config set active enterprise context," pachctl config set active-enterprise-context # Sets the currently active enterprise context.
Synopsis # Sets the currently active enterprise context.
pachctl config set active-enterprise-context &lt;context&gt; [flags] Options # -h, --help help for active-enterprise-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
177,Pachctl config set context," pachctl config set context # Set a context.
Synopsis # Set a context config from a given name and a JSON configuration file on stdin
pachctl config set context &lt;context&gt; [flags] Options # -h, --help help for context --overwrite Overwrite a context if it already exists. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
178,Pachctl config set metrics," pachctl config set metrics # Sets whether metrics are enabled.
Synopsis # Sets whether metrics are enabled.
pachctl config set metrics (true | false) [flags] Options # -h, --help help for metrics Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
179,Pachctl config update," pachctl config update # Commands for updating pachyderm config values
Synopsis # Commands for updating pachyderm config values
Options # -h, --help help for update Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
180,Pachctl config update context," pachctl config update context # Updates a context.
Synopsis # Updates an existing context config from a given name (or the currently-active context, if no name is given).
pachctl config update context [&lt;context&gt;] [flags] Options # --auth-info string Set a new k8s auth info. --cluster-name string Set a new cluster name. -h, --help help for context --namespace string Set a new namespace. --pachd-address string Set a new name pachd address. --project string Set a new project. --remove-cluster-deployment-id Remove the cluster deployment ID field, which will be repopulated on the next &#39;pachctl&#39; call using this context. --server-cas string Set new trusted CA certs. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
181,Pachctl connect," pachctl connect # Connect to a Pachyderm Cluster
Synopsis # Creates a Pachyderm context at the given address and sets it as active
pachctl connect &lt;address&gt; [flags] Options # -h, --help help for connect Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
182,Pachctl copy," pachctl copy # Copy a Pachyderm resource.
Synopsis # Copy a Pachyderm resource.
Options # -h, --help help for copy Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
183,Pachctl copy file," pachctl copy file # Copy files between pfs paths.
Synopsis # Copy files between pfs paths.
pachctl copy file &lt;src-repo&gt;@&lt;src-branch-or-commit&gt;:&lt;src-path&gt; &lt;dst-repo&gt;@&lt;dst-branch-or-commit&gt;:&lt;dst-path&gt; [flags] Examples # # copy between repos within the current project defined by the pachyderm context # defaults to the &#34;default&#34; project $ pachctl copy file srcRepo@master:/file destRepo@master:/file # copy within a specified project $ pachctl copy file srcRepo@master:/file destRepo@master:/file --project myProject # copy from the current project to a different project # here, srcRepo is in the current project, while destRepo is in myProject $ pachctl copy file srcRepo@master:/file destRepo@master:/file --dest-project myProject # copy from a different project to the current project # here, srcRepo is in myProject, while destRepo is in the current project $ pachctl copy file srcRepo@master:/file destRepo@master:/file --src-project myProject # copy between repos across two different projects # here, srcRepo is in project1, while destRepo is in project2 $ pachctl copy file srcRepo@master:/file destRepo@master:/file --src-project project1 --dest-project project2 Options # -a, --append Append to the existing content of the file, either from previous commits or previous calls to &#39;put file&#39; within this commit. --dest-project string Project in which the destination repo is located. This overrides --project. -h, --help help for file --project string Project in which both source and destination repos are located. (default &#34;joins&#34;) --src-project string Project in which the source repo is located. This overrides --project. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
184,Pachctl create," pachctl create # Create a new instance of a Pachyderm resource.
Synopsis # Create a new instance of a Pachyderm resource.
Options # -h, --help help for create Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
185,Pachctl create branch," pachctl create branch # Create a new branch, or update an existing branch, on a repo.
Synopsis # Create a new branch, or update an existing branch, on a repo, starting a commit on the branch will also create it, so there&rsquo;s often no need to call this.
pachctl create branch &lt;repo&gt;@&lt;branch&gt; [flags] Options # --head string The head of the newly created branch. Either pass the commit with format: &lt;branch-or-commit&gt;, or fully-qualified as &lt;repo&gt;@&lt;branch&gt;=&lt;id&gt; -h, --help help for branch --project string Project in which repo is located. (default &#34;joins&#34;) -p, --provenance []string The provenance for the branch. format: &lt;repo&gt;@&lt;branch&gt; (default []) -t, --trigger string The branch to trigger this branch on. --trigger-all Only trigger when all conditions are met, rather than when any are met. --trigger-commits int The number of commits to use in triggering. --trigger-cron string The cron spec to use in triggering. --trigger-size string The data size to use in triggering. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
186,Pachctl create pipeline," pachctl create pipeline # Create a new pipeline.
Synopsis # Create a new pipeline from a pipeline specification. For details on the format, see https://docs.pachyderm.com/latest/reference/pipeline_spec/.
pachctl create pipeline [flags] Options # --arg stringArray Top-level argument passed to the Jsonnet template in --jsonnet (which must be set if any --arg arguments are passed). Value must be of the form &#39;param=value&#39;. For multiple args, --arg may be set more than once. -f, --file string A JSON file (url or filepath) containing one or more pipelines. &#34;-&#34; reads from stdin (the default behavior). Exactly one of --file and --jsonnet must be set. -h, --help help for pipeline --jsonnet string BETA: A Jsonnet template file (url or filepath) for one or more pipelines. &#34;-&#34; reads from stdin. Exactly one of --file and --jsonnet must be set. Jsonnet templates must contain a top-level function; strings can be passed to this function with --arg (below) --project string The project in which to create the pipeline. (default &#34;joins&#34;) -p, --push-images If true, push local docker images into the docker registry. -r, --registry string The registry to push images to. (default &#34;index.docker.io&#34;) -u, --username string The username to push images as. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
187,Pachctl create project," pachctl create project # Create a new project.
Synopsis # Create a new project.
pachctl create project &lt;project&gt; [flags] Options # -d, --description string The description of the newly-created project. -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
188,Pachctl create repo," pachctl create repo # Create a new repo.
Synopsis # Create a new repo.
pachctl create repo &lt;repo&gt; [flags] Options # -d, --description string A description of the repo. -h, --help help for repo --project string The project to create the repo in. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
189,Pachctl create secret," pachctl create secret # Create a secret on the cluster.
Synopsis # Create a secret on the cluster.
pachctl create secret [flags] Options # -f, --file string File containing Kubernetes secret. -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
190,Pachctl debug," pachctl debug # Debug commands for analyzing a running cluster.
Synopsis # Debug commands for analyzing a running cluster.
Options # -h, --help help for debug Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
191,Pachctl debug analyze," pachctl debug analyze # Start a local pachd server to analyze a debug dump.
Synopsis # Start a local pachd server to analyze a debug dump.
pachctl debug analyze &lt;file&gt; [flags] Options # -h, --help help for analyze -p, --port int launch a debug server on the given port. If unset, choose a free port automatically Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
192,Pachctl debug binary," pachctl debug binary # Collect a set of binaries.
Synopsis # Collect a set of binaries.
pachctl debug binary &lt;file&gt; [flags] Options # -h, --help help for binary --pachd Only collect the binary from pachd. -p, --pipeline string Only collect the binary from the worker pods for the given pipeline. -w, --worker string Only collect the binary from the given worker pod. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
193,Pachctl debug dump," pachctl debug dump # Collect a standard set of debugging information.
Synopsis # Collect a standard set of debugging information.
pachctl debug dump &lt;file&gt; [flags] Options # --database Only collect the dump from pachd&#39;s database. -h, --help help for dump -l, --limit int Limit sets the limit for the number of commits / jobs that are returned for each repo / pipeline in the dump. --pachd Only collect the dump from pachd. -p, --pipeline string Only collect the dump from the worker pods for the given pipeline. --timeout duration Set an absolute timeout on the debug dump operation. (default 30m0s) -w, --worker string Only collect the dump from the given worker pod. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
194,Pachctl debug profile," pachctl debug profile # Collect a set of pprof profiles.
Synopsis # Collect a set of pprof profiles.
pachctl debug profile &lt;profile&gt; &lt;file&gt; [flags] Options # -d, --duration duration Duration to run a CPU profile for. (default 1m0s) -h, --help help for profile --pachd Only collect the profile from pachd. -p, --pipeline string Only collect the profile from the worker pods for the given pipeline. -w, --worker string Only collect the profile from the given worker pod. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
195,Pachctl delete," pachctl delete # Delete an existing Pachyderm resource.
Synopsis # Delete an existing Pachyderm resource.
Options # -h, --help help for delete Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
196,Pachctl delete all," pachctl delete all # Delete everything.
Synopsis # Delete all repos, commits, files, pipelines and jobs. This resets the cluster to its initial state.
pachctl delete all [flags] Options # -h, --help help for all Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
197,Pachctl delete branch," pachctl delete branch # Delete a branch
Synopsis # Delete a branch, while leaving the commits intact
pachctl delete branch &lt;repo&gt;@&lt;branch&gt; [flags] Options # -f, --force remove the branch regardless of errors; use with care -h, --help help for branch --project string Project in which repo is located. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
198,Pachctl delete commit," pachctl delete commit # Delete the sub-commits of a commit.
Synopsis # Delete the sub-commits of a commit. The data in the sub-commits will be lost. This operation is only supported if none of the sub-commits have children.
pachctl delete commit &lt;commit-id&gt; [flags] Options # -h, --help help for commit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
199,Pachctl delete file," pachctl delete file # Delete a file.
Synopsis # Delete a file.
pachctl delete file &lt;repo&gt;@&lt;branch-or-commit&gt;:&lt;path/in/pfs&gt; [flags] Options # -h, --help help for file --project string Project in which repo is located. (default &#34;joins&#34;) -r, --recursive Recursively delete the files in a directory. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
200,Pachctl delete job," pachctl delete job # Delete a job.
Synopsis # Delete a job.
pachctl delete job &lt;pipeline&gt;@&lt;job&gt; [flags] Options # -h, --help help for job --project string Project within which to delete job (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
201,Pachctl delete pipeline," pachctl delete pipeline # Delete a pipeline.
Synopsis # Delete a pipeline.
pachctl delete pipeline (&lt;pipeline&gt;|--all) [flags] Options # --all delete all pipelines --all-projects delete pipelines from all projects; only valid with --all -f, --force delete the pipeline regardless of errors; use with care -h, --help help for pipeline --keep-repo delete the pipeline, but keep the output repo data around (the pipeline cannot be recreated later with the same name unless the repo is deleted) --project string project containing project (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
202,Pachctl delete project," pachctl delete project # Delete a project.
Synopsis # Delete a project.
pachctl delete project &lt;project&gt; [flags] Options # -f, --force remove the project regardless of errors; use with care -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
203,Pachctl delete repo," pachctl delete repo # Delete a repo.
Synopsis # Delete a repo.
pachctl delete repo &lt;repo&gt; [flags] Options # --all remove all repos -A, --all-projects delete repos from all projects; only valid with --all -f, --force remove the repo regardless of errors; use with care -h, --help help for repo --project string project in which repo is located (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
204,Pachctl delete secret," pachctl delete secret # Delete a secret from the cluster.
Synopsis # Delete a secret from the cluster.
pachctl delete secret [flags] Options # -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
205,Pachctl delete transaction," pachctl delete transaction # Cancel and delete an existing transaction.
Synopsis # Cancel and delete an existing transaction.
pachctl delete transaction [&lt;transaction&gt;] [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
206,Pachctl diff," pachctl diff # Show the differences between two Pachyderm resources.
Synopsis # Show the differences between two Pachyderm resources.
Options # -h, --help help for diff Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
207,Pachctl diff file," pachctl diff file # Return a diff of two file trees stored in Pachyderm
Synopsis # Return a diff of two file trees stored in Pachyderm
pachctl diff file &lt;new-repo&gt;@&lt;new-branch-or-commit&gt;:&lt;new-path&gt; [&lt;old-repo&gt;@&lt;old-branch-or-commit&gt;:&lt;old-path&gt;] [flags] Examples # # Return the diff of the file &#34;path&#34; of the repo &#34;foo&#34; between the head of the # &#34;master&#34; branch and its parent. $ pachctl diff file foo@master:path # Return the diff between the master branches of repos foo and bar at paths # path1 and path2, respectively. $ pachctl diff file foo@master:path1 bar@master:path2 Options # --diff-command string Use a program other than git to diff files. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for file --name-only Show only the names of changed files. --no-pager Don&#39;t pipe output into a pager (i.e. less). --old-project string Project in which second, older repo is located. --project string Project in which first repo is located. (default &#34;joins&#34;) -s, --shallow Don&#39;t descend into sub directories. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
208,Pachctl draw," pachctl draw # Draw an ASCII representation of an existing Pachyderm resource.
Synopsis # Draw an ASCII representation of an existing Pachyderm resource.
Options # -h, --help help for draw Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
209,Pachctl draw pipeline," pachctl draw pipeline # Draw a DAG
Synopsis # Draw a DAG
pachctl draw pipeline [flags] Options # --box-width int Character width of each box in the DAG (default 11) -c, --commit string Commit at which you would to draw the DAG --edge-height int Number of vertical lines spanned by each edge (default 5) -h, --help help for pipeline --project string Project containing pipelines. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
210,Pachctl edit," pachctl edit # Edit the value of an existing Pachyderm resource.
Synopsis # Edit the value of an existing Pachyderm resource.
Options # -h, --help help for edit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
211,Pachctl edit pipeline," pachctl edit pipeline # Edit the manifest for a pipeline in your text editor.
Synopsis # Edit the manifest for a pipeline in your text editor.
pachctl edit pipeline &lt;pipeline&gt; [flags] Options # --editor string Editor to use for modifying the manifest. -h, --help help for pipeline -o, --output string Output format: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project of pipeline to edit. (default &#34;joins&#34;) --reprocess If true, reprocess datums that were already processed by previous version of the pipeline. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
212,Pachctl enterprise," pachctl enterprise # Enterprise commands enable Pachyderm Enterprise features
Synopsis # Enterprise commands enable Pachyderm Enterprise features
Options # -h, --help help for enterprise Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
213,Pachctl enterprise deactivate," pachctl enterprise deactivate # Deactivate the enterprise service
Synopsis # Deactivate the enterprise service
pachctl enterprise deactivate [flags] Options # -h, --help help for deactivate Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
214,Pachctl enterprise get state," pachctl enterprise get-state # Check whether the Pachyderm cluster has enterprise features activated
Synopsis # Check whether the Pachyderm cluster has enterprise features activated
pachctl enterprise get-state [flags] Options # --enterprise Activate auth on the active enterprise context -h, --help help for get-state Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
215,Pachctl enterprise heartbeat," pachctl enterprise heartbeat # Sync the enterprise state with the license server immediately.
Synopsis # Sync the enterprise state with the license server immediately.
pachctl enterprise heartbeat [flags] Options # --enterprise Make the enterprise server refresh its state -h, --help help for heartbeat Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
216,Pachctl enterprise pause status," pachctl enterprise pause-status # Get the pause status of the cluster.
Synopsis # Get the pause the cluster: normal, partially-paused or paused.
pachctl enterprise pause-status [flags] Options # -h, --help help for pause-status Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
217,Pachctl enterprise pause," pachctl enterprise pause # Pause the cluster.
Synopsis # Pause the cluster.
pachctl enterprise pause [flags] Options # -h, --help help for pause Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
218,Pachctl enterprise register," pachctl enterprise register # Register the cluster with an enterprise license server
Synopsis # Register the cluster with an enterprise license server
pachctl enterprise register [flags] Options # --cluster-deployment-id string the deployment id of the cluster being registered --enterprise-server-address string the address for the pachd to reach the enterprise server -h, --help help for register --id string the id for this cluster --pachd-address string the address for the enterprise server to reach this pachd --pachd-user-address string the address for a user to reach this pachd Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
219,Pachctl enterprise sync contexts," pachctl enterprise sync-contexts # Pull all available Pachyderm Cluster contexts into your pachctl config
Synopsis # Pull all available Pachyderm Cluster contexts into your pachctl config
pachctl enterprise sync-contexts [flags] Options # -h, --help help for sync-contexts Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
220,Pachctl enterprise unpause," pachctl enterprise unpause # Unpause the cluster.
Synopsis # Unpause the cluster.
pachctl enterprise unpause [flags] Options # -h, --help help for unpause Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
221,Pachctl exit," pachctl exit # Exit the pachctl shell.
Synopsis # Exit the pachctl shell.
pachctl exit [flags] Options # -h, --help help for exit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
222,Pachctl finish," pachctl finish # Finish a Pachyderm resource.
Synopsis # Finish a Pachyderm resource.
Options # -h, --help help for finish Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
223,Pachctl finish commit," pachctl finish commit # Finish a started commit.
Synopsis # Finish a started commit. Commit-id must be a writeable commit.
pachctl finish commit &lt;repo&gt;@&lt;branch-or-commit&gt; [flags] Options # --description string A description of this commit&#39;s contents (synonym for --message) -f, --force finish the commit even if it has provenance, which could break jobs; prefer &#39;stop job&#39; -h, --help help for commit -m, --message string A description of this commit&#39;s contents (overwrites any existing commit description) --project string Project in which repo is located. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
224,Pachctl finish transaction," pachctl finish transaction # Execute and clear the currently active transaction.
Synopsis # Execute and clear the currently active transaction.
pachctl finish transaction [&lt;transaction&gt;] [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
225,Pachctl fsck," pachctl fsck # Run a file system consistency check on pfs.
Synopsis # Run a file system consistency check on the pachyderm file system, ensuring the correct provenance relationships are satisfied.
pachctl fsck [flags] Options # -f, --fix Attempt to fix as many issues as possible. -h, --help help for fsck --project string Project in which repo is located. (default &#34;joins&#34;) --zombie string A single commit to check for zombie files --zombie-all Check all pipelines for zombie files: files corresponding to old inputs that were not properly deleted Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
226,Pachctl get," pachctl get # Get the raw data represented by a Pachyderm resource.
Synopsis # Get the raw data represented by a Pachyderm resource.
Options # -h, --help help for get Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
227,Pachctl get file," pachctl get file # Return the contents of a file.
Synopsis # Return the contents of a file.
pachctl get file &lt;repo&gt;@&lt;branch-or-commit&gt;:&lt;path/in/pfs&gt; [flags] Examples # # get a single file &#34;XXX&#34; on branch &#34;master&#34; in repo &#34;foo&#34; $ pachctl get file foo@master:XXX # get file &#34;XXX&#34; in the parent of the current head of branch &#34;master&#34; # in repo &#34;foo&#34; $ pachctl get file foo@master^:XXX # get file &#34;XXX&#34; in the grandparent of the current head of branch &#34;master&#34; # in repo &#34;foo&#34; $ pachctl get file foo@master^2:XXX # get file &#34;test[].txt&#34; on branch &#34;master&#34; in repo &#34;foo&#34; # the path is interpreted as a glob pattern: quote and protect regex characters $ pachctl get file &#39;foo@master:/test\[\].txt&#39; # get all files under the directory &#34;XXX&#34; on branch &#34;master&#34; in repo &#34;foo&#34; $ pachctl get file foo@master:XXX -r Options # -h, --help help for file --offset int The number of bytes in the file to skip ahead when reading. -o, --output string The path where data will be downloaded. --progress {true|false} Whether or not to print the progress bars. (default true) --project string Project in which repo is located. (default &#34;joins&#34;) -r, --recursive Download multiple files, or recursively download a directory. --retry {true|false} Whether to append the missing bytes to an existing file. No-op if the file doesn&#39;t exist. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
228,Pachctl glob," pachctl glob # Print a list of Pachyderm resources matching a glob pattern.
Synopsis # Print a list of Pachyderm resources matching a glob pattern.
Options # -h, --help help for glob Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
229,Pachctl glob file," pachctl glob file # Return files that match a glob pattern in a commit.
Synopsis # Return files that match a glob pattern in a commit (that is, match a glob pattern in a repo at the state represented by a commit). Glob patterns are documented here.
pachctl glob file &#34;&lt;repo&gt;@&lt;branch-or-commit&gt;:&lt;pattern&gt;&#34; [flags] Examples # # Return files in repo &#34;foo&#34; on branch &#34;master&#34; that start # with the character &#34;A&#34;. Note how the double quotation marks around the # parameter are necessary because otherwise your shell might interpret the &#34;*&#34;. $ pachctl glob file &#34;foo@master:A*&#34; # Return files in repo &#34;foo&#34; on branch &#34;master&#34; under directory &#34;data&#34;. $ pachctl glob file &#34;foo@master:data/*&#34; # If you only want to view all files on a given repo branch, use &#34;list file -f &lt;repo&gt;@&lt;branch&gt;&#34; instead. Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for file -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project in which repo is located. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
230,Pachctl idp," pachctl idp # Commands to manage identity provider integrations
Synopsis # Commands to manage identity provider integrations
Options # -h, --help help for idp Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
231,Pachctl idp create client," pachctl idp create-client # Create a new OIDC client.
Synopsis # Create a new OIDC client.
pachctl idp create-client [flags] Options # --config string The file to read the YAML-encoded client configuration from, or &#39;-&#39; for stdin. (default &#34;-&#34;) -h, --help help for create-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
232,Pachctl idp create connector," pachctl idp create-connector # Create a new identity provider connector.
Synopsis # Create a new identity provider connector.
pachctl idp create-connector [flags] Options # --config string The file to read the YAML-encoded connector configuration from, or &#39;-&#39; for stdin. (default &#34;-&#34;) -h, --help help for create-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
233,Pachctl idp delete client," pachctl idp delete-client # Delete an OIDC client.
Synopsis # Delete an OIDC client.
pachctl idp delete-client &lt;client ID&gt; [flags] Options # -h, --help help for delete-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
234,Pachctl idp delete connector," pachctl idp delete-connector # Delete an identity provider connector
Synopsis # Delete an identity provider connector
pachctl idp delete-connector [flags] Options # -h, --help help for delete-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
235,Pachctl idp get client," pachctl idp get-client # Get an OIDC client.
Synopsis # Get an OIDC client.
pachctl idp get-client &lt;client ID&gt; [flags] Options # -h, --help help for get-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
236,Pachctl idp get config," pachctl idp get-config # Get the identity server config
Synopsis # Get the identity server config
pachctl idp get-config [flags] Options # -h, --help help for get-config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
237,Pachctl idp get connector," pachctl idp get-connector # Get the config for an identity provider connector.
Synopsis # Get the config for an identity provider connector.
pachctl idp get-connector &lt;connector id&gt; [flags] Options # -h, --help help for get-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
238,Pachctl idp list client," pachctl idp list-client # List OIDC clients.
Synopsis # List OIDC clients.
pachctl idp list-client [flags] Options # -h, --help help for list-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
239,Pachctl idp list connector," pachctl idp list-connector # List identity provider connectors
Synopsis # List identity provider connectors
pachctl idp list-connector [flags] Options # -h, --help help for list-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
240,Pachctl idp set config," pachctl idp set-config # Set the identity server config
Synopsis # Set the identity server config
pachctl idp set-config [flags] Options # --config string The file to read the YAML-encoded configuration from, or &#39;-&#39; for stdin. (default &#34;-&#34;) -h, --help help for set-config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
241,Pachctl idp update client," pachctl idp update-client # Update an OIDC client.
Synopsis # Update an OIDC client.
pachctl idp update-client [flags] Options # --config string The file to read the YAML-encoded client configuration from, or &#39;-&#39; for stdin. (default &#34;-&#34;) -h, --help help for update-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
242,Pachctl idp update connector," pachctl idp update-connector # Update an existing identity provider connector.
Synopsis # Update an existing identity provider connector. Only fields which are specified are updated.
pachctl idp update-connector [flags] Options # --config string The file to read the YAML-encoded connector configuration from, or &#39;-&#39; for stdin. (default &#34;-&#34;) -h, --help help for update-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
243,Pachctl inspect," pachctl inspect # Show detailed information about a Pachyderm resource.
Synopsis # Show detailed information about a Pachyderm resource.
Options # -h, --help help for inspect Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
244,Pachctl inspect branch," pachctl inspect branch # Return info about a branch.
Synopsis # Return info about a branch.
pachctl inspect branch &lt;repo&gt;@&lt;branch&gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for branch -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project in which repo is located. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
245,Pachctl inspect cluster," pachctl inspect cluster # Returns info about the pachyderm cluster
Synopsis # Returns info about the pachyderm cluster
pachctl inspect cluster [flags] Options # -h, --help help for cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
246,Pachctl inspect commit," pachctl inspect commit # Return info about a commit.
Synopsis # Return info about a commit.
pachctl inspect commit &lt;repo&gt;@&lt;branch-or-commit&gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project in which repo is located. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
247,Pachctl inspect datum," pachctl inspect datum # Display detailed info about a single datum.
Synopsis # Display detailed info about a single datum. Requires the pipeline to have stats enabled.
pachctl inspect datum &lt;pipeline&gt;@&lt;job&gt; &lt;datum&gt; [flags] Options # -h, --help help for datum -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project containing the job (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
248,Pachctl inspect file," pachctl inspect file # Return info about a file.
Synopsis # Return info about a file.
pachctl inspect file &lt;repo&gt;@&lt;branch-or-commit&gt;:&lt;path/in/pfs&gt; [flags] Options # -h, --help help for file -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project in which repo is located. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
249,Pachctl inspect job," pachctl inspect job # Return info about a job.
Synopsis # Return info about a job.
pachctl inspect job &lt;pipeline&gt;@&lt;job&gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for job -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string project containing job (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
250,Pachctl inspect pipeline," pachctl inspect pipeline # Return info about a pipeline.
Synopsis # Return info about a pipeline.
pachctl inspect pipeline &lt;pipeline&gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for pipeline -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project of pipeline to inspect. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
251,Pachctl inspect project," pachctl inspect project # Inspect a project.
Synopsis # Inspect a project.
pachctl inspect project &lt;project&gt; [flags] Options # -h, --help help for project -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
252,Pachctl inspect repo," pachctl inspect repo # Return info about a repo.
Synopsis # Return info about a repo.
pachctl inspect repo &lt;repo&gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for repo -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project in which repo is located. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
253,Pachctl inspect secret," pachctl inspect secret # Inspect a secret from the cluster.
Synopsis # Inspect a secret from the cluster.
pachctl inspect secret [flags] Options # -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
254,Pachctl inspect transaction," pachctl inspect transaction # Print information about an open transaction.
Synopsis # Print information about an open transaction.
pachctl inspect transaction [&lt;transaction&gt;] [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for transaction -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
255,Pachctl kube events," pachctl kube-events # Return the kubernetes events.
Synopsis # Return the kubernetes events.
pachctl kube-events [flags] Options # -h, --help help for kube-events --raw Return log messages verbatim from server. --since string Return log messages more recent than &#34;since&#34;. (default &#34;0&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
256,Pachctl license," pachctl license # License commmands manage the Enterprise License service
Synopsis # License commands manage the Enterprise License service
Options # -h, --help help for license Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
257,Pachctl license activate," pachctl license activate # Activate the license server with an activation code
Synopsis # Activate the license server with an activation code
pachctl license activate [flags] Options # -h, --help help for activate --no-register Activate auth on the active enterprise context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
258,Pachctl license add cluster," pachctl license add-cluster # Register a new cluster with the license server.
Synopsis # Register a new cluster with the license server.
pachctl license add-cluster [flags] Options # --address string The host and port where the cluster can be reached -h, --help help for add-cluster --id string The id for the cluster to register --secret string The shared secret to use to authenticate this cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
259,Pachctl license delete all," pachctl license delete-all # Delete all data from the license server
Synopsis # Delete all data from the license server
pachctl license delete-all [flags] Options # -h, --help help for delete-all Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
260,Pachctl license delete cluster," pachctl license delete-cluster # Delete a cluster registered with the license server.
Synopsis # Delete a cluster registered with the license server.
pachctl license delete-cluster [flags] Options # -h, --help help for delete-cluster --id string The id for the cluster to delete Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
261,Pachctl license get state," pachctl license get-state # Get the configuration of the license service.
Synopsis # Get the configuration of the license service.
pachctl license get-state [flags] Options # -h, --help help for get-state Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
262,Pachctl license list clusters," pachctl license list-clusters # List clusters registered with the license server.
Synopsis # List clusters registered with the license server.
pachctl license list-clusters [flags] Options # -h, --help help for list-clusters Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
263,Pachctl license update cluster," pachctl license update-cluster # Update an existing cluster registered with the license server.
Synopsis # Update an existing cluster registered with the license server.
pachctl license update-cluster [flags] Options # --address string The host and port where the cluster can be reached by the enterprise server --cluster-deployment-id string The deployment id of the updated cluster -h, --help help for update-cluster --id string The id for the cluster to update --user-address string The host and port where the cluster can be reached by a user Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
264,Pachctl list," pachctl list # Print a list of Pachyderm resources of a specific type.
Synopsis # Print a list of Pachyderm resources of a specific type.
Options # -h, --help help for list Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
265,Pachctl list branch," pachctl list branch # Return all branches on a repo.
Synopsis # Return all branches on a repo.
pachctl list branch &lt;repo&gt; [flags] Options # -h, --help help for branch -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project in which repo is located. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
266,Pachctl list commit," pachctl list commit # Return a list of commits.
Synopsis # Return a list of commits, either across the entire pachyderm cluster or restricted to a single repo.
pachctl list commit [&lt;commit-id&gt;|&lt;repo&gt;[@&lt;branch-or-commit&gt;]] [flags] Examples # # return all commits $ pachctl list commit # return commits in repo &#34;foo&#34; $ pachctl list commit foo # return all sub-commits in a commit $ pachctl list commit &lt;commit-id&gt; # return commits in repo &#34;foo&#34; on branch &#34;master&#34; $ pachctl list commit foo@master # return the last 20 commits in repo &#34;foo&#34; on branch &#34;master&#34; $ pachctl list commit foo@master -n 20 # return commits in repo &#34;foo&#34; on branch &#34;master&#34; since commit XXX $ pachctl list commit foo@master --from XXX Options # --all return all types of commits, including aliases -x, --expand show one line for each sub-commmit and include more columns -f, --from string list all commits since this commit --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit -n, --number int list only this many commits; if set to zero, list all commits --origin string only return commits of a specific type -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project in which commit is located. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
267,Pachctl list datum," pachctl list datum # Return the datums in a job.
Synopsis # Return the datums in a job.
pachctl list datum &lt;pipeline&gt;@&lt;job&gt; [flags] Options # -f, --file string The JSON file containing the pipeline to list datums from, the pipeline need not exist -h, --help help for datum -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project containing the job (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
268,Pachctl list file," pachctl list file # Return the files in a directory.
Synopsis # Return the files in a directory.
pachctl list file &lt;repo&gt;@&lt;branch-or-commit&gt;[:&lt;path/in/pfs&gt;] [flags] Examples # # list top-level files on branch &#34;master&#34; in repo &#34;foo&#34; $ pachctl list file foo@master # list files under directory &#34;dir&#34; on branch &#34;master&#34; in repo &#34;foo&#34; $ pachctl list file foo@master:dir # list top-level files in the parent commit of the current head of &#34;master&#34; # in repo &#34;foo&#34; $ pachctl list file foo@master^ # list top-level files in the grandparent of the current head of &#34;master&#34; # in repo &#34;foo&#34; $ pachctl list file foo@master^2 # list file under directory &#34;dir[1]&#34; on branch &#34;master&#34; in repo &#34;foo&#34; # : quote and protect regex characters $ pachctl list file &#39;foo@master:dir\[1\]&#39; Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for file -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project in which repo is located. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
269,Pachctl list job," pachctl list job # Return info about jobs.
Synopsis # Return info about jobs.
pachctl list job [&lt;job-id&gt;] [flags] Examples # # Return a summary list of all jobs $ pachctl list job # Return all sub-jobs in a job $ pachctl list job &lt;job-id&gt; # Return all sub-jobs split across all pipelines $ pachctl list job --expand # Return only the sub-jobs from the most recent version of pipeline &#34;foo&#34; $ pachctl list job -p foo # Return all sub-jobs from all versions of pipeline &#34;foo&#34; $ pachctl list job -p foo --history all # Return all sub-jobs whose input commits include foo@XXX and bar@YYY $ pachctl list job -i foo@XXX -i bar@YYY # Return all sub-jobs in pipeline foo and whose input commits include bar@YYY $ pachctl list job -p foo -i bar@YYY Options # --all-projects Show jobs from all projects. -x, --expand Show one line for each sub-job and include more columns --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for job --history string Return jobs from historical versions of pipelines. (default &#34;none&#34;) -i, --input strings List jobs with a specific set of input commits. format: &lt;repo&gt;@&lt;branch-or-commit&gt; --no-pager Don&#39;t pipe output into a pager (i.e. less). -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) -p, --pipeline string Limit to jobs made by pipeline. --project string Limit to jobs in the project specified. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml --state stringArray Return only sub-jobs with the specified state. Can be repeated to include multiple states Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
270,Pachctl list pipeline," pachctl list pipeline # Return info about all pipelines.
Synopsis # Return info about all pipelines.
pachctl list pipeline [&lt;pipeline&gt;] [flags] Options # --all-projects Show pipelines form all projects. -c, --commit string List the pipelines as they existed at this commit. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for pipeline --history string Return revision history for pipelines. (default &#34;none&#34;) -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project containing pipelines. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml -s, --spec Output &#39;create pipeline&#39; compatibility specs. --state stringArray Return only pipelines with the specified state. Can be repeated to include multiple states Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
271,Pachctl list project," pachctl list project # Return all projects.
Synopsis # Return all projects.
pachctl list project &lt;repo&gt; [flags] Options # -h, --help help for project -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
272,Pachctl list repo," pachctl list repo # Return a list of repos.
Synopsis # Return a list of repos. By default, hide system repos like pipeline metadata
pachctl list repo [flags] Options # --all include system repos of all types -A, --all-projects show repos from all projects --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for repo -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string project in which repo is located (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml --type string only include repos of the given type (default &#34;user&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
273,Pachctl list secret," pachctl list secret # List all secrets from a namespace in the cluster.
Synopsis # List all secrets from a namespace in the cluster.
pachctl list secret [flags] Options # -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
274,Pachctl list transaction," pachctl list transaction # List transactions.
Synopsis # List transactions.
pachctl list transaction [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for transaction -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
275,Pachctl logs," pachctl logs # Return logs from a job.
Synopsis # Return logs from a job.
pachctl logs [--pipeline=&lt;pipeline&gt;|--job=&lt;pipeline&gt;@&lt;job&gt;] [--datum=&lt;datum&gt;] [flags] Examples # # Return logs emitted by recent jobs in the &#34;filter&#34; pipeline $ pachctl logs --pipeline=filter # Return logs emitted by the job aedfa12aedf $ pachctl logs --job=aedfa12aedf # Return logs emitted by the pipeline \&#34;filter\&#34; while processing /apple.txt and a file with the hash 123aef $ pachctl logs --pipeline=filter --inputs=/apple.txt,123aef Options # --datum string Filter for log lines for this datum (accepts datum ID) -f, --follow Follow logs as more are created. -h, --help help for logs --inputs string Filter for log lines generated while processing these files (accepts PFS paths or file hashes) -j, --job string Filter for log lines from this job (accepts job ID) --master Return log messages from the master process (pipeline must be set). -p, --pipeline string Filter the log for lines from this pipeline (accepts pipeline name) --project string Project containing the job. (default &#34;joins&#34;) --raw Return log messages verbatim from server. --since string Return log messages more recent than &#34;since&#34;. (default &#34;24h&#34;) -t, --tail int Lines of recent logs to display. --worker Return log messages from the worker process. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
276,Pachctl mount," pachctl mount # Mount pfs locally. This command blocks.
Synopsis # Mount pfs locally. This command blocks.
pachctl mount &lt;path/to/mount/point&gt; [flags] Options # -d, --debug Turn on debug messages. -h, --help help for mount --project string Project in which repo is located. (default &#34;default&#34;) -r, --repos []string Repos and branches / commits to mount, arguments should be of the form &#34;repo[@branch=commit][+w]&#34;, where the trailing flag &#34;+w&#34; indicates write. You can omit the branch when specifying a commit unless the same commit ID is on multiple branches in the repo. (default []) -w, --write Allow writing to pfs through the mount. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
277,Pachctl port forward," pachctl port-forward # Forward a port on the local machine to pachd. This command blocks.
Synopsis # Forward a port on the local machine to pachd. This command blocks.
pachctl port-forward [flags] Options # --console-port uint16 The local port to bind the console service to. (default 4000) --dex-port uint16 The local port to bind the identity service to. (default 30658) -h, --help help for port-forward --namespace string Kubernetes namespace Pachyderm is deployed in. --oidc-port uint16 The local port to bind pachd&#39;s OIDC callback to. (default 30657) -p, --port uint16 The local port to bind pachd to. (default 30650) --remote-console-port uint16 The remote port to bind the console service to. (default 4000) --remote-dex-port uint16 The local port to bind the identity service to. (default 1658) --remote-oidc-port uint16 The remote port that OIDC callback is bound to in the cluster. (default 1657) --remote-port uint16 The remote port that pachd is bound to in the cluster. (default 1650) --remote-s3gateway-port uint16 The remote port that the s3 gateway is bound to. (default 1600) -s, --s3gateway-port uint16 The local port to bind the s3gateway to. (default 30600) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
278,Pachctl put," pachctl put # Insert data into Pachyderm.
Synopsis # Insert data into Pachyderm.
Options # -h, --help help for put Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
279,Pachctl put file," pachctl put file # Put a file into the filesystem.
Synopsis # Put a file into the filesystem. This command supports a number of ways to insert data into PFS.
pachctl put file &lt;repo&gt;@&lt;branch-or-commit&gt;[:&lt;path/to/file&gt;] [flags] Examples # # Put data from stdin at repo@branch:/path $ echo &#34;data&#34; | pachctl put file repo@branch:/path # Put a file from the local filesystem at repo@branch:/file $ pachctl put file repo@branch -f file # Put a file from the local filesystem at repo@branch:/path $ pachctl put file repo@branch:/path -f file # Put the contents of a directory at repo@branch:/dir/file $ pachctl put file -r repo@branch -f dir # Put the contents of a directory at repo@branch:/path/file (without /dir) $ pachctl put file -r repo@branch:/path -f dir # Put the data from a URL at repo@branch:/example.png $ pachctl put file repo@branch -f http://host/example.png # Put the data from a URL at repo@branch:/dir/example.png $ pachctl put file repo@branch:/dir -f http://host/example.png # Put the data from an S3 bucket at repo@branch:/s3_object $ pachctl put file repo@branch -r -f s3://my_bucket # Put several files or URLs that are listed in file. # Files and URLs should be newline delimited. $ pachctl put file repo@branch -i file # Put several files or URLs that are listed at URL. # NOTE this URL can reference local files, so it could cause you to put sensitive # files into your Pachyderm cluster. $ pachctl put file repo@branch -i http://host/path Options # -a, --append Append to the existing content of the file, either from previous commits or previous calls to &#39;put file&#39; within this commit. --compress Compress data during upload. This parameter might help you upload your uncompressed data, such as CSV files, to Pachyderm faster. Use &#39;compress&#39; with caution, because if your data is already compressed, this parameter might slow down the upload speed instead of increasing. -f, --file strings The file to be put, it can be a local file or a URL. (default [-]) --full-path If true, use the entire path provided to -f as the target filename in PFS. By default only the base of the path is used. -h, --help help for file -i, --input-file string Read filepaths or URLs from a file. If - is used, paths are read from the standard input. -p, --parallelism int The maximum number of files that can be uploaded in parallel. (default 10) --progress Print progress bars. (default true) --project string Project in which repo is located. (default &#34;joins&#34;) -r, --recursive Recursively put the files in a directory. --untar If true, file(s) with the extension .tar are untarred and put as a separate file for each file within the tar stream(s). gzipped (.tar.gz or .tgz) tar file(s) are handled as well Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
280,Pachctl restart," pachctl restart # Cancel and restart an ongoing task.
Synopsis # Cancel and restart an ongoing task.
Options # -h, --help help for restart Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
281,Pachctl restart datum," pachctl restart datum # Restart a stuck datum during a currently running job.
Synopsis # Restart a stuck datum during a currently running job; does not solve failed datums. Optionally, you can configure a job to skip failed datums via the transform.err_cmd setting of your pipeline spec.
pachctl restart datum &lt;pipeline&gt;@&lt;job&gt; &lt;datum-path1&gt;,&lt;datum-path2&gt;,... [flags] Options # -h, --help help for datum --project string Project containing the datum job (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
282,Pachctl resume," pachctl resume # Resume a stopped task.
Synopsis # Resume a stopped task.
Options # -h, --help help for resume Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
283,Pachctl resume transaction," pachctl resume transaction # Set an existing transaction as active.
Synopsis # Set an existing transaction as active.
pachctl resume transaction &lt;transaction&gt; [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
284,Pachctl run," pachctl run # Manually run a Pachyderm resource.
Synopsis # Manually run a Pachyderm resource.
Options # -h, --help help for run Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
285,Pachctl run cron," pachctl run cron # Run an existing Pachyderm cron pipeline now
Synopsis # Run an existing Pachyderm cron pipeline now
pachctl run cron &lt;pipeline&gt; [flags] Examples # # Run a cron pipeline &#34;clock&#34; now $ pachctl run cron clock Options # -h, --help help for cron --project string Project containing pipeline. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
286,Pachctl run pfs load test," pachctl run pfs-load-test # Run a PFS load test.
Synopsis # Run a PFS load test.
pachctl run pfs-load-test &lt;spec-file&gt; [flags] Examples # Specification: -- CommitSpec -- count: int modifications: [ ModificationSpec ] fileSources: [ FileSourceSpec ] validator: ValidatorSpec -- ModificationSpec -- count: int putFile: PutFileSpec -- PutFileSpec -- count: int source: string -- FileSourceSpec -- name: string random: RandomFileSourceSpec -- RandomFileSourceSpec -- directory: RandomDirectorySpec sizes: [ SizeSpec ] incrementPath: bool -- RandomDirectorySpec -- depth: SizeSpec run: int -- SizeSpec -- min: int max: int prob: int [0, 100] -- ValidatorSpec -- frequency: FrequencySpec -- FrequencySpec -- count: int prob: int [0, 100] Example: count: 5 modifications: - count: 5 putFile: count: 5 source: &#34;random&#34; fileSources: - name: &#34;random&#34; random: directory: depth: 3 run: 3 size: - min: 1000 max: 10000 prob: 30 - min: 10000 max: 100000 prob: 30 - min: 1000000 max: 10000000 prob: 30 - min: 10000000 max: 100000000 prob: 10 validator: {} Options # -b, --branch string The branch to use for generating the load. -h, --help help for pfs-load-test --project string Project in which repo is located. (default &#34;joins&#34;) -s, --seed int The seed to use for generating the load. --state-id string The ID of the base state to use for the load. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
287,Pachctl run pps load test," pachctl run pps-load-test # Run a PPS load test.
Synopsis # Run a PPS load test.
pachctl run pps-load-test &lt;spec-file&gt; [flags] Options # -d, --dag string The DAG specification file to use for the load test -h, --help help for pps-load-test -p, --parallelism int The parallelism to use for the pipelines. --pod-patch string The pod patch file to use for the pipelines. -s, --seed int The seed to use for generating the load. --state-id string The ID of the base state to use for the load. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
288,Pachctl shell," pachctl shell # Run the pachyderm shell.
Synopsis # Run the pachyderm shell.
pachctl shell [flags] Options # -h, --help help for shell --max-completions int The maximum number of completions to show in the shell, defaults to 64. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
289,Pachctl squash," pachctl squash # Squash an existing Pachyderm resource.
Synopsis # Squash an existing Pachyderm resource.
Options # -h, --help help for squash Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
290,Pachctl squash commit," pachctl squash commit # Squash the sub-commits of a commit.
Synopsis # Squash the sub-commits of a commit. The data in the sub-commits will remain in their child commits. The squash will fail if it includes a commit with no children
pachctl squash commit &lt;commit-id&gt; [flags] Options # -h, --help help for commit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
291,Pachctl start," pachctl start # Start a Pachyderm resource.
Synopsis # Start a Pachyderm resource.
Options # -h, --help help for start Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
292,Pachctl start commit," pachctl start commit # Start a new commit.
Synopsis # Start a new commit with parent-commit as the parent on the given branch; if the branch does not exist, it will be created.
pachctl start commit &lt;repo&gt;@&lt;branch&gt; [flags] Examples # # Start a commit in repo &#34;test&#34; on branch &#34;master&#34; $ pachctl start commit test@master # Start a commit with &#34;master&#34; as the parent in repo &#34;test&#34;, on a new branch &#34;patch&#34;; essentially a fork. $ pachctl start commit test@patch -p master # Start a commit with XXX as the parent in repo &#34;test&#34; on the branch &#34;fork&#34; $ pachctl start commit test@fork -p XXX Options # --description string A description of this commit&#39;s contents (synonym for --message) -h, --help help for commit -m, --message string A description of this commit&#39;s contents -p, --parent string The parent of the new commit, unneeded if branch is specified and you want to use the previous head of the branch as the parent. --project string Project in which repo is located. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
293,Pachctl start pipeline," pachctl start pipeline # Restart a stopped pipeline.
Synopsis # Restart a stopped pipeline.
pachctl start pipeline &lt;pipeline&gt; [flags] Options # -h, --help help for pipeline --project string Project containing pipeline. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
294,Pachctl start transaction," pachctl start transaction # Start a new transaction.
Synopsis # Start a new transaction.
pachctl start transaction [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
295,Pachctl stop," pachctl stop # Cancel an ongoing task.
Synopsis # Cancel an ongoing task.
Options # -h, --help help for stop Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
296,Pachctl stop job," pachctl stop job # Stop a job.
Synopsis # Stop a job. The job will be stopped immediately.
pachctl stop job &lt;pipeline&gt;@&lt;job&gt; [flags] Options # -h, --help help for job --project string Project containing the job (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
297,Pachctl stop pipeline," pachctl stop pipeline # Stop a running pipeline.
Synopsis # Stop a running pipeline.
pachctl stop pipeline &lt;pipeline&gt; [flags] Options # -h, --help help for pipeline --project string Project containing pipeline. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
298,Pachctl stop transaction," pachctl stop transaction # Stop modifying the current transaction.
Synopsis # Stop modifying the current transaction.
pachctl stop transaction [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
299,Pachctl subscribe," pachctl subscribe # Wait for notifications of changes to a Pachyderm resource.
Synopsis # Wait for notifications of changes to a Pachyderm resource.
Options # -h, --help help for subscribe Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
300,Pachctl subscribe commit," pachctl subscribe commit # Print commits as they are created (finished).
Synopsis # Print commits as they are created in the specified repo and branch. By default, all existing commits on the specified branch are returned first. A commit is only considered &lsquo;created&rsquo; when it&rsquo;s been finished.
pachctl subscribe commit &lt;repo&gt;[@&lt;branch&gt;] [flags] Examples # # subscribe to commits in repo &#34;test&#34; on branch &#34;master&#34; $ pachctl subscribe commit test@master # subscribe to commits in repo &#34;test&#34; on branch &#34;master&#34;, but only since commit XXX. $ pachctl subscribe commit test@master --from XXX # subscribe to commits in repo &#34;test&#34; on branch &#34;master&#34;, but only for new commits created from now on. $ pachctl subscribe commit test@master --new Options # --all return all types of commits, including aliases --from string subscribe to all commits since this commit --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit --new subscribe to only new commits created from now on --origin string only return commits of a specific type -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project in which repo is located. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
301,Pachctl unmount," pachctl unmount # Unmount pfs.
Synopsis # Unmount pfs.
pachctl unmount &lt;path/to/mount/point&gt; [flags] Options # -a, --all unmount all pfs mounts -h, --help help for unmount Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
302,Pachctl update," pachctl update # Change the properties of an existing Pachyderm resource.
Synopsis # Change the properties of an existing Pachyderm resource.
Options # -h, --help help for update Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
303,Pachctl update pipeline," pachctl update pipeline # Update an existing Pachyderm pipeline.
Synopsis # Update a Pachyderm pipeline with a new pipeline specification. For details on the format, see https://docs.pachyderm.com/latest/reference/pipeline-spec/.
pachctl update pipeline [flags] Options # --arg stringArray Top-level argument passed to the Jsonnet template in --jsonnet (which must be set if any --arg arguments are passed). Value must be of the form &#39;param=value&#39;. For multiple args, --arg may be set more than once. -f, --file string A JSON file (url or filepath) containing one or more pipelines. &#34;-&#34; reads from stdin (the default behavior). Exactly one of --file and --jsonnet must be set. -h, --help help for pipeline --jsonnet string BETA: A Jsonnet template file (url or filepath) for one or more pipelines. &#34;-&#34; reads from stdin. Exactly one of --file and --jsonnet must be set. Jsonnet templates must contain a top-level function; strings can be passed to this function with --arg (below) --project string The project in which to update the pipeline. (default &#34;joins&#34;) -p, --push-images If true, push local docker images into the docker registry. -r, --registry string The registry to push images to. (default &#34;index.docker.io&#34;) --reprocess If true, reprocess datums that were already processed by previous version of the pipeline. -u, --username string The username to push images as. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
304,Pachctl update project," pachctl update project # Update a project.
Synopsis # Update a project.
pachctl update project &lt;project&gt; [flags] Options # -d, --description string The description of the updated project. -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
305,Pachctl update repo," pachctl update repo # Update a repo.
Synopsis # Update a repo.
pachctl update repo &lt;repo&gt; [flags] Options # -d, --description string A description of the repo. -h, --help help for repo --project string Project in which repo is located. (default &#34;joins&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
306,Pachctl version," pachctl version # Print Pachyderm version information.
Synopsis # Print Pachyderm version information.
pachctl version [flags] Options # --client-only If set, only print pachctl&#39;s version, but don&#39;t make any RPCs to pachd. Useful if pachd is unavailable --enterprise If set, &#39;pachctl version&#39; will run on the active enterprise context. -h, --help help for version -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml --timeout string If set, &#39;pachctl version&#39; will timeout after the given duration (formatted as a golang time duration--a number followed by ns, us, ms, s, m, or h). If --client-only is set, this flag is ignored. If unset, pachctl will use a default timeout; if set to 0s, the call will never time out. (default &#34;default&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
307,Pachctl wait," pachctl wait # Wait for the side-effects of a Pachyderm resource to propagate.
Synopsis # Wait for the side-effects of a Pachyderm resource to propagate.
Options # -h, --help help for wait Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
308,Pachctl wait commit," pachctl wait commit # Wait for the specified commit to finish and return it.
Synopsis # Wait for the specified commit to finish and return it.
pachctl wait commit &lt;repo&gt;@&lt;branch-or-commit&gt; [flags] Examples # # wait for the commit foo@XXX to finish and return it $ pachctl wait commit foo@XXX -b bar@baz Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project containing commit. (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
309,Pachctl wait job," pachctl wait job # Wait for a job to finish then return info about the job.
Synopsis # Wait for a job to finish then return info about the job.
pachctl wait job &lt;job&gt;|&lt;pipeline&gt;@&lt;job&gt; [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for job -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string project containing job (default &#34;joins&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs "
310,Configuration Specification,"This document outlines the fields in pachyderm configs. This should act as a reference. If you wish to change a config value, you should do so via pachctl config.
JSON format # { &#34;user_id&#34;: &#34;string&#34;, &#34;v2&#34;: { &#34;active_context&#34;: &#34;string&#34;, &#34;active_enterprise_context&#34;: &#34;string&#34;, &#34;contexts&#34;: { &#34;source&#34;: int, &#34;pachd_address&#34;: &#34;string&#34;, &#34;server_cas&#34;: &#34;string&#34;, &#34;session_token&#34;: &#34;string&#34;, &#34;active_transaction&#34;: &#34;string&#34;, &#34;cluster_name&#34;: &#34;string&#34;, &#34;auth_info&#34;: &#34;string&#34;, &#34;namspace&#34;: &#34;string&#34;, &#34;cluster_deployment_id&#34;: &#34;string&#34;, &#34;enterprise_server&#34;: bool, &#34;port_forwarders&#34;: { service_name: int, ... } ... }, &#34;metrics&#34;: bool } } If a field is not set, it will be omitted from JSON entirely. Following is an example of a simple config:
{ &#34;user_id&#34;: &#34;514cbe16-e615-46fe-92d9-3156f12885d7&#34;, &#34;v2&#34;: { &#34;active_context&#34;: &#34;default&#34;, &#34;contexts&#34;: { &#34;default&#34;: {} }, &#34;metrics&#34;: true } } Following is a walk-through of all the fields.
User ID # A UUID giving a unique ID for this user for metrics.
Metrics # Whether metrics is enabled.
Active Context # v2.active_context specifies the name of the currently active pachyderm context, as specified in v2.contexts.
Active Enterprise Context # v2.active_enterprise_context specifies the name of the currently active pachyderm enterprise context, as specified in v2.contexts. If left blank the v2.active_context value will be interpreted as the Active Enterprise Context.
Contexts # A map of context names to their configurations. Pachyderm contexts are akin to kubernetes contexts (and in fact reference the kubernetes context that they&rsquo;re associated with.)
Source # An integer that specifies where the config came from. This parameter is for internal use only and should not be modified.
Pachd Address # A host:port specification for connecting to pachd. If this is set, pachyderm will directly connect to the cluster, rather than resorting to kubernetes' port forwarding. If you can set this (because there&rsquo;s no firewall between you and the cluster), you should, as kubernetes&rsquo; port forwarder is not designed to handle large amounts of data.
Server CAs # Trusted root certificates for the cluster, formatted as a base64-encoded PEM. This is only set when TLS is enabled.
Session token # A secret token identifying the current pachctl user within their pachyderm cluster. This is included in all RPCs sent by pachctl, and used to determine if pachctl actions are authorized. This is only set when auth is enabled.
Active transaction # The currently active transaction for batching together pachctl commands. This can be set or cleared via many of the pachctl * transaction commands.
Cluster name # The name of the underlying Kubernetes cluster, extracted from the Kubernetes context.
Auth info # The name of the underlying Kubernetes cluster&rsquo;s auth credentials, extracted from the Kubernetes context.
Namespace # The underlying Kubernetes cluster&rsquo;s namespace, extracted from the Kubernetes context.
Cluster Deployment ID # The pachyderm cluster deployment ID that is used to ensure the operations run on the expected cluster.
Enterprise Server # Whether the context represents an enterprise server.
Port forwarders # A mapping of service name -&gt; local port. This field is populated when you run explicit port forwarding (pachctl port-forward), so that subsequent pachctl operations know to use the explicit port forwarder.
This field is removed when the pachctl port-forward operation completes. You might need to manually delete the field from your config if the process failed to remove the field automatically.
"
311,Helm Chart Values (HCVs),"Explore all of the available Helm chart values for Pachyderm in this section.
‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
312,Deploy Target HCVs," About # The Deploy Target section defines where you&rsquo;re deploying Pachyderm; this is typically located at the top of your values.yaml file.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: Amazon Custom Google Local Microsoft Minio # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;AMAZON&#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;CUSTOM&#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;GOOGLE&#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;LOCAL&#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;MICROSOFT&#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;MINIO&#34; ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
313,Global HCVs," About # The Global section configures the connection to the PostgreSQL database. By default, it uses the included Postgres service.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: With Secrets Without Secrets global: postgresql: postgresqlAuthType: &#34;md5&#34; # sets the auth type used with postgres &amp; pg-bounder; options include &#34;md5&#34; and &#34;scram-sha-256&#34; postgresqlUsername: &#34;pachyderm&#34; # defines the username to access the pachyderm and dex databases postgresqlExistingSecretName: &#34;&#34; # leave blank if using password postgresqlExistingSecretKey: &#34;&#34; # leave blank if using password postgresqlDatabase: &#34;pachyderm&#34; # defines the database name where pachyderm data will be stored postgresqlHost: &#34;postgres&#34; # defines the postgresql database host to connect to postgresqlPort: &#34;5432&#34; # defines he postgresql database port to connect to postgresqlSSL: &#34;disable&#34; # defines the SSL mode used to connect pg-bouncer to postgrs postgresqlSSLCACert: &#34;&#34; # defines the CA Certificate required to connect to Postgres postgresqlSSLSecret: &#34;&#34; # defines the TLS Secret with cert/key to connect to Postgres identityDatabaseFullNameOverride: &#34;&#34; # defines the DB name that dex connects to; defaults to &#34;Dex&#34; imagePullSecrets: [] # allows you to pull images from private repositories; also added to pipeline workers # Example: # imagePullSecrets: # - regcred customCaCerts: false # loads the cert file in pachd-tls-cert as the root cert for pachd, console, and enterprise-server proxy: &#34;&#34; # sets server address for outbound cluster traffic noProxy: &#34;&#34; # if proxy is set, allows a comma-separated list of destinations that bypass the proxy securityContexts: # set security context runAs users. If running on openshift, set enabled to false as openshift creates its own contexts. enabled: true global: postgresql: postgresqlAuthType: &#34;md5&#34; # sets the auth type used with postgres &amp; pg-bounder; options include &#34;md5&#34; and &#34;scram-sha-256&#34; postgresqlUsername: &#34;pachyderm&#34; # defines the username to access the pachyderm and dex databases postgresqlPostgresPassword: &#34;insecure-root-password&#34; # leave blank if using a secret postgresqlDatabase: &#34;pachyderm&#34; # defines the database name where pachyderm data will be stored postgresqlHost: &#34;postgres&#34; # defines the postgresql database host to connect to postgresqlPort: &#34;5432&#34; # defines he postgresql database port to connect to postgresqlSSL: &#34;disable&#34; # defines the SSL mode used to connect pg-bouncer to postgrs postgresqlSSLCACert: &#34;&#34; # defines the CA Certificate required to connect to Postgres postgresqlSSLSecret: &#34;&#34; # defines the TLS Secret with cert/key to connect to Postgres identityDatabaseFullNameOverride: &#34;&#34; # defines the DB name that dex connects to; defaults to &#34;Dex&#34; imagePullSecrets: [] # allows you to pull images from private repositories; also added to pipeline workers customCaCerts: false # loads the cert file in pachd-tls-cert as the root cert for pachd, console, and enterprise-server proxy: &#34;&#34; # sets server address for outbound cluster traffic noProxy: &#34;&#34; # if proxy is set, allows a comma-separated list of destinations that bypass the proxy # Set security context runAs users. If running on openshift, set enabled to false as openshift creates its own contexts securityContexts: enabled: true ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
314,Console HCVs," About # Console is the Graphical User Interface (GUI) for Pachyderm. Users that would prefer to navigate and manage through their project resources visually can connect to Console by authenticating against your configured OIDC. For personal-machine installations of Pachyderm, a user may access Console without authentication via localhost.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: Enabled No Metrics Enabled With Metrics Disabled console: enabled: true # deploys Console UI annotations: {} image: # defines which image to use for the console; replicates the --console-image &amp; --registry arguments to pachctl repository: &#34;pachyderm/haberdashery&#34; # defines image repo location pullPolicy: &#34;IfNotPresent&#34; tag: &#34;2.3.3-1&#34; # defines the image repo to pull from priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] podLabels: {} # specifies labels to add to the console pod. resources: # specifies the resource request and limits; unset by default. {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; config: # defines primary configuration settings, including authentication. reactAppRuntimeIssuerURI: &#34;&#34; # defines the pachd oauth address accessible to outside clients. oauthRedirectURI: &#34;&#34; # defines the oauth callback address within console that the pachd oauth service would redirect to. oauthClientID: &#34;console&#34; # defines the client identifier for the Console with pachd oauthClientSecret: &#34;&#34; # defines the secret configured for the client with pachd; if blank, autogenerated. oauthClientSecretSecretName: &#34;&#34; # uses the value of an existing k8s secret by pulling from the `OAUTH_CLIENT_SECRET` key. graphqlPort: 4000 # defines the http port that the console service will be accessible on. pachdAddress: &#34;pachd-peer:30653&#34; disableTelemetry: false # disables analytics and error data collection service: annotations: {} labels: {} # specifies labels to add to the console service. type: ClusterIP # specifies the Kubernetes type of the console service; default is `ClusterIP`. console: enabled: true # deploys Console UI annotations: {} image: # defines which image to use for the console; replicates the --console-image &amp; --registry arguments to pachctl repository: &#34;pachyderm/haberdashery&#34; # defines image repo location pullPolicy: &#34;IfNotPresent&#34; tag: &#34;2.3.3-1&#34; # defines the image repo to pull from priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] podLabels: {} # specifies labels to add to the console pod. resources: # specifies the resource request and limits; unset by default. {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; config: # defines primary configuration settings, including authentication. reactAppRuntimeIssuerURI: &#34;&#34; # defines the pachd oauth address accessible to outside clients. oauthRedirectURI: &#34;&#34; # defines the oauth callback address within console that the pachd oauth service would redirect to. oauthClientID: &#34;console&#34; # defines the client identifier for the Console with pachd oauthClientSecret: &#34;&#34; # defines the secret configured for the client with pachd; if blank, autogenerated. oauthClientSecretSecretName: &#34;&#34; # uses the value of an existing k8s secret by pulling from the `OAUTH_CLIENT_SECRET` key. graphqlPort: 4000 # defines the http port that the console service will be accessible on. pachdAddress: &#34;pachd-peer:30653&#34; disableTelemetry: true # disables analytics and error data collection service: annotations: {} labels: {} # specifies labels to add to the console service. type: ClusterIP # specifies the Kubernetes type of the console service; default is `ClusterIP`. console: enabled: false ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
315,Enterprise Server HCVs," About # Enterprise Server is a production management layer that centralizes the licensing registration of multiple Pachyderm clusters for Enterprise use and the setup of user authorization/authentication via OIDC.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: TLS Disabled TLS New Secret TLS Existing Secret ES Disabled enterpriseServer: enabled: true affinity: {} annotations: {} tolerations: [] priorityClassName: &#34;&#34; nodeSelector: {} service: type: ClusterIP apiGRPCPort: 31650 prometheusPort: 31656 oidcPort: 31657 identityPort: 31658 s3GatewayPort: 31600 tls: enabled: false resources: {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; podLabels: {} # specifies labels to add to the pachd pod. clusterDeploymentID: &#34;&#34; image: repository: &#34;pachyderm/pachd&#34; pullPolicy: &#34;IfNotPresent&#34; tag: &#34;&#34; # defaults to the chart‚Äôs specified appVersion. enterpriseServer: enabled: true affinity: {} annotations: {} tolerations: [] priorityClassName: &#34;&#34; nodeSelector: {} service: type: ClusterIP apiGRPCPort: 31650 prometheusPort: 31656 oidcPort: 31657 identityPort: 31658 s3GatewayPort: 31600 tls: enabled: true newSecret: create: true crt: &#34;&#34; key: &#34;&#34; resources: {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; podLabels: {} # specifies labels to add to the pachd pod. clusterDeploymentID: &#34;&#34; image: repository: &#34;pachyderm/pachd&#34; pullPolicy: &#34;IfNotPresent&#34; tag: &#34;&#34; # defaults to the chart‚Äôs specified appVersion. enterpriseServer: enabled: true affinity: {} annotations: {} tolerations: [] priorityClassName: &#34;&#34; nodeSelector: {} service: type: ClusterIP apiGRPCPort: 31650 prometheusPort: 31656 oidcPort: 31657 identityPort: 31658 s3GatewayPort: 31600 tls: enabled: true secretName: &#34;&#34; resources: {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; podLabels: {} # specifies labels to add to the pachd pod. clusterDeploymentID: &#34;&#34; image: repository: &#34;pachyderm/pachd&#34; pullPolicy: &#34;IfNotPresent&#34; tag: &#34;&#34; # defaults to the chart‚Äôs specified appVersion. enterpriseServer: enabled: false ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
316,ETCD HCVs," About # The ETCD section configures the ETCD cluster in the deployment.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
etcd: affinity: {} annotations: {} dynamicNodes: 1 # sets the number of nodes in the etcd StatefulSet; analogous to the --dynamic-etcd-nodes argument to pachctl image: repository: &#34;pachyderm/etcd&#34; tag: &#34;v3.5.1&#34; pullPolicy: &#34;IfNotPresent&#34; maxTxnOps: 10000 # sets the --max-txn-ops in the container args priorityClassName: &#34;&#34; nodeSelector: {} podLabels: {} # specifies labels to add to the etcd pod. resources: # specifies the resource request and limits {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; storageClass: &#34;&#34; # defines what existing storage class to use; analogous to --etcd-storage-class argument to pachctl storageSize: 10Gi # specifies the size of the volume to use for etcd. service: annotations: {} # specifies annotations to add to the etcd service. labels: {} # specifies labels to add to the etcd service. type: ClusterIP # specifies the Kubernetes type of the etcd service. tolerations: [] ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
317,Ingress HCVs," About # ‚ö†Ô∏è ingress will be removed from the helm chart once the deployment of Pachyderm with a proxy becomes mandatory.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: TLS Existing Secret TLS New Secret TLS Disabled ingress: enabled: true annotations: {} host: &#34;&#34; uriHttpsProtoOverride: false # if true, adds the https protocol to the ingress URI routes without configuring certs tls: enabled: true secretName: &#34;&#34; ingress: enabled: true annotations: {} host: &#34;&#34; uriHttpsProtoOverride: false # if true, adds the https protocol to the ingress URI routes without configuring certs tls: enabled: true newSecret: create: true crt: &#34;&#34; key: &#34;&#34; ingress: enabled: true annotations: {} host: &#34;&#34; uriHttpsProtoOverride: false # if true, adds the https protocol to the ingress URI routes without configuring certs tls: enabled: false ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
318,Loki HCVs," About # Loki Stack contains values that are passed to the loki-stack subchart. For more details on each service, see their official documentation:
Loki storage documentation Grafana documentation Promtail documentation Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
loki-stack: loki: serviceAccount: automountServiceAccountToken: false persistence: enabled: true accessModes: - ReadWriteOnce size: 10Gi # More info for setting up storage classes on various cloud providers: # AWS: https://docs.aws.amazon.com/eks/latest/userguide/storage-classes.html # GCP: https://cloud.google.com/compute/docs/disks/performance#disk_types # Azure: https://docs.microsoft.com/en-us/azure/aks/concepts-storage#storage-classes storageClassName: &#34;&#34; annotations: {} priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] config: limits_config: retention_period: 24h retention_stream: - selector: &#39;{suite=&#34;pachyderm&#34;}&#39; priority: 1 period: 168h # = 1 week grafana: enabled: false promtail: config: clients: - url: &#34;http://{{ .Release.Name }}-loki:3100/loki/api/v1/push&#34; snippets: # The scrapeConfigs section is copied from loki-stack-2.6.4 # The pipeline_stages.match stanza has been added to prevent multiple lokis in a cluster from mixing their logs. scrapeConfigs: | - job_name: kubernetes-pods pipeline_stages: {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }} - match: selector: &#39;{namespace!=&#34;{{ .Release.Namespace }}&#34;}&#39; action: drop kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: - __meta_kubernetes_pod_controller_name regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})? action: replace target_label: __tmp_controller_name - source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_name - __meta_kubernetes_pod_label_app - __tmp_controller_name - __meta_kubernetes_pod_name regex: ^;*([^;]+)(;.*)?$ action: replace target_label: app - source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_instance - __meta_kubernetes_pod_label_release regex: ^;*([^;]+)(;.*)?$ action: replace target_label: instance - source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_component - __meta_kubernetes_pod_label_component regex: ^;*([^;]+)(;.*)?$ action: replace target_label: component {{- if .Values.config.snippets.addScrapeJobLabel }} - replacement: kubernetes-pods target_label: scrape_job {{- end }} {{- toYaml .Values.config.snippets.common | nindent 4 }} {{- with .Values.config.snippets.extraRelabelConfigs }} {{- toYaml . | nindent 4 }} {{- end }} pipelineStages: - cri: {} common: # This is copy and paste of existing actions, so we don&#39;t lose them. # Cf. https://github.com/grafana/loki/issues/3519#issuecomment-1125998705 - action: replace source_labels: - __meta_kubernetes_pod_node_name target_label: node_name - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace replacement: $1 separator: / source_labels: - namespace - app target_label: job - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - action: replace replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - action: replace regex: true/(.*) replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash - __meta_kubernetes_pod_container_name target_label: __path__ - action: keep regex: pachyderm source_labels: - __meta_kubernetes_pod_label_suite # this gets all kubernetes labels as well - action: labelmap regex: __meta_kubernetes_pod_label_(.+) livenessProbe: failureThreshold: 5 tcpSocket: port: http-metrics initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
319,PachD HCVs," Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: With Secrets Without Secrets pachd: enabled: true preflightChecks: enabled: true # runs kube validation preflight checks. affinity: {} annotations: {} clusterDeploymentID: &#34;&#34; # sets Pachyderm cluster ID. configJob: annotations: {} goMaxProcs: 0 # passed as GOMAXPROCS to the pachd container. image: repository: &#34;pachyderm/pachd&#34; pullPolicy: &#34;IfNotPresent&#34; tag: &#34;&#34; # sets worker image tag; defaults to appVersion. logFormat: &#34;json&#34; logLevel: &#34;info&#34; lokiDeploy: true lokiLogging: true metrics: enabled: true endpoint: &#34;&#34; # provide the URL of the metrics endpoint. priorityClassName: &#34;&#34; nodeSelector: {} podLabels: {} # adds labels to the pachd pod. replicas: 1 # sets the number of pachd running pods resources: # specifies the resource requests &amp; limits {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; requireCriticalServersOnly: false externalService: enabled: false # Creates a service that&#39;s safe to expose. loadBalancerIP: &#34;&#34; apiGRPCPort: 30650 s3GatewayPort: 30600 annotations: {} service: labels: {} # adds labels to the pachd service. type: &#34;ClusterIP&#34; # specifies pachd service&#39;s Kubernetes type annotations: {} apiGRPCPort: 30650 prometheusPort: 30656 oidcPort: 30657 identityPort: 30658 s3GatewayPort: 30600 #apiGrpcPort: # expose: true # port: 30650 activateEnterpriseMember: false # connects to an existing enterprise server. activateAuth: true # bootstraps auth via the config job. enterpriseLicenseKey: &#34;&#34; # activates enterprise if provided. enterpriseLicenseKeySecretName: &#34;&#34; # pulls value from k8s secret key &#34;enterprise-license-key&#34; rootToken: &#34;&#34; # autogenerated if not provided; stored in k8s secret &#34;pachyderm-bootstrap-config.rootToken&#34; rootTokenSecretName: &#34;&#34; # passes rooToken value from k8s secret key &#34;root-token&#34; enterpriseSecret: &#34;&#34; # autogenerated if not provided; stored in k8s secret &#34;pachyderm-bootstrap-config.enterpriseSecret&#34; enterpriseSecretSecretName: &#34;&#34; # passes value from k8s secret key &#34;enterprise-secret&#34; oauthClientID: pachd oauthClientSecret: &#34;&#34; # autogenerated if not provided; stored in k8s secret &#34;pachyderm-bootstrap-config.authConfig.clientSecret&#34; oauthClientSecretSecretName: &#34;&#34; # passes value from k8s secret key &#34;pachd-oauth-client-secret&#34; oauthRedirectURI: &#34;&#34; enterpriseServerToken: &#34;&#34; # authenticates to a enterprise server &amp; registers this cluster as a member if activateEnterpriseMember is true. enterpriseServerTokenSecretName: &#34;&#34; # passes value from k8s secret key &#34;enterprise-server-token&#34; if activateEnterpriseMember is true. enterpriseServerAddress: &#34;&#34; enterpriseCallbackAddress: &#34;&#34; localhostIssuer: &#34;&#34; # Indicates to pachd whether dex is embedded in its process; &#34;true&#34;, &#34;false&#34;, or &#34;&#34; pachAuthClusterRoleBindings: {} # map initial users to their list of roles. # robot:wallie: # - repoReader # robot:eve: # - repoWriter additionalTrustedPeers: [] # configures identity service to recognize trusted peers. # - example-app serviceAccount: create: true additionalAnnotations: {} name: &#34;pachyderm&#34; storage: backend: &#34;&#34; # options: GOOGLE, AMAZON, MINIO, MICROSOFT or LOCAL amazon: bucket: &#34;&#34; # sets the S3 bucket to use. cloudFrontDistribution: &#34;&#34; # sets the CloudFront distribution in the storage secrets. customEndpoint: &#34;&#34; disableSSL: false id: &#34;&#34; # sets the Amazon access key ID logOptions: &#34;&#34; # case-sensitive comma-separated list: &#39;Debug&#39;, &#39;Signing&#39;, &#39;HTTPBody&#39;, &#39;RequestRetries&#39;, &#39;EventStreamBody&#39;, or &#39;all&#39; maxUploadParts: 10000 verifySSL: true partSize: &#34;5242880&#34; # sets part size for object storage uploads; must be a string. region: &#34;&#34; # sets AWS region retries: 10 reverse: true secret: &#34;&#34; # sets the Amazon secret access key to use. timeout: &#34;5m&#34; # sets the timeout for object storage requests. token: &#34;&#34; # sets the Amazon token to use. uploadACL: &#34;bucket-owner-full-control&#34; google: bucket: &#34;&#34; cred: &#34;&#34; # sets GCP service account private key as string. # cred: | # { # &#34;type&#34;: &#34;service_account&#34;, # &#34;project_id&#34;: &#34;‚Ä¶&#34;, # &#34;private_key_id&#34;: &#34;‚Ä¶&#34;, # &#34;private_key&#34;: &#34;-----BEGIN PRIVATE KEY-----\n‚Ä¶\n-----END PRIVATE KEY-----\n&#34;, # &#34;client_email&#34;: &#34;‚Ä¶@‚Ä¶.iam.gserviceaccount.com&#34;, # &#34;client_id&#34;: &#34;‚Ä¶&#34;, # &#34;auth_uri&#34;: &#34;https://accounts.google.com/o/oauth2/auth&#34;, # &#34;token_uri&#34;: &#34;https://oauth2.googleapis.com/token&#34;, # &#34;auth_provider_x509_cert_url&#34;: &#34;https://www.googleapis.com/oauth2/v1/certs&#34;, # &#34;client_x509_cert_url&#34;: &#34;https://www.googleapis.com/robot/v1/metadata/x509/‚Ä¶%40‚Ä¶.iam.gserviceaccount.com&#34; # } local: hostPath: &#34;&#34; # path where PFS metadata is stored; must end with &#34;/&#34;. requireRoot: true # root required for hostpath, but we run rootless in CI microsoft: container: &#34;&#34; id: &#34;&#34; secret: &#34;&#34; minio: bucket: &#34;&#34; # sets bucket name. endpoint: &#34;&#34; # format: hostname:port id: &#34;&#34; # username/id with readwrite access to the bucket. secret: &#34;&#34; # the secret/password of the user with readwrite access to the bucket. secure: &#34;false&#34; # enables https for minio if &#34;true&#34; signature: &#34;&#34; # Enables S3v2 support by setting signature to &#34;1&#34;; being deprecated. putFileConcurrencyLimit: 100 # sets the maximum number of files to upload or fetch from remote sources uploadConcurrencyLimit sets the maximum number of concurrent; analogous to --put-file-concurrency-limit argument to pachctl uploadConcurrencyLimit: 100 # object storage uploads per Pachd instance; analogous to --upload-concurrency-limit argument to pachctl compactionShardSizeThreshold: 0 # the total size of the files in a shard. compactionShardCountThreshold: 0 # the total number of files in a shard. ppsWorkerGRPCPort: 1080 storageGCPeriod: 0 # the number of seconds between PFS&#39;s garbage collection cycles; &lt;0 disables garbage collection; 0 defaults to pachyderm&#39;s internal config. storageChunkGCPeriod: 0 # the number of seconds between chunk garbage collection cycles; &lt;0 disables chunk garbage collection; 0 defaults to pachyderm&#39;s internal config. # There are three options for TLS: # 1. Disabled # 2. Enabled, existingSecret, specify secret name # 3. Enabled, newSecret, must specify cert, key and name tls: enabled: false secretName: &#34;&#34; newSecret: create: false crt: &#34;&#34; key: &#34;&#34; tolerations: [] worker: image: repository: &#34;pachyderm/worker&#34; pullPolicy: &#34;IfNotPresent&#34; # Worker tag is set under pachd.image.tag (they should be kept in lock step) serviceAccount: create: true additionalAnnotations: {} name: &#34;pachyderm-worker&#34; # sets the name of the worker service account; analogous to --worker-service-account argument to pachctl. rbac: create: true # indicates whether RBAC resources should be created; analogous to --no-rbac to pachctl pachd: enabled: true preflightChecks: # if enabled runs kube validation preflight checks. enabled: true affinity: {} annotations: {} # clusterDeploymentID sets the Pachyderm cluster ID. clusterDeploymentID: &#34;&#34; configJob: annotations: {} # goMaxProcs is passed as GOMAXPROCS to the pachd container. goMaxProcs: 0 image: repository: &#34;pachyderm/pachd&#34; pullPolicy: &#34;IfNotPresent&#34; # tag defaults to the chart‚Äôs specified appVersion. # This sets the worker image tag as well (they should be kept in lock step) tag: &#34;&#34; logFormat: &#34;json&#34; logLevel: &#34;info&#34; # If lokiDeploy is true, a Pachyderm-specific instance of Loki will # be deployed. lokiDeploy: true # lokiLogging enables Loki logging if set. lokiLogging: true metrics: # enabled sets the METRICS environment variable if set. enabled: true # endpoint should be the URL of the metrics endpoint. endpoint: &#34;&#34; priorityClassName: &#34;&#34; nodeSelector: {} # podLabels specifies labels to add to the pachd pod. podLabels: {} # resources specifies the resource requests and limits # replicas sets the number of pachd running pods replicas: 1 resources: {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; # requireCriticalServersOnly only requires the critical pachd # servers to startup and run without errors. It is analogous to the # --require-critical-servers-only argument to pachctl deploy. requireCriticalServersOnly: false # If enabled, External service creates a service which is safe to # be exposed externally externalService: enabled: false # (Optional) specify the existing IP Address of the load balancer loadBalancerIP: &#34;&#34; apiGRPCPort: 30650 s3GatewayPort: 30600 annotations: {} service: # labels specifies labels to add to the pachd service. labels: {} # type specifies the Kubernetes type of the pachd service. type: &#34;ClusterIP&#34; annotations: {} apiGRPCPort: 30650 prometheusPort: 30656 oidcPort: 30657 identityPort: 30658 s3GatewayPort: 30600 #apiGrpcPort: # expose: true # port: 30650 # DEPRECATED: activateEnterprise is no longer used. activateEnterprise: false ## if pachd.activateEnterpriseMember is set, enterprise will be activated and connected to an existing enterprise server. ## if pachd.enterpriseLicenseKey is set, enterprise will be activated. activateEnterpriseMember: false ## if pachd.activateAuth is set, auth will be bootstrapped by the config-job. activateAuth: true ## the license key used to activate enterprise features enterpriseLicenseKey: &#34;&#34; rootToken: &#34;&#34; enterpriseSecret: &#34;&#34; # enterpriseSecretSecretName is used to pass the enterprise secret value via an existing k8s secret. # The value is pulled from the key, &#34;enterprise-secret&#34;. enterpriseSecretSecretName: &#34;&#34; # if a secret is not provided, a secret will be autogenerated on install and stored in the k8s secret &#39;pachyderm-bootstrap-config.authConfig.clientSecret&#39; oauthClientID: pachd oauthClientSecret: &#34;&#34; # oauthClientSecretSecretName is used to set the OAuth Client Secret via an existing k8s secret. # The value is pulled from the key, &#34;pachd-oauth-client-secret&#34;. oauthClientSecretSecretName: &#34;&#34; oauthRedirectURI: &#34;&#34; # DEPRECATED: enterpriseRootToken is deprecated, in favor of enterpriseServerToken # NOTE only used if pachd.activateEnterpriseMember == true enterpriseRootToken: &#34;&#34; # enterpriseServerToken represents a token that can authenticate to a separate pachyderm enterprise server, # and is used to complete the enterprise member registration process for this pachyderm cluster. # The user backing this token should have either the licenseAdmin &amp; identityAdmin roles assigned, or # the clusterAdmin role. # NOTE: only used if pachd.activateEnterpriseMember == true enterpriseServerToken: &#34;&#34; # enterpriseServerTokenSecretName is used to pass the enterpriseServerToken value via an existing k8s secret. # The value is pulled from the key, &#34;enterprise-server-token&#34;. enterpriseServerTokenSecretName: &#34;&#34; # only used if pachd.activateEnterpriseMember == true enterpriseServerAddress: &#34;&#34; enterpriseCallbackAddress: &#34;&#34; # Indicates to pachd whether dex is embedded in its process. localhostIssuer: &#34;&#34; # &#34;true&#34;, &#34;false&#34;, or &#34;&#34; (used string as bool doesn&#39;t support empty value) # set the initial pachyderm cluster role bindings, mapping a user to their list of roles # ex. # pachAuthClusterRoleBindings: # robot:wallie: # - repoReader # robot:eve: # - repoWriter pachAuthClusterRoleBindings: {} # additionalTrustedPeers is used to configure the identity service to recognize additional OIDC clients as trusted peers of pachd. # For example, see the following example or the dex docs (https://dexidp.io/docs/custom-scopes-claims-clients/#cross-client-trust-and-authorized-party). # additionalTrustedPeers: # - example-app additionalTrustedPeers: [] serviceAccount: create: true additionalAnnotations: {} name: &#34;pachyderm&#34; #TODO Set default in helpers / Wire up in templates storage: # backend configures the storage backend to use. It must be one # of GOOGLE, AMAZON, MINIO, MICROSOFT or LOCAL. This is set automatically # if deployTarget is GOOGLE, AMAZON, MICROSOFT, or LOCAL backend: &#34;&#34; amazon: # bucket sets the S3 bucket to use. bucket: &#34;&#34; # cloudFrontDistribution sets the CloudFront distribution in the # storage secrets. It is analogous to the # --cloudfront-distribution argument to pachctl deploy. cloudFrontDistribution: &#34;&#34; customEndpoint: &#34;&#34; # disableSSL disables SSL. It is analogous to the --disable-ssl # argument to pachctl deploy. disableSSL: false # id sets the Amazon access key ID to use. Together with secret # and token, it implements the functionality of the # --credentials argument to pachctl deploy. id: &#34;&#34; # logOptions sets various log options in Pachyderm‚Äôs internal S3 # client. Comma-separated list containing zero or more of: # &#39;Debug&#39;, &#39;Signing&#39;, &#39;HTTPBody&#39;, &#39;RequestRetries&#39;, # &#39;RequestErrors&#39;, &#39;EventStreamBody&#39;, or &#39;all&#39; # (case-insensitive). See &#39;AWS SDK for Go&#39; docs for details. # logOptions is analogous to the --obj-log-options argument to # pachctl deploy. logOptions: &#34;&#34; # maxUploadParts sets the maximum number of upload parts. It is # analogous to the --max-upload-parts argument to pachctl # deploy. maxUploadParts: 10000 # verifySSL performs SSL certificate verification. It is the # inverse of the --no-verify-ssl argument to pachctl deploy. verifySSL: true # partSize sets the part size for object storage uploads. It is # analogous to the --part-size argument to pachctl deploy. It # has to be a string due to Helm and YAML parsing integers as # floats. Cf. https://github.com/helm/helm/issues/1707 partSize: &#34;5242880&#34; # region sets the AWS region to use. region: &#34;&#34; # retries sets the number of retries for object storage # requests. It is analogous to the --retries argument to # pachctl deploy. retries: 10 # reverse reverses object storage paths. It is analogous to the # --reverse argument to pachctl deploy. reverse: true # secret sets the Amazon secret access key to use. Together with id # and token, it implements the functionality of the # --credentials argument to pachctl deploy. secret: &#34;&#34; # timeout sets the timeout for object storage requests. It is # analogous to the --timeout argument to pachctl deploy. timeout: &#34;5m&#34; # token optionally sets the Amazon token to use. Together with # id and secret, it implements the functionality of the # --credentials argument to pachctl deploy. token: &#34;&#34; # uploadACL sets the upload ACL for object storage uploads. It # is analogous to the --upload-acl argument to pachctl deploy. uploadACL: &#34;bucket-owner-full-control&#34; google: bucket: &#34;&#34; # cred is a string containing a GCP service account private key, # in object (JSON or YAML) form. A simple way to pass this on # the command line is with the set-file flag, e.g.: # # helm install pachd -f my-values.yaml --set-file storage.google.cred=creds.json pachyderm/pachyderm cred: &#34;&#34; # Example: # cred: | # { # &#34;type&#34;: &#34;service_account&#34;, # &#34;project_id&#34;: &#34;‚Ä¶&#34;, # &#34;private_key_id&#34;: &#34;‚Ä¶&#34;, # &#34;private_key&#34;: &#34;-----BEGIN PRIVATE KEY-----\n‚Ä¶\n-----END PRIVATE KEY-----\n&#34;, # &#34;client_email&#34;: &#34;‚Ä¶@‚Ä¶.iam.gserviceaccount.com&#34;, # &#34;client_id&#34;: &#34;‚Ä¶&#34;, # &#34;auth_uri&#34;: &#34;https://accounts.google.com/o/oauth2/auth&#34;, # &#34;token_uri&#34;: &#34;https://oauth2.googleapis.com/token&#34;, # &#34;auth_provider_x509_cert_url&#34;: &#34;https://www.googleapis.com/oauth2/v1/certs&#34;, # &#34;client_x509_cert_url&#34;: &#34;https://www.googleapis.com/robot/v1/metadata/x509/‚Ä¶%40‚Ä¶.iam.gserviceaccount.com&#34; # } local: # hostPath indicates the path on the host where the PFS metadata # will be stored. It must end in /. It is analogous to the # --host-path argument to pachctl deploy. hostPath: &#34;&#34; requireRoot: true #Root required for hostpath, but we run rootless in CI microsoft: container: &#34;&#34; id: &#34;&#34; secret: &#34;&#34; minio: # minio bucket name bucket: &#34;&#34; # the minio endpoint. Should only be the hostname:port, no http/https. endpoint: &#34;&#34; # the username/id with readwrite access to the bucket. id: &#34;&#34; # the secret/password of the user with readwrite access to the bucket. secret: &#34;&#34; # enable https for minio with &#34;true&#34; defaults to &#34;false&#34; secure: &#34;&#34; # Enable S3v2 support by setting signature to &#34;1&#34;. This feature is being deprecated signature: &#34;&#34; # putFileConcurrencyLimit sets the maximum number of files to # upload or fetch from remote sources (HTTP, blob storage) using # PutFile concurrently. It is analogous to the # --put-file-concurrency-limit argument to pachctl deploy. putFileConcurrencyLimit: 100 # uploadConcurrencyLimit sets the maximum number of concurrent # object storage uploads per Pachd instance. It is analogous to # the --upload-concurrency-limit argument to pachctl deploy. uploadConcurrencyLimit: 100 # The shard size corresponds to the total size of the files in a shard. # The shard count corresponds to the total number of files in a shard. # If either criteria is met, a shard will be created. compactionShardSizeThreshold: 0 compactionShardCountThreshold: 0 ppsWorkerGRPCPort: 1080 # the number of seconds between PFS&#39;s garbage collection cycles. # if this value is set to 0, it will default to pachyderm&#39;s internal configuration. # if this value is less than 0, it will turn off garbage collection. storageGCPeriod: 0 # the number of seconds between chunk garbage collection cycles. # if this value is set to 0, it will default to pachyderm&#39;s internal configuration. # if this value is less than 0, it will turn off chunk garbage collection. storageChunkGCPeriod: 0 # There are three options for TLS: # 1. Disabled # 2. Enabled, existingSecret, specify secret name # 3. Enabled, newSecret, must specify cert, key and name tls: enabled: false secretName: &#34;&#34; newSecret: create: false crt: &#34;&#34; key: &#34;&#34; tolerations: [] worker: image: repository: &#34;pachyderm/worker&#34; pullPolicy: &#34;IfNotPresent&#34; # Worker tag is set under pachd.image.tag (they should be kept in lock step) serviceAccount: create: true additionalAnnotations: {} # name sets the name of the worker service account. Analogous to # the --worker-service-account argument to pachctl deploy. name: &#34;pachyderm-worker&#34; #TODO Set default in helpers / Wire up in templates rbac: # create indicates whether RBAC resources should be created. # Setting it to false is analogous to passing --no-rbac to pachctl # deploy. create: true ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
320,PachW HCVs," About # PachW enables fine-grained control of where compaction and object-storage interaction occur by running storage tasks in a dedicated Kubernetes deployment. Users can configure PachW&rsquo;s min and max replicas as well as define nodeSelectors, tolerations, and resource requests. Using PachW allows power users to save on costs by claiming fewer resources and running storage tasks on less expensive nodes.
‚ö†Ô∏è If you are upgrading to 2.5.0+ for the first time and you wish to use PachW, you must calculate how many maxReplicas you need. By default, PachW is set to maxReplicas:1 &mdash; however, that is not sufficient for production runs.
maxReplicas # You should set the maxReplicas value to at least match the number of pipeline replicas that you have. For high performance, we suggest taking the following approach:
number of pipelines * highest parallelism spec * 1.5 = maxReplicas
Let&rsquo;s say you have 6 pipelines. One of these pipelines has a parallelism spec value of 6, and the rest are 5 or fewer.
6 * 6 * 1.5 = 54
minReplicas # Workloads that constantly process storage and compaction tasks because they are committing rapidly may want to increase minReplicas to have instances on standby.
nodeSelectors # Workloads that utilize GPUs and other expensive resources may want to add a node selector to scope PachW instances to less expensive nodes.
Values # Options: Enabled With Minimum With Specific Resources As Sidecars (Legacy) pachw: inheritFromPachd: true # defaults below configuration options like &#39;resources&#39; and &#39;tolerations&#39; to values from pachd maxReplicas: 1 minReplicas: 0 inSidecars: false #tolerations: [] #affinity: {} #nodeSelector: {} pachw: inheritFromPachd: true # defaults below configuration options like &#39;resources&#39; and &#39;tolerations&#39; to values from pachd maxReplicas: 6 # set to match the number of pipeline replicas you have; sample formula: pipeline count * parallelism = target maxReplicas minReplicas: 1 #tolerations: [] #affinity: {} #nodeSelector: {} #resources: # sets kubernetes resource configuration for pachw pods. If not defined, config from pachd is reused. We recommend defining resources when running pachw with a high value of maxReplicas (when formula is: target maxReplicas * 1.5). #limits: #cpu: &#34;1&#34; #memory: &#34;2G&#34; #requests: #cpu: &#34;1&#34; #memory: &#34;2G&#34; pachw: inheritFromPachd: false # defaults below configuration options like &#39;resources&#39; and &#39;tolerations&#39; to values from pachd maxReplicas: 6 # set to match the number of pipeline replicas you have; sample formula: pipeline count * parallelism = target maxReplicas minReplicas: 1 #tolerations: [] #affinity: {} #nodeSelector: {} resources: # sets kubernetes resource configuration for pachw pods. If not defined, config from pachd is reused. We recommend defining resources when running pachw with a high value of maxReplicas (when formula is: target maxReplicas * 1.5). limits: cpu: &#34;1&#34; memory: &#34;2G&#34; requests: cpu: &#34;1&#34; memory: &#34;2G&#34; pachw: inheritFromPachd: true # defaults below configuration options like &#39;resources&#39; and &#39;tolerations&#39; to values from pachd inSidecars: true # processes storage related tasks in pipeline storage sidecars like version 2.4.2 or less. maxReplicas: 1 #tolerations: [] #affinity: {} #nodeSelector: {} ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
321,Kube Event Tail HCVs," About # Kube Event Tail deploys a lightweight app that watches Kubernetes events and echoes them into logs.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: All Events Namespace Events Disabled kubeEventTail: enabled: true clusterScope: false # if true, watches just events in its namespace image: repository: pachyderm/kube-event-tail pullPolicy: &#34;IfNotPresent&#34; tag: &#34;v0.0.6&#34; resources: limits: cpu: &#34;1&#34; memory: 100Mi requests: cpu: 100m memory: 45Mi kubeEventTail: enabled: true clusterScope: true # if true, watches just events in its namespace image: repository: pachyderm/kube-event-tail pullPolicy: &#34;IfNotPresent&#34; tag: &#34;v0.0.6&#34; resources: limits: cpu: &#34;1&#34; memory: 100Mi requests: cpu: 100m memory: 45Mi kubeEventTail: enabled: false ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
322,PGBouncer HCVs," About # The PGBouncer section configures a PGBouncer Postgres connection pooler.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
pgbouncer: service: type: ClusterIP # defines the Kubernetes service type. annotations: {} priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] image: repository: pachyderm/pgbouncer tag: 1.16.1-debian-10-r82 resources: # defines resources in standard kubernetes format; unset by default. {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; maxConnections: 1000 # defines the maximum number of concurrent connections into pgbouncer. defaultPoolSize: 20 # specifies the maximum number of concurrent connections from pgbouncer to the postgresql database. ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
323,PostgreSQL Subchart HCVs," About # The PostgresQL section controls the Bitnami PostgreSQL subchart. Pachyderm runs on Kubernetes, is backed by an object store of your choice, and comes with a bundled version of PostgreSQL (metadata storage) by default.
We recommended disabling this bundled PostgreSQL and using a managed database instance (such as RDS, CloudSQL, or PostgreSQL Server) for production environments.
See storage class details for your provider:
AWS | Min: 500Gi (GP2) / 1,500 IOP GCP | Min: 50Gi / 1,500 IOPS Azure | Min: 256Gi / 1,100 IOPS Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: Production Personal Machine postgresql: enabled: false # if false, you must specify a PostgreSQL database server connection @ global.postgresql postgresql: enabled: true # if false, you must specify a PostgreSQL database server connection @ global.postgresql image: tag: &#34;13.3.0&#34; # DEPRECATED from pachyderm 2.1.5 initdbScripts: dex.sh: | #!/bin/bash set -e psql -v ON_ERROR_STOP=1 --username &#34;$POSTGRES_USER&#34; --dbname &#34;$POSTGRES_DB&#34; &lt;&lt;-EOSQL CREATE DATABASE dex; GRANT ALL PRIVILEGES ON DATABASE dex TO &#34;$POSTGRES_USER&#34;; EOSQL fullnameOverride: postgres persistence: # Specify the storage class for the postgresql Persistent Volume (PV) storageClass: &#34;&#34; # specifies the size of the volume to use for postgresql size: 10Gi labels: suite: pachyderm primary: priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] readReplicas: priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
324,CloudSQL Auth Proxy HCVs," About # The CloudSQL Auth Proxy section configures the CloudSQL Auth Proxy for deploying Pachyderm on GCP with CloudSQL.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: Enabled Disabled cloudsqlAuthProxy: connectionName: &#34;&#34; # may be found by running `gcloud sql instances describe INSTANCE_NAME --project PROJECT_ID` serviceAccount: &#34;&#34; # defines the account used to connect to the cloudSql instance iamLogin: false port: 5432 # the cloudql database port to expose. The default is `5432` enabled: true # controls whether to deploy the cloudsqlAuthProxy. Default is false. image: repository: &#34;gcr.io/cloudsql-docker/gce-proxy&#34; # the image repo to pull from; replicates --registry to pachctl pullPolicy: &#34;IfNotPresent&#34; tag: &#34;1.23.0&#34; # the image repo to pull from; replicates the --dash-image argument to pachctl deploy. priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] podLabels: {} # specifies labels to add to the dash pod. resources: {} # specifies the resource request and limits. # requests: # # The proxy&#39;s memory use scales linearly with the number of active # # connections. Fewer open connections will use less memory. Adjust # # this value based on your application&#39;s requirements. # memory: &#34;&#34; # # The proxy&#39;s CPU use scales linearly with the amount of IO between # # the database and the application. Adjust this value based on your # # application&#39;s requirements. # cpu: &#34;&#34; service: labels: {} # specifies labels to add to the cloudsql auth proxy service. type: ClusterIP # specifies the Kubernetes type of the cloudsql auth proxy service. The default is `ClusterIP`. cloudsqlAuthProxy: enabled: false ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
325,OpenID Connect HCVs," About # The OIDC section of the helm chart enables you to set up authentication through upstream IDPs. To use authentication, you must have an Enterprise license.
We recommend setting up this section alongside the Enterprise Server section of your Helm chart so that you can easily scale multiple clusters using the same authentication configurations.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: Mock IDP Upstream IDP Additional Clients oidc: issuerURI: &#34;&#34; # inferred if running locally or using ingress requireVerifiedEmail: false # if true, email verification is required to authenticate IDTokenExpiry: 24h # if set, specifies the duration where OIDC ID Tokens are valid; parsed into golang&#39;s time.Duration: https://pkg.go.dev/time#example-ParseDuration RotationTokenExpiry: 48h # If set, enables OIDC rotation tokens, and specifies the duration where they are valid. userAccessibleOauthIssuerHost: &#34;&#34; # (Optional) Only set in cases where the issuerURI is not user accessible (ie. localhost install) mockIDP: true # if true, ignores upstreamIDPs in favor of a placeholder IDP with the username/password of &#34;admin&#34;/&#34;password&#34; oidc: issuerURI: &#34;&#34; # inferred if running locally or using ingress requireVerifiedEmail: false # if true, email verification is required to authenticate IDTokenExpiry: 24h # if set, specifies the duration where OIDC ID Tokens are valid; parsed into golang&#39;s time.Duration: https://pkg.go.dev/time#example-ParseDuration RotationTokenExpiry: 48h # If set, enables OIDC rotation tokens, and specifies the duration where they are valid. userAccessibleOauthIssuerHost: &#34;&#34; # (Optional) Only set in cases where the issuerURI is not user accessible (ie. localhost install) upstreamIDPs: # defines a list of Identity Providers to use for authentication. https://dexidp.io/docs/connectors/ - id: idpConnector config: issuer: &#34;&#34; clientID: &#34;&#34; clientSecret: &#34;&#34; redirectURI: &#34;http://localhost:30658/callback&#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: idpConnector type: oidc - id: okta config: issuer: &#34;https://dev-84362674.okta.com&#34; clientID: &#34;client_id&#34; clientSecret: &#34;notsecret&#34; redirectURI: &#34;http://localhost:30658/callback&#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: okta type: oidc upstreamIDPsSecretName: &#34;&#34; # passes the upstreamIDPs value via an existing k8s secret (key: `upstream-idps`) dexCredentialSecretName: &#34;&#34; # mounts a credential file to the pachd pod at /dexcreds/ (e.g., serviceAccountFilePath: /dexcreds/googleAuth.json); required for some dex configs like Google. mockIDP: false # if true, ignores upstreamIDPs in favor of a placeholder IDP with the username/password of &#34;admin&#34;/&#34;password&#34; oidc: issuerURI: &#34;&#34; # inferred if running locally or using ingress requireVerifiedEmail: false # if true, email verification is required to authenticate IDTokenExpiry: 24h # if set, specifies the duration where OIDC ID Tokens are valid; parsed into golang&#39;s time.Duration: https://pkg.go.dev/time#example-ParseDuration RotationTokenExpiry: 48h # If set, enables OIDC rotation tokens, and specifies the duration where they are valid. userAccessibleOauthIssuerHost: &#34;&#34; # (Optional) Only set in cases where the issuerURI is not user accessible (ie. localhost install) upstreamIDPs: # defines a list of Identity Providers to use for authentication. https://dexidp.io/docs/connectors/ - id: idpConnector config: issuer: &#34;&#34; clientID: &#34;&#34; clientSecret: &#34;&#34; redirectURI: &#34;http://localhost:30658/callback&#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: idpConnector type: oidc - id: okta config: issuer: &#34;https://dev-84362674.okta.com&#34; clientID: &#34;client_id&#34; clientSecret: &#34;notsecret&#34; redirectURI: &#34;http://localhost:30658/callback&#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: okta type: oidc upstreamIDPsSecretName: &#34;&#34; # passes the upstreamIDPs value via an existing k8s secret (key: `upstream-idps`) dexCredentialSecretName: &#34;&#34; # mounts a credential file to the pachd pod at /dexcreds/ (e.g., serviceAccountFilePath: /dexcreds/googleAuth.json); required for some dex configs like Google. mockIDP: false # if true, ignores upstreamIDPs in favor of a placeholder IDP with the username/password of &#34;admin&#34;/&#34;password&#34; additionalOIDCClient: - id: example-app secret: example-app-secret name: &#39;Example App&#39; redirectURIs: - &#39;http://127.0.0.1:5555/callback&#39; additionalClientsSecretName: &#34;&#34; ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
326,Test Connection HCVs," About # The Test Connection section is used by Pachyderm to test the connection during installation. This config is used by organizations that do not have permission to pull Docker images directly from the Internet, and instead need to mirror locally.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
testConnection: image: repository: alpine tag: latest ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
327,Proxy HCVs," About # Proxy is a service that handles all Pachyderm traffic (S3, Console, OIDC, Dex, GRPC) on a single port; It&rsquo;s great for exposing directly to the Internet.
‚ö†Ô∏è Proxy will soon be the mandatory way to interact with Pachyderm, replacing Ingress. See Upgrade to Embedded Proxy for more details.
Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.
Options: Load Balancer With Legacy Ports Node Port proxy: enabled: true host: &#34;&#34; # the external hostname (including port if nonstandard) that the proxy will be reachable at. replicas: 1 # each replica can handle 50,000 concurrent connections. There is an affinity rule to prefer scheduling the proxy pods on the same node as pachd, so a number here that matches the number of pachd replicas is a fine configuration. (Note that we don&#39;t guarantee to keep the proxy&lt;-&gt;pachd traffic on-node or even in-region.) image: repository: &#34;envoyproxy/envoy&#34; tag: &#34;v1.22.0&#34; pullPolicy: &#34;IfNotPresent&#34; resources: requests: cpu: 100m memory: 512Mi limits: memory: 512Mi # proxy sheds traffic before using 500MB of RAM. labels: {} annotations: {} service: # configures the service that routes traffic to the proxy. type: LoadBalancer # can be ClusterIP, NodePort, or LoadBalancer. loadBalancerIP: &#34;&#34; # If the service is a LoadBalancer, you can specify the IP address to use; defaults to 80. httpPort: 80 # The port to serve plain HTTP traffic on. httpsPort: 443 # The port to serve HTTPS traffic on, if enabled below. annotations: {} labels: {} # adds labels to the service itself (not the selector!). tls: # Incompatible with legacy ports. enabled: false secretName: &#34;&#34; # must contain &#34;tls.key&#34; and &#34;tls.crt&#34; keys; generate with kubectl create secret tls &lt;name&gt; --key=tls.key --cert=tls.cert&#34; secret: {} # generate the secret from values here. This is intended only for unit tests. proxy: enabled: true host: &#34;&#34; # the external hostname (including port if nonstandard) that the proxy will be reachable at. replicas: 1 # each replica can handle 50,000 concurrent connections. There is an affinity rule to prefer scheduling the proxy pods on the same node as pachd, so a number here that matches the number of pachd replicas is a fine configuration. (Note that we don&#39;t guarantee to keep the proxy&lt;-&gt;pachd traffic on-node or even in-region.) image: repository: &#34;envoyproxy/envoy&#34; tag: &#34;v1.22.0&#34; pullPolicy: &#34;IfNotPresent&#34; resources: requests: cpu: 100m memory: 512Mi limits: memory: 512Mi # proxy sheds traffic before using 500MB of RAM. labels: {} annotations: {} service: # configures the service that routes traffic to the proxy. type: LoadBalancer # can be ClusterIP, NodePort, or LoadBalancer. loadBalancerIP: &#34;&#34; # If the service is a LoadBalancer, you can specify the IP address to use; defaults to 80. httpPort: 80 # The port to serve plain HTTP traffic on. httpsPort: 443 # The port to serve HTTPS traffic on, if enabled below. annotations: {} labels: {} # adds labels to the service itself (not the selector!). legacyPorts: # proxy can serve backend services on a numbered port if not set to 0. If this service is of type NodePort, the port numbers here will be used for the node port, and will need to be in the node port range. console: 0 # legacy 30080, conflicts with default httpNodePort grpc: 0 # legacy 30650 s3Gateway: 0 # legacy 30600 oidc: 0 # legacy 30657 identity: 0 # legacy 30658 metrics: 0 # legacy 30656 proxy: enabled: true host: &#34;&#34; # the external hostname (including port if nonstandard) that the proxy will be reachable at. replicas: 1 # each replica can handle 50,000 concurrent connections. There is an affinity rule to prefer scheduling the proxy pods on the same node as pachd, so a number here that matches the number of pachd replicas is a fine configuration. (Note that we don&#39;t guarantee to keep the proxy&lt;-&gt;pachd traffic on-node or even in-region.) image: repository: &#34;envoyproxy/envoy&#34; tag: &#34;v1.22.0&#34; pullPolicy: &#34;IfNotPresent&#34; resources: requests: cpu: 100m memory: 512Mi limits: memory: 512Mi # proxy sheds traffic before using 500MB of RAM. labels: {} annotations: {} service: # configures the service that routes traffic to the proxy. type: NodePort # can be ClusterIP, NodePort, or LoadBalancer. httpPort: 80 # The port to serve plain HTTP traffic on. httpsPort: 443 # The port to serve HTTPS traffic on, if enabled below. httpNodePort: 30080 # If the service is a NodePort, you can specify the port to receive HTTP traffic on. httpsNodePort: 30443 annotations: {} labels: {} # adds labels to the service itself (not the selector!). legacyPorts: # proxy can serve backend services on a numbered port if not set to 0. If this service is of type NodePort, the port numbers here will be used for the node port, and will need to be in the node port range. console: 0 # legacy 30080, conflicts with default httpNodePort grpc: 0 # legacy 30650 s3Gateway: 0 # legacy 30600 oidc: 0 # legacy 30657 identity: 0 # legacy 30658 metrics: 0 # legacy 30656 ‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.
"
328,Language Clients,"pachctl is the command-line tool you use to interact with a Pachyderm cluster in your terminal. However, external applications might need to interact with Pachyderm directly through our APIs.
In this case, Pachyderm offers language specific SDKs in Go, Python, and JS.
Go Client # The Pachyderm team officially supports the Go client. It implements most of the functionalities provided with the pachctl CLI tool.
Generate And Serve The godocs Locally # Golang&rsquo;s package (godoc), installed by default by the Go installer, can generate the Go client&rsquo;s documentation from the go code.
To generate the docs:
Set your GOPATH:
export PATH=$(go env GOPATH)/bin:$PATH In Pachyderm&rsquo;s root directory, start the godocs server:
go run golang.org/x/tools/cmd/godoc -http=:6060 -goroot=&#34;&lt;your go root directory - for example: /Users/yourusername/pachyderm&gt;&#34; See https://pkg.go.dev/golang.org/x/tools/cmd/godoc for the complete list of flags available.
In your favorite browser, run localhost:6060/pkg/
‚ö†Ô∏è A compatible version of gRPC is needed when using the Go client. You can identify the compatible version by searching for the version number next to replace google.golang.org/grpc =&gt; google.golang.org/grpc in https://github.com/pachyderm/pachyderm/blob/master/go.mod then:
go get google.golang.org/grpc cd $GOPATH/src/google.golang.org/grpc git checkout v1.29.1 Running Go Examples # The Pachyderm godocs reference (see generation instructions above) provides examples of how you can use the Go client API. You need to have a running Pachyderm cluster to run these examples.
Make sure that you use your pachd_address in client.NewFromAddress(&quot;&lt;your-pachd-address&gt;:30650&quot;). For example, if you are testing on minikube, run minikube ip to get this information.
See the OpenCV Example in Go for more information.
Python Client # The Python client python-pachyderm is officially supported by the Pachyderm team. It implements most of the functionalities provided with the pachctl CLI tool allowing you to easily integrate operations like create repo, put a file, or create pipeline into your python applications.
‚ÑπÔ∏è Use python-pachyderm v7.3 with Pachyderm 2.5.x.
You will find all you need to get you started or dive into the details of the available modules and functions in the API documentation, namely:
The installation instructions and links to PyPI. A quick &ldquo;Hello World&rdquo; example to jumpstart your understanding of the API. Links to python-pachyderm main Github repository with a list of useful examples. As well as the entire reference API. Node Client # Our Javascript client node-pachyderm is a library officially supported by Pachyderm and used in production by Pachyderm Console.
Today, we provide only read operations as shown in Console. Over time, we will add additional functionality to the SDK. However, there are no near-term plans to reach parity with python-pachyderm yet.
Please get in touch with us if you are interested in contributing or ask your questions on our dedicated slack channel.
You will find installations instructions and a first quick overview of how to use the library in our public repository. Check also our opencv example.
Other languages # Pachyderm uses a simple protocol buffer API. Protobufs support other languages, any of which can be used to programmatically use Pachyderm. We have not built clients for them yet. It is an easy way to contribute to Pachyderm if you are looking to get involved.
"
329,Pachyderm Supported Releases & Features,"Pachyderm lists the status for each release and feature, so that you can understand expectations for support and stability.
Supported Releases # Pachyderm supports the latest Generally Available (GA) release and the previous two major and minor GA releases. Releases three or more major and minor versions back are considered End of Life (EOL).
Release Status by Version # Version Release Status Support 2.5.x GA Yes 2.4.x GA Yes 2.3.x GA Yes 2.2.x GA No 2.1.x GA No 2.0.x GA No 1.13.x GA No 1.12.x EOL No 1.11.x EOL No 1.10.x EOL No &lt; 1.9.11 EOL No Releases Under Development # A release under development may undergo several pre-release stages before becoming Generally Available (GA). These pre-releases enable the Pachyderm team to do development and testing in partnership with our users before a release is considered ready for a Generally Availability (GA).
alpha &gt; beta &gt; Release Candidate (RC) &gt; Generally Available (GA)
Release Status # Alpha # alpha releases are a pre-release version of a product, intended for development and testing purposes only. alpha releases include many bugs and unfinished features, and are only suitable for early technical feedback. alpha releases should not be used in a production environment.
Beta # beta releases are a pre-release version of a product, intended for development and testing purposes only, and include a wider range of users than an alpha release. beta releases should not be used in a production environment.
Release Candidate (RC) # Release Candidate or RC releases are a pre-release version of a product, intended for users to prepare for a GA release. RC releases should not be used in a production environment.
Generally Available (GA) # Generally Available or GA releases are considered stable and intended for production usage.
Contain new features, fixed defects, and patched security vulnerabilities. Support is available from Pachyderm. End of Life (EOL) # End of Life or EOL indicates the release will no longer receive support.
Documentation will be archived. Release artifacts will remain available. We keep release artifacts on Github and Docker Hub. Support is no longer available for End of Life (EOL) releases. Support can assist with upgrading to a newer version. Supported Features # Stable # stable indicates that the Pachyderm team believes the feature is ready for use in a production environment.
The feature&rsquo;s API is stable and unlikely to change. There are no major defects for the feature. The Pachyderm team believes there is a sufficient amount of testing, including automated tests, community testing, and user production environments. Support is available from Pachyderm. Experimental # experimental indicates that a feature has not met the Pachyderm team&rsquo;s criteria for production use. Therefore, these features should be used with caution in production environments. experimental features are likely to change, have outstanding defects, and/or missing documentation. Users considering using experimental features should contact Pachyderm for guidance.
Production use is not recommended without guidance from Pachyderm. These features may have missing documentation, lack of examples, and lack of content. Support is available from Pachyderm, which may be limited in scope based on our guidance. Deprecated # deprecated indicates that a feature is no longer developed. Users of deprecated features are encouraged to upgrade or migrate to newer versions or compatible features. deprecated features become End of Life (EOL) features after 6 months.
Users continuing to use deprecated features should contact support to migrate to features. Support is available from Pachyderm. End of Life (EOL) Features # End of Life or EOL indicates that a feature is no longer supported.
Documentation will be archived. Support is no longer available for End of Life (EOL) features. Support can assist upgrading to a newer version. Experimental Features # Feature Version Date Service Pipelines 1.9.9 2019-11-06 JupyterLab Extension 0.6.3 2022-09-21 Deprecated Features # Feature Version EOL Date End of Life (EOL) Features # Feature Version EOL Date Build Pipelines 2.0.0 2021-07-25 Git Inputs 2.0.0 2021-07-25 pachctl deploy 2.0.0 2021-07-25 Spouts: Named Pipes 2.0.0 2021-07-25 Vault Plugin 2.0.0 2021-07-25 pachctl put file --split 2.0.0 2021-07-25 MaxQueueSize 2.0.0 2021-07-25 S3v2 signatures 1.12.0 2021-01-05 atom inputs 1.9.0 2019-06-12 "
330,Pipeline Scaling Limits (CE),"Our free Pachyderm Community Edition contains built-in scaling limitations and parallelism thresholds. To scale beyond these limits, request a Pachyderm Enterprise trial token and enjoy unlimited scaling, and more.
üìñ You might qualify for a free Enterprise license.
Pachyderm offers activation keys for proofs-of-concept, startups, academic, nonprofit, or open-source projects. Tell us about your project!.
Scaling Limits # Number of concurrent pipelines deployed Number of workers for each pipeline Community Users can deploy up to 16 pipelines. Community Users can run up to 8 workers in parallel on each pipeline. What happens when you exceed those limits? # As a general rule, Pachyderm provides an error message in the STDERR whenever a limit is encountered that prevents you from successfully running a command. In that case, the alert message links to a free trial request form.
Limit on the number of pipelines # When exceeding the number of pipelines:
pachctl create pipeline fails once the maximum number of pipelines is reached.
pachctl update pipeline and pachctl edit pipeline succeed on existing pipelines, fail when attempting to create pipelines beyond the limit.
‚ÑπÔ∏è If update pipeline fails for any other reason, it does not log any message related to pipeline limits.
All of the commands listed above create a distinct message to STDERR and to the pachd logs. This message includes information such as the limit on the number of pipelines in the Community Edition, the total number of pipelines deployed, and provides a link to request an Enterprise key to lift those limitations.
all other list, run, start, stop pipeline commands&rsquo; behavior remains unchanged. Limit on the number of workers per pipeline # When constant parallelism &gt; 8:
pachctl create pipeline and pachctl update pipeline fail. A message to STDERR and pachd logs is generated. You will need to update your pipeline specification file accordingly or activate an Enterprise license. What happens when your license expires? # If your Enterprise License has expired and you have more than 16 pipelines, all existing pipelines continue to work. However, you will not be able to create additional pipelines. Same behavior if you upgrade your cluster.
‚ö†Ô∏è Restoring or installing Pachyderm with an expired license will fail.
‚ÑπÔ∏è Pipelines automatically generated by the system (for example cron&hellip;) are not considered when assessing the total number of pipelines deployed. The limit applies to user-created pipelines only.
"
331,Pipeline Specification (PPS),"This document discusses each of the fields present in a pipeline specification. To see how to use a pipeline spec to create a pipeline, refer to the create pipeline section.
Before You Start # Pachyderm&rsquo;s pipeline specifications can be written in JSON or YAML. Pachyderm uses its json parser if the first character is {. A pipeline specification file can contain multiple pipeline declarations at once. Minimal Spec # Generally speaking, the only attributes that are strictly required for all scenarios are pipeline.name and transform. Beyond those, other attributes are conditionally required based on your pipeline&rsquo;s use case. The following are a few examples of common use cases along with their minimally required attributes.
Use Case: Cron Egress (DB) Egress (Storage) Input Service Spout S3 { &#34;pipeline&#34;: { &#34;project&#34;: 1, &#34;name&#34;: &#34;wordcount&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;cron&#34;: { { &#34;name&#34;: string, &#34;spec&#34;: string, &#34;repo&#34;: string, &#34;start&#34;: time, &#34;overwrite&#34;: bool } } } } { &#34;pipeline&#34;: { &#34;project&#34;: 1, &#34;name&#34;: &#34;wordcount&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;egress&#34;: { &#34;sql_database&#34;: { &#34;url&#34;: string, &#34;file_format&#34;: { &#34;type&#34;: string, &#34;columns&#34;: [string] }, &#34;secret&#34;: { &#34;name&#34;: string, &#34;key&#34;: &#34;PACHYDERM_SQL_PASSWORD&#34; } } }, } { &#34;pipeline&#34;: { &#34;project&#34;: 1, &#34;name&#34;: &#34;wordcount&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;egress&#34;: { &#34;URL&#34;: &#34;s3://bucket/dir&#34; }, } { &#34;pipeline&#34;: { &#34;project&#34;: 1, &#34;name&#34;: &#34;wordcount&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } } } { &#34;pipeline&#34;: { &#34;project&#34;: 1, &#34;name&#34;: &#34;wordcount&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;service&#34;: { &#34;internal_port&#34;: int, &#34;external_port&#34;: int }, } { &#34;pipeline&#34;: { &#34;project&#34;: 1, &#34;name&#34;: &#34;wordcount&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;spout&#34;: { }, } { &#34;pipeline&#34;: { &#34;project&#34;: 1, &#34;name&#34;: &#34;wordcount&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;s3_out&#34;: true, } "
332,Autoscaling PPS," Spec # &#34;autoscaling&#34;: bool Behavior # The autoscaling attribute in a Pachyderm Pipeline Spec is used to specify whether the pipeline should automatically scale up or down based on the processing load.
If the autoscaling attribute is set to true, Pachyderm will monitor the processing load of the pipeline, and automatically scale up or down the number of worker nodes as needed to keep up with the demand. This can help to ensure that the pipeline is always running at optimal efficiency, without wasting resources when the load is low.
The maximum number of workers is controlled by the parallelism_spec. A pipeline with no outstanding jobs will go into standby. A pipeline in a standby state consumes no resources. When to Use # You should consider using the autoscaling attribute in a Pachyderm Pipeline Spec when you have a workload that has variable processing requirements or when the processing load of your pipeline is difficult to predict.
Example scenarios:
Processing unpredictable workloads: If you have a workload that has variable processing requirements, it can be difficult to predict the number of worker nodes that will be needed to keep up with the demand. In this case, you could use the autoscaling attribute to automatically scale the number of worker nodes up or down based on the processing load.
Processing large datasets: If you have a pipeline that is processing a large dataset, it can be difficult to predict the processing requirements for the pipeline. In this case, you could use the autoscaling attribute to automatically scale the number of worker nodes based on the processing load, in order to keep up with the demand.
Handling bursty workloads: If you have a workload that has periods of high demand followed by periods of low demand, it can be difficult to predict the processing requirements for the pipeline. In this case, you could use the autoscaling attribute to automatically scale the number of worker nodes up or down based on the processing load, in order to handle the bursty demand.
"
333,Datum Set Spec PPS," Spec # &#34;datum_set_spec&#34;: { &#34;number&#34;: int, &#34;size_bytes&#34;: int, &#34;chunks_per_worker&#34;: int, } Attributes # Attribute Description number The desired number of datums in each datum set. If specified, each datum set will contain the specified number of datums. If the total number of input datums is not evenly divisible by the number of datums per set, the last datum set may contain fewer datums than the others. size_bytes The desired target size of each datum set in bytes. If specified, Pachyderm will attempt to create datum sets with the specified size, though the actual size may vary due to the size of the input files. per_worker The desired number of datum sets that each worker should process at a time. This field is similar to number, but specifies the number of sets per worker instead of the number of datums per set. 4 datum sets per worker is the default. Behavior # The datum_set_spec attribute in a Pachyderm Pipeline Spec is used to control how the input data is partitioned into individual datum sets for processing. Datum sets are the unit of work that workers claim, and each worker can claim 1 or more datums. Once done processing, it commits a full datum set.
number if nonzero, specifies that each datum set should contain number datums. Sets may contain fewer if the total number of datums don&rsquo;t divide evenly. If you lower the number to 1 it&rsquo;ll update after every datum, the cost is extra load on etcd which can slow other stuff down.
size_bytes , if nonzero, specifies a target size for each set of datums. Sets may be larger or smaller than size_bytes, but will usually be pretty close to size_bytes in size.
per_worker, if nonzero, specifies how many datum sets should be created for each worker. It can&rsquo;t be set with number or size_bytes.
When to Use # You should consider using the datum_set_spec attribute in your Pachyderm pipeline when you are experiencing stragglers, which are situations where most of the workers are idle but a few are still processing jobs. This can happen when the work is not divided up in a balanced way, which can cause some workers to be overloaded with work while others are idle.
"
334,Datum Timeout PPS," Spec # &#34;datum_timeout&#34;: string, Behavior # The datum_timeout attribute in a Pachyderm pipeline is used to specify the maximum amount of time that a worker is allowed to process a single datum in the pipeline.
When a worker begins processing a datum, Pachyderm starts a timer that tracks the elapsed time since the datum was first assigned to the worker. If the worker has not finished processing the datum before the datum_timeout period has elapsed, Pachyderm will automatically mark the datum as failed and reassign it to another worker to retry. This helps to ensure that slow or problematic datums do not hold up the processing of the entire pipeline.
Other considerations:
Not set by default, allowing a datum to process for as long as needed. Takes precedence over the parallelism or number of datums; no single datum is allowed to exceed this value. The value must be a string that represents a time value, such as 1s, 5m, or 15h. When to Use # You should consider using the datum_timeout attribute in your Pachyderm pipeline when you are processing large or complex datums that may take a long time to process, and you want to avoid having individual datums hold up the processing of the entire pipeline.
For example, if you are processing images or videos that are particularly large, or if your pipeline is doing complex machine learning or deep learning operations that can take a long time to run on individual datums, setting a reasonable datum_timeout can help ensure that your pipeline continues to make progress even if some datums are slow or problematic.
"
335,Datum Tries PPS," Spec # &#34;datum_tries&#34;: int, Behavior # The datum_tries attribute in a Pachyderm pipeline specifies the maximum number of times a datum can be retried if it fails to process. When a datum fails to process, either because of an error in the processing logic or because it exceeds the datum_timeout value, Pachyderm will automatically retry the datum, up to the number of times specified in datum_tries.
Each retry of a datum is treated as a new attempt, and the datum is added back to the job queue for processing. The retry process is transparent to the user and happens automatically within the Pachyderm system.
Other considerations:
Setting to 1 attempts a job once with no retries. If the operation succeeds in retry attempts, then the job is marked as successful. Otherwise, the job is marked as failed. When to Use # You should consider setting a higher datum_tries count if your pipeline has a large number of datums that are prone to errors or timeouts, or if the datums you are working with have to be imported or fetched (via data ingress) from an external source.
"
336,Description PPS," Spec # &#34;description&#34;: string, Behavior # description is displayed in your pipeline details when viewed from pachCTL or console.
When to Use # It&rsquo;s recommended to always provide meaningful descriptions to your Pachyderm resources.
"
337,Egress PPS," Spec # &#34;egress&#34;: { // Egress to an object store &#34;URL&#34;: &#34;s3://bucket/dir&#34; // Egress to a database &#34;sql_database&#34;: { &#34;url&#34;: string, &#34;file_format&#34;: { &#34;type&#34;: string, &#34;columns&#34;: [string] }, &#34;secret&#34;: { &#34;name&#34;: string, &#34;key&#34;: &#34;PACHYDERM_SQL_PASSWORD&#34; } } }, Attributes # Attribute Description URL The URL of the object store where the pipeline&rsquo;s output data should be written. sql_database An optional field that is used to specify how the pipeline should write output data to a SQL database. url The URL of the SQL database, in the format postgresql://user:password@host:port/database. file_format The file format of the output data, which can be specified as csv or tsv. This field also includes the column names that should be included in the output. secret The name and key of the Kubernetes secret that contains the password for the SQL database. Behavior # The egress field in a Pachyderm Pipeline Spec is used to specify how the pipeline should write the output data. The egress field supports two types of outputs: writing to an object store and writing to a SQL database.
Data is pushed after the user code finishes running but before the job is marked as successful. For more information, see Egress Data to an object store or Egress Data to a database.
This is required if the pipeline needs to write output data to an external storage system.
When to Use # You should use the egress field in a Pachyderm Pipeline Spec when you need to write the output data from your pipeline to an external storage system, such as an object store or a SQL database.
Example scenarios:
Long-term data storage: If you need to store the output data from your pipeline for a long time, you can use the egress field to write the data to an object store, such as Amazon S3 or Google Cloud Storage.
Data sharing: If you need to share the output data from your pipeline with external users or systems, you can use the egress field to write the data to an object store that is accessible to those users or systems.
Analytics and reporting: If you need to perform further analytics or reporting on the output data from your pipeline, you can use the egress field to write the data to a SQL database that can be used for those purposes.
"
338,Input Cron PPS," Spec # &#34;input&#34;: { &#34;cron&#34;: { { &#34;name&#34;: string, &#34;spec&#34;: string, &#34;repo&#34;: string, &#34;start&#34;: time, &#34;overwrite&#34;: bool } } } Attributes # Attribute Description name The name of the cron job, which should be unique within the Pachyderm cluster. spec The cron schedule for the job, specified using the standard cron format or macros. See schedule macros for examples. Pachyderm also supports non-standard schedules, such as &quot;@daily&quot;. repo The name of the input repository that the cron job should read data from; default:&lt;pipeline-name&gt;_&lt;input-name&gt; start An optional field that specifies the start time for the cron job. This is useful for running the job on a specific date in the future. If not specified, starts immediately. Specifying a time enables you to run on matching times from the past or skip times from the present and only start running on matching times in the future. Format the time value according to RFC3339. overwrite An optional field that defines whether you want the timestamp file to be overwritten on each tick; defaults to simply writing new files on each tick. By default, when &quot;overwrite&quot; is disabled, ticks accumulate in the cron input repo. When &quot;overwrite&quot; is enabled, Pachyderm erases the old ticks and adds new ticks with each commit. If you do not add any manual ticks or run pachctl run cron, only one tick file per commit (for the latest tick) is added to the input repo. Behavior # The input field in a Pachyderm Pipeline Spec is used to specify the inputs to the pipeline, which are the Pachyderm repositories that the pipeline should read data from. The input field can include both static and dynamic inputs.
The cron field within the input field is used to specify a dynamic input that is based on a cron schedule. This is useful for pipelines that need to process data on a regular schedule, such as daily or hourly.
A repo is created for each cron input. When a Cron input triggers, pachd commits a single file, named by the current RFC3339 timestamp to the repo which contains the time which satisfied the spec.
When to Use # You should use a cron input in a Pachyderm Pipeline Spec when you need to process data on a regular schedule, such as hourly or daily. A cron input allows you to specify a schedule for the pipeline to run, and Pachyderm will automatically trigger the pipeline at the specified times.
Example scenarios:
Batch processing: If you have a large volume of data that needs to be processed on a regular schedule, a cron input can be used to trigger the processing automatically, without the need for manual intervention.
Data aggregation: If you need to collect data from different sources and process it on a regular schedule, a cron input can be used to automate the data collection and processing.
Report generation: If you need to generate reports on a regular schedule, a cron input can be used to trigger the report generation process automatically.
"
339,Input Cross PPS," Spec # &#34;input&#34;: { &#34;cross&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ... ] } Attributes # Attribute Description name The name of the PFS input that appears in the INPUT field when you run the pachctl list pipeline command. If an input name is not specified, it defaults to the name of the repo. repo Specifies the name of the Pachyderm repository that contains the input data. branch The branch to watch for commits. If left blank, Pachyderm sets this value to master. glob A wildcard pattern that defines how a dataset is broken up into datums for further processing. When you use a glob pattern in a group input, it creates a naming convention that Pachyderm uses to group the files. lazy Controls how the data is exposed to jobs. The default is false which means the job eagerly downloads the data it needs to process and exposes it as normal files on disk. If lazy is set to true, data is exposed as named pipes instead, and no data is downloaded until the job opens the pipe and reads it. If the pipe is never opened, then no data is downloaded. empty_files Controls how files are exposed to jobs. If set to true, it causes files from this PFS to be presented as empty files. This is useful in shuffle pipelines where you want to read the names of files and reorganize them by using symlinks. s3 Indicates whether the input data is stored in an S3 object store. Behavior # input.cross is an array of inputs to cross. The inputs do not have to be pfs inputs. They can also be union and cross inputs.
A cross input creates tuples of the datums in the inputs. In the example below, each input includes individual datums, such as if foo and bar were in the same repository with the glob pattern set to /*. Alternatively, each of these datums might have come from separate repositories with the glob pattern set to / and being the only file system objects in these repositories.
| inputA | inputB | inputA ‚®Ø inputB | | ------ | ------ | --------------- | | foo | fizz | (foo, fizz) | | bar | buzz | (foo, buzz) | | | | (bar, fizz) | | | | (bar, buzz) | The cross inputs above do not take a name and maintain the names of the sub-inputs. In the example above, you would see files under /pfs/inputA/... and /pfs/inputB/....
When to Use # You should use a cross input in a Pachyderm Pipeline Spec when you need to perform operations on combinations of data from multiple Pachyderm repositories. The cross input allows you to generate a set of combinations of files between two or more repositories, which can be used as the input to your pipeline.
Example scenarios:
Data analysis: If you have data from multiple sources that you need to combine and analyze, a cross input can be used to generate a set of combinations of data that can be used as the input to your analysis.
Machine learning: If you need to train a machine learning model on combinations of data from multiple sources, a cross input can be used to generate a set of combinations of data that can be used as the input to your model.
Report generation: If you need to generate reports that combine data from multiple sources, a cross input can be used to generate a set of combinations of data that can be used as the input to your report generation process.
"
340,Input Group PPS," Spec # &#34;input&#34;: { &#34;group&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;group_by&#34;: string, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;group_by&#34;: string, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ] } Attributes # Attribute Description name The name of the PFS input that appears in the INPUT field when you run the pachctl list pipeline command. If an input name is not specified, it defaults to the name of the repo. repo Specifies the name of the Pachyderm repository that contains the input data. branch The branch to watch for commits. If left blank, Pachyderm sets this value to master. glob A wildcard pattern that defines how a dataset is broken up into datums for further processing. When you use a glob pattern in a group input, it creates a naming convention that Pachyderm uses to group the files. group_by A parameter that is used to group input files by a specific pattern. lazy Controls how the data is exposed to jobs. The default is false which means the job eagerly downloads the data it needs to process and exposes it as normal files on disk. If lazy is set to true, data is exposed as named pipes instead, and no data is downloaded until the job opens the pipe and reads it. If the pipe is never opened, then no data is downloaded. empty_files Controls how files are exposed to jobs. If set to true, it causes files from this PFS to be presented as empty files. This is useful in shuffle pipelines where you want to read the names of files and reorganize them by using symlinks. s3 Indicates whether the input data is stored in an S3 object store. Behavior # The group input in a Pachyderm Pipeline Spec allows you to group input files by a specific pattern.
To use the group input, you specify one or more PFS inputs with a group_by parameter. This parameter specifies a pattern or field to use for grouping the input files. The resulting groups are then passed to your pipeline as a series of grouped datums, where each datum is a single group of files.
You can specify multiple group input fields in a Pachyderm Pipeline Spec, each with their own group_by parameter. This allows you to group files by multiple fields or patterns, and pass each group to your pipeline as a separate datum.
The glob and group_by parameters must be configured.
When to Use # You should consider using the group input in a Pachyderm Pipeline Spec when you have large datasets with multiple files that you want to partition or group by a specific field or pattern. This can be useful in a variety of scenarios, such as when you need to perform complex data analysis on a large dataset, or when you need to group files by some attribute or characteristic in order to facilitate further processing.
Example scenarios:
Partitioning data by time: If you have a large dataset that spans a long period of time, you might want to partition it by day, week, or month in order to perform time-based analysis or modeling. In this case, you could use the group input field to group files by date or time, and then process each group separately.
Grouping data by user or account: If you have a dataset that includes data from multiple users or accounts, you might want to group the data by user or account in order to perform user-based analysis or modeling. In this case, you could use the group input field to group files by user or account, and then process each group separately.
Partitioning data by geography: If you have a dataset that includes data from multiple geographic regions, you might want to partition it by region in order to perform location-based analysis or modeling. In this case, you could use the group input field to group files by region, and then process each group separately.
"
341,Input Join PPS," Spec # &#34;input&#34;: { &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;join_on&#34;: string, &#34;outer_join&#34;: bool, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;join_on&#34;: string, &#34;outer_join&#34;: bool, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ] } Behavior # A join input must have the glob and join_on parameters configured to work properly. A join can combine multiple PFS inputs. You can optionally add &quot;outer_join&quot;: true to your PFS input. In that case, you will alter the join&rsquo;s behavior from a default &ldquo;inner-join&rdquo; (creates a datum if there is a match only) to a &ldquo;outer-join&rdquo; (the repos marked as &quot;outer_join&quot;: true will see a datum even if there is no match). You can set 0 to many PFS input to &quot;outer_join&quot;: true within your join. "
342,Input PFS PPS," Spec # &#34;input&#34;: { &#34;pfs&#34;: { &#34;project&#34;: string, &#34;name&#34;: string, &#34;repo&#34;: string, &#34;repo_type&#34;:string, &#34;branch&#34;: string, &#34;commit&#34;:string, &#34;glob&#34;: string, &#34;join_on&#34;:string, &#34;outer_join&#34;: bool, &#34;group_by&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool, &#34;trigger&#34;: { &#34;branch&#34;: string, &#34;all&#34;: bool, &#34;cron_spec&#34;: string, }, } } Behavior # input.pfs.name is the name of the input. An input with the name XXX is visible under the path /pfs/XXX when a job runs. Input names must be unique if the inputs are crossed, but they may be duplicated between PFSInputs that are combined by using the union operator. This is because when PFSInputs are combined, you only ever see a datum from one input at a time. Overlapping the names of combined inputs allows you to write simpler code since you no longer need to consider which input directory a particular datum comes from. If an input&rsquo;s name is not specified, it defaults to the name of the repo. Therefore, if you have two crossed inputs from the same repo, you must give at least one of them a unique name.
"
343,Input Union PPS," Spec # &#34;input&#34;: { &#34;union&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ... ] } Behavior # input.union is an array of inputs to combine. The inputs do not have to be pfs inputs. They can also be union and cross inputs.
Union inputs take the union of other inputs. In the example below, each input includes individual datums, such as if foo and bar were in the same repository with the glob pattern set to /*. Alternatively, each of these datums might have come from separate repositories with the glob pattern set to / and being the only file system objects in these repositories.
| inputA | inputB | inputA ‚à™ inputB | | ------ | ------ | --------------- | | foo | fizz | foo | | bar | buzz | fizz | | | | bar | | | | buzz | The union inputs do not take a name and maintain the names of the sub-inputs. In the example above, you would see files under /pfs/inputA/... or /pfs/inputB/..., but never both at the same time. When you write code to address this behavior, make sure that your code first determines which input directory is present. Starting with Pachyderm 1.5.3, we recommend that you give your inputs the same Name. That way your code only needs to handle data being present in that directory. This only works if your code does not need to be aware of which of the underlying inputs the data comes from.
"
344,Job Timeout PPS," Spec # &#34;job_timeout&#34;: string, Behavior # Work that is not complete by set timeout is interrupted. Value must be a string that represents a time value, such as 1s, 5m, or 15h. Differs from datum_timeout in that the limit is applied across all workers and all datums. If not set, a job will run indefinitely until it succeeds or fails. "
345,Metadata PPS," Spec # &#34;metadata&#34;: { &#34;annotations&#34;: { &#34;annotation&#34;: string }, &#34;labels&#34;: { &#34;label&#34;: string } }, Behavior # Labels help organize and track cluster objects by creating groups of pods based on a given dimension.
Annotations enable you to specify any arbitrary metadata.
Both parameters require a key-value pair. Do not confuse this parameter with pod_patch, which adds metadata to the user container of the pipeline pod. For more information, see Labels and Selectors and Kubernetes Annotations in the Kubernetes documentation.
When to Use # Use metadata for operation ergonomics and to simplify the querying of Kubernetes objects.
"
346,Output Branch PPS," Spec # &#34;output_branch&#34;: string, Behavior # Set to master by default. When to Use # Use this setting to output commits to dev or testing branches.
"
347,Parallelism Spec PPS," Spec # &#34;parallelism_spec&#34;: { &#34;constant&#34;: int }, Behavior # Pachyderm starts the number of workers that you specify. For example, set &quot;constant&quot;:10 to use 10 workers.
The default value is 1 When to Use # ‚ö†Ô∏è Because spouts and services are designed to be single instances, do not modify the default parallism_spec value for these pipelines.
"
348,Pod Patch PPS," Spec # &#34;pod_patch&#34;: string, Behavior # pod_patch is similar to pod_spec but is applied as a JSON Patch. Note, this means that the process outlined above of modifying an existing pod spec and then manually blanking unchanged fields won&rsquo;t work, you&rsquo;ll need to create a correctly formatted patch by diffing the two pod specs.
"
349,Pod Spec PPS," Spec # &#34;pod_spec&#34;: string, Behavior # pod_spec is an advanced option that allows you to set fields in the pod spec that haven&rsquo;t been explicitly exposed in the rest of the pipeline spec. A good way to figure out what JSON you should pass is to create a pod in Kubernetes with the proper settings, then do:
kubectl get po/&lt;pod-name&gt; -o json | jq .spec this will give you a correctly formatted piece of JSON, you should then remove the extraneous fields that Kubernetes injects or that can be set else where.
The JSON is applied after the other parameters for the pod_spec have already been set as a JSON Merge Patch. This means that you can modify things such as the storage and user containers.
"
350,Reprocess Spec PPS," Spec # &#34;reprocess_spec&#34;: string, Behavior # &quot;reprocess_spec&quot;: &quot;until_success&quot; is the default behavior. To mitigate datums failing for transient connection reasons, Pachyderm automatically retries user code three (3) times before marking a datum as failed. Additionally, you can set the datum_tries field to determine the number of times a job attempts to run on a datum when a failure occurs.
Let&rsquo;s compare &quot;until_success&quot; and &quot;every_job&quot;:
Say we have 2 identical pipelines (reprocess_until_success.json and reprocess_at_every_job.json) but for the &quot;reprocess_spec&quot; field set to &quot;every_job&quot; in reprocess_at_every_job.json.
Both use the same input repo and have a glob pattern set to /*.
When adding 3 text files to the input repo (file1.txt, file2.txt, file3.txt), the 2 pipelines (reprocess_until_success and reprocess_at_every_job) will process the 3 datums (here, the glob pattern /* creates one datum per file). Now, let&rsquo;s add a 4th file file4.txt to our input repo or modify the content of file2.txt for example. Case of our default reprocess_until_success.json pipeline: A quick check at the list datum on the job id shows 4 datums, of which 3 were skipped. (Only the changed file was processed) Case of reprocess_at_every_job.json: A quick check at the list datum on the job id shows that all 4 datums were reprocessed, none were skipped. ‚ö†Ô∏è &quot;reprocess_spec&quot;: &quot;every_job will not take advantage of Pachyderm&rsquo;s default de-duplication. In effect, this can lead to slower pipeline performance. Before using this setting, consider other options such as including metadata in your file, naming your files with a timestamp, UUID, or other unique identifiers in order to take advantage of de-duplication. Review how datum processing works to understand more.
When to Use # Per default, Pachyderm avoids repeated processing of unchanged datums (i.e., it processes only the datums that have changed and skip the unchanged datums). This incremental behavior ensures efficient resource utilization. However, you might need to alter this behavior for specific use cases and force the reprocessing of all of your datums systematically. This is especially useful when your pipeline makes an external call to other resources, such as a deployment or triggering an external pipeline system. Set &quot;reprocess_spec&quot;: &quot;every_job&quot; in order to enable this behavior.
"
351,Resource Limits PPS," Spec # &#34;resource_limits&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, Behavior # resource_limits describes the upper threshold of allowed resources a given worker can consume. If a worker exceeds this value, it will be evicted.
The gpu field is a number that describes how many GPUs each worker needs. Only whole number are supported, Kubernetes does not allow multiplexing of GPUs. Unlike the other resource fields, GPUs only have meaning in Limits, by requesting a GPU the worker will have sole access to that GPU while it is running. It&rsquo;s recommended to enable autoscaling if you are using GPUs so other processes in the cluster will have access to the GPUs while the pipeline has nothing to process. For more information about scheduling GPUs see the Kubernetes docs on the subject.
"
352,Resource Requests PPS," Spec # &#34;resource_requests&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, Behavior # resource_requests describes the amount of resources that the pipeline workers will consume. Knowing this in advance enables Pachyderm to schedule big jobs on separate machines, so that they do not conflict, slow down, or terminate.
This parameter is optional, and if you do not explicitly add it in the pipeline spec, Pachyderm creates Kubernetes containers with the following default resources:
The user and storage containers request 0 CPU, 0 disk space, and 64MB of memory. The init container requests the same amount of CPU, memory, and disk space that is set for the user container. The resource_requests parameter enables you to overwrite these default values.
The memory field is a string that describes the amount of memory, in bytes, that each worker needs. Allowed SI suffixes include M, K, G, Mi, Ki, Gi, and other.
For example, a worker that needs to read a 1GB file into memory might set &quot;memory&quot;: &quot;1.2G&quot; with a little extra for the code to use in addition to the file. Workers for this pipeline will be placed on machines with at least 1.2GB of free memory, and other large workers will be prevented from using it, if they also set their resource_requests.
The cpu field is a number that describes the amount of CPU time in cpu seconds/real seconds that each worker needs. Setting &quot;cpu&quot;: 0.5 indicates that the worker should get 500ms of CPU time per second. Setting &quot;cpu&quot;: 2 indicates that the worker gets 2000ms of CPU time per second. In other words, it is using 2 CPUs, though worker threads might spend 500ms on four physical CPUs instead of one second on two physical CPUs.
The disk field is a string that describes the amount of ephemeral disk space, in bytes, that each worker needs. Allowed SI suffixes include M, K, G, Mi, Ki, Gi, and other.
In both cases, the resource requests are not upper bounds. If the worker uses more memory than it is requested, it does not mean that it will be shut down. However, if the whole node runs out of memory, Kubernetes starts deleting pods that have been placed on it and exceeded their memory request, to reclaim memory. To prevent deletion of your worker node, you must set your memory request to a sufficiently large value. However, if the total memory requested by all workers in the system is too large, Kubernetes cannot schedule new workers because no machine has enough unclaimed memory. cpu works similarly, but for CPU time.
For more information about resource requests and limits see the Kubernetes docs on the subject.
"
353,s3 Out PPS," Spec # &#34;s3_out&#34;: bool, Behavior # s3_out allows your pipeline code to write results out to an S3 gateway endpoint instead of the typical pfs/out directory. When this parameter is set to true, Pachyderm includes a sidecar S3 gateway instance container in the same pod as the pipeline container. The address of the output repository will be s3://&lt;output_repo&gt;.
If you want to expose an input repository through an S3 gateway, see input.pfs.s3 in PFS Input.
When to Use # You should use the s3 Out attribute when you&rsquo;d like to access and store the results of your Pachyderm transformations externally.
"
354,Scheduling Spec PPS," Spec # &#34;scheduling_spec&#34;: { &#34;node_selector&#34;: {string: string}, &#34;priority_class_name&#34;: string }, Attributes # Attribute Description node_selector Allows you to select which nodes your pipeline will run on. Refer to the Kubernetes docs on node selectors for more information about how this works. priority_class_name Allows you to select the priority class for the pipeline, which is how Kubernetes chooses to schedule and de-schedule the pipeline. Refer to the Kubernetes docs on priority and preemption for more information about how this works. Behavior # When you include a node_selector in the scheduling_spec, it tells Kubernetes to schedule the pipeline&rsquo;s Pods on nodes that match the specified key-value pairs. For example, if you specify {&quot;gpu&quot;: &quot;true&quot;} in the node_selector, Kubernetes will only schedule the pipeline&rsquo;s Pods on nodes that have a label gpu=true. This is useful when you have specific hardware or other node-specific requirements for your pipeline.
When you specify a priority_class_name in the scheduling_spec, it tells Kubernetes to assign the specified priority class to the pipeline&rsquo;s Pods. The priority class determines the priority of the Pods relative to other Pods in the cluster, and can affect the order in which Pods are scheduled and the resources they are allocated. For example, if you have a high-priority pipeline that needs to complete as quickly as possible, you can assign it a higher priority class than other Pods in the cluster to ensure that it gets scheduled and allocated resources first.
When to Use # You should use the scheduling_spec field in a Pachyderm Pipeline Spec when you have specific requirements for where and when your pipeline runs. This can include requirements related to hardware, node labels, scheduling priority, and other factors.
Example requirements:
Hardware requirements: If your pipeline requires specific hardware, such as GPUs, you can use the node_selector field to ensure that your pipeline runs on nodes that have the necessary hardware.
Node labels: If you have specific requirements for node labels, such as data locality, you can use the node_selector field to schedule your pipeline on nodes with the appropriate labels.
Priority: If you have a high-priority pipeline that needs to complete as quickly as possible, you can use the priority_class_name field to assign a higher priority class to your pipeline&rsquo;s Pods.
Resource constraints: If your pipeline requires a large amount of resources, such as CPU or memory, you can use the node_selector field to ensure that your pipeline runs on nodes with sufficient resources.
"
355,Service PPS," Spec # &#34;service&#34;: { &#34;internal_port&#34;: int, &#34;external_port&#34;: int }, Attributes # Attribute Description internal_port The port that the user code binds to inside the container. external_port The port on which it is exposed through the NodePorts functionality of Kubernetes services. Behavior # When enabled, transform.cmd is not expected to exit and will restart if it does. The service becomes exposed outside the container using a Kubernetes service. You can access the service at http://&lt;kubernetes-host&gt;:&lt;external_port&gt;. When to Use # You should use the service field in a Pachyderm Pipeline Spec when you want to expose your pipeline as a Kubernetes service, and allow other Kubernetes services or external clients to connect to it.
Example scenarios:
Microservices architecture: If you are building a microservices architecture, you may want to expose individual pipelines as services that can be accessed by other services in the cluster. By using the service field to expose your pipeline as a Kubernetes service, you can easily connect it to other services in the cluster.
Client access: If you want to allow external clients to access the output of your pipeline, you can use the service field to expose your pipeline as a Kubernetes service and provide clients with the service&rsquo;s IP address and external_port.
Load balancing: By exposing your pipeline as a Kubernetes service, you can take advantage of Kubernetes&rsquo; built-in load balancing capabilities. Kubernetes automatically load balances traffic to the service&rsquo;s IP address and external_port across all the replicas of the pipeline&rsquo;s container.
"
356,Sidecar Resource Limits PPS," Spec # &#34;sidecar_resource_limits&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, Attributes # Attribute Description cpu The maximum number of CPU cores that the sidecar container can use. memory The maximum amount of memory that the sidecar container can use. This can be specified in bytes, or with a unit such as &ldquo;Mi&rdquo; or &ldquo;Gi&rdquo;. gpu An optional field that specifies the number and type of GPUs that the sidecar container can use. type The type of GPU to use, such as &ldquo;nvidia&rdquo; or &ldquo;amd&rdquo;. number The number of GPUs that the sidecar container can use. disk The maximum amount of disk space that the sidecar container can use. This can be specified in bytes, or with a unit such as &ldquo;Mi&rdquo; or &ldquo;Gi&rdquo;. Behavior # The sidecar_resource_limits field in a Pachyderm Pipeline Spec is used to specify the resource limits for any sidecar containers that are run alongside the main pipeline container.
In a Pachyderm Pipeline, sidecar containers can be used to perform additional tasks alongside the main pipeline container, such as logging, monitoring, or handling external dependencies. By specifying resource limits for these sidecar containers, you can ensure that they don&rsquo;t consume too many resources and impact the performance of the main pipeline container.
This field can also be useful in deployments where Kubernetes automatically applies resource limits to containers, which might conflict with Pachyderm pipelines&rsquo; resource requests. Such a deployment might fail if Pachyderm requests more than the default Kubernetes limit. The sidecar_resource_limits enables you to explicitly specify these resources to fix the issue.
When to Use # You should use the sidecar_resource_limits field in a Pachyderm Pipeline Spec when you have sidecar containers that perform additional tasks alongside the main pipeline container, and you want to set resource limits for those sidecar containers.
Example scenarios:
Logging: If you have a sidecar container that is responsible for logging, you may want to limit its CPU and memory usage to prevent it from consuming too many resources and impacting the performance of the main pipeline container.
Monitoring: If you have a sidecar container that is responsible for monitoring the pipeline, you may want to limit its CPU and memory usage to prevent it from competing with the main pipeline container for resources.
External dependencies: If you have a sidecar container that provides external dependencies, such as a database, you may want to limit its CPU and memory usage to ensure that the main pipeline container has sufficient resources to perform its task.
"
357,Spec Commit PPS," Spec # &#34;spec_commit&#34;: { &#34;option&#34;: false, &#34;branch&#34;: { &#34;option&#34;: false, &#34;repo&#34;: { &#34;option&#34;: false, &#34;name&#34;: string, &#34;type&#34;: string, &#34;project&#34;:{ &#34;option&#34;: false, &#34;name&#34;: string, }, }, &#34;name&#34;: string }, &#34;id&#34;: string, } When to Use # You do not need to ever configure this attribute; its details are auto-generated.
"
358,Spout PPS," Spec # &#34;spout&#34;: { \\ Optionally, you can combine a spout with a service: &#34;service&#34;: { &#34;internal_port&#34;: int, &#34;external_port&#34;: int } }, Attributes # Attribute Description service An optional field that is used to specify how to expose the spout as a Kubernetes service. internal_port Used for the spout&rsquo;s container. external_port Used for the Kubernetes service that exposes the spout. Behavior # Does not have a PFS input; instead, it consumes data from an outside source. Can have a service added to it. See Service. When to Use # You should use the spout field in a Pachyderm Pipeline Spec when you want to read data from an external source that is not stored in a Pachyderm repository. This can be useful in situations where you need to read data from a service that is not integrated with Pachyderm, such as an external API or a message queue.
Example scenarios:
Data ingestion: If you have an external data source, such as a web service, that you want to read data from and process with Pachyderm, you can use the spout field to read the data into Pachyderm.
Real-time data processing: If you need to process data in real-time and want to continuously read data from an external source, you can use the spout field to read the data into Pachyderm and process it as it arrives.
Data integration: If you have data stored in an external system, such as a message queue or a streaming service, and you want to integrate it with data stored in Pachyderm, you can use the spout field to read the data from the external system and process it in Pachyderm.
"
359,Transform PPS," Spec # &#34;transform&#34;: { &#34;image&#34;: string, &#34;cmd&#34;: [ string ], &#34;err_cmd&#34;: [ string ], &#34;env&#34;: { string: string }, &#34;secrets&#34;: [ { &#34;name&#34;: string, &#34;mount_path&#34;: string }, { &#34;name&#34;: string, &#34;env_var&#34;: string, &#34;key&#34;: string } ], &#34;image_pull_secrets&#34;: [ string ], &#34;stdin&#34;: [ string ], &#34;err_stdin&#34;: [ string ], &#34;accept_return_code&#34;: [ int ], &#34;debug&#34;: bool, &#34;user&#34;: string, &#34;working_dir&#34;: string, &#34;dockerfile&#34;: string, &#34;memory_volume&#34;: bool, }, Attributes # Attribute Description cmd Passes a command to the Docker run invocation. stdin Passes an array of lines to your command on stdin. err_cmd Passes a command executed on failed datums. err_stdin Passes an array of lines to your error command on stdin. env Enables a key-value map of environment variables that Pachyderm injects into the container. secrets Passes an array of secrets to embed sensitive data. image_pull_secrets Passes an array of secrets that are mounted before the containers are created. accept_return_code Passes an array of return codes that are considered acceptable when your Docker command exits. debug Enables debug logging for the pipeline user Sets the user that your code runs as. working_dir Sets the directory that your command runs from. memory_volume Sets pachyderm-worker&rsquo;s emptyDir.Medium to Memory, allowing Kubernetes to mount a memory-backed volume (tmpfs). Behavior # cmd is not run inside a shell which means that wildcard globbing (*), pipes (|), and file redirects (&gt; and &gt;&gt;) do not work. To specify these settings, you can set cmd to be a shell of your choice, such as sh and pass a shell script to stdin. err_cmd can be used to ignore failed datums while still writing successful datums to the output repo, instead of failing the whole job when some datums fail. The transform.err_cmd command has the same limitations as transform.cmd. stdin lines do not have to end in newline characters. The following environment variables are automatically injected into the container: PACH_JOB_ID ‚Äì the ID of the current job. PACH_OUTPUT_COMMIT_ID ‚Äì the ID of the commit in the output repo for the current job. &lt;input&gt;_COMMIT - the ID of the input commit. For example, if your input is the images repo, this will be images_COMMIT. secrets reference Kubernetes secrets by name and specify a path to map the secrets or an environment variable (env_var) that the value should be bound to. 0 is always considered a successful exit code. tmpfs is cleared on node reboot and any files you write count against your container&rsquo;s memory limit. This may be useful for workloads that are IO heavy or use memory caches. When to Use # You must always use the transform attribute when making a pipeline.
"
360,S3 Gateway API,"This section outlines the operations exposed by Pachyderm&rsquo;s HTTP API S3 Gateway.
üìñ Since 1.13.3, all operations mentioning &lt;branch&gt;.&lt;repo&gt; also accept the syntax &lt;commit&gt;.&lt;repo&gt; and &lt;commit&gt;.&lt;branch&gt;.&lt;repo&gt;.
ListBuckets # Route: GET /.
Lists all of the branches across all of the repos as S3 buckets.
DeleteBucket # Route: DELETE /&lt;branch&gt;.&lt;repo&gt;/.
Deletes the branch. If it is the last branch in the repo, the repo is also deleted. Unlike S3, you can delete non-empty branches.
ListObjects # Route: GET /&lt;branch&gt;.&lt;repo&gt;/
Only S3&rsquo;s list objects v1 is supported.
PFS directories are represented via CommonPrefixes. This largely mirrors how S3 is used in practice, but leads to a couple of differences:
If you set the delimiter parameter, it must be /. Empty directories are included in listed results. With regard to listed results:
Due to PFS peculiarities, the LastModified field references when the most recent commit to the branch happened, which may or may not have modified the specific object listed. The HTTP ETag field does not use MD5, but is a cryptographically secure hash of the file contents. The S3 StorageClass and Owner fields always have the same filler value. GetBucketLocation # Route: GET /&lt;branch&gt;.&lt;repo&gt;/?location
This will always serve the same location for every bucket, but the endpoint is implemented to provide better compatibility with S3 clients.
GetBucketVersioning # Route: GET /&lt;branch&gt;.&lt;repo&gt;/?versioning
This will get whether versioning is enabled, which is always true.
ListMultipartUploads # Route: GET /&lt;branch&gt;.&lt;repo&gt;/?uploads
Lists the in-progress multipart uploads in the given branch. The delimiter query parameter is not supported.
CreateBucket # Route: PUT /&lt;branch&gt;.&lt;repo&gt;/.
If the repo does not exist, it is created. If the branch does not exist, it is likewise created. As per S3&rsquo;s behavior in some regions (but not all), trying to create the same bucket twice will return a BucketAlreadyOwnedByYou error.
DeleteObjects # Route: POST /&lt;branch&gt;.&lt;repo&gt;/?delete.
Deletes multiple files specified in the request payload.
DeleteObject # Route: DELETE /&lt;branch&gt;.&lt;repo&gt;/&lt;filepath&gt;.
Deletes the PFS file filepath in an atomic commit on the HEAD of branch.
GetObject # Route: GET /&lt;branch&gt;.&lt;repo&gt;/&lt;filepath&gt;.
By default, this request gets the HEAD version of the file. You can use s3&rsquo;s versioning API to get the object at a non-HEAD commit by specifying either a specific commit ID, or by using the caret syntax &ndash; for example, HEAD^.
There is support for range queries and conditional requests, however error response bodies for bad requests using these headers are not standard S3 XML.
With regard to HTTP response headers:
Due to PFS peculiarities, the HTTP Last-Modified header references when the most recent commit to the branch happened, which may or may not have modified this specific object. The HTTP ETag does not use MD5, but is a cryptographically secure hash of the file contents. PutObject # Route: PUT /&lt;branch&gt;.&lt;repo&gt;/&lt;filepath&gt;.
Writes the PFS file at filepath in an atomic commit on the HEAD of branch.
Any existing file content is overwritten. Unlike S3, there is no limit to upload size.
Unlike s3, a 64mb max size is not enforced on this endpoint. Especially, as the file upload size gets larger, we recommend setting the Content-MD5 request header to ensure data integrity.
AbortMultipartUpload # Route: DELETE /&lt;branch&gt;.&lt;repo&gt;?uploadId=&lt;uploadId&gt;
Aborts an in-progress multipart upload.
CompleteMultipartUpload # Route: POST /&lt;branch&gt;.&lt;repo&gt;?uploadId=&lt;uploadId&gt;
Completes a multipart upload. If ETags are included in the request payload, they must be of the same format as returned by the S3 gateway when the multipart chunks are included. If they are md5 hashes or any other hash algorithm, they are ignored.
CreateMultipartUpload # Route: POST /&lt;branch&gt;.&lt;repo&gt;?uploads
Initiates a multipart upload.
ListParts # Route: GET /&lt;branch&gt;.&lt;repo&gt;?uploadId=&lt;uploadId&gt;
Lists the parts of an in-progress multipart upload.
UploadPart # Route: PUT /&lt;branch&gt;.&lt;repo&gt;?uploadId=&lt;uploadId&gt;&amp;partNumber=&lt;partNumber&gt;
Uploads a chunk of a multipart upload.
"
361,Enterprise Edition," üí° Get your free-trial Enterprise License token by filling in this form or scheduling some time with one of our experts.
"
362,Activate Enterprise via Helm," Before You Start # You must have a Pachyderm Enterprise License Key. You must have pachctl and Pachyderm installed. You must have the Pachyderm Helm repo downloaded. How to Activate Enterprise Pachyderm via Helm # Activation Method: License License Secret Open your Helm values.yml file. Find the the pachd.enterpriseLicenseKey attribute. Input your enterprise key. Upgrade your cluster by running the following command: helm upgrade pachyderm pachyderm/pachyderm -f values.yml Once deployed, Pachyderm stores your provided Enterprise license as the platform secret pachyderm-license in the key enterprise-license-key.
Create a secret for your Enterprise license. Open your Helm values.yml file. Find the the pachd.enterpriseLicenseKeySecretName attribute. Input your license&rsquo;s secret name. Upgrade your cluster by running the following command: helm upgrade pachyderm pachyderm/pachyderm -f values.yml "
363,Activate Enterprise via Pachctl," Before You Start # You must have a Pachyderm Enterprise License Key. You must have pachctl and Pachyderm installed. You must have the Pachyderm Helm repo downloaded. How to Activate Enterprise Pachyderm via Pachctl # Open your terminal. Input the following command: echo &lt;your-activation-token&gt; | pachctl license activate Verify the status of the enterprise activation: pachctl enterprise get-state # ACTIVE You have unlocked Pachyderm&rsquo;s enterprise features.
"
364,Authentication & Authorization,"Pachyderm has an embedded Open ID Connect based on Dex, allowing for vendor-neutral authentication using your existing credentials from various back-ends. See compatible connectors.
Auth Token Duration # Pachd auth tokens duration is set to a 30 days default in pachd environment variable SESSION_DURATION_MINUTES.
"
365,Activate Authorization," Before You Start # You must be using Pachyderm Enterprise to set up authentication and authorization. Activate User Access Management # Activate authentication using the following command: pachctl auth activate # Pachyderm root token: # 54778a770c554d0fb84563033c9cb808 Save the root token value in a secure place. You can use this token in the future to log in to the initial root admin user by entering the following comand:
pachctl auth use-auth-token # Please paste your Pachyderm auth token: As a Root User (or initial admin), you can now configure Pachyderm to work with the identity management provider (IdP) of your choice.
License Expiration # When an Enterprise License expires, a Pachyderm cluster with enabled User Access Management goes into an admin-only state. In this state, only ClusterAdmins have access to the data stored in Pachyderm. This safety measure keeps sensitive data protected, even when an enterprise subscription becomes stale. To return the cluster to its previous state, run pachctl license activate and submit your new code.
"
366,Set Up IdP Connectors,"You can enable users to authenticate to a Pachyderm cluster using their favorite Identity Providers by following the articles in this section.
"
367,Log In via IdP, Before You Start # Your organization must have an active Enterprise License Key. You must have an IdP Connector set up. You must have pachctl installed. How to Log in to a Cluster via IdP # Open a terminal. Input the following command: pachctl auth login Select the connector you wish to use. Provide your credentials 
368,Check IdP User, Before You Start # Your organization must have an active Enterprise license key. You must have pachctl installed. How to Check Your Current User # Open a terminal. Run the following command: pachctl auth whoami # You are &#34;user:one-pachyderm-user@gmail.com&#34; # session expires: 08 May 21 13:59 EDT 
369,Revoke User Access," Before You Start # You must have clusterAdmin role permissions. How to Revoke User Access # ‚ö†Ô∏è You must remove the revoked user from your IdP user registry after completing the steps in this guide.
Revoke a Specific Token # pachctl auth revoke --token=&lt;pach token&gt; Revoke All Tokens # pachctl auth revoke --user=idp:usernamen@pachyderm.io "
370,Deactivate Authorization,"When you deactivate authorization, all permissions granted to users on Pachyderm resources are removed. Everyone that can connect to Pachyderm is back to being a clusterAdmin (can access and modify all data in all repos).
Before You Start # You must be logged in as a clusterAdmin. How to Deactivate Auth # pachctl auth deactivate "
371,Authorization,"You can use Pachyderm&rsquo;s Role-Based Access Control (RBAC) model to configure authorization for your users. Users can be assigned roles that grant certain permissions for interacting with Pachyderm&rsquo;s resources.
Users Types # Pachyderm has 5 user types:
User Type Description clusterAdmin IdP User Any user or group of users authenticated by your Identity Provider to access Pachyderm. Robot User A Service account used for third party applications/systems integrating with Pachyderm APIs/Clients. Pipeline User An internal Service Account used for Pipelines when interacting with Pachyderm resources. All Cluster Users A general subject that represents everyone who has logged in to a cluster. Pachyderm defines 4 prefixes depending on the type of user:
robot user group pipeline (as mentioned above, this prefix will not be used in the context of granting privileges to users. However, it does exist. We are listing it here to give an exhauxtive list of all prefixes.) Aditionnally, the &ldquo;everyone&rdquo; user allClusterUsers has no specific prefix. See the example below to learn how to assign repoReader access to allClusterUsers on a repo.
Resource Types # Pachyderm has 3 resource types:
Resource Type Description Cluster A set of nodes for running containerized applications. Containers allow users to run repeatable and standardized code. Project A project is a container of 1 or more DAGs that allows for users to organize their repos. Projects allow multiple teams to work in a cluster. Repo A repository is where data is stored and contains both files and folders. Repos tracks all changes to the data and creates a history of data changes. Role Types # Pachyderm has 3 role types:
Role Type Description Cluster Roles Granted at the cluster level. Project Roles Granted at the project level. Repo Roles Granted at the repo level or at the cluster level. "
372,Add Roles to User," Before You Start # Review the permissions assigned to each role. Confirm you have the right role(s) to grant a user access to a given resource. This guide assumes resources (projects, repositories) have already been created in your cluster. How to Assign Roles to a User # Open your terminal. Connect as the root user using the following command: pachctl auth use-auth-token Input your root token.
Run one of the following commands to assign a role:
Resource Type: Project Repo Other All pachctl auth set project &lt;project-name&gt; &lt;role-name&gt; user:&lt;username@email.com&gt; pachctl auth set repo &lt;repo-name&gt; &lt;role-name&gt; user:&lt;username@email.com&gt; pachctl auth set enterprise clusterAdmin user:&lt;email&gt; pachctl auth set &lt;resource&gt; &lt;resource-name&gt; [role1,role2 | none ] &lt;prefix:subject&gt; Confirm access by running the following command: Resource Type: Project Repo pachctl auth get project &lt;project-name&gt; pachctl auth get repo &lt;repo-name&gt; You can also use these steps to update a users permissions.
"
373,Add Roles to Group," Before You Start # Your IdP must support groups to use these instructions. Review the permissions assigned to each role. This guide assumes resources (projects, repositories) have already been created in your cluster. This guide uses Auth0 as an example IdP. How to Assign Roles to a Group # Enable group management in your IdP of choice . Update your connector config to include the appropriate attributes. Syntax: JSON YAML { &#34;type&#34;: &#34;oidc&#34;, &#34;id&#34;: &#34;auth0&#34;, &#34;name&#34;: &#34;Auth0&#34;, &#34;version&#34;: 1, &#34;config&#34;:{ &#34;issuer&#34;: &#34;https://dev-k34x5yjn.us.auth0.com/&#34;, &#34;clientID&#34;: &#34;hegmOc5rTotLPu5ByRDXOvBAzgs3wuw5&#34;, &#34;clientSecret&#34;: &#34;7xk8O71Uhp5T-bJp_aP2Squwlh4zZTJs65URPma-2UT7n1iigDaMUD9ArhUR-2aL&#34;, &#34;redirectURI&#34;: &#34;http(s)://&lt;insert-external-ip-or-dns-name&gt;/dex/callback&#34;, &#34;scopes&#34;: [&#34;groups&#34;, &#34;email&#34;, &#34;profile&#34;], &#34;claimMapping&#34;:{ &#34;groups&#34;: &#34;http://pachyderm.com/groups&#34; }, &#34;insecureEnableGroups&#34;: true } } type: oidc id: auth0 name: Auth0 version: 1 config: issuer: https://dev-k34x5yjn.us.auth0.com/ clientID: hegmOc5rTotLPu5ByRDXOvBAzgs3wuw5 clientSecret: 7xk8O71Uhp5T-bJp_aP2Squwlh4zZTJs65URPma-2UT7n1iigDaMUD9ArhUR-2aL redirectURI: http(s)://&lt;insert-external-ip-or-dns-name&gt;/dex/callback scopes: - groups - email - profile claimMapping: groups: http://pachyderm.com/groups insecureEnableGroups: true Update the config by running the following command: pachctl idp update-connector &lt;connector-id&gt; --version 2 Grant the group roles by running the following command: pachctl auth set &lt;resource-type&gt; &lt;resource-name&gt; &lt;role-name&gt; group:&lt;group-name&gt; Confirm the group&rsquo;s roles were updated for the given resource: Resource Type: Project Repo pachctl auth get project &lt;project-name&gt; pachctl auth get repo &lt;repo-name&gt; üí° The command pachctl auth get-groups lists the groups that have been defined on your cluster.
"
374,Pachyderm IAM,"This page describes how Pachyderm&rsquo;s Identity and Access Management (IAM) system works and how you can use it to manage access in Pachyderm. Use IAM to grant granular access to specific Pachyderm resources.
How IAM Works # IAM works by managing access for users (human or robot) through assigned roles. Roles contain a set of granular permissions (create, read, update, delete) for a given resource. In Pachyderm, resources include clusters, projects, and repositories.
A user can have many roles, and some roles encompass the permissions of other roles. For example, if you have a clusterAdminRole, all other permissions belonging to more restricted roles are included.
üí° You can use the command pachctl auth roles-for-permission &lt;permission&gt; to look up which roles provide a given permission.
Admin Roles # clusterAdminRole # The ClusterAdminRole includes all of the previous permissions, plus the following:
Permission CLUSTER_MODIFY_BINDINGS CLUSTER_GET_BINDINGS CLUSTER_AUTH_ACTIVATE CLUSTER_AUTH_DEACTIVATE CLUSTER_AUTH_GET_CONFIG CLUSTER_AUTH_SET_CONFIG CLUSTER_AUTH_MODIFY_GROUP_MEMBERS CLUSTER_AUTH_GET_GROUPS CLUSTER_AUTH_GET_GROUP_USERS CLUSTER_AUTH_EXTRACT_TOKENS CLUSTER_AUTH_RESTORE_TOKEN CLUSTER_AUTH_ROTATE_ROOT_TOKEN CLUSTER_AUTH_DELETE_EXPIRED_TOKENS CLUSTER_AUTH_GET_PERMISSIONS_FOR_PRINCIPAL CLUSTER_AUTH_REVOKE_USER_TOKENS CLUSTER_ENTERPRISE_ACTIVATE CLUSTER_ENTERPRISE_HEARTBEAT CLUSTER_ENTERPRISE_GET_CODE CLUSTER_ENTERPRISE_DEACTIVATE CLUSTER_DELETE_ALL CLUSTER_ENTERPRISE_PAUSE oidcAppAdminRole # Permission CLUSTER_IDENTITY_DELETE_OIDC_CLIENT CLUSTER_IDENTITY_CREATE_OIDC_CLIENT CLUSTER_IDENTITY_UPDATE_OIDC_CLIENT CLUSTER_IDENTITY_LIST_OIDC_CLIENTS CLUSTER_IDENTITY_GET_OIDC_CLIENT idpAdminRole # Permission CLUSTER_IDENTITY_CREATE_IDP CLUSTER_IDENTITY_UPDATE_IDP CLUSTER_IDENTITY_LIST_IDPS CLUSTER_IDENTITY_GET_IDP CLUSTER_IDENTITY_DELETE_IDP secretAdminRole # Permission CLUSTER_CREATE_SECRET CLUSTER_LIST_SECRETS SECRET_INSPECT SECRET_DELETE identityAdminRole # Permission CLUSTER_IDENTITY_SET_CONFIG CLUSTER_IDENTITY_GET_CONFIG licenseAdminRole # Permission CLUSTER_LICENSE_ACTIVATE CLUSTER_LICENSE_GET_CODE CLUSTER_LICENSE_ADD_CLUSTER CLUSTER_LICENSE_UPDATE_CLUSTER CLUSTER_LICENSE_DELETE_CLUSTER CLUSTER_LICENSE_LIST_CLUSTERS Project Roles # All users have the PROJECT_LIST_REPO and PROJECT_CREATE_REPO permissions by default.
ProjectOwnerRole # Permission PROJECT_DELETE PROJECT_MODIFY_BINDINGS ProjectCreatorRole # Permission PROJECT_CREATE Repo Roles # RepoReaderRole # Permission REPO_READ REPO_INSPECT_COMMIT REPO_LIST_COMMIT REPO_LIST_BRANCH REPO_LIST_FILE REPO_INSPECT_FILE REPO_ADD_PIPELINE_READER REPO_REMOVE_PIPELINE_READER PIPELINE_LIST_JOB RepoWriterRole # The RepoWriterRole includes all of the RepoReaderRole permissions, plus the following:
Permission REPO_WRITE REPO_DELETE_COMMIT REPO_CREATE_BRANCH REPO_DELETE_BRANCH REPO_ADD_PIPELINE_WRITER RepoOwnerRole # The RepoOwnerRole includes all of the RepoWriterRole and RepoReaderRole permissions, plus the following:
Permission REPO_MODIFY_BINDINGS REPO_DELETE Misc Roles # debuggerRole # Permission CLUSTER_DEBUG_DUMP CLUSTER_GET_PACHD_LOGS robotUserRole # Permission CLUSTER_AUTH_GET_ROBOT_TOKEN pachdLogReaderRole # Permission CLUSTER_GET_PACHD_LOGS "
375,Enterprise Server (ES),"You can manage your enterprise licensing and identity provider (IdP) integrations through the Enterprise Server. A Enterprise Server can have multiple Pachyderm clustered registered to it.
"
376,Activate ES for Multi-Cluster,"This guide deploys Enterprise Server as a standalone cluster within a multi-cluster deployment.
Before You Start # There are a few minor differences to note when deploying an Enterprise Server when compared to a standard Pachyderm cluster:
No deployment target is necessary in your Helm chart since there is no object store The Enterprise Server cluster contains the dex database Each registered cluster requires its own PostgresSQL pachyderm database How to Activate Enterprise for Multi-Cluster # Create a separate Kubernetes namespace dedicated to your enterprise server: kubectl create namespace enterprise-server kubectl config set-context --current --namespace=enterprise-server Create a Helm chart enterprise-server-values.yml file for your enterprise server (see Helm Chart Reference Guide). Deploy the Enterprise Server cluster: helm install enterprise-server pachyderm/pachyderm --f enterprise-server-values.yml Verify deployment: kubectl get all --namespace enterprise-server Reference Diagram # The following diagram gives you a quick overview of an organization with multiple Pachyderm clusters behind a single Enterprise Server. "
377,Activate ES for Single-Cluster,"You can register an existing single-cluster Pachyderm instance to the embedded Enterprise Server that comes included with pachd using the steps in this guide. Doing so enables you to also activate authentication and set up IdP connectors.
Before You Start # You must have an Enterprise license key You must have an active Pachyderm cluster How to Activate Enterprise Server # Open your terminal. Activate Enterprise Server: echo &lt;enterprise-license-key-value&gt; | pachctl license activate Activate Authentication: pachctl auth activate --enterprise Set up your Identity Provider (IdP). "
378,Register a Cluster via Helm," Before You Start # You must have an Enterprise license key You must have the Pachyderm Helm repo downloaded. How to Register a Cluster # Open your Helm values.yml file. Update the pachd section with the following attributes: pachd: activateEnterpriseMember: true enterpriseServerAddress: &#34;grpc://&lt;ENTERPRISE_SERVER_ADDRESS&gt;&#34; enterpriseCallbackAddress: &#34;grpc://&lt;PACHD_ADDRESS&gt;&#34; enterpriseServerToken: &#34;&lt;ENTERPRISE-SERVER-TOKEN&gt;&#34; # the same root token of the enterprise cluster # Alternatively, use a secret enterpriseServerTokenSecretName: &#34;&lt;Name of you secret containing enterpriseServerToken&gt;&#34; Upgrade the cluster: helm upgrade pachyderm pachyderm/pachyderm -f values.yml "
379,Register a Cluster via Pachctl," Before You Start # You must have an Enterprise license key You must have an active Pachyderm cluster You must have the Pachyderm Helm repo downloaded. How to Register a Cluster # Open your terminal. Run the following command: pachctl enterprise register --id &lt;my-pachd-config-name&gt; --enterprise-server-address &lt;pach-enterprise-IP&gt;:650 --pachd-address &lt;pachd-IP&gt;:650 Attribute Description --id the name of the context pointing to your cluster in ~/.pachyderm/config.json. --enterprise-server-address the host and port where pachd can reach the enterprise server. --pachd-address the host and port where the enterprise server can reach pachd. This may be internal to the kubernetes cluster, or over the internet. View all registered clusters with your enterprise server: pachctl license list-clusters # Using enterprise context: my-enterprise-context-name # id: john # address: ae1ba915f8b5b477c98cd26c67d7563b-66539067.us-west-2.elb.amazonaws.com:650 # version: 2.0.0 # auth_enabled: true # last_heartbeat: 2021-05-21 18:37:36.072156 +0000 UTC # --- # id: doe # address: 34.71.247.191:650 # version: 2.0.0 # auth_enabled: true # last_heartbeat: 2021-05-21 18:43:42.157027 +0000 UTC # --- Activate Authentication: pachctl auth activate --enterprise Set up your Identity Provider (IdP). "
380,Server Management," Contexts # The enterprise server has a separate context in the pachctl config file (~/.pachyderm/config.json).
Pachctl has an active pachd context (the cluster it is binded to), and separately an active enterprise context.
To check the active enterprise context, run:
pachctl config get active-enterprise-context ‚ö†Ô∏è In a single-cluster deployment, the active enterprise context will be the same as the enterprise context. The pachctl license and pachctl idp commands run against the enterprise context. pachctl auth commands accept an --enterprise flag to run against the enterprise context. Configuring IDPs # To configure IDP integrations, use pachctl idp create-connector as documented in the Pachyderm Integration with Identity Providers page.
Manage your Enterprise Server # Add Users As Administrators # By default, only the root token (Root User) can administer the Enterprise Server. Run the following command to add more ClusterAdmin to your Enterprise Server:
pachctl auth set enterprise clusterAdmin user:&lt;email&gt; List All Registered Clusters # pachctl license list-clusters The output includes the pachd version, whether auth is enabled, and the last heartbeat:
id: pach-2 address: 34.71.247.191:650 version: 2.0.0 auth_enabled: true last_heartbeat: 2021-05-21 18:43:42.157027 +0000 UTC Synchronize all available contexts in your ~/.pachyderm/config.json file # In the case where the enterprise server of your organization has multiple pachd instances, you can use the following command to ‚Äúdiscover‚Äù other pachd instances. It will automatically update your ~/.pachyderm/config.json file with all the contexts you can connect to.
pachctl enterprise sync-contexts Update The Enterprise License # To apply a new license and have it picked up by all clusters, run:
pachctl license activate --no-register Unregister A Cluster # To unregister a given cluster from your Enterprise Server, run:
pachctl license delete-cluster --id &lt;cluster id&gt; Undeploy # To undeploy a Cluster registered with an Enterprise Server: Unregister the cluster as mentioned above (pachctl license delete-cluster) Then, undeploy it: helm uninstall "
381,Server Setup," ‚ÑπÔ∏è For POCs and smaller organizations with one single Pachyderm cluster, the Enterprise Server services can be run embedded in pachd. A separate deployment is not necessary. An organization with a single Pachyderm cluster can run the Enterprise Server services embedded within pachd.
The setup of an Enterprise Server requires to:
Deploy it. Activate your Enterprise Key and enable Auth. Register your newly created or existing Pachyderm clusters with your enterprise server. Optional: Enable Auth on each cluster. ‚ö†Ô∏è We are now shipping Pachyderm with an embedded proxy allowing your nterprise server to expose one single port externally. This deployment setup is optional.
If you choose to deploy your enterprise server with a Proxy, check out our new recommended architecture and deployment instructions as they alter the instructions below.
1. Deploy An Enterprise Server # Deploying and configuring an enterprise server can be done in one of two flavors:
Provide all licensing and authentication configurations as a part of the Helm deployment. Or, install a bare-bones version of Pachyderm with Helm, then use pachctl commands to set up licensing and authentication. As Part Of A Regular Pachyderm Helm Deployment # Update your values.yaml with your enterprise license key and auth configurations (for an example on localhost, see the example values.yaml here) or check our minimal example below to your values.yaml.
‚ö†Ô∏è If a pachyderm cluster will also be installed in the same kubernetes cluster, they should be installed in different namespaces: kubectl create namespace enterprise helm install ... --set enterpriseServer.enabled=true --namespace enterprise This command deploys postgres, etcd and a deployment and service called pach-enterprise. pach-enterprise uses the same docker image and pachd binary, but it listens on a different set of ports (31650, 31657, 31658) to avoid conflicts with pachd.
Check the state of your deployment by running: kubectl get all --namespace enterprise System Response
NAME READY STATUS RESTARTS AGE pod/etcd-5fd7c675b6-46kz7 1/1 Running 0 113m pod/pach-enterprise-6dc9cb8f66-rs44t 1/1 Running 0 105m pod/postgres-6bfd7bfc47-9mz28 1/1 Running 0 113m values.yaml for a stand-alone Enterprise Server as part of a multi-cluster deployment # Deploying a stand-alone enterprise server requires setting the helm parameter `enterpriseServer.enabled` to `true` and the `pachd.enabled` to `false`. enterpriseServer: enabled: true pachd: enabled: false enterpriseLicenseKey: &#34;&lt;ENTERPRISE-LICENSE-KEY&gt;&#34; # Alternatively, you can pass your license in a secret enterpriseLicenseKeySecretName: &#34;&lt;enterprise License key secret name&gt;&#34; oauthClientID: &#34;pachd&#34; oauthRedirectURI: &#34;http://&lt;PACHD-IP&gt;:30657/authorization-code/callback&#34; ## if a secret name is not provided in `oauthClientSecretSecretName`, a secret containing `oauthClientSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-auth` under the key `auth-config&#39; oauthClientSecret: &#34;&#34; oauthClientSecretSecretName: &#34;&#34; ## if a secret name is not provided in `enterpriseSecretSecretName`, a secret containing `enterpriseSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-enterprise` under the key `enterprise-secret&#39; enterpriseSecretSecretName: &#34;&#34; enterpriseSecret: &#34;&#34; activateAuth: true ## if a secret name is not provided in `rootTokenSecretName`, a secret containing `rootToken` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-auth` under the key `rootToken&#39; rootTokenSecretName: &#34;&#34; rootToken: &#34;&#34; externalService: enabled: true oidc: issuerURI: &#34;http://&lt;PACHD-IP&gt;:30658/&#34; ## userAccessibleOauthIssuerHost is necessary in localhost settings or anytime the registered Issuer address isn&#39;t accessible outside the cluster # userAccessibleOauthIssuerHost: &#34;localhost:30658&#34; ## if `mockIDP` is set to true, `pachd.upstreamIDPs` will be ignored in favor of a testing placeholder IDP with username/password: admin/password mockIDP: false ## to set up upstream IDPs, set pachd.mockIDP to false, ## and populate the pachd.upstreamIDPs with an array of Dex Connector configurations. ## See the example below or https://dexidp.io/docs/connectors/ upstreamIDPs: - id: idpConnector jsonConfig: &gt;- { &#34;issuer&#34;: &#34;&lt;ISSUER&gt;&#34;, &#34;clientID&#34;: &#34;&lt;CLIENT-ID&gt;&#34;, &#34;clientSecret&#34;: &#34;&lt;CLIENT-SECRET&gt;&#34;, &#34;redirectURI&#34;: &#34;http://&lt;PACHD-IP&gt;:30658/callback&#34;, &#34;insecureEnableGroups&#34;: true, &#34;insecureSkipEmailVerified&#34;: true, &#34;insecureSkipIssuerCallbackDomainCheck&#34;: true, &#34;forwardedLoginParams&#34;: [&#34;login_hint&#34;] } name: idpConnector type: oidc values.yaml for an embedded single-cluster deployment # pachd: enterpriseLicenseKey: &#34;&lt;ENTERPRISE-LICENSE-KEY&gt;&#34; # Alternatively, you can pass your license in a secret enterpriseLicenseKeySecretName: &#34;&lt;enterprise License key secret name&gt;&#34; oauthClientID: &#34;pachd&#34; oauthRedirectURI: &#34;http://&lt;PACHD-IP&gt;:30657/authorization-code/callback&#34; ## if a secret name is not provided in `oauthClientSecretSecretName`, a secret containing `oauthClientSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-auth` under the key `auth-config&#39; oauthClientSecret: &#34;&#34; oauthClientSecretSecretName: &#34;&#34; ## if a secret name is not provided in `enterpriseSecretSecretName`, a secret containing `enterpriseSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-enterprise` under the key `enterprise-secret&#39; enterpriseSecretSecretName: &#34;&#34; enterpriseSecret: &#34;&#34; activateAuth: true ## if a secret name is not provided in `rootTokenSecretName`, a secret containing `rootToken` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-auth` under the key `rootToken&#39; rootTokenSecretName: &#34;&#34; rootToken: &#34;&#34; externalService: enabled: true oidc: issuerURI: &#34;http://&lt;PACHD-IP&gt;:30658/&#34; ## userAccessibleOauthIssuerHost is necessary in localhost settings or anytime the registered Issuer address isn&#39;t accessible outside the cluster # userAccessibleOauthIssuerHost: &#34;localhost:30658&#34; ## if `mockIDP` is set to true, `pachd.upstreamIDPs` will be ignored in favor of a testing placeholder IDP with username/password: admin/password mockIDP: false ## to set up upstream IDPs, set pachd.mockIDP to false, ## and populate the pachd.upstreamIDPs with an array of Dex Connector configurations. ## See the example below or https://dexidp.io/docs/connectors/ upstreamIDPs: - id: idpConnector jsonConfig: &gt;- { &#34;issuer&#34;: &#34;&lt;ISSUER&gt;&#34;, &#34;clientID&#34;: &#34;&lt;CLIENT-ID&gt;&#34;, &#34;clientSecret&#34;: &#34;&lt;CLIENT-SECRET&gt;&#34;, &#34;redirectURI&#34;: &#34;http://&lt;PACHD-IP&gt;:30658/callback&#34;, &#34;insecureEnableGroups&#34;: true, &#34;insecureSkipEmailVerified&#34;: true, &#34;insecureSkipIssuerCallbackDomainCheck&#34;: true, &#34;forwardedLoginParams&#34;: [&#34;login_hint&#34;] } name: idpConnector type: oidc This results in a single pachd pod, with authentication enabled, and an IDP integration configured.
‚ÑπÔ∏è Update the following values as follows:
PACHD-IP: The address of Pachyderm&rsquo;s IP. Retrieve Pachyderm external IP address if necessary. ISSUER, CLIENT-ID, CLIENT-SECRET: Refer to our Identity Provider Configuration page.
Check the list of all available helm values at your disposal in our reference documentation or on Github.
‚ö†Ô∏è When enterprise is enabled through Helm, auth is automatically activated (i.e., you do not need to run pachctl auth activate) and a pachyderm-auth k8s secret is created containing a rootToken key. Use {{&quot;kubectl get secret pachyderm-auth -o go-template='{{.data.rootToken | base64decode }}'&quot;}} to retrieve it and save it where you see fit. However, this secret is only used when configuring through helm:
If you run pachctl auth activate, the secret is not updated. Instead, the rootToken is printed in your STDOUT for you to save.
Same behavior if you activate enterprise manually (pachctl license activate) then activate authentication (pachctl auth activate). Set the helm value pachd.activateAuth to false to prevent the automatic bootstrap of auth on the cluster.
On An Existing Pachyderm Cluster # To enable the Enterprise Server on an existing cluster:
activate your enterprise key and authentication as described in the following chapter. then proceed to configuring IDP integrations. 2. Activate Enterprise Licensing And Enable Authentication # Use your enterprise key to activate your enterprise server: echo &lt;your-activation-token&gt; | pachctl license activate Then enable Authentication at the Enterprise Server level: pachctl auth activate --enterprise ‚ö†Ô∏è Enabling Auth will return a root token for the enterprise server. This is separate from the root tokens for each pachd (cluster). They should all be stored securely.
Once the enterprise server is deployed, deploy your cluster(s) helm install... and register it(them) with the enterprise server. Note that you have the option to register your clusters directly in your values.yaml when deploying or after its deployment, using pachctl.
You might want to expose your cluster(s) to the internet. Check the setup of a Load Balancer in our deployment section.
3. Register Your Cluster With The Enterprise Server # Similarly to the enterprise server, we can configure our pachyderm clusters to leverage Helm for licensing and authentication in one of two flavors:
Provide enterprise registration information as a part of the Helm deployment of a cluster. Register a cluster with the Enterprise Server using pachctl commands. Register Clusters With Helm # Add the enterprise server&rsquo;s root token, and network addresses to the values.yaml of each cluster you plan to deploy and register, for the cluster and enterprise server to communicate (for an example on localhost, see the example values.yaml here), or insert our minimal example below to your values.yaml.
values.yaml with activation of an enterprise license and authentication # pachd: activateEnterpriseMember: true enterpriseServerAddress: &#34;grpc://&lt;ENTERPRISE_SERVER_ADDRESS&gt;&#34; enterpriseCallbackAddress: &#34;grpc://&lt;PACHD_ADDRESS&gt;&#34; enterpriseServerToken: &#34;&lt;ENTERPRISE-SERVER-TOKEN&gt;&#34; # the same root token of the enterprise cluster # Alternatively, use a secret enterpriseServerTokenSecretName: &#34;&lt;Name of you secret containing enterpriseServerToken&gt;&#34; ‚ö†Ô∏è When setting your enterprise server info as part of the Helm deployment of a cluster, auth is automatically activated unless the helm value pachd.activateAuth was intentionally set to false. (i.e., you can skip step 4).
In this case, a pachyderm-auth k8s secret is automatically created containing an entry for your rootToken in the key rootToken. Use the following to retrieve it and save it where you see fit:
{{&#34;kubectl get secret pachyderm-auth -o go-template=&#39;{{.data.rootToken | base64decode }}&#39;&#34;}} Register Clusters With pachctl # Run this command for each of the clusters you wish to register using pachctl:
pachctl enterprise register --id &lt;my-pachd-config-name&gt; --enterprise-server-address &lt;pach-enterprise-IP&gt;:650 --pachd-address &lt;pachd-IP&gt;:650 --id is the name of the context pointing to your cluster in ~/.pachyderm/config.json.
--enterprise-server-address is the host and port where pachd can reach the enterprise server. In production, the enterprise server may be exposed on the internet.
--pachd-address is the host and port where the enterprise server can reach pachd. This may be internal to the kubernetes cluster, or over the internet.
Display the list of all registered clusters with your enterprise server:
pachctl license list-clusters Using enterprise context: my-enterprise-context-name id: john address: ae1ba915f8b5b477c98cd26c67d7563b-66539067.us-west-2.elb.amazonaws.com:650 version: 2.0.0 auth_enabled: true last_heartbeat: 2021-05-21 18:37:36.072156 +0000 UTC --- id: doe address: 34.71.247.191:650 version: 2.0.0 auth_enabled: true last_heartbeat: 2021-05-21 18:43:42.157027 +0000 UTC --- 4. Enable Auth On Each Cluster # Finally, if your clusters were registered with the Enterprise Server using pachctl, you might choose to activate auth on each (or some) of them. This is an optional step. Clusters can be registered with the enterprise server without authentication being enabled.
Before enabling authentication, set up the issuer in the idp config between the enterprise server and your cluster:
echo &#34;issuer: http://&lt;enterprise-server-IP&gt;:658&#34; | pachctl idp set-config --config - Check that your config has been updated properly: pachctl idp get-config
For each registered cluster you want to enable auth on:
pachctl auth activate --client-id &lt;my-pachd-config-name&gt; --redirect http://&lt;pachd-IP&gt;:657/authorization-code/callback ‚ÑπÔ∏è Note the /authorization-code/callback appended after &lt;pachd-IP&gt;:657 in --redirect. --client-id is to pachctl auth activate what --id is to pachctl enterprise register: In both cases, enter &lt;my-pachd-config-name&gt;. Make sure than your enterprise context is set up properly: pachctl config get active-enterprise-context If not:
pachctl config set active-enterprise-context &lt;my-enterprise-context-name&gt; To manage you server, its context, or connect your IdP, visit the Manage your Enterprise Server page.
"
382,Features Overview,"Pachyderm Enterprise helps you scale and manage Pachyderm data pipelines by removing all scaling limits and providing you with additional features not available in the Community Edition.
‚ÑπÔ∏è Want to try Pachyderm Enterprise, or simply have a few questions? Get in touch with us at sales@pachyderm.io or on our Slack.
Additional Features # Authentication: Authenticate against your favorite OIDC providers. Role-Based Access Control (RBAC): Use RBAC on pachyderm resources (clusters, projects, repos), silo data, and prevent unintended changes on production pipelines. Enterprise Server: Simplify licensing and Identity Provider management by using one Enterprise server to register many Pachyderm clusters. Additionally, you have access to a pachctl command that pauses (pachctl enterprise pause) and unpauses (pachctl enterprise unpause) your cluster for a backup and restore. "
383,Troubleshooting Guides,"This section describe troubleshooting guidelines that should help you in troubleshooting your deployment and pipelines.
Pachyderm has a built-in logging system that collects information about events in your Pachyderm environment at pipeline, datum, and job level. See pachctl logs.
To troubleshoot the cluster itself, use the kubectl tool troubleshooting tips. A few basic commands that you can use include the following:
Get the list of all Kubernetes objects:
kubectl get all Get the information about a pod:
kubectl describe pod &lt;podname&gt; "
384,General Troubleshooting," Cannot connect via pachctl - context deadline exceeded # Symptom # You may be using the pachd address config value or environment variable to specify how pachctl talks to your Pachyderm cluster, or you may be forwarding the pachyderm port. In any event, you might see something similar to:
pachctl version COMPONENT VERSION pachctl 2.5.1 context deadline exceeded Also, you might get this message if pachd is not running.
Recourse # It&rsquo;s possible that the connection is just taking a while. Occasionally this can happen if your cluster is far away (deployed in a region across the country). Check your internet connection.
It&rsquo;s also possible that you haven&rsquo;t poked a hole in the firewall to access the node on this port. Usually to do that you adjust a security rule (in AWS parlance a security group). For example, on AWS, if you find your node in the web console and click on it, you should see a link to the associated security group. Inspect that group. There should be a way to &ldquo;add a rule&rdquo; to the group. You&rsquo;ll want to enable TCP access (ingress) on port 30650. You&rsquo;ll usually be asked which incoming IPs should be whitelisted. You can choose to use your own, or enable it for everyone (0.0.0.0/0).
Certificate Error When Using Kubectl # Symptom # This can happen on any request using kubectl (e.g. kubectl get all). In particular you&rsquo;ll see:
kubectl version Client Version: version.Info{Major:&#34;1&#34;, Minor:&#34;6&#34;, GitVersion:&#34;v1.6.4&#34;, GitCommit:&#34;d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae&#34;, GitTreeState:&#34;clean&#34;, BuildDate:&#34;2017-05-19T20:41:24Z&#34;, GoVersion:&#34;go1.8.1&#34;, Compiler:&#34;gc&#34;, Platform:&#34;darwin/amd64&#34;} Unable to connect to the server: x509: certificate signed by unknown authority Recourse # Check if you&rsquo;re on any sort of VPN or other egress proxy that would break SSL. Also, there is a possibility that your credentials have expired. In the case where you&rsquo;re using GKE and gcloud, renew your credentials via:
kubectl get all Unable to connect to the server: x509: certificate signed by unknown authority gcloud container clusters get-credentials my-cluster-name-dev Fetching cluster endpoint and auth data. kubeconfig entry generated for my-cluster-name-dev. kubectl config current-context gke_my-org_us-east1-b_my-cluster-name-dev Uploads and Downloads are Slow # Symptom # Any pachctl put file or pachctl get file commands are slow.
Recourse # If you do not explicitly set the pachd address config value, pachctl will default to using port forwarding, which throttles traffic to ~1MB/s. If you need to do large downloads/uploads you should consider using pachd address config value. You&rsquo;ll also want to make sure you&rsquo;ve allowed ingress access through any firewalls to your k8s cluster.
Naming a Repo with an Unsupported Symbol # Symptom # A Pachyderm repo was accidentally named starting with a dash (-) and the repository is treated as a command flag instead of a repository.
Recourse # Pachyderm supports standard bash utilities that you can use to resolve this and similar problems. For example, in this case, you can specify double dashes (--) to delete the repository. Double dashes signify the end of options and tell the shell to process the rest arguments as filenames and objects.
For more information, see man bash.
Failed Uploads # Symptom # A file upload, particularly a recursive one of many files, fails. You may see log messages containing the following in either pipeline logs, pachd logs, or from the pachctl command locally:
pachctl errror: an error occurred forwarding XXXXX -&gt; 650: error forwarding port 650 pachctl error: EOF pachd or worker: all SubConns are in TransientFailure, latest connection error: connection error: desc = \&quot;transport: Error while dialing dial tcp 127.0.0.1:653: connect: connection refused\&quot;; retrying in XXXX.XXXXXs&quot;} Recourse # Either pachd or your pipeline&rsquo;s worker sidecar may be getting OOM killed as it grows while getting data from object storage.
You can give the storage container more resources by increasing the cache_size parameter in your pipeline spec. Increase it to what you can afford; its default is 64M.(If you‚Äôre using a release prior to 1.10.0 and you have cluster-wide or namepace policies on resource limits, you may need to manually edit the pipeline RC.)
If it still gets OOM killed by k8s, there are a couple of environment variables you can set in your pachd deployment to limit the amount of memory the sidecar and pachd use.
STORAGE_UPLOAD_CONCURRENCY_LIMIT limits the parallelism to put files into the storage backend. Default is 100. STORAGE_PUT_FILE_CONCURRENCY_LIMIT limits the number of parallel downloads pachd will initiate. Default is also 100. You may use a binary search technique to hone in on a value appropriate for a production pipeline:
for cache_size, max it out. If it works, halve it. If its OOM killed, increase the value by 50%. and so on for the CONCURRENCY_LIMITS, halve and increase by 50% until you get a value that works.
"
385,Troubleshooting Deployments,"A common issue related to a deployment: getting a CrashLoopBackoff error.
Pod stuck in CrashLoopBackoff # Symptoms # The pachd pod keeps crashing/restarting:
kubectl get all NAME READY STATUS RESTARTS AGE po/etcd-281005231-qlkzw 1/1 Running 0 7m po/pachd-1333950811-0sm1p 0/1 CrashLoopBackOff 6 7m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/etcd 100.70.40.162 &lt;nodes&gt; 2379:30938/TCP 7m svc/kubernetes 100.64.0.1 &lt;none&gt; 443/TCP 9m svc/pachd 100.70.227.151 &lt;nodes&gt; 650:30650/TCP,651:30651/TCP 7m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/etcd 1 1 1 1 7m deploy/pachd 1 1 1 0 7m NAME DESIRED CURRENT READY AGE rs/etcd-281005231 1 1 1 7m rs/pachd-1333950811 1 1 0 7m Recourse # First describe the pod:
kubectl describe po/pachd-1333950811-0sm1p If you see an error including Error attaching EBS volume or similar, see the recourse for that error here under the corresponding section below. If you don&rsquo;t see that error, but do see something like:
1m 3s 9 {kubelet ip-172-20-48-123.us-west-2.compute.internal} Warning FailedSync Error syncing pod, skipping: failed to &#34;StartContainer&#34; for &#34;pachd&#34; with CrashLoopBackOff: &#34;Back-off 2m40s restarting failed container=pachd pod=pachd-1333950811-0sm1p_default(a92b6665-506a-11e7-8e07-02e3d74c49ac)&#34; it means Kubernetes tried running pachd, but pachd generated an internal error. To see the specifics of this internal error, check the logs for the pachd pod:
kubectl logs po/pachd-1333950811-0sm1p ‚ÑπÔ∏è If you&rsquo;re using a log aggregator service (e.g. the default in GKE), you won&rsquo;t see any logs when using kubectl logs ... in this way. You will need to look at your logs UI (e.g. in GKE&rsquo;s case the stackdriver console).
These logs will most likely reveal the issue directly, or at the very least, a good indicator as to what&rsquo;s causing the problem. For example, you might see, BucketRegionError: incorrect region, the bucket is not in 'us-west-2' region. In that case, your object store bucket in a different region than your pachyderm cluster and the fix would be to recreate the bucket in the same region as your pachydermm cluster.
If the error / recourse isn&rsquo;t obvious from the error message, post the error as well as the pachd logs in our Slack channel, or open a GitHub Issue and provide the necessary details prompted by the issue template. Please do be sure provide these logs either way as it is extremely helpful in resolving the issue.
Pod stuck in CrashLoopBackoff - with error attaching volume # Symptoms # A pod (could be the pachd pod or a worker pod) fails to startup, and is stuck in CrashLoopBackoff. If you execute kubectl describe po/pachd-xxxx, you&rsquo;ll see an error message like the following at the bottom of the output:
30s 30s 1 {attachdetach } Warning FailedMount Failed to attach volume &#34;etcd-volume&#34; on node &#34;ip-172-20-44-17.us-west-2.compute.internal&#34; with: Error attaching EBS volume &#34;vol-0c1d403ac05096dfe&#34; to instance &#34;i-0a12e00c0f3fb047d&#34;: VolumeInUse: vol-0c1d403ac05096dfe is already attached to an instance This would indicate that the persistent volume claim is failing to get attached to the node in your kubernetes cluster.
Recourse # Your best bet is to manually detach the volume and restart the pod.
For example, to resolve this issue when Pachyderm is deployed to AWS, pull up your AWS web console and look up the node mentioned in the error message (ip-172-20-44-17.us-west-2.compute.internal in our case). Then on the bottom pane for the attached volume. Follow the link to the attached volume, and detach the volume. You may need to &ldquo;Force Detach&rdquo; it.
Once it&rsquo;s detached (and marked as available). Restart the pod by killing it, e.g:
kubectl delete po/pachd-xxx It will take a moment for a new pod to get scheduled.
"
386,Troubleshooting Pipelines," Introduction # Job failures can occur for a variety of reasons, but they generally categorize into 4 failure types:
You hit one of the Pachyderm Community Edition Scaling Limits. User-code-related: An error in the user code running inside the container or the json pipeline config. Data-related: A problem with the input data such as incorrect file type or file name. System- or infrastructure-related: An error in Pachyderm or Kubernetes such as missing credentials, transient network errors, or resource constraints (for example, out-of-memory&ndash;OOM&ndash;killed). In this document, we&rsquo;ll show you the tools for determining what kind of failure it is. For each of the failure modes, we‚Äôll describe Pachyderm‚Äôs and Kubernetes‚Äôs specific retry and error-reporting behaviors as well as typical user triaging methodologies.
Failed jobs in a pipeline will propagate information to downstream pipelines with empty commits to preserve provenance and make tracing the failed job easier. A failed job is no longer running.
In this document, we&rsquo;ll describe what you&rsquo;ll see, how Pachyderm will respond, and techniques for triaging each of those three categories of failure.
At the bottom of the document, we&rsquo;ll provide specific troubleshooting steps for specific scenarios.
Pipeline exists but never runs All your pods or jobs get evicted Determining the kind of failure # First off, you can see the status of Pachyderm&rsquo;s jobs with pachctl list job --expand, which will show you the status of all jobs. For a failed job, use pachctl inspect job &lt;job-id&gt; to find out more about the failure. The different categories of failures are addressed below.
Community Edition Scaling Limits # If you are running on the Community Edition, you might have hit the limit set on the number of pipelines and/or parallel workers.
That scenario is quite easy to troubleshoot:
Check your number of pipelines and parallelism settings (&quot;parallelism_spec&quot; attribute in your pipeline specification files) against our limits.
Additionally, your stderr and pipeline logs (pachctl log -p &lt;pipeline name&gt; --master or pachctl log -p &lt;pipeline name&gt; --worker) should contain one or both of those messages:
number of pipelines limit exceeded: Pachyderm Community Edition requires an activation key to create more than 16 total pipelines (you have X). Use the command pachctl license activate to enter your key.
Pachyderm offers readily available activation keys for proofs-of-concept, startups, academic, nonprofit, or open-source projects. Tell us about your project to get one.
max number of workers exceeded: This pipeline will only create a total of 8 workers (you specified X). Pachyderm Community Edition requires an activation key to create pipelines with constant parallelism greater than 8. Use the command pachctl license activate to enter your key.
Pachyderm offers readily available activation keys for proofs-of-concept, startups, academic, nonprofit, or open-source projects. Tell us about your project to get one.
To lift those limitations, Request an Enterprise Edition trial token. Check out our Enterprise features for more details on our Enterprise Offer.
User Code Failures # When there‚Äôs an error in user code, the typical error message you‚Äôll see is
failed to process datum &lt;UUID&gt; with error: &lt;user code error&gt; This means pachyderm successfully got to the point where it was running user code, but that code exited with a non-zero error code. If any datum in a pipeline fails, the entire job will be marked as failed, but datums that did not fail will not need to be reprocessed on future jobs. You can use pachctl inspect datum &lt;job-id&gt; &lt;datum-id&gt; or pachctl logs with the --pipeline, --job or --datum flags to get more details.
There are some cases where users may want mark a datum as successful even for a non-zero error code by setting the transform.accept_return_code field in the pipeline config .
Retries # Pachyderm will automatically retry user code three (3) times before marking the datum as failed. This mitigates datums failing for transient connection reasons.
Triage # pachctl logs --job=&lt;job_ID&gt; or pachctl logs --pipeline=&lt;pipeline_name&gt; will print out any logs from your user code to help you triage the issue. Kubernetes will rotate logs occasionally so if nothing is being returned, you‚Äôll need to make sure that you have a persistent log collection tool running in your cluster.
In cases where user code is failing, changes first need to be made to the code and followed by updating the pachyderm pipeline. This involves building a new docker container with the corrected code, modifying the pachyderm pipeline config to use the new image, and then calling pachctl update pipeline -f updated_pipeline_config.json. Depending on the issue/error, user may or may not want to also include the --reprocess flag with update pipeline.
Data Failures # When there‚Äôs an error in the data, this will typically manifest in a user code error such as
failed to process datum &lt;UUID&gt; with error: &lt;user code error&gt; This means pachyderm successfully got to the point where it was running user code, but that code exited with a non-zero error code, usually due to being unable to find a file or a path, a misformatted file, or incorrect fields/data within a file. If any datum in a pipeline fails, the entire job will be marked as failed. Datums that did not fail will not need to be reprocessed on future jobs.
Retries # Just like with user code failures, Pachyderm will automatically retry running a datum 3 times before marking the datum as failed. This mitigates datums failing for transient connection reasons.
Triage # Data failures can be triaged in a few different way depending on the nature of the failure and design of the pipeline.
In some cases, where malformed datums are expected to happen occasionally, they can be ‚Äúswallowed‚Äù (e.g. marked as successful using transform.accept_return_codes or written out to a ‚Äúfailed_datums‚Äù directory and handled within user code). This would simply require the necessary updates to the user code and pipeline config as described above. For cases where your code detects bad input data, a &ldquo;dead letter queue&rdquo; design pattern may be needed. Many pachyderm developers use a special directory in each output repo for &ldquo;bad data&rdquo; and pipelines with globs for detecting bad data direct that data for automated and manual intervention.
If a few files as part of the input commit are causing the failure, they can simply be removed from the HEAD commit with start commit, delete file, finish commit. The files can also be corrected in this manner as well. This method is similar to a revert in Git &ndash; the ‚Äúbad‚Äù data will still live in the older commits in Pachyderm, but will not be part of the HEAD commit and therefore not processed by the pipeline.
System-level Failures # System-level failures are the most varied and often hardest to debug. We‚Äôll outline a few common patterns and triage steps. Generally, you‚Äôll need to look at deeper logs to find these errors using pachctl logs --pipeline=&lt;pipeline_name&gt; --raw and/or --master and kubectl logs pod &lt;pod_name&gt;.
Here are some of the most common system-level failures:
Malformed or missing credentials such that a pipeline cannot connect to object storage, registry, or other external service. In the best case, you‚Äôll see permission denied errors, but in some cases you‚Äôll only see ‚Äúdoes not exist‚Äù errors (this is common reading from object stores) Out-of-memory (OOM) killed or other resource constraint issues such as not being able to schedule pods on available cluster resources. Network issues trying to connect Pachd, etcd, or other internal or external resources Failure to find or pull a docker image from the registry Retries # For system-level failures, Pachyderm or Kubernetes will generally continually retry the operation with exponential backoff. If a job is stuck in a given state (e.g. starting, merging) or a pod is in CrashLoopBackoff, those are common signs of a system-level failure mode.
Triage # Triaging system failures varies as widely as the issues do themselves. Here are options for the common issues mentioned previously.
Credentials: check your secrets in k8s, make sure they‚Äôre added correctly to the pipeline config, and double check your roles/perms within the cluster OOM: Increase the memory limit/request or node size for your pipeline. If you are very resource constrained, making your datums smaller to require less resources may be necessary. Network: Check to make sure etcd and pachd are up and running, that k8s DNS is correctly configured for pods to resolve each other and outside resources, firewalls and other networking configurations allow k8s components to reach each other, and ingress controllers are configured correctly Check your container image name in the pipeline config and image_pull_secret. Specific scenarios # All pods or jobs get evicted # Symptom # After creating a pipeline, a job starts but never progresses through any datums.
Recourse # Run kubectl get pods and see if the command returns pods that are marked Evicted. If you run kubectl describe &lt;pod-name&gt; with one of those evicted pods, you might get an error saying that it was evicted due to disk pressure. This means that your nodes are not configured with a big enough root volume size. You need to make sure that each node&rsquo;s root volume is big enough to store the biggest datum you expect to process anywhere on your DAG plus the size of the output files that will be written for that datum.
Let&rsquo;s say you have a repo with 100 folders. You have a single pipeline with this repo as an input, and the glob pattern is /*. That means each folder will be processed as a single datum. If the biggest folder is 50GB and your pipeline&rsquo;s output is about three times as big, then your root volume size needs to be bigger than:
50 GB (to accommodate the input) + 50 GB x 3 (to accommodate the output) = 200GB In this case we would recommend 250GB to be safe. If your root volume size is less than 50GB (many defaults are 20GB), this pipeline will fail when downloading the input. The pod may get evicted and rescheduled to a different node, where the same thing will happen.
Pipeline exists but never runs # Symptom # You can see the pipeline via pachctl list pipeline, but if you look at the job via pachctl list job --expand, it&rsquo;s marked as running with 0/0 datums having been processed.
If you inspect the job via pachctl inspect job &lt;pipeline_name&gt;@&lt;jobID&gt;, you don&rsquo;t see any worker set.
E.g:
Worker Status: WORKER JOB DATUM STARTED ... If you do kubectl get pod you see the worker pod for your pipeline, e.g:
po/pipeline-foo-5-v1-273zc But it&rsquo;s state is Pending or CrashLoopBackoff.
Recourse # First make sure that there is no parent job still running. Do pachctl list job --expand| grep yourPipelineName to see if there are pending jobs on this pipeline that were kicked off prior to your job. A parent job is the job that corresponds to the parent output commit of this pipeline. A job will block until all parent jobs complete.
If there are no parent jobs that are still running, then continue debugging:
Describe the pod via:
kubectl describe po/pipeline-foo-5-v1-273zc If the state is CrashLoopBackoff, you&rsquo;re looking for a descriptive error message. One such cause for this behavior might be if you specified an image for your pipeline that does not exist.
If the state is Pending it&rsquo;s likely the cluster doesn&rsquo;t have enough resources. In this case, you&rsquo;ll see a could not schedule type of error message which should describe which resource you&rsquo;re low on. This is more likely to happen if you&rsquo;ve set resource requests (cpu/mem/gpu) for your pipelines. In this case, you&rsquo;ll just need to scale up your resources. You can use your cloud provider&rsquo;s auto scaling groups to increase the size of your instance group. It can take up to 10 minutes for the changes to go into effect.
Cannot Delete Pipelines with an etcd Error # Failed to delete a pipeline with an etcdserver error.
Symptom # Deleting pipelines fails with the following error:
pachctl delete pipeline pipeline-name etcdserver: too many operations in txn request (XXXXXX comparisons, YYYYYYY writes: hint: set --max-txn-ops on the ETCD cluster to at least the largest of those values) Recourse # When a Pachyderm cluster reaches a certain scale, you need to adjust the default parameters provided for certain etcd flags. Depending on how you deployed Pachyderm, you need to either edit the etcd Deployment or StatefulSet.
kubectl edit deploy etcd or
kubectl edit statefulset etcd In the spec/template/containers/command path, set the value for max-txn-ops to a value appropriate for your cluster, in line with the advice in the error above: larger than the greater of XXXXXX or YYYYYYY.
Pipeline is stuck in starting # Symptom # After starting a pipeline, running the pachctl list pipeline command returns the starting status for a very long time. The kubectl get pods command returns the pipeline pods in a pending state indefinitely.
Recourse # Run the kubectl describe pod &lt;pipeline-pod&gt; and analyze the information in the output of that command. Often, this type of error is associated with insufficient amount of CPU, memory, and GPU resources in your cluster.
"
387,Coding Conventions,"Interested in contributing to Pachyderm&rsquo;s code? Learn the conventions here! For setup instructions, see Setup for Contributors.
Languages # The Pachyderm repository is written using Go, Shell, and Make. Exceptions to this are:
/examples: For showcasing how to use the product in various languages. /doc: For building documentation using a python-based static site generator (MkDocs). Shell # See the Shell Style Guide for standard conventions. Add set -eou pipefail to your scripts. Go # See the Effective Go Style Guide for standard conventions.
Naming # Consider the package name when naming an interface to avoid redundancy. For example, storage.Interface is better than storage.StorageInterface. Do not use uppercase characters, underscores, or dashes in package names. The package foo line should match the name of the directory in which the .go file exists. Importers can use a different name if they need to disambiguate. When multiple locks are present, give each lock a distinct name following Go conventions (e.g., stateLock, mapLock). Go Modules/Third-Party Code # See the Go Modules Usage and Troubleshooting Guide for managing Go modules. Go dependencies are managed with go modules. Use go get foo to add or update a package; for more specific versions, use go get foo@v1.2.3, go get foo@master, or go get foo@e3702bed2. YAML # See the Helm Best Practices guide series. Review # See the Go Code Review Comments guide for a list of common comments. See the Go Test Comments guide for a list of common test code comments. Make sure CI is passing for your branch. Checks # Run checks using make lint. Testing # All packages and significant functionality must come with test coverage. Local unit tests should pass before pushing to GitHub (make localtest or make integration-tests for integrations). Use short flag for local tests only. Avoid waiting for asynchronous things to happen; If possible, use a method of waiting directly (e.g. &lsquo;flush commit&rsquo; is much better than repeatedly trying to read from a commit). Run single tests or tests from a single package; the Go tool only supports tests that match a regular expression (for example, go test -v ./src/path/to/package -run ^TestMyTest). Documentation # When writing documentation, follow the Style Guide conventions. PRs that have only documentation changes, such as typos, is a great place to start and we welcome your help! "
388,Contributor Setup," General requirements # First, go through the general Local Installation Instructions. Additionally, make sure you have the following installed:
golang 1.12+ docker jq pv shellcheck Bash helpers # To stay up to date, we recommend doing the following.
First clone the code: (Note, as of 07/11/19 pachyderm is using go modules and recommends cloning the code outside of the $GOPATH, we use the location ~/workspace as an example, but the code can live anywhere)
cd ~/workspace git clone git@github.com:pachyderm/pachyderm Then update your ~/.bash_profile by adding the line:
source ~/workspace/pachyderm/etc/contributing/bash_helpers And you&rsquo;ll stay up to date!
Special macOS configuration # File descriptor limit # If you&rsquo;re running tests locally, you&rsquo;ll need to up your file descriptor limit. To do this, first setup a LaunchDaemon to up the limit with sudo privileges:
sudo cp ~/workspace/pachyderm/etc/contributing/com.apple.launchd.limit.plist /Library/LaunchDaemons/ Once you restart, this will take effect. To see the limits, run:
launchctl limit maxfiles Before the change is in place you&rsquo;ll see something like 256 unlimited. After the change you&rsquo;ll see a much bigger number in the first field. This ups the system wide limit, but you&rsquo;ll also need to set a per-process limit.
Second, up the per process limit by adding something like this to your ~/.bash_profile :
ulimit -n 12288 Unfortunately, even after setting that limit it never seems to report the updated version. So if you try
ulimit And just see unlimited, don&rsquo;t worry, it took effect.
To make sure all of these settings are working, you can test that you have the proper setup by running:
make test-pfs-server If this fails with a timeout, you&rsquo;ll probably also see &rsquo;too many files&rsquo; type of errors. If that test passes, you&rsquo;re all good!
Timeout helper # You&rsquo;ll need the timeout utility to run the make launch task. To install on mac, do:
brew install coreutils And then make sure to prepend the following to your path:
PATH=&#34;/usr/local/opt/coreutils/libexec/gnubin:$PATH&#34; Dev cluster # Now launch the dev cluster: make launch-dev-vm.
And check it&rsquo;s status: kubectl get all.
pachctl # This will install the dev version of pachctl:
cd ~/workspace/pachyderm make install pachctl version And make sure that $GOPATH/bin is on your $PATH somewhere
Getting some images in place for local test runs # The following commands will put some images that some of the tests rely on in place in your minikube cluster:
For pachyderm_entrypoint container:
make docker-build-test-entrypoint ./etc/kube/push-to-minikube.sh pachyderm_entrypoint For pachyderm/python-build container:
(cd etc/pipeline-build; make push-to-minikube) Running tests # Now that we have a dev cluster, it&rsquo;s nice to be able to run some tests locally as we are developing.
To run some specific tests, just use go test directly, e.g:
go test -v ./src/server/cmd/pachctl/cmd We don&rsquo;t recommend trying to run all the tests locally, they take a while. Use CI for that.
Fully resetting # Instead of running the makefile targets to re-compile pachctl and redeploy a dev cluster, we have a script that you can use to fully reset your pachyderm environment:
All existing cluster data is deleted If possible, the virtual machine that the cluster is running on is wiped out pachctl is recompiled The dev cluster is re-deployed This reset is a bit more time consuming than running one-off Makefile targets, but comprehensively ensures that the cluster is in its expected state, and is especially helpful when you&rsquo;re first getting started with contributions and don&rsquo;t yet have a complete intuition on the various ways a cluster may get in an unexpected state. It&rsquo;s been tested on docker for mac and minikube, but likely works in other kubernetes environments as well.
To run it, simply call ./etc/reset.py from the pachyderm repo root.
"
389,Developing on Windows with VSCode," Before You Start # Installation Requirements # You must have all of the following installed before you can start development:
Docker Desktop Go v1.15.x+ GoReleaser Git HyperV jq kubectl Make minikube ShellCheck VSCode Terminal Settings # Open VS Code. Open your terminal (ctrl+`). Add the following to your settings.json: &#34;terminal.integrated.shell.windows&#34;: &#34;C:\\Program Files\\Git\\bin\\bash.exe&#34;, This path may vary depending on where your git bash actually exists.
Getting started # Open a terminal and navigate to a directory you&rsquo;d like to store Pachyderm. Clone the pachyderm repo using git clone https://github.com/pachyderm/pachyderm. Launch Docker Desktop (with Kubernetes enabled) or start minikube. Provision ~10 GB of memory and ~4CPUs. Via minikube: minikube start --memory=10000mb --cpus=4 --disk-size=40000mb --driver=hyperv Via Docker Desktop: Open Docker Desktop and navigate to Preferences &gt; Resources &gt; Advanced. Build your pachyderm pachd and worker images via the task docker-build. Option 1: Navigate to Terminal &gt; Run Task&hellip; Option 2: Press ctrl+p and input task docker-build Build and install pachctl. Launch a Pachyderm cluster by running the task launch-dev. If the service does not come up promptly (the script never says all the pods are ready), see the Debugging section.
Debugging # Common Commands # The following commands are used frequently when working with Pachyderm:
kubectl get all: lists resources in the &lsquo;default&rsquo; namespace, where we deploy locally. kubectl logs -p &lt;pod&gt;: gets the logs from the previous attempt at running a pod; a good place to find errors. minikube logs: gets the logs from minikube itself, useful when a pod runs into a CreateContainerError. docker container ls: lists recently used or in-use docker containers; used to get logs more directly. docker logs &lt;container&gt;: gets the logs from a specific docker container. Gotchas # Docker can get confused by command-line windows-style paths; it reads : as a mode and fails to parse. You may want to export MSYS_NO_PATHCONV=1 to prevent the automated conversion of unix-to-windows paths. Kubernetes resource specs (specifically hostPath) do not work if you use a windows-style path. Instead, you must use a unix-style path where the drive letter is the first directory, e.g. /C/path/to/file. Etcd may fail to mmap files when in a directory shared with the host system. Full Restart # Minikube # If you&rsquo;d like to completely restart, use the following terminal commands:
minikube delete kubectl delete pvc -l suite=pachyderm minikube start --memory=10000mb --cpus=4 --disk-size=40000mb "
390,Documentation Style Guide,"Thank you for taking an interest in contributing to Pachyderm&rsquo;s docs! üêò üìñ
This style guide provides editorial guidelines for writing clear and consistent Pachyderm product documentation. See our contribution guide for instructions on how to draft and submit changes.
Audience # Pachyderm has two main audiences:
MLOPs Engineers: They install and configure Pachyderm to transform data using pipelines. Data Scientists &amp; Data/ML Engineers: They plan the development of pipelines and consume the outputs of Pachyderm&rsquo;s data processing to feed AI/ML models. Be sure to provide links to pre-requisite or contextual materials whenever possible, as everyone&rsquo;s experience level and career journey is different.
Voice &amp; Tone # Pachyderm&rsquo;s voice in documentation should consistently convey a personality that is friendly, knowledgeable, and empathetic.
The tone of voice may vary depending on the type of content being written. For example, a danger notice may use an urgent and serious tone while a tutorial may use an energetic and instructive tone. Make sure the tone of your documentation aligns with the content. If you aren&rsquo;t sure what tone to convey, ask yourself: &ldquo;What is the reader likely feeling when presented this content? Why are they here?&rdquo; and adjust your language to the most appropriate tone.
Language &amp; Grammar # The following guidelines are to be followed loosely; use your best judgment when approaching content &ndash; there are always exceptions.
Use Active Voice # Write in active voice to enforce clarity and simplicity.
üëéüö´ Don&rsquo;t üëç‚úÖ Do The update was failing due to a configuration issue. The update failed due to a configuration issue. A POST request is sent and a response is returned. Send a POST request; the endpoint sends a response. You can break this rule to emphasize an object (the image is installed) or de-emphasize a subject (5 errors were found in this article).
Use Global English # Write documentation using Global English. Global English makes comprehension easier for all audiences by avoiding regional idioms/expressions and standardizing spelling words using the US English variant.
Put Conditional Clauses First # Order conditional clauses first when drafting sentences; this empowers the reader to skip to the next step when the condition does not apply.
üëéüö´ Don&rsquo;t üëç‚úÖ Do See this page for more information on how to use this feature. For more information, see How to Use Console. Enable the CustomField setting if you want to map custom fields. To map custom fields, enable the CustomField setting. Write Accessibly # Be mindful of how you describe software behavior and users; in particular, avoid ableist language. Use generic &ldquo;they/them&rdquo; and &ldquo;you&rdquo; when describing users or actors; avoid the use of &ldquo;obviously&rdquo;, &ldquo;simply&rdquo;, &ldquo;easily&rdquo; &ndash; every reader has a different level of expertise and familiarity with key concepts/tools.
üëéüö´ Don&rsquo;t üëç‚úÖ Do To start, simply enter the following command: To start, enter the following command: Configuring this setting just requires a simple API call. Make an API call to configure this setting. The results, without this setting enabled, might look crazy. Your results may be inconsistent or unreliable without this setting enabled. Formatting &amp; Punctuation # Markdown # All documentation is written using Markdown syntax in .md files. See this official Markdown Cheat Sheet for a quick introduction to the syntax.
Code Blocks # Use ``` to wrap long code samples into a code block.
This is a code block. Headers # Use title casing for all header titles.
Capitalize the first and last word. Capitalize adjectives, adverbs, nouns, pronouns, and subordinate conjuctions. Lowercase articles (a, an, the) and coordinating conjunctions (and, but,for, nor, or, so, yet). üëéüö´ Don&rsquo;t üëç‚úÖ Do How to develop sentient ai How to Develop Sentient AI How to use the pachctl cli How to Use the Pachctl CLI Links # Use meaningful link descriptions, such as the original article&rsquo;s title or a one-line summarization of its contents.
Examples:
- See the [Pachyderm Technical Documentation Style Guide](../docs-style-guide) - Use the [official Pachyderm style guide](../docs-style-guide). Lists # Use numbered lists for sequential items, such as instructions. Use unbulleted lists for all other list types (like this list). Commas # Use the serial or Oxford comma in a list of three or more items to minimize any chance of confusion.
üëéüö´ Don&rsquo;t üëç‚úÖ Do I like swimming, biking and singing. I like swimming, biking, and singing. I only trust my parents, Madonna and Shakira. I only trust my parents, Madonna, and Shakira. UI Elements # Bold UI elements when mentioned in a set of instructions (a numbered list).
Navigate to Settings &gt; Desktop. Scroll to Push Notifications. Organization # Use Nested Headers # Remember to use all header sizes (h1-h6) to organize information. Each header should be a subset of topics or examples more narrow in scope than its parents. This enables readers to both gain more context and mentally parse the information at a glance.
Publish Modular Topics # Avoid mixing objectives or use cases in one article; instead, organize and separate your content so that it is task-based. If there are many substasks (such as in a long-form tutorial), organize your content so that each major step is itself an article.
Examples:
The below outlines are 4 articles, with the parent article linking to each modular sub-topic.
How to Locally Deploy Pachyderm MacOS Local Deployment Guide Linux Local Deployment Guide Windows Local Deployment Guide Images &amp; Diagrams # Visualizations are helpful in learning complex workflows and UIs; however, they can expire quickly and take a lot of effort to maintain. Ask yourself the following questions when deciding whether or not to add visualizations:
Is the user interface complex enough to warrant a screenshot? Can a diagram convey this concept more efficiently than words? How frequently does this visual need to be updated? Also, remember to add alt-text to your visualizations for screen readers.
Mermaid # You can build diagrams using mermaid.js in our documentation by:
Adding mermaid: true to the frontmatter of an article. Drafting a diagram, like so: ```mermaid graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; ``` graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; Want to make an update to this style guide? Select Edit on Github and leave a suggestion as a pull request!
"
