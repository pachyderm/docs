,fname,text
0,Overview," Pachyderm is a data science platform that provides data-driven pipelines with version control and autoscaling. It is container-native, allowing developers to use the languages and libraries that are best suited to their needs, and runs across all major cloud providers and on-premises installations.
The platform is built on Kubernetes and integrates with standard tools for CI/CD, logging, authentication, and data APIs, making it scalable and incredibly flexible. Pachyderm’s data-driven pipelines allow you to automatically trigger data processing based on changes in your data, and the platform’s autoscaling capabilities ensure that resource utilization is optimized, maximizing developer efficiency.
"
1,Key Features," Key Features and Benefits # The following are the key features of Pachyderm that make it a powerful data processing platform.
Data-driven Pipelines # Automatically trigger pipelines based on changes in the data. Orchestrate batch or real-time data pipelines. Only process dependent changes in the data. Reproducibility and data lineage across all pipelines. Version Control # Track every change to your data automatically. Works with any file type. Supports collaboration through a git-like structure of commits. Autoscaling and Deduplication # Autoscale jobs based on resource demand. Automatically parallelize large data sets. Automatically deduplicate data across repositories. Flexibility and Infrastructure Agnosticism # Use existing cloud or on-premises infrastructure. Process any data type, size, or scale in batch or real-time pipelines. Container-native architecture allows for developer autonomy. Integrates with existing tools and services, including CI/CD, logging, authentication, and data APIs. "
2,Target Audience," Target Audience # Pachyderm is designed for data engineers and data scientists who are managing and processing large amounts of data in a scalable and efficient manner. Pachyderm is ideal for organizations working with big data and require robust, version-controlled, reproducible, and distributed data pipelines.
It is particularly useful for large unstructured data processing jobs, such as dataset curation for computer vision, speech recognition, video analytics, NLP, and many others.
Non-Target Audience # Pachyderm is not intended for users who do not require large-scale data processing and analysis. For instance, data scientists who are just starting with a small project may not need Pachyderm&rsquo;s distributed system. Additionally, users with limited experience with containerization, cloud computing, and distributed systems may find it challenging to use Pachyderm effectively.
"
3,Basic Concepts," Pachyderm File System # The Pachyderm File System (PFS) is the backbone of the Pachyderm data platform, providing a secure, scalable, and efficient way to store and manage large amounts of data. It is a version-controlled data management system that enables users to store any type of data in any format and scale, from a single file to a directory of files. The PFS is built on top of Postgres and S3, ensuring that your data is secure, consistent, and easily accessible. With PFS, users can version their data and work collaboratively with their teams, using branches and commits to manage and track changes over time.
Repositories (Repo) # Pachyderm repositories are version controlled, meaning that they keep track of changes to the data stored within them. Each repository can contain any type of data, including individual files or directories of files, and can handle data of any scale.
Learn more about Repositories
Branches # Branches in Pachyderm are similar to branches in Git. They are pointers to commits that move along a growing chain of commits. This allows you to work with different versions of your data within the same repository.
Learn more about Branches
Commits # A commit in Pachyderm is created automatically whenever data is added to or deleted from a repository. Each commit preserves the state of all files in the repository at the time of the commit, similar to a snapshot. Each commit is uniquely identifiable by a UUID and is immutable, meaning that the source data can never change.
Learn more about Commits
Pachyderm Pipeline System # The Pachyderm Pipeline System (PPS) is a core component of the Pachyderm platform, designed to run robust data pipelines in a scalable and reproducible manner. With PPS, you can define, execute, and monitor complex data transformations using code that is run in Docker containers. The output of each pipeline is version-controlled in a Pachyderm data repository, providing a complete, auditable history of all processing steps. In this way, PPS provides a flexible, data-driven solution for managing your data processing needs, while keeping data and processing results secure, reproducible, and scalable.
Learn more about the PPS
Pipelines # Pachyderm pipelines are used to transform data from Pachyderm repositories. The output data is versioned in a Pachyderm data repository, and the code for the transformation is run in Docker containers. Pipelines are triggered by new commits to a branch, making them data-driven.
Learn more about Pipelines
Jobs # A job in Pachyderm is the execution of a pipeline with a new commit. The data is distributed and parallelized computation is performed across a cluster. Each job is uniquely identified, making it possible to reproduce the results of a specific job.
Learn more about Jobs
Datum # A datum in Pachyderm is a unit of computation for a job. It is used to distribute the processing workloads and to define how data can be split for parallel processing.
Learn more about Datums
"
4,Intro to Data Versioning," Introduction to Data Versioning # On this page we want to give a brief overview of how to use and interact with versioned data inside Pachyderm. Collectively, this is often referred to as the Pachyderm File System (PFS).
Repositories # Data versioning in Pachyderm starts with creating a data repository. Pachyderm data repos are similar to Git repositories in that they provide a place to track changes made to a set of files.
Using the Pachyderm CLI (pachctl) we would create a repository called data with the create repo command.
pachctl create repo data Once a repo is created, data can be added, deleted, or updated to a branch and all changes are versioned with commits.
Commits # In Pachyderm, commits are made to branches of a repo. For example, in the following session if we add a file to our data repository, that file will be captured in a commit.
$ pachctl put file data@master -f my_file.bin $ pachctl list commit images@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master 6806cce 4 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE /my_file.bin file 57.27KiB If we then delete that file, it is removed from the active state of the branch, but the commit still exists.
$ pachctl delete file data@master:/my_file.bin $ pachctl list commit data@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master ff1867a 3 seconds ago 0B USER data master 6806cce 20 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE Then if we add the file back, we&rsquo;ll see a third commit.
$ pachctl create file data@master:/my_file.bin $ pachctl list commit data@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master 0ec029b 20 seconds ago 57.27KiB USER data master ff1867a 3 seconds ago 0B USER data master 6806cce 20 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE /my_file.bin file 57.27KiB Visualizing the commit history for the master branch looks like the following.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; tag: &#34;master&#34; Branches are a critical for tracking commits. The branch functions as a pointer to the most recent commit to the branch. For instance, when we create a new commit on the master branch (pachctl put file data@master -f my_new_file), we would create a new commit and our master branch would point at it.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;master&#34; As we&rsquo;ve already seen, we can reference the HEAD of the branch, with the syntax, data@master.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; type:HIGHLIGHT tag: &#34;HEAD&#34; Navigating Commits # Here we&rsquo;ll introduce the basics of how to navigate commits. Navigating these commits is an important aspect of working with PFS, and allows you to easily manage the history and evolution of your data.
One useful feature for navigating commits in PFS is the ability to refer to a previous commit using ancestry syntax. This syntax allows you to specify a commit relative to the current one, making it easy to compare and manipulate different versions of your data.
This makes it simple to switch between different versions of your data, and to perform operations like diffing, branching, and merging.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master^&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; type:HIGHLIGHT commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; To refer to the commit 2 before the HEAD:
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master^^&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; type:HIGHLIGHT commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; Similarly, we can abbreviate this with the following syntax:
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master^2&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; type:HIGHLIGHT commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; We can reference the commits in numerical order using .n, where n is the commit number.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master.1&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; type:HIGHLIGHT commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; %%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master.-1&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; type:HIGHLIGHT commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; Branches # In Pachyderm, branches are used to track changes in a repository. You can think of a branch as a tag on a specific commit. Branches are associated with a particular commit and are updated as new commits are made (moving the HEAD of that branch to its most recent commit). This also means that at any time, you can change the commit that a branch is associated with, affecting branch history.
Here&rsquo;s an example of a repo with three branches, each with its own history of commits:
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit commit branch v1.0 commit commit commit branch v1.1 commit commit commit tag:&#34;v1.1:HEAD&#34; checkout v1.0 commit tag:&#34;v1.0:HEAD&#34; checkout master commit tag:&#34;master:HEAD&#34; &ldquo;Merging&rdquo; Branches # The concept of merging binary data from different commits is complex. Ultimately, there are too many edge cases to do it reliably for every type of binary data, because computing a diff between two commits is ultimately meaningless unless you know how to compare the data. For example, we know that text files can be compared line-by-line or a bitmap image pixel by pixel, but how would we compute a diff for, say, binary model files?
Additionally, the output of a merge is usually a master copy, the official set of files desired. We rarely combine multiple pieces of image data to make one image, and if we are, we have usually created a technique for doing so. In the end, some files will be deleted, some updated, and some added.
Instead, merging data, means creating a new commit with the desired combination of files and pointing our branch at that commit. In order to maintain a proper history, we would also want to make sure that the parent of that commit is relevant to what we want as well.
For example, in this situation, we have created a branch, dev, based on the 1-2833cd3 commit. We have committed multiple times to the dev branch, but nothing to master.
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit id:&#34;0-96e9b89&#34; commit id:&#34;1-2833cd3&#34; tag:&#34;master:HEAD&#34; branch dev commit id:&#34;2-25a8daf&#34; commit id:&#34;3-6413afc&#34; commit id:&#34;4-41a750b&#34; tag:&#34;dev:HEAD&#34; In this case it is simple to simply move the master branch to follow the most recent commit on dev, 4-41a750b.
pachctl create branch data@master --head 41a750b
Which would look like this:
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit id:&#34;0-96e9b89&#34; commit id:&#34;1-2833cd3&#34; branch dev commit id:&#34;2-25a8daf&#34; commit id:&#34;3-6413afc&#34; commit id:&#34;4-41a750b&#34; tag:&#34;master:HEAD, dev:HEAD&#34; Or from the history perspective of the respective branches:
%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit id:&#34;0-96e9b89&#34; commit id:&#34;1-2833cd3&#34; branch dev commit id:&#34;2-25a8daf&#34; commit id:&#34;3-6413afc&#34; checkout dev commit tag:&#34;dev:HEAD&#34; id:&#34;4-41a750b&#34; checkout master commit id:&#34;2-25a8daf &#34; commit id:&#34;3-6413afc &#34; commit tag:&#34;master:HEAD&#34; id:&#34;4-41a750b &#34; Branches are useful for many reasons, but in Pachyderm they also form the foundation of the pipeline system. New commits on branches can be used to trigger pipelines to run, resulting in one of the key differentiators, data-driven pipelines.
"
5,Intro to Pipelines," Introduction to Pipelines # The Pachyderm Pipeline System (PPS) is a powerful tool for automating data transformations. With PPS, pipelines can be automatically triggered whenever input data changes, meaning that data transformations happen automatically in response to changes in your data, without the need for manual intervention.
Pipelines in Pachyderm are defined by a pipeline specification and run on Kubernetes. The output of a pipeline is stored in a versioned data repository, which allows you to reproduce any transformation that occurs in Pachyderm.
Pipelines can be combined into a computational DAG (directed acyclic graph), with each pipeline being triggered when an upstream commit is finished. This allows you to build complex workflows that can process large amounts of data efficiently and with minimal manual intervention.
Pipeline Specification # This is a Pachyderm pipeline definition in YAML. It describes a pipeline called transform that takes data from the data repository and transforms it using a Python script my_transform_code.py.
pipeline: name: transform input: pfs: repo: data glob: &#34;/*&#34; transform: image: my-transform-image:v1.0 cmd: - python - &#34;/my_transform_code.py&#34; - &#34;--input&#34; - &#34;/pfs/data/&#34; - &#34;--output&#34; - &#34;/pfs/out/&#34; Here&rsquo;s a breakdown of the different sections of the pipeline definition:
pipeline specifies the name of the pipeline (in this case, it&rsquo;s transform). This name will also be used as the name for the output data repository. input specifies the input for the pipeline. In this case, the input is taken from the data repository in Pachyderm. glob is used to specify how the files from the repository map to datums for processing. In this case, /* is used to specify all files in the repository can be processed individually. transform specifies the code and image to use for processing the input data. The image field specifies the Docker image to use for the pipeline. In this example, the image is named my-transform-image with a tag of v1.0. The cmd field specifies the command to run inside the container. In this example, the command is python /my_transform_code.py, which runs a Python script named my_transform_code.py. The script is passed the --input flag pointing to the input data directory, and the --output flag pointing to the output data directory. /pfs/data/ and /pfs/out/ are directories created by Pachyderm. The input directory will contain an individual datum when the job is running, and anything put into the output directory will be committed to the output repositories when the job is complete. So, in summary, this pipeline definition defines a pipeline called transform that takes all files in the data repository, runs a Python script to transform them, and outputs the results to the out repository.
Datums and Jobs # Pipelines can distribute work across a cluster to parallelize computation. Each time data is committed to a Pachyderm repository, a job is created for each pipeline with that repo as an input to process the data.
To determine how to distribute data and computational work, datums are used. A datum is an indivisible unit of data required by the pipeline, defined according to the pipeline spec. The datums will be distributed across the cluster to be processed by workers.
ℹ️ Only one job per pipeline will be created per commit, but there may be many datums per job.
For example, say you have a bunch of images that you want to normalize to a single size. You could iterate through each image and use opencv to change the size of it. No image depends on any other image, so this task can be parallelized by treating each image as an individual unit of work, a datum.
Next, let’s say you want to create a collage from those images. Now, we need to consider all of the images together to combine them. In this case, the collection of images would be a single datum, since they are all required for the process.
Pachyderm input specifications can handle both of these situations with the glob section of the Pipeline Specification.
Basic Glob Patterns # In this section we&rsquo;ll introduce glob patterns and datums in a couple of examples.
In the basic glob pattern example below, the input glob pattern is /*. This pattern matches each image at the top level of the images@master branch as an individual unit of work.
pipeline: name: resize description: A pipeline that resizes an image. input: pfs: glob: /* repo: images transform: cmd: - python - resize.py - --input - /pfs/images/* - --output - /pfs/out/ image: pachyderm/opencv When the pipeline is executed, it retrieves the datums defined in the input specification. For each datum, the worker downloads the necessary files into the Docker container at the start of its execution, and then performs the transform. Once the execution is complete, the output for each execution is combined into a commit and written to the output data repository.
In this example, the input glob pattern is /. This pattern matches everything at the top level of the images@master branch as an individual unit of work.
pipeline: name: collage description: A pipeline that creates a collage for a collection of images. input: pfs: glob: / repo: images transform: cmd: - python - collage.py - --input - /pfs/images/* - --output - /pfs/out/ image: pachyderm/opencv When this pipeline runs, it retrieves a single datum from the input specification. The job runs the single datum, downloading all the files from the images@master into the Docker container, and performs the transform. The result is then committed to the output data repository.
Advanced Glob Patterns # Datums can also be created from advanced operations, such as Join, Cross, Group, Union, and others to combine glob patterns from multiple data repositories. This allows us to create complex datum definitions, enabling sophisticated data processing pipelines.
Pipeline Communication (Advanced) # A much more detailed look at how Pachyderm actually triggers pipelines is shown in the sequence diagram below. This is a much more advanced level of detail, but knowing how the different pieces of the platform interact can be useful.
Before we look at the diagram, it may be helpful to provide a brief recap of the main participants involved:
User: The user is the person interacting with Pachyderm, typically through the command line interface (CLI) or one of the client libraries. PFS (Pachyderm File System): PFS is the underlying file system that stores all of the data in Pachyderm. It provides version control and lineage tracking for all data inside it. PPS (Pachyderm Pipeline System): PPS is how code gets applied to the data in Pachyderm. It manages the computational graph, which describes the dependencies between different steps of the data processing pipeline. Worker: Workers are Kubernetes pods that executes the jobs defined by PPS. Each worker runs a container image that contains the code for a specific pipeline. The worker will iterate through the datums it is given and apply user code to it. sequenceDiagram participant User participant PPS participant PFS participant Worker User-&gt;&gt;PFS: pachctl create repo foo activate PFS Note over PFS: create branch foo@master deactivate PFS User-&gt;&gt;PPS: pachctl create pipeline bar activate PPS PPS-&gt;&gt;PFS: create branch bar@master &lt;br&gt; (provenant on foo@master) PPS-&gt;&gt;Worker: create pipeline worker master Worker-&gt;&gt;PFS: subscribe to bar@master &lt;br&gt; (because it&#39;s subvenant on foo@master) deactivate PPS User-&gt;&gt;PFS: pachctl put file -f foo@master data.txt activate PFS Note over PFS: start commit PFS-&gt;&gt;PFS: propagate commit &lt;br&gt; (start downstream commits) Note over PFS: copy data.txt to open commit Note over PFS: finish commit PFS--&gt;&gt;Worker: subscribed commit returns deactivate PFS Note over Worker: Pipeline Triggered activate Worker Worker-&gt;&gt;PPS: Create job Worker-&gt;&gt;PFS: request datums for commit PFS--&gt;&gt;Worker: Datum list loop Each datum PFS-&gt;&gt;Worker: download datum Note over Worker: Process datum with user code Worker-&gt;&gt;PFS: copy data to open output commit end Worker-&gt;&gt;PFS: Finish commit Worker-&gt;&gt;PPS: Finish job deactivate Worker This diagram illustrates the data flow and interaction between the user, the Pachyderm Pipeline System (PPS), the Pachyderm File System (PFS), and a worker node when creating and running a Pachyderm pipeline. Note, this is simplified for the single worker case. The multi-worker and autoscaling mechanisms are more complex.
The sequence of events begins with the user creating a PFS repo called foo and a PPS pipeline called bar with the foo repo as its input. When the pipeline is created, PPS creates a branch called bar@master, which is provenant on the foo@master branch in PFS. A worker pod is then created in the Kubernetes cluster by PPS, which subscribes to the bar@master branch.
When the user puts a file named data.txt into the foo@master branch, PFS starts a new commit and propagates the commit, opening downstream commits for anything impacted. The worker receives the subscribed commit and when it finishes, triggers the pipeline.
The triggered pipeline creates a job for the pipeline, requesting datums for the output commit. For each datum, the worker downloads the data, processes it with the user&rsquo;s code, and writes the output to an open output commit in PFS. Once all datums have been processed, the worker finishes the output commit and the job is marked as complete.
"
6,Get Started," What is Pachyderm? # Pachyderm is a data-centric pipeline and data versioning application written in go that runs on top of a Kubernetes cluster.
Local vs Cloud Installation # Local Cloud Used for learning &amp; testing. Used in production environments. Allocates your local machine&rsquo;s resources to spin up a K8s cluster. Uses a cloud provider (AWS, Azure, GCP) to to spin up K8s clusters. Uses Docker Desktop, Minikube, or Kind. Uses EKS, GKE, or AKS Free. Metered by cloud provider. "
7,Local Getting Started Guides," What is a Local Installation? # A local installation means that you will allocate resources from your local machine (e.g., your laptop) to spin up a Kubernetes cluster to run Pachyderm. This installation method is not for a production setup, but is great for personal use, testing, and product exploration.
Which Guide Should I Use? # Both the Docker Desktop and Minikube installation guides support MacOS, Windows, and Linux. If this is your first time using Kubernetes, try Docker Desktop &mdash; if you are experienced with Kubernetes, you can deploy using a variety of solutions not listed here (KinD, Rancher Desktop, Podman, etc.).
💡 Binary Files (Advanced Users)
You can download the latest binary files from GitHub for a direct installation of pachctl and the mount-server.
"
8,Docker Desktop," Before You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:
Open a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each of the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; All installation steps after 1. Install Docker Desktop must be run through the WSL terminal (Ubuntu) and not in Powershell.
You are now ready to continue to Step 1.
You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; 1. Install Docker Desktop # Install Docker Desktop for your machine. Navigate to Settings for Mac, Windows, or Linux. Adjust your resources (~4 CPUs and ~12GB Memory) Enable Kubernetes Select Apply &amp; Restart. 2. Install Pachctl CLI # Operating System: MacOs, Windows, &amp; Darwin Debian brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.5 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.5.1/pachctl_2.5.1_amd64.deb &amp;&amp; sudo dpkg -i /tmp/pachctl.deb 3. Install &amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install PachD: Version: Community Edition Enterprise helm install pachd pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:
helm install pachd pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --set pachd.enterpriseLicenseKey=$(cat license.txt) --set ingress.host=localhost This unlocks Enterprise features but also requires user authentication to access Console. A mock user is created by default to get you started, with the username: admin and password: password.
This may take several minutes to complete.
4. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 5. Connect to Cluster # pachctl connect grpc://localhost:80 ℹ️ If the connection commands did not work together, run each separately.
Optionally open your browser and navigate to the Console UI.
💡 You can check your Pachyderm version and connection to pachd at any time with the following command:
pachctl version COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
9,Minikube,"Minikube is a tool that quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. It&rsquo;s a great solution for trying out Pachyderm locally.
Before You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have Docker Desktop installed with Kubernetes enabled. You must have Docker Desktop installed with Kubernetes enabled. You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:
Open a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; All installation steps after this point must be run through the WSL terminal (Ubuntu) and not in Powershell.
You are now ready to continue to Step 1.
You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have Docker Desktop installed. 1. Install Docker # brew install docker See the official Docker getting started guide for the most up-to-date installation steps.
2. Install &amp; Start Minikube # Install # brew install minikube See the official Minikube getting started guide for the most up-to-date installation steps.
Start # Launch Docker Desktop. Start Minikube: minikube start 3. Install Pachctl CLI # brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.5 4. Install &amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install PachD: Version: Community Edition Enterprise helm install pachd pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:
helm install pachd pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --set pachd.enterpriseLicenseKey=$(cat license.txt) --set ingress.host=localhost This unlocks Enterprise features but also requires user authentication to access Console. A mock user is created by default to get you started, with the username: admin and password: password.
This may take several minutes to complete.
5. Verify Installation # Run the following command in a new terminal to check the status of your pods: kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE default console-6b9bb8766d-f2gm4 1/1 Running 0 41m default etcd-0 1/1 Running 0 41m default pachd-76896d6b5d-kmfvw 1/1 Running 0 41m default pachd-loki-0 1/1 Running 0 41m default pachd-promtail-rm5ss 1/1 Running 0 41m default pachyderm-kube-event-tail-b9b554fb6-dpcsr 1/1 Running 0 41m default pg-bouncer-5c9494c678-z57qh 1/1 Running 0 41m default postgres-0 1/1 Running 0 41m kube-system coredns-6d4b75cb6d-jnl5f 1/1 Running 3 (42m ago) 97d kube-system etcd-minikube 1/1 Running 4 (42m ago) 97d kube-system kube-apiserver-minikube 1/1 Running 3 (42m ago) 97d kube-system kube-controller-manager-minikube 1/1 Running 4 (42m ago) 97d kube-system kube-proxy-bngzv 1/1 Running 3 (42m ago) 97d kube-system kube-scheduler-minikube 1/1 Running 3 (42m ago) 97d kube-system storage-provisioner 1/1 Running 5 (42m ago) 97d kubernetes-dashboard dashboard-metrics-scraper-78dbd9dbf5-swttf 1/1 Running 3 (42m ago) 97d kubernetes-dashboard kubernetes-dashboard-5fd5574d9f-c7ptx 1/1 Running 4 (42m ago) 97d Re-run this command after a few minutes if pachd is not ready. 6. Connect to Cluster # pachctl connect grpc://localhost:80 ℹ️ If the connection commands did not work together, run each separately.
Optionally open your browser and navigate to the Console UI.
💡 You can check your Pachyderm version and connection to pachd at any time with the following command:
pachctl version COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
10,AWS + Pachyderm," Before You Start # This guide assumes that you have already tried Pachyderm locally and have all of the following installed:
Kubectl Pachctl Helm AWS CLI Eksctl 1. Create an EKS Cluster # Use the eksctl tool to deploy an EKS Cluster: eksctl create cluster --name pachyderm-cluster --region &lt;region&gt; -profile &lt;your named profile&gt; Verify deployment: kubectl get all 2. Create an S3 Bucket # Run the following command: aws s3api create-bucket --bucket ${BUCKET_NAME} --region ${AWS_REGION} Verify. aws s3 ls 3. Enable Persistent Volumes Creation # Create an IAM OIDC provider for your cluster. Install the Amazon EBS Container Storage Interface (CSI) driver on your cluster. Create a gp3 storage class manifest file (e.g., gp3-storageclass.yaml) kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp3 annotations: storageclass.kubernetes.io/is-default-class: &#34;true&#34; provisioner: kubernetes.io/aws-ebs parameters: type: gp3 fsType: ext4 Set gp3 to your default storage class. kubectl apply -f gp3-storageclass.yaml Verify that it has been set as your default. kubectl get storageclass 4. Create a Values.yaml # Version: Community Edition Enterprise deployTarget: &#34;AMAZON&#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: amazon: bucket: &#34;bucket_name&#34; # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; region: &#34;us-east-2&#34; externalService: enabled: true console: enabled: true deployTarget: &#34;AMAZON&#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: amazon: bucket: &#34;bucket_name&#34; # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; region: &#34;us-east-2&#34; # pachyderm enterprise key enterpriseLicenseKey: &#34;YOUR_ENTERPRISE_TOKEN&#34; console: enabled: true 5. Configure Helm # Run the following to add the Pachyderm repo to Helm:
helm repo add pach https://helm.pachyderm.com helm repo update helm install pachd pach/pachyderm -f my_pachyderm_values.yaml 6. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 7. Connect to Cluster # pachctl connect grpc://localhost:80 ℹ️ If the connection commands did not work together, run each separately.
Optionally open your browser and navigate to the Console UI.
💡 You can check your Pachyderm version and connection to pachd at any time with the following command:
pachctl version COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
11,Azure + Pachyderm," Before You Start # This guide assumes that you have already tried Pachyderm locally and have all of the following installed:
Kubectl Pachctl Helm Azure CLI. 1. Create an AKS Cluster # You can deploy Kubernetes on Azure by following the official Azure Kubernetes Service documentation, use the quickstart walkthrough, or follow the steps in this section.
At a minimum, you will need to specify the parameters below:
Variable Description RESOURCE_GROUP A unique name for the resource group where Pachyderm is deployed. For example, pach-resource-group. LOCATION An Azure availability zone where AKS is available. For example, centralus. NODE_SIZE The size of the Kubernetes virtual machine (VM) instances. To avoid performance issues, Pachyderm recommends that you set this value to at least Standard_DS4_v2 which gives you 8 CPUs, 28 Gib of Memory, 56 Gib SSD.
In any case, use VMs that support premium storage. See Azure VM sizes for details around which sizes support Premium storage. CLUSTER_NAME A unique name for the Pachyderm cluster. For example, pach-aks-cluster. You can choose to follow the guided steps in Azure Service Portal&rsquo;s Kubernetes Services or use Azure CLI.
Log in to Azure:
az login This command opens a browser window. Log in with your Azure credentials. Resources can now be provisioned on the Azure subscription linked to your account.
Create an Azure resource group or retrieve an existing group.
az group create --name ${RESOURCE_GROUP} --location ${LOCATION} Example:
az group create --name test-group --location centralus System Response:
{ &#34;id&#34;: &#34;/subscriptions/6c9f2e1e-0eba-4421-b4cc-172f959ee110/resourceGroups/pach-resource-group&#34;, &#34;location&#34;: &#34;centralus&#34;, &#34;managedBy&#34;: null, &#34;name&#34;: &#34;pach-resource-group&#34;, &#34;properties&#34;: { &#34;provisioningState&#34;: &#34;Succeeded&#34; }, &#34;tags&#34;: null, &#34;type&#34;: null } Create an AKS cluster in the resource group/location:
For more configuration options: Find the list of all available flags of the az aks create command.
az aks create --resource-group ${RESOURCE_GROUP} --name ${CLUSTER_NAME} --node-vm-size ${NODE_SIZE} --node-count &lt;node_pool_count&gt; --location ${LOCATION} Example:
az aks create --resource-group test-group --name test-cluster --generate-ssh-keys --node-vm-size Standard_DS4_v2 --location centralus Confirm the version of the Kubernetes server by running kubectl version.
ℹ️ &ldquo;See Also:&rdquo; - Azure Virtual Machine sizes
Once your Kubernetes cluster is up, and your infrastructure configured, you are ready to prepare for the installation of Pachyderm. Some of the steps below will require you to keep updating the values.yaml started during the setup of the recommended infrastructure:
2. Create a Storage Container # Pachyderm needs an Azure Storage Container (Object store) to store your data.
To access your data, Pachyderm uses a Storage Account with permissioned access to your desired container. You can either use an existing account or create a new one in your default subscription, then use the JSON key associated with the account and pass it on to Pachyderm.
Set up the following variables:
STORAGE_ACCOUNT: The name of the storage account where you store your data. CONTAINER_NAME: The name of the Azure blob container where you store your data. Create an Azure storage account:
az storage account create \ --resource-group=&#34;${RESOURCE_GROUP}&#34; \ --location=&#34;${LOCATION}&#34; \ --sku=Premium_LRS \ --name=&#34;${STORAGE_ACCOUNT}&#34; \ --kind=BlockBlobStorage System response:
{ &#34;accessTier&#34;: null, &#34;creationTime&#34;: &#34;2019-06-20T16:05:55.616832+00:00&#34;, &#34;customDomain&#34;: null, &#34;enableAzureFilesAadIntegration&#34;: null, &#34;enableHttpsTrafficOnly&#34;: false, &#34;encryption&#34;: { &#34;keySource&#34;: &#34;Microsoft.Storage&#34;, &#34;keyVaultProperties&#34;: null, &#34;services&#34;: { &#34;blob&#34;: { &#34;enabled&#34;: true, ... Make sure that you set Stock Keeping Unit (SKU) to Premium_LRS and the kind parameter is set to BlockBlobStorage. This configuration results in a storage that uses SSDs rather than standard Hard Disk Drives (HDD). If you set this parameter to an HDD-based storage option, your Pachyderm cluster will be too slow and might malfunction.
Verify that your storage account has been successfully created:
az storage account list Obtain the key for the storage account (STORAGE_ACCOUNT) and the resource group to be used to deploy Pachyderm:
STORAGE_KEY=&#34;$(az storage account keys list \ --account-name=&#34;${STORAGE_ACCOUNT}&#34; \ --resource-group=&#34;${RESOURCE_GROUP}&#34; \ --output=json \ | jq &#39;.[0].value&#39; -r )&#34; ℹ️ Find the generated key in the Storage accounts &gt; Access keys section in the Azure Portal or by running the following command az storage account keys list --account-name=${STORAGE_ACCOUNT}.
Create a new storage container within your storage account:
az storage container create --name ${CONTAINER_NAME} \ --account-name ${STORAGE_ACCOUNT} \ --account-key &#34;${STORAGE_KEY}&#34; 3. Create a Values.yaml # Version: Community Edition Enterprise deployTarget: &#34;MICROSOFT&#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: microsoft: # storage container name container: &#34;blah&#34; # storage account name id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # storage account key secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; externalService: enabled: true console: enabled: true deployTarget: &#34;MICROSOFT&#34; proxy: enabled: true service: type: LoadBalancer ingress: host: &lt;insert-external-ip-address-or-dns-name&gt; pachd: storage: microsoft: # storage container name container: &#34;blah&#34; # storage account name id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # storage account key secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; # pachyderm enterprise key enterpriseLicenseKey: &#34;YOUR_ENTERPRISE_TOKEN&#34; console: enabled: true 4. Configure Helm # Run the following to add the Pachyderm repo to Helm:
helm repo add pach https://helm.pachyderm.com helm repo update helm install pachd pach/pachyderm -f my_pachyderm_values.yaml 5. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 6. Connect to Cluster # pachctl connect grpc://localhost:80 ℹ️ If the connection commands did not work together, run each separately.
Optionally open your browser and navigate to the Console UI.
💡 You can check your Pachyderm version and connection to pachd at any time with the following command:
pachctl version COMPONENT VERSION pachctl 2.5.1 pachd 2.5.1 "
12,GCP + Pachyderm," Before You Start # This guide assumes that:
You have already tried Pachyderm locally and have some familiarity with Kubectl, Helm, Google Cloud SDK and jq. You have access to a Google Cloud account linked to an active billing account. ⚠️ This is not a production-level setup guide; see the Google Cloud Platform deploy guide for in-depth setup.
1. Create a New Project # Log in to Google Cloud Console. Create a new project (e.g.,pachyderm-quickstart-project). Enable the Compute Engine API. You are now ready to create a GKE Cluster.
2. Run Setup Script # You can run this setup script either through the Cloud Shell or in a local terminal via the gcloud cli. Running this script creates all of the following:
One GKE cluster Workload identity service accounts Permissions A static IP address The cloud SQL instance and databases One cloud storage bucket One file called ${NAME}.values.yaml in the current directory It also installs Pachyderm into the cluster.
3. Connect to Cluster # Run the following: pachctl config import-kube local --overwrite pachctl config set active-context local pachctl port-forward Open your browser at localhost:4000. 💡 You can also connect to Console via Google&rsquo;s Cloud Shell:
"
13,Install Pachctl Auto-completion,"Pachyderm autocompletion allows you to automatically finish partially typed commands by pressing TAB. Autocompletion needs to be installed separately when pachctl is already available on your client machine.
Pachyderm autocompletion is supported for bash and zsh shells. You must have either of them preinstalled before installing Pachyderm autocompletion.
💡 Type pachctl completion --help to display help information about the command.
Command Shell: Zsh Bash Verify that bash-completion is installed on your machine. For example, if you have installed bash completion by using Homebrew, type:
brew info bash-completion This command returns information about the directory in which bash-completion and bash completion scripts are installed. For example, /usr/local/etc/bash_completion.d/. You need to specify the path to bash_completion.d as the path to which install pachctl autocompletion. Also, the output of the info command might have a suggestion to include the path to bash-completion into your ~/.bash_profile file.
Install pachctl autocompletion:
pachctl completion bash --install --path &lt;path/to/bash-completion&gt; For example, if you specify the path to bash-completion as /usr/local/etc/bash_completion.d/pachctl, your system response looks like this:
System response:
Bash completions installed in /usr/local/etc/bash_completion.d/pachctl, you must restart bash to enable completions. Restart your terminal.
pachctl autocomplete should now be enabled in your system.
Verify that zsh-completions are installed on your machine. For example, if you have installed zsh completion by using Homebrew, type:
brew info zsh-completions You should see the directory in which zsh-completions are installed and instructions to add the correct path in the ~/.zshrc file. Make sure you add the required path. If you do not have the ~/.zshrc file on your computer, create one. For more information about setting up zsh completions, see zsh-completions.
Install pachctl autocompletion for zsh:
pachctl completion zsh --install --path &lt;path/to/zsh-completions&gt; Example:
pachctl completion zsh --install --path /usr/local/share/zsh-completions/_pachctl System response:
Completions installed in &#34;_pachctl&#34;, you must restart your terminal to enable them. Restart your terminal.
pachctl autocomplete should now be enabled in your system.
"
14,Beginner Tutorial," Before You Start # Install Pachyderm either locally our within the cloud. Install Pachyderm Shell. Join our Slack Community so you can ask any questions you may have! Context # Pachyderm creates a Kubernetes cluster that you interact with using either the pachctl CLI or through Console, a GUI.
pachctl is great for users already experienced with using a CLI. Console is great for beginners and helps with visualizing relationships between projects, repos, and pipelines. Within the cluster, you can create projects that contain repos and pipelines. Pipelines can be single-stage or multi-stage; multi-stage pipelines are commonly referred to as DAGs.
Tutorial: Image processing with OpenCV # In this tutorial you&rsquo;ll create an image edge detection pipeline that processes new data as it is added and outputs the results.
1. Create a Project # To keep our work organized, we&rsquo;re going to create a project named openCV and set it to our currently active context.
pachctl create project openCV pachctl config update context --project openCV You can always check to confirm which project has been set to your context by running the following commands:
# prints current context name (local) pachctl config get active-context # prints local&#39;s context details pachctl config get context local # { # &#34;source&#34;: &#34;IMPORTED&#34;, # &#34;cluster_name&#34;: &#34;docker-desktop&#34;, # &#34;auth_info&#34;: &#34;docker-desktop&#34;, # &#34;cluster_deployment_id&#34;: &#34;dev&#34;, # &#34;project&#34;: &#34;openCV&#34; # } 2. Create a Repo # Repos should be dedicated to a single source of data such as log messages from a particular service, a users table, or training data.
pachctl create repo images You can verify that the repository was created by running the following command:
pachctl list repo # NAME CREATED SIZE (MASTER) ACCESS LEVEL # images 4 seconds ago ≤ 0B [repoOwner] 3. Add Data # In Pachyderm, you write data to an explicit commit. Commits are immutable snapshots of your data which give Pachyderm its version control properties. You can add, remove, or update files in a given commit.
Upload an Image File # We&rsquo;re going to use the pachctl put file command, along with the -f flag, to upload an image.
pachctl put file images@master:liberty.png -f http://imgur.com/46Q8nDz.png pachctl put file automatically starts and finishes a commit for you so you can add files more easily.
💡 If you want to add many files over a period of time, you can do pachctl start commit and pachctl finish commit yourself.
You can confirm the commit using the following command:
pachctl list commit images # REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION # openCV/images master 37559e89ed0c4a0cb354649524050851 10 seconds ago 57.27KiB USER You can also view the filename in the commit using the following command:
pachctl list file images@master # NAME TYPE SIZE # /liberty.png file 57.27KiB View Image # In Terminal # Operating System: MacOS Linux pachctl get file images@master:liberty.png | open -f -a Preview.app pachctl get file images@master:liberty.png | display In Console # In your Console, click on the images repo to visualize its commit and inspect its file:
4. Create a Pipeline # Now that you have some data in your repo, it is time to do something with it using a pipeline.
Pipelines process data and are defined using a JSON pipeline specification. For this tutorial, we&rsquo;ve already created the spec for you.
Review Pipeline Spec # Take a moment to review the details of the provided pipeline spec so that you&rsquo;ll know how to create one on your own in the future.
{ // The `pipeline` section contains a `name` for identification; this name is also used to create a corresponding output repo. &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;description&#34;: &#34;A pipeline that performs image edge detection by using the OpenCV library.&#34;, // The `transform` section allows you to specify the docker `image` you want to use (`pachyderm/opencv:1.0`)and the `cmd` that defines the entry point (`edges.py`). &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/edges.py&#34; ], &#34;image&#34;: &#34;pachyderm/opencv:1.0&#34; }, // The input section specifies repos visible to the running pipeline, and how to process the data from the repos. // Commits to these repos trigger the pipeline to create new processing jobs. In this case, `images` is the repo, and `/*` is the glob pattern. &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;images&#34;, // The glob pattern defines how the input data will be transformed into datum if you want to distribute computation. `/*` means that each file can be processed individually. &#34;glob&#34;: &#34;/*&#34; } } } The following extract is the Python code run in this pipeline:
import cv2 import numpy as np from matplotlib import pyplot as plt import os # make_edges reads an image from /pfs/images and outputs the result of running # edge detection on that image to /pfs/out. Note that /pfs/images and # /pfs/out are special directories that Pachyderm injects into the container. def make_edges(image): img = cv2.imread(image) tail = os.path.split(image)[1] edges = cv2.Canny(img,100,200) plt.imsave(os.path.join(&#34;/pfs/out&#34;, os.path.splitext(tail)[0]+&#39;.png&#39;), edges, cmap = &#39;gray&#39;) # walk /pfs/images and call make_edges on every file found for dirpath, dirs, files in os.walk(&#34;/pfs/images&#34;): for file in files: make_edges(os.path.join(dirpath, file)) The code simply walks over all the images in /pfs/images, performs edge detection, and writes the result to /pfs/out.
/pfs/images and /pfs/out are special local directories that Pachyderm creates within the container automatically. Input data is stored in /pfs/&lt;input_repo_name&gt;. ℹ️ Your code must write out to /pfs/out (see the function make_edges(image) above). Pachyderm gathers data written to /pfs/out, versions it, and maps it to the pipeline&rsquo;s output repo of the same name.
Now, let&rsquo;s create the pipeline in Pachyderm:
pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/2.5.x/examples/opencv/edges.json Again, check the end result in your Console: What Happens When You Create a Pipeline # When you create a pipeline, Pachyderm transforms all current and future data added to your input repo using your user code. This process is known as a job. The initial job downloads the specified Docker image that is used for all future jobs.
View the job: pachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # 23378d899d3d45738f55df3809841145 1 ▇▇▇▇▇▇▇▇ 5 seconds ago 5 seconds ago Check the state of your pipeline: pachctl list pipeline # NAME VERSION INPUT CREATED STATE / LAST JOB DESCRIPTION # edges 1 images:/* 2 minutes ago running / success A pipeline that performs image edge detection by using the OpenCV library. List your repositories: pachctl list repo # NAME CREATED SIZE (MASTER) ACCESS LEVEL # edges 10 minutes ago ≤ 22.22KiB [repoOwner] Output repo for pipeline edges. # images 3 hours ago ≤ 57.27KiB [repoOwner] Reading the Output # We can view the output data from the edges repo in the same fashion that we viewed the input data.
Operating System: MacOS Linux pachctl get file edges@master:liberty.png | open -f -a Preview.app pachctl get file edges@master:liberty.png | display Processing More Data # Create two new commits: pachctl put file images@master:AT-AT.png -f http://imgur.com/8MN9Kg0.png pachctl put file images@master:kitten.png -f http://imgur.com/g2QnNqa.png View the list of jobs that have started: pachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # 1c1a9d7d36944eabb4f6f14ebca25bf1 1 ▇▇▇▇▇▇▇▇ 31 seconds ago 31 seconds ago # fe5c4f70ac4347fd9c5934f0a9c44651 1 ▇▇▇▇▇▇▇▇ 47 seconds ago 47 seconds ago # 23378d899d3d45738f55df3809841145 1 ▇▇▇▇▇▇▇▇ 12 minutes ago 12 minutes ago View the output data: Operating System: MacOS Linux pachctl get file edges@master:AT-AT.png | open -f -a Preview.app pachctl get file edges@master:kitten.png | open -f -a Preview.app pachctl get file edges@master:AT-AT.png | display pachctl get file edges@master:kitten.png | display 5. Create a DAG # Currently, we&rsquo;ve only set up a single-stage pipeline. Let&rsquo;s create a multi-stage pipeline (also known as a DAG) by adding a montage pipeline that takes our both original and edge-detected images and arranges them into a single montage of images:
Below is the pipeline spec for this new pipeline:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;montage&#34; }, &#34;description&#34;: &#34;A pipeline that combines images from the `images` and `edges` repositories into a montage.&#34;, &#34;input&#34;: { &#34;cross&#34;: [ { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/&#34;, &#34;repo&#34;: &#34;images&#34; } }, { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/&#34;, &#34;repo&#34;: &#34;edges&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;sh&#34; ], &#34;image&#34;: &#34;v4tech/imagemagick&#34;, &#34;stdin&#34;: [ &#34;montage -shadow -background SkyBlue -geometry 300x300+2+2 $(find /pfs -type f | sort) /pfs/out/montage.png&#34; ] } } This montage pipeline spec is similar to our edges pipeline except for the following differences:
We are using a different Docker image that has imagemagick installed. We are executing a sh command with stdin instead of a python script in the pipeline&rsquo;s transform section. We have multiple input data repositories (images and edges). In the montage pipeline we are combining our multiple input data repositories using a cross pattern. This cross pattern creates a single pairing of our input images with our edge detected images.
Create the montage pipeline: pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/2.5.x/examples/opencv/montage.json View the triggered jobs: pachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # 01e0c8040e18429daf7f67ce34c3a5d7 1 ▇▇▇▇▇▇▇▇ 11 seconds ago 11 seconds ago # 1c1a9d7d36944eabb4f6f14ebca25bf1 1 ▇▇▇▇▇▇▇▇ 12 minutes ago 12 minutes ago # fe5c4f70ac4347fd9c5934f0a9c44651 1 ▇▇▇▇▇▇▇▇ 12 minutes ago 12 minutes ago # 23378d899d3d45738f55df3809841145 1 ▇▇▇▇▇▇▇▇ 24 minutes ago 24 minutes ago View the generated montage image: Operating System: MacOS Linux pachctl get file montage@master:montage.png | open -f -a Preview.app pachctl get file montage@master:montage.png | display "
15,Concepts,"This section details the foundational concepts of Pachyderm&rsquo;s data versioning and pipeline semantics broken down into two main components:
Pachyderm File System (PFS) manages Pachyderm&rsquo;s data and versioning system. Pachyderm Pipeline System (PPS) enables you to perform various transformations on your data. "
16,Deferred Processing,"While a Pachyderm pipeline is running, it processes any new data that you commit to its input branch. However, in some cases, you want to commit data more frequently than you want to process it.
Because Pachyderm pipelines do not reprocess the data that has already been processed, in most cases, this is not an issue. But, some pipelines might need to process everything from scratch. For example, you might want to commit data every hour, but only want to retrain a machine learning model on that data daily because it needs to train on all the data from scratch.
In these cases, you can leverage a massive performance benefit from deferred processing. This section covers how to achieve that and control what gets processed.
Pachyderm controls what is being processed by using the filesystem, rather than at the pipeline level. Although pipelines are inflexible, they are simple and always try to process the data at the heads of their input branches. In contrast, the filesystem is very flexible and gives you the ability to commit data in different places and then efficiently move and rename the data so that it gets processed when you want.
Configure a Staging Branch in an Input repository # When you want to load data into Pachyderm without triggering a pipeline, you can upload it to a staging branch and then submit accumulated changes in one batch by re-pointing the HEAD of your master branch to a commit in the staging branch.
Although, in this section, the branch in which you consolidate changes is called staging, you can name it as you like.
ℹ️ You can have multiple staging branches. For example, dev1, dev2, staging&hellip;
In the example below, we first create a repository called data on which we configure a staging branch:
💡 A simple pipeline subscribes to the master branch of the repo data:
{ &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*&#34;, } } Create a repository. For example, data.
pachctl create repo data Create a master branch.
pachctl create branch data@master View the created branch:
pachctl list commit data REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION data master 8090bfb4d4fe44158eac12199c37a591 About a minute ago 0B AUTO Pachyderm automatically created an empty HEAD commit on the new branch, as you can see from the zero-byte size and AUTO commit origin. When you commit data to the master branch, the pipeline immediately starts a job to process it. However, if you want to commit something without immediately processing it, you need to commit it to a different branch.
Commit a file to the staging branch:
pachctl put file data@staging -f &lt;file&gt; Pachyderm automatically creates the staging branch. Your repo now has 2 branches, staging and master. In this example, the staging name is used, but you can name the branch as you want.
Verify that the branches were created:
pachctl list branch data BRANCH HEAD TRIGGER staging f3506f0fab6e483e8338754081109e69 - master 8090bfb4d4fe44158eac12199c37a591 - The master branch still has the same HEAD commit. No jobs have started to process the new file, because there are no pipelines that take staging as inputs. You can continue to commit to staging to add new data to the branch, and the pipeline will not process anything.
When you are ready to process the data, update the master branch to point it to the head of the staging branch:
pachctl create branch data@master --head staging List your branches to verify that the master branch&rsquo;s HEAD commit has changed:
pachctl list branch data staging f3506f0fab6e483e8338754081109e69 master f3506f0fab6e483e8338754081109e69 The master and staging branches now have the same HEAD commit. This means that your pipeline has data to process.
Verify that the pipeline has new jobs:
pachctl list job test@f3506f0fab6e483e8338754081109e69 ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE f3506f0fab6e483e8338754081109e69 test 32 seconds ago Less than a second 0 6 + 0 / 6 108B 24B success You should see one job that Pachyderm created for all the changes you have submitted to the staging branch, with the same ID. While the commits to the staging branch are ancestors of the current HEAD in master, they were never the actual HEAD of master themselves, so they do not get processed. This behavior works for most of the use cases because commits in Pachyderm are generally additive, so processing the HEAD commit also processes data from previous commits.
Process Specific Commits # Sometimes you want to process specific intermediary commits that are not in the HEAD of the branch. To do this, you need to set master to have these commits as HEAD. For example, if you submitted ten commits in the staging branch and you want to process the seventh, third, and most recent commits, you need to run the following commands respectively:
pachctl create branch data@master --head staging^7 pachctl create branch data@master --head staging^3 pachctl create branch data@master --head staging When you run the commands above, Pachyderm creates a job for each of the commands one after another. Therefore, when one job is completed, Pachyderm starts the next one. To verify that Pachyderm created jobs for these commands, run pachctl list job -p &lt;pipeline_name&gt; --history all.
Change the HEAD of your Branch # You can move backward to previous commits as easily as advancing to the latest commits. For example, if you want to change the final output to be the result of processing staging^1, you can roll back your HEAD commit by running the following command:
pachctl create branch data@master --head staging^1 This command starts a new job to process staging^1. The HEAD commit on your output repo will be the result of processing staging^1 instead of staging.
Copy Files from One Branch to Another # Using a staging branch allows you to defer processing. To use this functionality you need to know your input commits in advance. However, sometimes you want to be able to commit data in an ad-hoc, disorganized manner and then organize it later. Instead of pointing your master branch to a commit in a staging branch, you can copy individual files from staging to master. When you run copy file, Pachyderm only copies references to the files and does not move the actual data for the files around.
To copy files from one branch to another, complete the following steps:
Start a commit:
pachctl start commit data@master Copy files:
pachctl copy file data@staging:file1 data@master:file1 pachctl copy file data@staging:file2 data@master:file2 ... Close the commit:
pachctl finish commit data@master ℹ️ While the commit is open, you can run pachctl delete file if you want to remove something from the parent commit or pachctl put file if you want to upload something that is not in a repo yet.
Deferred Processing in Output Repositories # You can perform the same deferred processing operations with data in output repositories. To do so, rather than committing to a staging branch, configure the output_branch field in your pipeline specification.
To configure deferred processing in an output repository, complete the following steps:
In the pipeline specification, add the output_branch field with the name of the branch in which you want to accumulate your data before processing:
&#34;output_branch&#34;: &#34;staging&#34; When you want to process data, run:
pachctl create branch pipeline@master --head staging Automate Deferred Processing With Branch Triggers # Typically, repointing from one branch to another happens when a certain condition is met. For example, you might want to repoint your branch when you have a specific number of commits, or when the amount of unprocessed data reaches a certain size, or at a specific time interval, such as daily, or other. This can be automated using branch triggers. A trigger is a relationship between two branches, such as master and staging in the examples above, that says: when the head commit of staging meets a certain condition it should trigger master to update its head to that same commit. In other words it does pachctl create branch data@master --head staging automatically when the trigger condition is met.
Building on the example above, to make master automatically trigger when there&rsquo;s 1 Megabyte of new data on staging, run:
pachctl create branch data@master --trigger staging --trigger-size 1MB pachctl list branch data BRANCH HEAD TRIGGER staging 8b5f3eb8dc4346dcbd1a547f537982a6 - master 8090bfb4d4fe44158eac12199c37a591 staging on Size(1MB) When you run that command, it may or may not set the head of master. It depends on the difference between the size of the head of staging and the existing head of master, or 0 if it doesn&rsquo;t exist. Notice that in the example above staging had an existing head with less than a MB of data in it so master is still empty. If you don&rsquo;t see staging when you list branch that&rsquo;s ok, triggers can point to branches that don&rsquo;t exist yet. The head of master will update if you add a MB of new data to staging:
dd if=/dev/urandom bs=1MiB count=1 | pachctl put file data@staging:/file pachctl list branch data BRANCH HEAD TRIGGER staging 64b70e6aeda84845858c42d755023673 - master 64b70e6aeda84845858c42d755023673 staging on Size(1MB) Triggers automate deferred processing, but they don&rsquo;t prevent manually updating the head of a branch. If you ever want to trigger master even though the trigger condition hasn&rsquo;t been met you can run:
pachctl create branch data@master --head staging Notice that you don&rsquo;t need to re-specify the trigger when you call create branch to change the head. If you do want to clear the trigger delete the branch and recreate it.
There are three conditions on which you can trigger the repointing of a branch.
time, using a cron specification (&ndash;trigger-cron) size (&ndash;trigger-size) number of commits (&ndash;trigger-commits) When more than one is specified, a branch repoint will be triggered when any of the conditions is met. To guarantee that they all must be met, add &ndash;trigger-all.
To experiment further, see the full triggers example.
Embed Triggers in Pipelines # Triggers can also be specified in the pipeline spec and automatically created when the pipeline is created. For example, this is the edges pipeline from our our OpenCV demo modified to only trigger when there is a 1 Megabyte of new images:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;description&#34;: &#34;A pipeline that performs image edge detection by using the OpenCV library.&#34;, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;images&#34;, &#34;trigger&#34;: { &#34;size&#34;: &#34;1MB&#34; } } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/edges.py&#34; ], &#34;image&#34;: &#34;pachyderm/opencv&#34; } } When you create this pipeline, Pachyderm will also create a branch in the input repo that specifies the trigger and the pipeline will use that branch as its input. The name of the branch is auto-generated with the form &lt;pipeline-name&gt;-trigger-n. You can manually update the heads of these branches to trigger processing just like in the previous example.
ℹ️ Deleting or updating a pipeline will not clean up the trigger branch that it has created. In fact, the trigger branch has a lifetime that is not tied to the pipeline&rsquo;s lifetime. There is no guarantee that other pipelines are not using that trigger branch. A trigger branch can, however, be deleted manually (pachctl delete branch &lt;repo&gt;@&lt;branch&gt;).
More advanced automation # More advanced use cases might not be covered by the trigger methods above. For those, you need to create a Kubernetes application that uses Pachyderm APIs and watches the repositories for the specified condition. When the condition is met, the application switches the Pachyderm branch from staging to master.
"
17,Distributed Computing,"Distributing your computations across multiple workers is a fundamental part of any big data processing. When you build production-scale pipelines, you need to adjust the number of workers and resources that are allocated to each job to optimize throughput.
A Pachyderm worker is an identical Kubernetes pod that runs the Docker image that you specified in the pipeline spec. Your analysis code does not affect how Pachyderm distributes the workload among workers. Instead, Pachyderm spreads out the data that needs to be processed across the various workers and makes that data available for your code.
When you create a pipeline, Pachyderm spins up worker pods that continuously run in the cluster waiting for new data to be available for processing. You can change this behavior by setting &quot;autoscaling&quot; :true. Therefore, you do not need to recreate and schedule workers for every new job.
For each job, all the datums are queued up and then distributed across the available workers. When a worker finishes processing its datum, it grabs a new datum from the queue until all the datums complete processing. If a worker pod crashes, its datums are redistributed to other workers for maximum fault tolerance.
The following animation shows how distributed computing works:
In the diagram above, you have three Pachyderm worker pods that process your data. When a pod finishes processing a datum, it automatically takes another datum from the queue to process it. Datums might be different in size and, therefore, some of them might be processed faster than others.
Each datum goes through the following processing phases inside a Pachyderm worker pod:
Phase Description Downloading The Pachyderm worker pod downloads the datum contents into Pachyderm. Processing The Pachyderm worker pod runs the contents of the datum against your code. Uploading The Pachyderm worker pod uploads the results of processing into an output repository. When a datum completes a phase, the Pachyderm worker moves it to the next one while another datum from the queue takes its place in the processing sequence.
The following animation displays what happens inside a pod during the datum processing:
Parallelism # You can control the number of worker pods that Pachyderm runs in a pipeline by defining the parallelism parameter in the pipeline specification.
example # &#34;parallelism_spec&#34;: { // Exactly one of these two fields should be set &#34;constant&#34;: int Pachyderm has the following parallelism strategies that you can set in the pipeline spec:
Strategy Description constant Pachyderm starts the specified number of workers. For example, if you set &quot;constant&quot;:10, Pachyderm spreads the computation workload among ten workers. By default, Pachyderm sets parallelism to “constant&quot;: 1, which means that it spawns one worker per Kubernetes node for this pipeline.
Autoscaling # Pipelines that will not have a constant flow of data to process should use the autoscaling feature by setting &quot;autoscaling&quot;: true in the pipeline spec.
Doing so will cause the pipeline to go into standby when there is nothing for the workers to do. In standby a pipeline will have no workers and will consume no resources; it will just wait for data to come in for it to process.
When data does come in, the pipeline will exit its standby status and spin up workers to process the new data. Initially, a single worker will spin up and layout a distributed processing plan for the job. Then it will start working on the job, and if there is more work that could happen in parallel, it will spin up more workers to run in parallel, up to the limit defined by the parallelism_spec.
Multiple jobs can run in parallel and cause new workers to spin up. For example, if a job comes in with a single datum, it will cause a single worker to spin up. If another job with a single datum comes in while the first job is still running, another worker will spin up to work on the second job. Again this is bounded by the limit defined in the parallelism_spec.
One limitation of autoscaling is that it cannot dynamically scale down. Suppose a job with many datums is near completion, only one worker is still working while the others are idle. Pachyderm does not yet have a way for the idle workers to steal work, and there are a few issues that prevent us from spinning down the idle workers. Kubernetes does not have a good way to scale down a controller and specify which pods should be killed, so scaling down may kill the worker pod that is still doing work. This means another worker will have to restart that work from scratch, and the job will take longer. Additionally, we want to keep the workers around to participate in the distributed merge process at the end of the job.
ℹ️ See Also:
Glob Pattern Pipeline Specification "
18,Global Identifier," Definition # Pachyderm provides users with a simple way to follow a change throughout their DAG (i.e., traverse Provenance and Subvenance).
Pachyderm associates a commit ID to each new commit. You can quickly check this new commit by running pachctl list commit repo@branch. All resulting downstream commits and jobs in your DAG will then share that same ID (Global Identifier).
📖 The commits and jobs sharing the same ID represent a logically-related set of objects. The ID of a commit is also:
the ID of any commits created along due to provenance relationships, and the ID of any jobs triggered by the creation of those commits. This ability to track down related commits and jobs with one global identifier brought the need to introduce a new scope to our original concepts of job and commit. The nuance in the scope of a commit or a job ( &ldquo;Global&rdquo; or &ldquo;Local&rdquo;) gives the term two possible meanings.
CONCEPT SCOPE DEFINITION Commit Global A commit with global scope (global commit) represents the set of all provenance-dependent commits sharing the same ID. You can retrieve a global commit by running pachctl list commit &lt;commitID&gt;. Commit Local Repo The same term of commit, applied to the more focused scope of a repo (pachctl list commit &lt;repo&gt;@&lt;commitID&gt; or pachctl list commit &lt;repo&gt;@&lt;branch&gt;=&lt;commitID&gt;), represents &ldquo;the Git-like&rdquo; record of one commit in a single branch of a repository&rsquo;s file system. Job Global A job with global scope (global job) is the set of jobs triggered due to commits in a global commit.
You can retrieve a global job by running pachctl list job &lt;commitID&gt;. Job Local Pipeline Narrowing down the scope to a single pipeline (pachctl list job &lt;pipeline&gt;@&lt;commitID&gt;) shifts the meaning to the execution of a given job in a pipeline of your DAG. List All Global Commits And Global Jobs # You can list all global commits by running the following command:
pachctl list commit Each global commit displays how many (sub) commits it is made of.
ID SUBCOMMITS PROGRESS CREATED MODIFIED 1035715e796f45caae7a1d3ffd1f93ca 7 ▇▇▇▇▇▇▇▇ 7 seconds ago 7 seconds ago 28363be08a8f4786b6dd0d3b142edd56 6 ▇▇▇▇▇▇▇▇ 24 seconds ago 24 seconds ago e050771b5c6f4082aed48a059e1ac203 4 ▇▇▇▇▇▇▇▇ 24 seconds ago 24 seconds ago Similarly, if you run the equivalent command for global jobs:
pachctl list job you will notice that the job IDs are shared with the global commit IDs.
ID SUBJOBS PROGRESS CREATED MODIFIED 1035715e796f45caae7a1d3ffd1f93ca 2 ▇▇▇▇▇▇▇▇ 55 seconds ago 55 seconds ago 28363be08a8f4786b6dd0d3b142edd56 1 ▇▇▇▇▇▇▇▇ About a minute ago About a minute ago e050771b5c6f4082aed48a059e1ac203 1 ▇▇▇▇▇▇▇▇ About a minute ago About a minute ago For example, in this example, 7 commits and 2 jobs are involved in the changes occured in the global commit ID 1035715e796f45caae7a1d3ffd1f93ca.
ℹ️ The progress bar is equally divided to the number of steps, or pipelines, you have in your DAG. In the example above,1035715e796f45caae7a1d3ffd1f93ca is two steps. If one of the sub-jobs fails, you will see the progress bar turn red for that pipeline step. To troubleshoot, look into that particular pipeline execution.
List All Commits And Jobs With A Global ID # To list all (sub) commits involved in a global commit:
pachctl list commit 1035715e796f45caae7a1d3ffd1f93ca REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION images master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 238.3KiB USER edges.spec master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 244B ALIAS montage.spec master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 405B ALIAS montage.meta master 1035715e796f45caae7a1d3ffd1f93ca 4 minutes ago 1.656MiB AUTO edges master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 133.6KiB AUTO edges.meta master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 373.9KiB AUTO montage master 1035715e796f45caae7a1d3ffd1f93ca 4 minutes ago 1.292MiB AUTO Similarly, change commit in job to list all (sub) jobs linked to your global job ID.
pachctl list job 1035715e796f45caae7a1d3ffd1f93ca ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE 1035715e796f45caae7a1d3ffd1f93ca montage 5 minutes ago 4 seconds 0 1 + 0 / 1 79.49KiB 381.1KiB success 1035715e796f45caae7a1d3ffd1f93ca edges 5 minutes ago 2 seconds 0 1 + 0 / 1 57.27KiB 22.22KiB success For each pipeline execution (sub job) within this global job, Pachyderm shows the time since each sub job started and its duration, the number of datums in the PROGRESS section, and other information. The format of the progress column is DATUMS PROCESSED + DATUMS SKIPPED / TOTAL DATUMS.
For more information, see Datum Processing States.
ℹ️ The global commit and global job above are the result of a pachctl put file images@master -i images.txt in the images repo of the open cv example.
The following diagram illustrates the global commit and its various components: Let&rsquo;s take a look at the origin of each commit.
ℹ️ Check the list of all commit origins in the Commit page.
Inspect the commit ID 1035715e796f45caae7a1d3ffd1f93ca in the images repo, the repo in which our change (put file) has originated:
pachctl inspect commit images@1035715e796f45caae7a1d3ffd1f93ca --raw Note that this original commit is of USER origin (i.e., the result of a user change).
&#34;origin&#34;: { &#34;kind&#34;: &#34;USER&#34; }, Inspect the following commit 1035715e796f45caae7a1d3ffd1f93ca produced in the output repos of the edges pipeline:
pachctl inspect commit edges@1035715e796f45caae7a1d3ffd1f93ca --raw { &#34;commit&#34;: { &#34;branch&#34;: { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; }, &#34;id&#34;: &#34;1035715e796f45caae7a1d3ffd1f93ca&#34; }, &#34;origin&#34;: { &#34;kind&#34;: &#34;AUTO&#34; }, &#34;parent_commit&#34;: { &#34;branch&#34;: { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; }, &#34;id&#34;: &#34;28363be08a8f4786b6dd0d3b142edd56&#34; }, &#34;started&#34;: &#34;2021-07-07T13:52:34.140584032Z&#34;, &#34;finished&#34;: &#34;2021-07-07T13:52:36.507625440Z&#34;, &#34;direct_provenance&#34;: [ { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;spec&#34; }, &#34;name&#34;: &#34;master&#34; }, { &#34;repo&#34;: { &#34;name&#34;: &#34;images&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; } ], &#34;details&#34;: { &#34;size_bytes&#34;: &#34;22754&#34; } } Note that the origin of the commit is of kind AUTO as it has been trigerred by the arrival of a commit in the upstream repo images.
&#34;origin&#34;: { &#34;kind&#34;: &#34;AUTO&#34; }, The same origin (AUTO ) applies to the commits sharing that same ID in the montage output repo as well as edges.meta and montage.meta system repos.
📖 Check the list of all types of repos in the Repo page.
Besides the USER and AUTO commits, notice a set of ALIAS commits in edges.spec and montage.spec: pachctl inspect commit edges.spec@336f02bdbbbb446e91ba27d2d2b516c6 --raw The version of each pipeline within their respective .spec repos are neither the result of a user change, nor of an automatic change. They have, however, contributed to the creation of the previous AUTO commits. To make sure that we have a complete view of all the data and pipeline versions involved in all the commits resulting from the initial put file, their version is kept as ALIAS commits under the same global ID.
For a full view of GlobalID in action, take a look at our GlobalID illustration.
Track Provenance Downstream # Pachyderm provides the wait commit &lt;commitID&gt; command that enables you to track your commits downstream as they are produced.
Unlike the list commit &lt;commitID&gt;, each line is printed as soon as a new (sub) commit of your global commit finishes.
Change commit in job to list the jobs related to your global job as they finish processing a commit.
Squash And Delete Commit # See squash commit and delete commit in the Delete a Commit / Delete Data page of the How-Tos section of this Documentation.
"
19,Pipeline Concepts,"Pachyderm Pipeline System (PPS) is the computational component of the Pachyderm platform that enables you to perform various transformations on your data.
"
20,Datum," ℹ️ TLDR: Datums define what input data is seen by your code. It can be all data at once, each directory independently, individual files one by one, or combined data from multiple inputs together.
Definition # A datum is the smallest indivisible unit of computation within a job. A job can have one, many or no datums. Each datum is processed independently with a single execution of the user code on one of the pipeline worker pods. The files output by all of the datums are then combined together to create the final output commit.
Zero-Datum Jobs # A &ldquo;zero-datum&rdquo; job is a job that is successfully executed but has no matching files to transform with the provided user code.
Data distribution # Think of datums as a way to divide your input data and distribute processing workloads. They are instrumental in optimizing your pipeline performance.
You define how your data is spread among workers by specifying pipeline inputs for your pipeline in its pipeline specification file.
Based on this specification file, the data in the input of your pipeline is turned into datums, each of which can contain 1-to-many files. Pachyderm provides a wide variety of ways to define the granularity of each datum.
For example, you can configure a whole branch of an input repository to be one datum, each top-level filesystem object of a given branch to be a separate datum, specific paths on a given branch can be datums, etc&hellip; You can also create a combination of the above by aggregating multiple input.
Pipeline Inputs # This section details the tools at your disposal to &ldquo;break down&rdquo; your data and fit your specific use case.
PFS Input and Glob Pattern # The most primitive input of a pipeline is a PFS input, defined, at a minimum, by:
a repo containing the data you want your pipeline to consider a branch to watch for commits and a glob pattern to determine how the input data is partitioned. A pipeline input can have one or multiple PFS inputs. In the latter case, Pachyderm provides a variety of options to aggregate several PFS inputs together.
"
21,Cross & Union Inputs,"Pachyderm enables you to combine multiple PFS inputs by using the union and cross operators in the pipeline specification.
You can think of union as a disjoint union binary operator and cross as a cartesian product binary operator.
This section describes how to use cross and union in your pipelines and how you can optimize your code when you work with them.
Union Input # The union input combines each of the datums in the input repos as one set of datums. The number of datums that are processed is the sum of all the datums in each repo.
For example, you have two input repos, A and B. Each of these repositories contain three files with the following names.
Repository A has the following structure:
A ├── 1.txt ├── 2.txt └── 3.txt Repository B has the following structure:
B ├── 4.txt ├── 5.txt └── 6.txt If you want your pipeline to process each file independently as a separate datum, use a glob pattern of /*. Each glob is applied to each input independently. The input section in the pipeline spec might have the following structure:
&#34;input&#34;: { &#34;union&#34;: [ { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;A&#34; } }, { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;B&#34; } } ] } In this example, each Pachyderm repository has those three files in the root directory, so three datums from each input. Therefore, the union of A and B has six datums in total. Your pipeline processes the following datums without any specific order:
/pfs/A/1.txt /pfs/A/2.txt /pfs/A/3.txt /pfs/B/4.txt /pfs/B/5.txt /pfs/B/6.txt ℹ️ Each datum in a pipeline is processed independently by a single execution of your code. In this example, your code runs six times, and each datum is available to it one at a time. For example, your code processes pfs/A/1.txt in one of the runs and pfs/B/5.txt in a different run, and so on. In a union, two or more datums are never available to your code at the same time. You can simplify your union code by using the name property as described below.
Simplifying the Union Pipelines Code # In the example above, your code needs to read into the pfs/A or pfs/B directory because only one of them is present in any given datum. To simplify your code, you can add the name field to the pfs object and give the same name to each of the input repos. For example, you can add, the name field with the value C to the input repositories A and B:
&#34;input&#34;: { &#34;union&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: &#34;C&#34;, &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;A&#34; } }, { &#34;pfs&#34;: { &#34;name&#34;: &#34;C&#34;, &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;B&#34; } } ] } Then, in the pipeline, all datums appear in the same directory.
/pfs/C/1.txt # from A /pfs/C/2.txt # from A /pfs/C/3.txt # from A /pfs/C/4.txt # from B /pfs/C/5.txt # from B /pfs/C/6.txt # from B Cross Input # In a cross input, Pachyderm exposes every combination of datums, or a cross-product, from each of your input repositories to your code in a single run.
In other words, a cross input pairs every datum in one repository with each datum in another, creating sets of datums. Your transformation code is provided one of these sets at the time to process.
For example, you have repositories A and B with three datums, each with the following structure:
ℹ️ For this example, the glob pattern is set to /*.
Repository A has three files at the top level:
A ├── 1.txt ├── 2.txt └── 3.txt Repository B has three files at the top level:
B ├── 4.txt ├── 5.txt └── 6.txt Because you have three datums in each repo, Pachyderm exposes a total of nine combinations of datums to your code.
💡 In cross pipelines, both pfs/A and pfs/B directories are visible during each code run.
Run 1: /pfs/A/1.txt /pfs/B/4.txt Run 2: /pfs/A/1.txt /pfs/B/5.txt ... Run 9: /pfs/A/3.txt /pfs/B/6.txt ℹ️ In cross inputs, if you use the name field, your two inputs cannot have the same name. This could cause file system collisions.
ℹ️ See Also:
Cross Input in a pipeline specification Union Input in a pipeline specification Distributed hyperparameter tuning example "
22,Datum Processing,"A datum is a Pachyderm abstraction that helps in optimizing pipeline processing. Datums exist only as a pipeline processing property and are not filesystem objects. You can never copy a datum. They are a representation of a unit of work.
Job Processing and Datums # When new data comes in (in the form of commit(s) in its input repo(s)), a Pachyderm pipeline automatically starts a new job. Each Pachyderm job consists of the following stages:
1: Creation of input datums # In this stage, Pachyderm creates datums based on the input data according to the pipeline input(s) set in the pipeline specification file.
2: Transformation # The pipeline uses your code to process the datums.
3: Creation of output files # Your code writes output file(s) in the pfs/out output directory that Pachyderm creates automatically for each pipeline&rsquo;s job.
4: Final commit in the pipeline&rsquo;s output repo # ℹ️ The output produced by a pipeline&rsquo;s job is written to an output repo of the same name (i.e., output repo name = pipeline name).
The content of all /pfs/out is combined in a commit to the pipeline&rsquo;s output repo. This generally means unioning all the files together.
💡 The Single Datum Provenance Rule.
If two outputted files have the same name (i.e., two datums wrote to the same output file, creating a conflict), then an error is raised, resulting in your pipeline failure.
Avoid this anti-pattern from the start by having each datum write in separate files. Pachyderm provides an environment variable PACH_DATUM_ID that stores the datum ID. This variable is available in the pipeline&rsquo;s user code. To ensure that each datum outputs distinct file paths, you can use this variable in the name of your outputted files.
5. Next: Add a Reduce (Merge) pipeline # If you need files from different datums merged into single files in a particular way:
add a pipeline that groups the files from the previous output repo into single datums using the appropriate glob pattern. then merge them as intended using your code. The example that follows illustrates this two steps approach.
Example: Two Steps Map/Reduce Pattern and Single Datum Provenance Rule # In this example, we highlight a two pipelines pattern where a first pipeline&rsquo;s glob pattern splits an incoming commit into three datums (called &ldquo;Datum1&rdquo; (Red), &ldquo;Datum2&rdquo; (Blue), &ldquo;Datum3&rdquo; (Purple)), each producing two files each. The files can then be further appended or overwritten with other files to create the final result. Below, a second pipeline appends the content of all files in each directory into one final document.
ℹ️ In the example, the files are named after the datum itself. Depending on your use case, there might be more logical ways to name the files produced by a datum. However, in any case, make sure that this name is unique for each datum to avoid duplicate files with the same file path. Each file is put in specific directories. This directory structure has been thought to facilitate the aggregation of the content in the following pipeline. Think about your directory structure so that the next glob pattern will aggregate your data as needed. Let&rsquo;s now create a new commit and overwrite a file in datum 2, Pachyderm detects three datums. However, because datum 1 and datum 3 are unchanged, it skips processing these datums. Pachyderm detects that something has changed in datum 2. It is unaware of any details of the change; therefore, it processes the whole datum 2(') (here in yellow) and outputs 3 files. Then, the following pipeline aggregates these data to create the final result.
Incrementality # In Pachyderm, unchanged datums are never re-processed. For example, if you have multiple datums, and only one datum was modified; Pachyderm processes that datum only and skips processing other datums. This incremental behavior ensures efficient resource utilization.
Overwriting files # By default, Pachyderm overwrites new data. For example, if you have a file foo in the repository A and add the same file foo to that repository again by using the pachctl put file command, Pachyderm will overwrite that file in the repo.
For more information, and learn how to change this behavior, see File.
Note: Data persistence between datums # Pachyderm only controls and wipes the /pfs directories between datums. If scratch/temp space is used during execution, the user needs to be careful to clean that up. Not cleaning temporary directories may cause unexpected bugs where one datum accesses temporary files that were previously used by another datum!
"
23,Datum Processing States,"When a pipeline runs, it processes your datums. Some of them get processed successfully and some might be skipped or even fail. Generally, processed datums fall into either successful or failure state category.
The following table describes the processing states of datums that can occur in Pachyderm:
Successful States
State Description Success The datum has been successfully processed in this job. Skipped The datum has been successfully processed in a previous job, has not changed since then, and therefore, it was skipped in the current job. Failure States
State Description Failed The datum failed to be processed. Any failed datum in a job fails the whole job. Recovered The datum failed, but was recovered by the user&rsquo;s error handling code. Although the datum is marked as recovered, Pachyderm does not process it in the downstream pipelines. A recovered datum does not fail the whole job. Just like failed datums, recovered datums are retried on the next run of the pipeline. You can view the information about datum processing states in the output of the pachctl list job &lt;jobID&gt; command:
ℹ️ Datums that failed are still included in the total, but not shown in the progress indicator.
"
24,Glob Pattern,"Defining how your data is spread among workers is one of the most important aspects of distributed computation.
Pachyderm uses glob patterns to provide flexibility to define data distribution.
ℹ️ Pachyderm&rsquo;s concept of glob patterns is similar to Unix glob patterns. For example, the ls *.md command matches all files with the .md file extension.
The glob pattern applies to all of the directories/files in the branch specified by the pfs section of the pipeline specification (referred to as PFS inputs). The directories/files that match are the datums that will be processed by the worker(s) that run your pipeline code.
⚠️ You must configure a glob pattern for each PFS input of a pipeline specification.
We have listed some commonly used glob patterns. We will later illustrate their use in an example:
Glob Pattern Datum created / Pachyderm denotes the whole repository as a single datum and sends all input data to a single worker node to be processed together. /* Pachyderm defines each top-level files / directories in the input repo, as a separate datum. For example, if you have a repository with ten files and no directory structure, Pachyderm identifies each file as a single datum and processes them independently. /*/* Pachyderm processes each file / directory in each subdirectories as a separate datum. /** Pachyderm processes each file in all directories and subdirectories as a separate datum. Glob patterns also let you take only a particular subset of the files / directories, a specific branch&hellip; as an input instead of the whole repo. We will elaborate on this more in the following example.
If you have more than one input repo in your pipeline, or want to consider more than one branch in a repo, or any combination of the above, you can define a different glob pattern for each PFS input. You can additionally combine the resulting datums by using the cross, union, join, or group operator to create the final datums that your code processes. For more information, see Cross and Union, Join, Group.
Example of Defining Datums # Let&rsquo;s consider an input repo with the following structure where each top-level directory represents a US state with a json file for each city in that state:
/California /San-Francisco.json /Los-Angeles.json ... /Colorado /Denver.json /Boulder.json ... /Washington /Seattle.json /Vancouver.json Now let&rsquo;s consider what the following glob patterns would match respectively:
Glob Pattern Corresponding match Example / This pattern matches /, the root directory itself, meaning all the data would be one large datum. All changes in any of the files and directories trigger Pachyderm to process the whole repository contents as a single datum. If you add a new file Sacramento.json to the California/ directory, Pachyderm processes all changed files and directories in the repo as a single datum. /* This pattern matches everything under the root directory. It defines one datum per state, which means that all the cities for a given state are processed together by a single worker, but each state is processed independently. If you add a new file Sacramento.json to the California/ directory, Pachyderm processes the California/ datum only. /Colorado/* This pattern matches files only under the /Colorado directory. It defines one datum per city. If you add a new file Alamosa.json to the Colorado/ directory and Sacramento.json to the California/ directory, Pachyderm processes the Alamosa.json datum only. /C* This pattern matches all files under the root directory that start with the character C. In the example, the California and Colorado directories will each define a datum. /*/* This pattern matches everything that&rsquo;s two levels deep relative to the root. If we add County sub-directories to our states, /California/LosAngeles/LosAngeles.json, /California/LosAngeles/Malibu.json and /California/SanDiego/LaMosa.json for example, then this pattern would match each of those 3 .json files individually. /** The match is applied at all levels of your directory structure. This is a recursive glob pattern. Let&rsquo;s look at the additional example below for more detail. Example # The case of the /** glob pattern
Say we have the following repo structure:
/nope1.txt /test1.txt /foo-1 /nope2.txt /test2.txt /foo-2 /foo-2_1 /nope3.txt /test3.txt /anothertest.txt &hellip;and apply the following pattern to our input repo:
&#34;glob&#34;: &#34;/**test*.txt&#34; We are recursively matching all .txt files containing test starting from our input repo&rsquo;s root directory. In this case, the resulting datums will be:
- /test1.txt - /foo-1/test2.txt - /foo-2/foo-2_1/test3.txt - /foo-2/foo-2_1/anothertest.txt 📖 To understand how Pachyderm scales, read Distributed Computing. To learn about Datums&rsquo; incremental processing, read our Datum Processing section. Test a Glob pattern # You can use the pachctl glob file command to preview which filesystem objects a pipeline defines as datums. This command helps you to test various glob patterns before you use them in a pipeline.
If you set the glob property to /, Pachyderm detects all top-level filesystem objects in the train repository as one datum:
Example # pachctl glob file train@master:/ System Response:
NAME TYPE SIZE / dir 15.11KiB If you set the glob property to /*, Pachyderm detects each top-level filesystem object in the train repository as a separate datum:
Example # pachctl glob file train@master:/* System Response:
NAME TYPE SIZE /IssueSummarization.py file 1.224KiB /requirements.txt file 74B /seq2seq_utils.py file 13.81KiB Test your Datums # The granularity of your datums defines how your data will be distributed across the available workers allocated to a job. Pachyderm allows you to check those datums:
for a pipeline currently being developed for a past job Testing your glob pattern before creating a pipeline # You can use the pachctl list datum -f &lt;my_pipeline_spec.json&gt; command to preview the datums defined by a pipeline given its specification file.
ℹ️ The pipeline does not need to have been created for the command to return the list of datums. This &ldquo;dry run&rdquo; helps you adjust your glob pattern when creating your pipeline.
Example # pachctl list datum -f edges.json System Response:
ID FILES STATUS TIME - images@b8687e9720f04b7ab53ae8c64541003b:/46Q8nDz.jpg - - - images@b8687e9720f04b7ab53ae8c64541003b:/8MN9Kg0.jpg - - - images@b8687e9720f04b7ab53ae8c64541003b:/Togu2RY.jpg - - - images@b8687e9720f04b7ab53ae8c64541003b:/g2QnNqa.jpg - - - images@b8687e9720f04b7ab53ae8c64541003b:/w7RVTsv.jpg - - Running list datum on a past job # You can use the pachctl list datum &lt;pipeline&gt;@&lt;job_ID&gt; command to check the datums processed by a given job.
Example # pachctl list datum edges@b8687e9720f04b7ab53ae8c64541003b System Response:
ID FILES STATUS TIME a4149cd1907145f982e0eb49c50af3f1d4d8fecaa8647d62f2d9d93e30578df8 images@b8687e9720f04b7ab53ae8c64541003b:/w7RVTsv.jpg success Less than a second e2b4628dd88b179051ba0576e06fac12ae2e4d16165296212d0e98de501d17df images@b8687e9720f04b7ab53ae8c64541003b:/Togu2RY.jpg success Less than a second 353b6d2a5ac78f56facc7979e190affbb8f75c6f74da84b758216a8df77db473 images@7856142de8714c11b004610ea7af2378:/8MN9Kg0.jpg skipped Less than a second b751702850acad5502dc51c3e7e7a1ac10ba2199fdb839989cd0c5430ee10b84 images@fc9a12ee149a4499a1a7da0a31971b37:/46Q8nDz.jpg skipped Less than a second de9e3703322eff2ab90e89ff01a18c448af9870f17e78438c5b0f56588af9c44 images@7856142de8714c11b004610ea7af2378:/g2QnNqa.jpg skipped Less than a second In this example, you can see that the job b8687e9720f04b7ab53ae8c64541003b only processed 2 datums from the images input repo. The rest was skipped as it had been processed by previous jobs already. Notice that the ID of the datums is now showing.
📖 Stats and Datum Metadata
Running list datum on a given job execution of a pipeline allows you to additionally display the STATUS (running, failed, success) and TIME of each datum.
You might want to follow up with inspect datum pipeline@job_number datum ID to detail the files that a specific datum includes.
pachctl inspect datum edges@b8687e9720f04b7ab53ae8c64541003b a4149cd1907145f982e0eb49c50af3f1d4d8fecaa8647d62f2d9d93e30578df8 System Response:
ID	a4149cd1907145f982e0eb49c50af3f1d4d8fecaa8647d62f2d9d93e30578df8 Job ID	b8687e9720f04b7ab53ae8c64541003b State	SUCCESS Data Downloaded	606.3KiB Data Uploaded	26.96KiB Total Time	582.000134ms Download Time	40.062075ms Process Time	535.387088ms Upload Time	6.550971ms PFS State: REPO COMMIT PATH edges.meta b8687e9720f04b7ab53ae8c64541003b /pfs/a4149cd1907145f982e0eb49c50af3f1d4d8fecaa8647d62f2d9d93e30578df8 Inputs: REPO COMMIT PATH images b8687e9720f04b7ab53ae8c64541003b /w7RVTsv.jpg Add --raw for a full JSON version of the datum&rsquo;s metadata.
"
25,Group Input,"A group is a special type of pipeline input that enables you to aggregate files that reside in one or separate Pachyderm repositories and match a particular naming pattern. The group operator must be used in combination with a glob pattern that reflects a specific naming convention.
By analogy, a Pachyderm group is similar to a database group-by, but it matches on file paths only, not the content of the files.
Unlike the join datum that will always contain a single match (even partial) from each input repo, a group creates one datum for each set of matching files accross its input repos. You can use group to aggregate data that is not adequately captured by your directory structure or to control the granularity of your datums through file name-matching.
When you configure a group input, you must specify a glob pattern that includes a capture group. The capture group defines the specific string in the file path that is used to match files in other grouped repos. Capture groups work analogously to the regex capture group. You define the capture group inside parenthesis. Capture groups are numbered from left to right and can also be nested within each other. Numbering for nested capture groups is based on their opening parenthesis.
Below you can find a few examples of applying a glob pattern with a capture group to a file path. For example, if you have the following file path:
/foo/bar-123/ABC.txt The following glob patterns in a joint input create the following capture groups:
Regular expression Capture groups /(*) foo /*/bar-(*) 123 /(*)/*/(??)*.txt Capture group 1: foo, capture group 2: AB. /*/(bar-(123))/* Capture group 1: bar-123, capture group 2: 123. Also, groups require you to specify a replacement group in the group_by parameter to define which capture groups you want to try to match.
For example, $1 indicates that you want Pachyderm to match based on capture group 1. Similarly, $2 matches the capture group 2. $1$2 means that it must match both capture groups 1 and 2.
If Pachyderm does not find any matching files, you get a zero-datum job.
You can test your glob pattern and capture groups by using the pachctl list datum -f &lt;your_pipeline_spec.json&gt; command as described in List Datum.
💡 The content of the capture group defined in the group_by parameter is available to your pipeline&rsquo;s code in an environment variable: PACH_DATUM_&lt;input.name&gt;_GROUP_BY.
Example # For example, a repository labresults contains the lab results of patients. The files at the root of your repository have the following naming convention. You want to group your lab results by patientID.
labresults repo:
├── LIPID-patientID1-labID1.txt (1) ├── LIPID-patientID2-labID1.txt (2) ├── LIPID-patientID1-labID2.txt (3) ├── LIPID-patientID3-labID3.txt (4) ├── LIPID-patientID1-labID3.txt (5) ├── LIPID-patientID2-labID3.txt (6) Pachyderm runs your code on the set of files that match the glob pattern and capture groups.
The following example shows how you can use group to aggregate all the lab results of each patient.
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;group&#34; }, &#34;input&#34;: { &#34;group&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;labresults&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*-(*)-lab*.txt&#34;, &#34;group_by&#34;: &#34;$1&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;bash&#34; ], &#34;stdin&#34;: [ &#34;wc&#34; ,&#34;-l&#34; ,&#34;/pfs/labresults/*&#34; ] } } } The glob pattern for the labresults repository, /*-(*)-lab*.txt, selects all files with a patientID match in the root directory.
The pipeline will process 3 datums for this job.
all files containing patientID1 (1, 3, 5) are grouped in one datum, a second datum will be made of (2, 6) for patientID2 and a third with (4) for patientID3 The pachctl list datum -f &lt;your_pipeline_spec.json&gt; command is a useful tool to check your datums:
ID FILES STATUS TIME - labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID1-labID1.txt, labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID1-labID3.txt, labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID1-labID2.txt - - - labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID2-labID1.txt, labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID2-labID3.txt - - - labresults@722665ed49474db0aab5cbe4d8a20ff8:/LIPID-patientID3-labID3.txt To experiment further, see the full group example.
"
26,Join Input,"A join is a special type of pipeline input that enables you to combine files that reside in separate Pachyderm repositories and match a particular naming pattern. The join operator must be used in combination with a glob pattern that reflects a specific naming convention. Note that in Pachyderm, matches are made on file paths only, not the files&rsquo; content.
Pachyderm supports two types of joins:
A default join setting, similar to a database equi-join or inner-join operation. Unlike the cross input, which creates datums from every combination of files in each input repository, inner joins only create datums where there is a match. You can use inner joins to combine data from different Pachyderm repositories and ensure that only specific files from each repo are processed together. If Pachyderm does not find any matching files, you get a zero-datum job. Pachyderm also supports a join close to a database outer-join, allowing you to create datums for all files in a repo, even if there is no match. The outer-join behavior can be set on any repository in your join. When you configure a join input (inner or outer), you must specify a glob pattern that includes a capture group. The capture group defines the specific string in the file path that is used to match files in other joined repos. Capture groups work analogously to the regex capture group. You define the capture group inside parenthesis. Capture groups are numbered from left to right and can also be nested within each other. Numbering for nested capture groups is based on their opening parenthesis.
Below you can find a few examples of applying a glob pattern with a capture group to a file path. For example, if you have the following file path:
/foo/bar-123/ABC.txt The following glob patterns in a joint input create the following capture groups:
Regular expression Capture groups /(*) foo /*/bar-(*) 123 /(*)/*/(??)*.txt Capture group 1: foo, capture group 2: AB. /*/(bar-(123))/* Capture group 1: bar-123, capture group 2: 123. Also, joins require you to specify a replacement group in the join_on parameter to define which capture groups you want to try to match.
For example, $1 indicates that you want Pachyderm to match based on capture group 1. Similarly, $2 matches the capture group 2. $1$2 means that it must match both capture groups 1 and 2.
See the full join input configuration in the pipeline specification.
You can test your glob pattern and capture groups by using the pachctl list datum -f &lt;your_pipeline_spec.json&gt; command as described in List Datum.
💡 The content of the capture group defined in the join_on parameter is available to your pipeline&rsquo;s code in an environment variable: PACH_DATUM_&lt;input.name&gt;_JOIN_ON.
Inner Join # Per default, a join input has an inner-join behavior.
Inner Join Example # For example, you have two repositories. One with sensor readings and the other with parameters. The repositories have the following structures:
readings repo:
├── ID1234 ├── file1.txt ├── file2.txt ├── file3.txt ├── file4.txt ├── file5.txt parameters repo:
├── file1.txt ├── file2.txt ├── file3.txt ├── file4.txt ├── file5.txt ├── file6.txt ├── file7.txt ├── file8.txt Pachyderm runs your code only on the pairs of files that match the glob pattern and capture groups.
The following example shows how you can use joins to group matching IDs:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;joins&#34; }, &#34;input&#34;: { &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;readings&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;parameters&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/joins.py&#34;], &#34;image&#34;: &#34;joins-example&#34; } } The glob pattern for the readings repository, /*/(*).txt, indicates all matching files in the ID sub-directory. In the parameters repository, the glob pattern /(*).txt selects all the matching files in the root directory. All files with indices from 1 to 5 match. The files with indices from 6 to 8 do not match. Therefore, you only get five datums for this job.
To experiment further, see the full joins example.
Outer Join # Pachyderm also supports outer joins. Outer joins include everything an inner join does plus the files that didn&rsquo;t match anything. Inputs can be set to outer semantics independently. So while there isn&rsquo;t an explicit notion of &ldquo;left&rdquo; or &ldquo;right&rdquo; outer joins, you can still get those semantics, and even extend them to multiway joins.
Outer Join Example # Building off the example above, notice that there are 3 files in the parameters repo, file6.txt, file7.txt and file8.txt, which don&rsquo;t match any files in the readings repo. In an inner join, those files are omitted. If you still want to see the files without a match, you can use an outer join. The change to the pipeline spec is simple:
{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;joins&#34; }, &#34;input&#34;: { &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;readings&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;parameters&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34;, &#34;outer_join&#34;: true } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/joins.py&#34;], &#34;image&#34;: &#34;joins-example&#34; } } Your code will see the joined pairs that it saw before. In addition to those five datums, your code will also see three new ones: one for each of the files in parameters that didn&rsquo;t match. Note that this means that your code needs to handle (not crash) the case where input files are missing from /pfs/readings.
To experiment further, see the full join example.
"
27,Metadata," Datum Statistics # Pachyderm stores information about each datum that a pipeline processes, including timing information, size information, and /pfs snapshots. You can view these statistics by running the pachctl inspect datum command (or its language client equivalents).
In particular, Pachyderm provides the following information for each datum processed by your pipelines:
The amount of data that was uploaded and downloaded The time spend uploading and downloading data The total time spend processing Success/failure information, including any error encountered for failed datums The directory structure of input data that was seen by the job. Use the pachctl list datum &lt;pipeline&gt;@&lt;job ID&gt; to retrieve the list of datums processed by a given job, and pick the datum ID you want to inspect. That information can be useful when troubleshooting a failed job.
Meta Repo # Once a pipeline has finished a job, you can access additional execution metadata about the datums processed in the associated meta system repo. Note that all the inspect datum information above is stored in this meta repo, along with a couple more. For example, you can find the reason in meta/&lt;datumID&gt;/meta: the error message when the datum failed.
See the detail of the meta repo structure below.
📖 A meta repo contains 2 directories:
/meta/: The meta directory holds datums&rsquo; statistics /pfs: The pfs directory holds the input data of datums, and their resulting output data pachctl list file edges.meta@master System response:
NAME TAG TYPE SIZE /meta/ dir 1.956KiB /pfs/ dir 371.9KiB Meta directory # The meta directory holds each datum&rsquo;s JSON metadata, and can be accessed using a get file:
Example # pachctl get file edges.meta@master:/meta/002f991aa9db9f0c44a92a30dff8ab22e788f86cc851bec80d5a74e05ad12868/meta | jq System response:
{ &#34;job&#34;: { &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;id&#34;: &#34;efca9595bdde4c0ba46a444a5877fdfe&#34; }, &#34;inputs&#34;: [ { &#34;fileInfo&#34;: { ... } ], &#34;hash&#34;: &#34;28e6675faba53383ac84b899d853bb0781c6b13a90686758ce5b3644af28cb62f763&#34;, &#34;stats&#34;: { &#34;downloadTime&#34;: &#34;0.103591200s&#34;, &#34;processTime&#34;: &#34;0.374824700s&#34;, &#34;uploadTime&#34;: &#34;0.001807800s&#34;, &#34;downloadBytes&#34;: &#34;80588&#34;, &#34;uploadBytes&#34;: &#34;38046&#34; }, &#34;index&#34;: &#34;1&#34; } Usepachctl list file edges.meta@master:/meta/ to list the files in the meta directory.
Pfs Directory # The pfs directory has both the input files of datums, and the resulting output files that were committed to the output repo:
Example # pachctl list file montage.meta@master:/pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/ System response:
NAME TAG TYPE SIZE /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/edges/ dir 133.6KiB /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/images/ dir 238.3KiB /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/out/ dir 1.292MiB Use pachctl list file montage.meta@master:/pfs/ to list the files in the pfs directory.
"
28,Job," ⚠️ Note that Pachyderm uses two different scopes when referring to a job.
A &ldquo;global&rdquo; scope tracking down your entire provenance chain. Refer to GlobalID for more details. And a &ldquo;local&rdquo; scope in which a job (also referred to as sub job) is an execution of one particular pipeline. The following page details the latter.
Definition # A Pachyderm job is an execution of a pipeline that triggers when new data is detected in an input repository.
ℹ️ When a commit is made to the input repo of a pipeline, jobs are created for all of the downstream pipelines of a DAG. Those jobs are not running yet; each one is waiting until the prior pipeline(s) that it depends on in your DAG produces their output, which then becomes the input for the waiting pipeline.
Each job runs your code against the current commit in a &lt;repo&gt;@&lt;branch&gt; and then submits the results to the output repository of the pipeline as a single output commit. A pipeline triggers a new job every time you submit new changes, a commit, into your input source.
Each job has an alphanumeric identifier (ID) that you can reference in the &lt;pipeline&gt;@&lt;jobID&gt; format.
You can obtain information about all jobs sharing the same ID (Global ID) by running list jobs &lt;jobID&gt; or restrict to a particular pipeline list jobs -p &lt;pipeline&gt;, or inspect jobs &lt;pipeline&gt;@&lt;jobID&gt; --raw.
Job Statuses # Find a list of all possible job stages below and a state diagram detailing how a job transitions from one state to another.
Stage Description CREATED An input commit exists, but the job has not been started by a worker yet. STARTING The worker has allocated resources for the job (that is, the job counts towards parallelism), but it is still waiting on the inputs to be ready. UNRUNNABLE The job could not be run, because one or more of its inputs is the result of a failed or unrunnable job. As a simple example, say that pipelines Y and Z both depend on the output from pipeline X. If pipeline X fails, both pipeline Y and Z will pass from STARTING to UNRUNNABLE to signify that they had to be cancelled because of upstream failures. RUNNING The worker is processing datums. EGRESS The worker has completed all the datums and is uploading the output to the egress endpoint. FINISHING After all of the datum processing and egress (if any) is done, the job transitions to a finishing state where all of the post-processing tasks such as compaction are performed. FAILURE The worker encountered too many errors when processing a datum. KILLED The job timed out, or a user called StopJob SUCCESS None of the bad stuff happened. Below, the state transition diagram of a job:
List Jobs # They are various ways to list jobs in Pachyderm, depending on the expected outcome:
The pachctl list jobs command returns the list of all global jobs.
The pachctl list jobs &lt;jobID&gt; command returns the list of all jobs sharing the same &lt;jobID&gt;.
Note that you can also track your jobs downstream as they complete by running pachctl wait jobs &lt;jobID&gt;.
The pachctl list jobs -p &lt;pipeline&gt; command returns the list of all the jobs run in a given pipeline.
Example # pachctl list jobs -p edges System Response:
ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE fd9454d06d8e4fa38a75c8cd20b39538 edges 20 hours ago 5 seconds 0 2 + 1 / 3 181.1KiB 111.4KiB success 5a78358d4b53494cbba4550428f2fe98 edges 20 hours ago 2 seconds 0 1 + 0 / 1 57.27KiB 22.22KiB success 7dcd77a2f7f34ff384a6096d1139e922 edges 20 hours ago Less than a second 0 0 + 0 / 0 0B 0B success For each (sub) job, Pachyderm shows the time the pipeline started, its duration, data downloaded and uploaded, the STATE of the pipeline execution, and the number of datums in the PROGRESS section. The format of the progress column is DATUMS PROCESSED + DATUMS SKIPPED / TOTAL DATUMS.
See Datum Processing States for details on Datum states.
Inspect Job # The pachctl inspect jobs &lt;pipeline&gt;@&lt;jobID&gt; command enables you to view detailed information about a specific (sub)job in a given pipeline (state, number of datums processed/failed/skipped, data downloaded, uploaded, process time, image:tag used to transform your data, etc&hellip;). Along with checking the logs, it is especially useful when troubleshooting a failed job.
Example # Add a --raw flag to output a detailed JSON version of the job.
pachctl inspect jobs edges@fd9454d06d8e4fa38a75c8cd20b39538 --raw System Response:
{ &#34;job&#34;: { &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;id&#34;: &#34;fd9454d06d8e4fa38a75c8cd20b39538&#34; }, &#34;pipeline_version&#34;: &#34;1&#34;, &#34;output_commit&#34;: { &#34;branch&#34;: { &#34;repo&#34;: { &#34;name&#34;: &#34;edges&#34;, &#34;type&#34;: &#34;user&#34; }, &#34;name&#34;: &#34;master&#34; }, &#34;id&#34;: &#34;fd9454d06d8e4fa38a75c8cd20b39538&#34; }, &#34;data_processed&#34;: &#34;2&#34;, &#34;data_skipped&#34;: &#34;1&#34;, &#34;data_total&#34;: &#34;3&#34;, &#34;stats&#34;: { &#34;download_time&#34;: &#34;0.113263653s&#34;, &#34;process_time&#34;: &#34;1.020472976s&#34;, &#34;upload_time&#34;: &#34;0.010323995s&#34;, &#34;download_bytes&#34;: &#34;185424&#34;, &#34;upload_bytes&#34;: &#34;114041&#34; }, &#34;state&#34;: &#34;JOB_SUCCESS&#34;, &#34;created&#34;: &#34;2021-08-02T20:13:10.461841493Z&#34;, &#34;started&#34;: &#34;2021-08-02T20:13:32.870023561Z&#34;, &#34;finished&#34;: &#34;2021-08-02T20:13:38.691891860Z&#34;, &#34;details&#34;: { &#34;transform&#34;: { &#34;image&#34;: &#34;pachyderm/opencv:1.0&#34;, &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/edges.py&#34; ] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;name&#34;: &#34;images&#34;, &#34;repo&#34;: &#34;images&#34;, &#34;repo_type&#34;: &#34;user&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;commit&#34;: &#34;fd9454d06d8e4fa38a75c8cd20b39538&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;salt&#34;: &#34;27bbe39ccae54cc2976e3f960a2e1f94&#34;, &#34;datum_tries&#34;: &#34;3&#34; } } "
29,Pipeline,"A pipeline is a Pachyderm primitive that is responsible for reading data from a specified source, such as a Pachyderm repo, transforming it according to the pipeline configuration, and writing the result to an output repo.
A pipeline subscribes to a branch in one or more input repositories. Every time the branch has a new commit, the pipeline executes a job that runs your code to completion and writes the results to a commit in the output repository. Every pipeline automatically creates an output repository by the same name as the pipeline. For example, a pipeline named model writes all results to the model output repo.
In Pachyderm, a Pipeline is an individual execution step. You can chain multiple pipelines together to create a directed acyclic graph (DAG).
You define a pipeline declaratively, using a JSON or YAML file. Pipeline specification files follow Pachyderm&rsquo;s pipeline reference specification file.
A minimum pipeline specification must include the following parameters:
name — The name of your data pipeline. Set a meaningful name for your pipeline, such as the name of the transformation that the pipeline performs. For example, split or edges. Pachyderm automatically creates an output repository with the same name. A pipeline name must be an alphanumeric string that is less than 63 characters long and can include dashes and underscores. No other special characters allowed.
input — A location of the data that you want to process, such as a Pachyderm repository. You can specify multiple input repositories and set up the data to be combined in various ways. For more information, see Cross and Union, Join, Group. One very important property that is defined in the input field is the glob pattern that specifies how Pachyderm breaks the data into individual processing units, called Datums. For more information, see Datum.
transform — Specifies the code that you want to run against your data. The transform section must include an image field that defines the Docker image that you want to run, as well as a cmd field for the specific code within the container that you want to execute, such as a Python script.
example # { &#34;pipeline&#34;: { &#34;name&#34;: &#34;wordcount&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;python3&#34;, &#34;/my_python_code.py&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } } } "
